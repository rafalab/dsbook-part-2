[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preface\nThis is the website for the Statistics and Prediction Algorithms Through Case Studies, the second part of Introduction to Data Science.\nThe website for the first part, Data Wrangling and Visualization with R, is here.\nThis book started out as part of the class notes used in the HarvardX Data Science Series.\nA hardcopy version of the first edition of the book, which combined both parts, is available from CRC Press.\nA free PDF of the October 24, 2019 version of the book, which combined both parts, is available from Leanpub1.\nThe Quarto code used to generate the book is available on GitHub.\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0.\nWe make announcements related to the book on X. For updates follow @rafalab.",
    "crumbs": [
      "Front Matter",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "https://leanpub.com/datasciencebook↩︎",
    "crumbs": [
      "Front Matter",
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The phrase data science began gaining significant popularity around 2012, thanks in part to the publication titled “Data Scientist: The Most Alluring Profession of the 21st Century”1(https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century). This coincided with the rise of a new kind of endeavor in the technology sector and in academic projects during the 2000s: extracting insights from messy, complex, and large datasets, which had become increasingly prevalent with the advent of digital data storage.\nExamples include combining data from multiple political pollsters to improve election predictions, scraping athletic department websites to evaluate baseball prospects, analyzing movie ratings from millions of streaming service users to make personalized recommendations, developing software to read zip codes by digitizing handwritten digits, and using advanced measurement technologies to understand the molecular causes of diseases. This book is centered around these and other practical examples.\nAchieving success in these instances requires collaboration among experts with complementary skills. In this book, our primary focus is on data analysis, specifically, the statistical thinking that allows us to draw meaningful conclusions from data. To understand how to analyze data effectively in these examples, we will cover key mathematical concepts. Many of these concepts are not new, some were originally developed for different purposes, but they have proven adaptable and useful across a wide range of applications.\nThe mathematical difficulty in this book varies by topic, and we assume you are already familiar with the required material. However, the recommended readings at the end of each part can help fill in any gaps. The math is not an end in itself, it is a tool for articulating and deepening our understanding of statistical ideas and thow they are useful for data analysis.\nThe same is true for the code. We alternate between base R, data.table, and the tidyverse, choosing whichever is most appropriate for the task at hand. Given how capable large language models (LLMs) have become at writing and explaining code, we do not focus on syntax or programming best practices. Instead, we include code because it connects statistical ideas to data, and therefore to real-world applications.\nFor example, running Monte Carlo simulations on your own computer and generating your own data can make abstract probability concepts tangible. In general, we encourage you to experiment: change parameters, modify the code, or design your own simulations to test and refine your understanding. This process of active exploration is central to developing intuition about randomness, uncertainty, and inference.\nWhile LLMs can now produce working code for many specific problems, they are not yet capable of doing the main thing we teach here: thinking statistically about data-driven questions. This skill involves formulating questions, visually exploring data, identifying sources of variability, and reasoning about uncertainty. These are human-centered tasks that remain at the heart of data analysis.\nThe book is divided into six parts: Summary Statistics, Probability, Statistical Inference, Linear Models, High-Dimensional Data, and Machine Learning. The first two parts introduce key statistical ideas through illustrative examples, while the later parts focus on real-world case studies that demonstrate how these ideas come together in practice. Readers already comfortable with probability and statistical theory who are mainly interested in the applied perspective may wish to skip ahead to those later sections.\nEach part is organized into concise chapters designed to fit within a single lecture and paired with exercises for practice. All datasets used in the book are available in the dslabs package, and the complete Quarto source files can be found on GitHub2.",
    "crumbs": [
      "Front Matter",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century↩︎\n[https://github.com/rafalab/dsbook-part-2]↩︎",
    "crumbs": [
      "Front Matter",
      "Introduction"
    ]
  },
  {
    "objectID": "summaries/intro-summaries.html",
    "href": "summaries/intro-summaries.html",
    "title": "Summary Statistics",
    "section": "",
    "text": "We begin with one of the simplest and most powerful tools in data analysis: summarizing data. This part of the book introduces techniques that help us describe and understand datasets without relying on probability models. These summaries will later provide the intuition needed to understand statistical modeling and inference.\nIn the first chapter, we focus on distributions, visual representations such as histograms and density plots, that reveal patterns of variation, symmetry, and outliers. In the second chapter, we move from pictures to numbers, introducing numerical summaries such as the average and standard deviation, which quantify the center and spread of a distribution. To motivate these summaries, we introduce the normal distribution.\nHowever, these summarize are not ideal for all datasets. Outliers and skewed distributions can make certain summaries, like the average and standard deviation, less appropriate or cause them to no longer represent what we think they do. For this reason, we also introduce rank-based summaries such as the median and interquartile range, which are more robust to outliers and provide complementary perspectives on data. Together, these ideas form the foundation of data analysis: describing what we see before we attempt to model why it happens.",
    "crumbs": [
      "Summary Statistics"
    ]
  },
  {
    "objectID": "summaries/distributions.html",
    "href": "summaries/distributions.html",
    "title": "1  Distributions",
    "section": "",
    "text": "1.1 Variable types\nBefore we begin, we clarify that this chapter will not cover distributions in the context of probability, which is the mathematical framework used to model uncertainty. We postpone that discussion until the next part of the book, where we will use it throughout. Here, we focus on distributions as a way to describe and summarize data we’ve already collected. Our goal is to understand the variation and patterns present in a dataset, without yet making probabilistic claims or predictions. The concepts introduced in this chapter will later serve as the foundation for understanding probability distributions, where similar tools are used to describe the likely outcomes of random processes.\nTo illustrate the concepts needed to understand distribution and how they relate to summary statistics, we will pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame included in the dslabs package.\nOne way to convey the heights to ET is to simply send him a list with all the heights. However, there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights. We examine the female height data in Section 3.3.\nAt this stage, we are thinking of distributions purely as tools for summarizing and describing data. In the next part of the book, we will connect these same ideas to probability, where distributions help us model uncertainty. For now, our goal is simply to explore the patterns and variation present in the data we do have.\nIt turns out that, in some cases, the two summary statistics introduced in the next chapter, the average and the standard deviation, are all we need to understand the data. We will learn data visualization techniques that will help us determine when this two-number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.\nKnowing what variable type we are working with is important because it will help us determining the most effective way to summarize our data and display its distribution.\nWe will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.\nWhen data values represent groups instead of numbers, we refer to the data as categorical. Two simple examples are sex (male or female) and US regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers, such as spiciness (mild, medium, hot). These ordered categorical data are referred to as ordinal data.\nNumerical data can be continuous or discrete. A continuous variable can take on any value within a range. For example, if measured with sufficient precision, height is continuous because the difference between two individuals (even identical twins) can be an arbitrarily small fraction. However, if height is rounded to the nearest inch, it becomes discrete since values must be a whole number. Other examples of discrete data are the number of heads when tossing 10 coins, the number of cigarettes a person smokes a day, or the number of customers visiting a store in an hour. In these cases, the values are countable and cannot take on fractional values. Unlike continuous data, which can be measured with increasing precision, discrete data are limited to distinct, separate values.\nIn practice, there are occasions in which discrete variables are treated as continuous. For example, a digital measuring instrument might record data as 16-bit integers. However, because it is rare to obtain exactly the same reading more than once, it is convenient to treat the data as continuous. Another example is country population size. Although population counts are always whole numbers, it is uncommon for two jurisdictions to have exactly the same population. For this reason, population size is also commonly treated as continuous.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#sec-pmf-intro",
    "href": "summaries/distributions.html#sec-pmf-intro",
    "title": "1  Distributions",
    "section": "\n1.2 Relative frequency distribution",
    "text": "1.2 Relative frequency distribution\nThe most basic statistical summary of a list of numbers or categories is its distribution. The simplest way to think of a distribution is as a compact description of values with many entries. This concept should not be new for readers of this book. For example, with categorical data, the relative frequency distribution simply describes the proportion of each unique category:\n\\[\np_k = \\frac{\\mbox{number of times category }k \\mbox{ appears in the list}}{n}\n\\]\nwith \\(n\\) the length of the list.\nHere is an example with US state regions:\n\nprop.table(table(state.region))\n#&gt; state.region\n#&gt;     Northeast         South North Central          West \n#&gt;          0.18          0.32          0.24          0.26\n\nWhen the data are discrete numerical values, the relative frequency distribution is defined in the same way, except it is more common to use the following notation since the frequency can be considered a function of numerical values:\n\\[\nf(x) = \\frac{\\mbox{number of times value } x \\mbox{ appears in the list}}{n}\n\\]\nHere is the distribution for US state populations rounded to the nearest million, which are discrete numerical values:",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#sec-ecdf-intro",
    "href": "summaries/distributions.html#sec-ecdf-intro",
    "title": "1  Distributions",
    "section": "\n1.3 Empirical cumulative distribution functions",
    "text": "1.3 Empirical cumulative distribution functions\nWhen the data is continuous, the task of constructing a summary based on a distribution is more challenging because reporting the frequency of each entry is not an effective summary: most entries are unique so the relative frequency distribution does not summarize much.\nHere are the relative frequencies of US state populations:\n\n\n\n\n\n\n\n\nSince there are 50 states, each with a unique population value \\(x\\), the plot shows \\(f(x) = \\frac{1}{50} = 0.02\\) for each state.\nTo define a distribution for continuous numeric data we define a function that reports the proportion of the data entries \\(x\\) that are below \\(a\\), for all possible values of \\(a\\). This function is called the empirical cumulative distribution function (eCDF) and often denoted with \\(F\\):\n\\[ F(a) = \\mbox{Proportion of data points that are less than or equal to }a\\]\nHere is the eCDF for state population data above:\n\n\n\n\n\n\n\n\nSimilar to how a frequency table summarizes categorical data, the empirical cumulative distribution function (eCDF) summarizes continuous data. The eCDF provides a clear picture of how the values are distributed and highlights key characteristics of the dataset. For instance, in the population data, we can see that about half of the states have populations greater than 5,000,000, while most states fall below 20,000,000. This type of summary helps us quickly identify medians, ranges, and other important distributional features.\nIn our heights case study, most students reported heights rounded to the nearest inch. However, some did not. For example, one student reported a height of 68.503937007874 inches, and another reported 68.8976377952756 inches. These values correspond to 174 cm and 175 cm converted to inches, respectively, suggesting that some students knew their heights to the closest centimeter and reported this number divided by 2.54.\nAs a result the data is continuous and the relative frequency distribution is somewhat confusing:\n\n\n\n\n\n\n\n\nAlthough this approach preserves all the information from the list of observations, it can give misleading impressions if extrapolating insights to a broader population. For example, it might suggest there are no people with heights between 74 and 74.5 inches or that significantly more people are exactly 72 inches tall than 71.5 inches, patterns we know aren’t accurate. In contrast, the eCDF provides a more useful visual summary:\n\n\n\n\n\n\n\n\nFrom a quick glance at the figure, we can already infer some general characteristics of the data. For example, about half of the males are shorter than 69 inches, since \\(F(69) \\approx 0.5\\), and being shorter than 60 inches or taller than 80 inches is very rare. Moreover, because we can compute the proportion of individuals with heights between any two values \\(a\\) and \\(b\\) as \\(F(b) - F(a)\\), the plot above contains all the information needed to reconstruct the entire dataset.\nParaphrasing the saying “a picture is worth a thousand words,” in this case, a picture is as informative as 812 numbers.\n\n\n\n\n\n\nThe reason we add the word empirical is because, as we will see in Section 6.1, the cumulative distribution function (CDF) can be defined mathematically, meaning without any data.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#histograms",
    "href": "summaries/distributions.html#histograms",
    "title": "1  Distributions",
    "section": "\n1.4 Histograms",
    "text": "1.4 Histograms\nAlthough the eCDF is a fundamental concept in statistics, it is not commonly used in practice. The main reason is that it does not easily highlight key distribution characteristics, such as the central tendency, symmetry, or the range that contains 95% of the values. In contrast, histograms are widely preferred because they make these features much more interpretable. While histograms sacrifice a small amount of information, they provide a much clearer summary of the data.\nTo construct a histogram, we first divide the range of the data into non-overlapping bins. Each bin acts as a category, and its height is chosen so that the area of the bar, calculated as the height multiplied by the bin width, represents the relative frequency of observations within that bin. Although a histogram resembles a bar plot, it differs in that the x-axis represents numerical values rather than categorical ones.\nIf we round the reported heights to the nearest inch and create a relative frequency distribution, we obtain a histogram with bins defined in one-inch intervals:\n\\[(49.5, 50.5], (50.5, 51.5], (51.5, 52.5], (52.5, 53.5], \\dots, (82.5, 83.5]\\]\nWe can do this with the following ggplot2 code:\n\nlibrary(tidyverse)\nheights |&gt; filter(sex == \"Male\") |&gt; \n  mutate(height = round(height)) |&gt;\n  count(height) |&gt; mutate(f = n / sum(n)) |&gt; \n  ggplot(aes(height, f)) + geom_col()\n\n\n\n\n\n\n\nThe R base hist function also allows us quickly construct a histogram. We can define custom bin intervals which is particularly useful for grouping extreme values and avoiding empty bins:\n\nwith(heights, hist(height[sex == \"Male\"], breaks = c(50, 55, 60:80, 85)))\n\n\n\n\n\n\n\nIf we were to send either of these plots to an extraterrestrial unfamiliar with human height distributions, they would immediately gain key insights about the data. First, the height values range from 50 to 84 inches, with over 95% of observations falling between 63 and 75 inches. Second, the distribution is approximately symmetric around 69 inches. Additionally, by summing bin counts, one could estimate the proportion of data falling within any given interval. Thus, a histogram provides an intuitive summary that preserves most of the information from the original 812 height measurements using only about 30 bin counts.\nWhat information is lost? A histogram treats all values within a bin as if they were the same when computing bar heights. For instance, values of 64.0, 64.1, and 64.2 inches are grouped together. However, since these differences are barely perceptible, the practical impact is negligible, and we achieve a meaningful summary using just 23 numbers.\n\n\n\n\n\n\nBy default, histograms often display frequency on the y-axis instead of relative frequency. The ggplot2 package follows this convention:\n\nheights |&gt; filter(sex == \"Male\") |&gt; ggplot(aes(x = height)) + \n  geom_histogram(breaks = c(50, 55, 60:80, 85), fill = \"grey\", color = \"black\")\n\n\n\n\n\n\n\nNote we use fill and color to mimic the behavior or hist.\nWhen displaying frequencies, it is important to use equally spaced bins, as larger bins will naturally contain more observations and may distort the visual representation of the data. To instead display relative frequency, you can modify the code by setting y = after_stat(density):\n\nheights |&gt; filter(sex == \"Male\") |&gt; \n  ggplot(aes(x = height, y = after_stat(density))) + \n  geom_histogram(breaks = c(50, 55, 60:80, 85), fill = \"grey\", color = \"black\")\n\n\n\n\n\n\n\n\n\n\nChoosing bin widths\nThe appearance and interpretation of a histogram depend on the choice of bins, and in the case of equally sized bins, their width. Different bin widths can make the same data look quite different, too narrow, and the histogram may appear noisy or fragmented; too wide, and important features of the distribution may be hidden. The choice of bin width should be guided by the context of the data. For example, in the case of height, most people report values rounded to the nearest half inch, so using bins smaller than that does not make sense. At the same time, because height differences of more than an inch are noticeable in real life, bins larger than about two inches would obscure meaningful variation. For the extreme regions of the distribution, such as below 60 inches or above 80 inches, where there are few observations, it is reasonable to use wider bins to keep the summary clear. Every dataset requires a context-specific decision: the goal is to choose bins that reveal structure without introducing noise, maximizing the information conveyed by a single figure.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#smoothed-density-plots",
    "href": "summaries/distributions.html#smoothed-density-plots",
    "title": "1  Distributions",
    "section": "\n1.5 Smoothed density plots",
    "text": "1.5 Smoothed density plots\nSmooth density plots are similar to histograms, but the data is not divided into bins. Here is what a smooth density plot looks like for our heights data:\n\nheights |&gt; filter(sex == \"Male\") |&gt; \n  ggplot(aes(height)) + geom_density(alpha = 0.2, fill = \"#00BFC4\")\n\n\n\n\n\n\n\nIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed.\nTo understand smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics.\nThe main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any other list of values, and what we truly want to report to ET is this larger distribution, as it is much more general. Unfortunately, we don’t get to see it.\nHowever, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below, we present histograms for this hypothetical data with bins of size 1, 0.5, and 0.25. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts. The smaller we make the bins, the smoother the histogram becomes:\n\n\n\n\n\n\n\n\nThe smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small.\nNow, back to reality. We don’t have millions of measurements. Instead, we have 812 and we can’t make a histogram with very small bins.\nTherefore, we make a histogram using bin sizes appropriate for our data, computing frequencies rather than counts. Additionally, we draw a smooth curve that passes through the tops of the histogram bars. The following plots demonstrate the steps that lead to a smooth density:\n\n\n\n\n\n\n\n\nChoosing the bandwidth\nSmooth is a relative term. The degree of smoothness in a density curve is not fixed, it depends on a parameter, often called the bandwidth, that controls how much we smooth the curve. Most functions that compute smooth densities include an option to adjust this bandwidth. The examples below show the same histogram with two different bandwidth choices, illustrating how the amount of smoothing can change the appearance of the density curve.\n\n\n\n\n\n\n\n\nJust as a histogram’s appearance depends on the chosen bin width, a smoothed density depends on the bandwidth, the smoothing parameter. Different bandwidths can make the same data look noisy (too small) or overly smooth (too large). We should pick a bandwidth that reflects the context and what we believe about the underlying distribution. For heights, for example, it’s reasonable to expect nearby values, within an inch, to occur with similar frequency, the proportion at 72 inches should be closer to that at 71 than to that at 65 or 78, so a moderate level of smoothing is justified. The aim is the same as with bin width: reveal structure without adding noise or hiding meaningful features.\nInterpreting the y-axis\nInterpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine that we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:\n\n\n\n\n\n\n\n\nThe proportion of this area is about 0.3, meaning that about 30% of male heights are between 65 and 68 inches.\nBy understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the material covered up to this point, you can complete exercises 1 through 10.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#describing-distributions",
    "href": "summaries/distributions.html#describing-distributions",
    "title": "1  Distributions",
    "section": "\n1.6 Describing distributions",
    "text": "1.6 Describing distributions\nWhen visualizing data with a histogram or density plot, we often describe the shape of a distribution using a few key terms and concepts. In this section, our goal is simply to familiarize you with this vocabulary, terms that you’ll encounter in most discussions of data analysis.\nA mode is a value or region where the data are most concentrated, the peak of the distribution. Some distributions have one peak (unimodal), while others have two or more (bimodal or multimodal), which can indicate distinct subgroups within the data.\nA symmetric distribution has as many data points to both sides of the mode. A distribution is skewed if it is asymmetric.\nThe tails of a distribution are the regions that extend toward the smallest and largest values.\nA right-skewed (positively skewed) distribution has a long tail toward larger values, for example, income, where most people earn modest amounts but a few earn much more.\nA left-skewed (negatively skewed) distribution has a longer tail toward smaller values, such as test scores on an easy exam where most students score high but a few perform poorly.\nIn the next chapter, we introduce the normal distribution, a symmetric, bell-shaped distribution that serves as a useful reference point. Distributions with heavy tails contain more extreme observations than the normal distribution, while light-tailed distributions contain fewer.\nHere are some simulated examples illustrating the terms described above:",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#from-visual-to-numerical-summaries",
    "href": "summaries/distributions.html#from-visual-to-numerical-summaries",
    "title": "1  Distributions",
    "section": "\n1.7 From visual to numerical summaries",
    "text": "1.7 From visual to numerical summaries\nAs we’ve seen, visual tools such as histograms, density plots, and qqplots help us understand the shape of a distribution, its center, spread, symmetry, and tails. However, graphical summaries are often complemented by numerical summaries, which distill much of this information into just a few key numbers. In the next chapter, we introduce two of the most useful numerical summaries, the average and the standard deviation, which provide all the information you need when the data are approximately normal. We will also explore alternative measures that are more appropriate when distributions deviate from normality. The visualization techniques introduced here will guide us in deciding when simple numerical summaries are sufficient and when more detailed descriptions are needed.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#exercises",
    "href": "summaries/distributions.html#exercises",
    "title": "1  Distributions",
    "section": "\n1.8 Exercises",
    "text": "1.8 Exercises\n1. In the murders dataset, the region is a categorical variable and the following is its distribution:\n\n\n\n\n\n\n\n\nTo the closest 5%, what proportion of the states are in the North Central region?\n2. Which of the following is true:\n\nThe graph above is a histogram.\nThe graph above shows only four numbers with a bar plot.\nCategories are not numbers, so it does not make sense to graph the distribution.\nThe colors, not the height of the bars, describe the distribution.\n\n3. The plot below shows the eCDF for male heights:\n\n\n\n\n\n\n\n\nBased on the plot, what percentage of males are shorter than 75 inches?\n\n100%\n95%\n80%\n72 inches\n\n4. To the closest inch, what height m has the property that 1/2 of the male students are taller than m and 1/2 are shorter?\n\n61 inches\n64 inches\n69 inches\n74 inches\n\n5. Here is an eCDF of the murder rates (number of murders per 100,000 people) across states in 2010:\n\n\n\n\n\n\n\n\nKnowing that there are 51 observations (50 states and DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people?\n\n1\n5\n10\n50\n\n6. Based on the eCDF above, which of the following statements are true:\n\nAbout half the states have murder rates above 7 per 100,000 and the other half below.\nMost states have murder rates below 2 per 100,000.\nAll the states have murder rates above 2 per 100,000.\nWith the exception of 4 states, the murder rates are below 5 per 100,000.\n\n7. Below is a histogram of male heights in our heights dataset:\n\n\n\n\n\n\n\n\nBased on this plot, how many males are between 63.5 and 65.5?\n\n10\n24\n47\n100\n\n8. About what percentage are shorter than 60 inches?\n\n1%\n10%\n25%\n50%\n\n9. Based on the density plot below, about what proportion of US states have populations larger than 10 million?\n\n\n\n\n\n\n\n\n\n0.02\n0.15\n0.50\n0.55\n\n10. Below are three density plots. Is it possible that they are from the same dataset?\n\n\n\n\n\n\n\n\nWhich of the following statements is true:\n\nIt is impossible that they are from the same dataset.\nThey are from the same dataset, but the plots are different due to coding errors.\nThey are the same dataset, but the first and second plot under smooth and the third over smooths.\nThey are the same dataset, but the first is not in the log scale, the second under smooths, and the third over smooths.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/numerical-summaries.html",
    "href": "summaries/numerical-summaries.html",
    "title": "2  Numerical Summaries",
    "section": "",
    "text": "2.1 The normal distribution\nHistograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. One reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for these occurrences, which we will describe later. Here we focus on how the normal distribution helps us summarize data.\nRather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mathrm{Pr}(a &lt; x \\leq b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma} \\right)^2} \\, dx\\]\nBut you don’t need to memorize the formula to use the normal distribution in practics. The most important characterstics is that it is completely defined by just two parameters: \\(\\mu\\) and \\(\\sigma\\). The rest of the symbols in the formula represent the interval ends, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\). These two parameters, \\(\\mu\\) and \\(\\sigma\\), are referred to as the mean and the standard deviation (SD) of the distribution, respectively (and are the Greek letters for \\(m\\) and \\(s\\)). In R the fucntion pnorm let’s us compute the \\(\\mathrm{Pr}(x \\leq a)\\) for any \\(a,\\mu,\\) and \\(\\sigma\\):\npnorm(a, mu, sigma)\nThe distribution is symmetric, centered at \\(\\mu\\), and most values (about 95%) are within \\(2\\sigma\\) from \\(\\mu\\). Here is what the normal distribution looks like when the \\(\\mu = 0\\) and \\(\\sigma = 1\\):\nThe fact that the distribution is defined by just two parameters implies that if a dataset’s distribution is approximated by the normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the mean and the standard deviation.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "summaries/numerical-summaries.html#sec-average-and-sd-def",
    "href": "summaries/numerical-summaries.html#sec-average-and-sd-def",
    "title": "2  Numerical Summaries",
    "section": "\n2.2 The average and standard deviation",
    "text": "2.2 The average and standard deviation\nDiscrete distributions also have means and standard deviations. We will discuss these terms in more detail in Chapter 7. For a list of values \\(x_1, \\dots, x_n\\) the mean, call it \\(\\mu_x\\) is defined by\n\\[\n\\mu_x = \\frac{1}{n}\\sum_{i=1}^n x_i\n\\] Note that this is equivalent to the average of the \\(x\\)s.\nThe standard deviation is defined with\n\\[\n\\sigma_x = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu_x)^2}\n\\] which can be thought of as the average distance of the points \\(x_i\\) to the mean \\(\\mu_x\\).\nLet’s compute the mean and stardard deviations for the height for males:\n\nlibrary(tidyverse)\nlibrary(rafalib)\nx &lt;- with(heights, height[sex == \"Male\"])\n\nThe pre-built functions mean and sd can be used here:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n\n\n\n\n\n\n\nFor reasons explained in Section 10.2.1, sd(x) divides by length(x)-1 rather than length(x). But note that when length(x) is large, sd(x) and sqrt(sum((x-mu)^2)/length(x)) are practically equal.\n\n\n\nIf the distribution of the values stored in x is well approximated by a normal distribution, it makes sense to use the normal distribution with mean m &lt;- mean(x) and standard deviation s &lt;- sd(x). This means that the entire distribution of x can be summarized with just these two numbers!\nThe plot below shows the smooth density of the observed data (blue) and the corresponding normal approximation (black line) with mean = 69.3 and SD = 3.6:\n\n\n\n\n\n\n\n\nThe normal distribution appears to be a good approximation. For example, the probabilities computed using pnorm closely match those estimated directly from the data.\nTo check this, we can compare the proportion of observations falling within a given interval, say \\((a, b]\\), as predicted by the normal model to the actual proportion observed in the data. The proportion of values in the interval \\([a, b]\\) can be computed with:\n\nmean(x &gt; a & x &lt;= b)\n\nHere, the logical expression x &gt;= a & x &lt;= b returns TRUE for all values of x in the interval and FALSE otherwise. Because the mean function automatically converts TRUE to 1 and FALSE to 0, the average of this logical vector is simply the proportion of observations that fall within the interval. We will use this convenient approach throughout the book.\nThe following examples illustrate how closely the observed proportions match the theoretical probabilities from the normal distribution.\nThe probability of being shorter than the average student:\n\npnorm(m, m, s)\n#&gt; [1] 0.5\nmean(x &lt;= m)\n#&gt; [1] 0.515\n\nThe probability of being within two standard deviations of the mean:\n\npnorm(76.5, m, s) - pnorm(62.1, m, s)\n#&gt; [1] 0.954\nmean(x &gt; 62.1 & x &lt;= 76.5)\n#&gt; [1] 0.95\n\n\n\n\n\n\n\nWhen using the normal approximation with discrete data (for example, when heights are rounded to the nearest inch), note that mean(x &lt; a) and mean(x &lt;= a) may differ. In such cases, it’s common to apply a continuity correction, using pnorm(a + delta/2, m, s) to approximate mean(x &lt;= a), where delta is the bin width (in our example, 1 inch). In our dataset, however, some students reported more precise measurements, so the degree of discretization varies. As a result, the normal approximation should be applied with care.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "summaries/numerical-summaries.html#standard-units",
    "href": "summaries/numerical-summaries.html#standard-units",
    "title": "2  Numerical Summaries",
    "section": "\n2.3 Standard units",
    "text": "2.3 Standard units\nFor data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector x, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of x, respectively. Why is this convenient?\nFirst, revisit the formula for the normal distribution and observe that what is being exponentiated is \\(-z^2/2\\) with \\(z\\) equivalent to \\(x\\) in standard units. Because the maximum of \\(e^{-z^2/2}\\) is at \\(z = 0\\), this explains why the maximum of the distribution occurs at the mean. It also explains the symmetry since \\(- z^2/2\\) is symmetric around 0. Second, note that by converting the normally distributed data to standard units, we can quickly ascertain whether, for example, a person is about average (\\(z = 0\\)), one of the tallest (\\(z \\approx 2\\)), one of the shortest (\\(z \\approx -2\\)), or an extremely rare occurrence (\\(z &gt; 3\\) or \\(z &lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.\nIn R, we can transform to standard units using the function scale:\n\nz &lt;- scale(x)\n\nTo see how many males are within 2 SDs from the average, we simply type:\n\nmean(abs(z) &lt; 2)\n#&gt; [1] 0.95\n\nThe proportion is about 95%, which is what the normal distribution predicts!\n\n\n\n\n\n\nIn statistics, the letter \\(z\\) is commonly used to denote values expressed in standard units. When referring to random outcomes rather than specific observed values, we use an uppercase \\(Z\\).\nThroughout this book, we use \\(z\\) and \\(Z\\) in this way whenever we describe quantities that have been standardized. This notation helps connect examples across different chapters.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "summaries/numerical-summaries.html#robust-summaries",
    "href": "summaries/numerical-summaries.html#robust-summaries",
    "title": "2  Numerical Summaries",
    "section": "\n2.4 Robust summaries",
    "text": "2.4 Robust summaries\nNote that the heights we explored in the Chapter 1 are not the original heights reported by students. A second challenge involves exploring the original reported heights, which are also included in the dslabs package in the reported_heights object. We will see that due to errors in reporting, using robust summaries are necessary to produce useful summaries.\nOutliers\nOutliers are very common in real-world data analysis. Data recording can be complex and it is common to observe data points generated in error. For example, an old monitoring device may read out nonsensical measurements before completely failing. Human error is also a source of outliers, in particular when data entry is done manually. For example, an individual may mistakenly enter their height in centimeters instead of inches or put the decimal in the wrong place.\nHow do we distinguish an outlier from measurements that were too big or too small simply due to expected variability? This is not always an easy question to answer, but we try to provide some guidance. Let’s begin with a simple case.\nSuppose a colleague is charged with collecting demography data for a group of males. The data report height in feet and are stored in the object:\n\nlibrary(dslabs)\nstr(outlier_example)\n#&gt;  num [1:500] 5.59 5.8 5.54 6.15 5.83 5.54 5.87 5.93 5.89 5.67 ...\n\nOur colleague uses the fact that heights are usually well approximated by a normal distribution and summarizes the data with average and standard deviation:\n\nmean(outlier_example)\n#&gt; [1] 6.1\n\nOur colleage writes a report on the interesting fact that this group of males is much taller than usual. The average height is over six feet tall! However, using your data analysis skills you notice something else that is unexpected: the standard deviation is over 7 feet.\n\nsd(outlier_example)\n#&gt; [1] 7.8\n\nAdding and subtracting two standard deviations, you note that 95% of this population will have heights between -9.49, 21.7 feet, which does not make sense. A quick plot reveals the problem:\n\nboxplot(outlier_example)\n\n\n\n\n\n\n\nThere appears to be at least one value that is nonsensical, since we know that a height of 180 feet is impossible. The boxplot detects this point as an outlier.\nThe Median\nOutliers can make the average arbitrarily large. With 500 data points, increasing a single observation by \\(\\Delta \\times\\) 500 raises the average by \\(\\Delta\\). In contrast, the median is robust to outliers: no matter how extreme one value becomes, the median remains unchanged as long as it does not cross the middle of the data.\nThe median of our example data is:\n\nmedian(outlier_example)\n#&gt; [1] 5.74\n\nwhich corresponds to about 5 feet and 9 inches. The median is often a better measure of typical value when the data contain outliers.\nMedian Absolute Deviation\nJust as outliers can distort the average, they can also make the standard deviation misleadingly large. A more robust measure of spread is the median absolute deviation (MAD).\nTo compute the MAD, we first find the median, then calculate the absolute deviation of each value from the median, and finally take the median of those deviations. To make it comparable to the standard deviation when data are normally distributed, we multiply this value by 1.4826. (The mad function in R applies this correction automatically.)\nFor our height data, the MAD is:\n\nmad(outlier_example)\n#&gt; [1] 0.237\n\nwhich corresponds to about 3 inches.\nThe MAD provides a more stable estimate of variability when data include outliers. In such cases, it offers a truer sense of the dataset’s typical spread.\nThe Interquartile Range (IQR)\nWhen data contain outliers or do not follow a normal distribution, the interquartile range (IQR) provides another way to summarize spread that is more robust than the standard deviation. The IQR represents the range that contains the middle 50% of the data, spanning from the 25th percentile (first quartile) to the 75th percentile (third quartile), and always includes the median.\nLike the median and MAD, the IQR is resistant to extreme values: no matter how large or small an outlier becomes, the IQR remains largely unaffected. For normally distributed data, dividing the IQR by 1.349 gives an estimate of the standard deviation we would expect if no outliers were present.\nIn our example, this approximation works well. The IQR-based estimate of the standard deviation is:\n\nIQR(outlier_example)/1.349\n#&gt; [1] 0.245\n\nwhich corresponds to about 3 inches.\nIn addition to summarizing spread, the IQR allows us to formally define what we mean by an outlier, a concept we will introduce in the next section.\nA Data-Driven Definition of Outliers\nA precise, data-driven definition of an outlier was introduced by John Tukey, who also developed the boxplot, a visualization we will introduce in the next section. To construct a boxplot, we first need a formal way to identify which data points count as outliers.\nLet the first and third quartiles be denoted as \\(Q_1 = Q(0.25)\\) and \\(Q_3 = Q(0.75)\\), respectively. Tukey defined an observation as an outlier if it falls outside the following range:\n\\[\n[Q_1 - 1.5 \\times \\text{IQR}, ; Q_3 + 1.5 \\times \\text{IQR}], \\quad \\text{where } \\text{IQR} = Q_3 - Q_1\n\\]\nWhen the data follow a standard normal distribution, this range is:\n\nq3 &lt;- qnorm(0.75); q1 &lt;- qnorm(0.25); iqr &lt;- q3 - q1\nc(q1 - 1.5*iqr, q3 + 1.5*iqr)\n#&gt; [1] -2.7  2.7\n\nUsing pnorm, we can compute the proportion of data expected to fall within this range:\n\nr &lt;- c(q1 - 1.5*iqr, q3 + 1.5*iqr)\nround((pnorm(r[2]) - pnorm(r[1]))*100, 3)\n#&gt; [1] 99.3\n\nThus, about 99.302% of the data lie inside this interval for a normal distribution. In other words, we expect roughly 7 out of every 1,000 observations from a perfectly normal dataset to fall outside this range, not because they are erroneous, but simply due to random variation.\nTo make the definition stricter, Tukey proposed using a wider multiplier, such as 3 instead of 1.5, to flag far-out outliers. For normally distributed data, the range\n\nr &lt;- c(q1 - 3*iqr , q3 + 3*iqr)\nround((pnorm(r[2]) - pnorm(r[1]))*100, 4)\n#&gt; [1] 100\n\ncaptures about 99.9998% of all values, meaning that only about two in a million observations would fall outside it. In ggplot geom_boxplot() function, this multiplier is controlled by the coef argument, which defaults to 1.5.\nLet’s apply Tukey’s rule to our height data. The extreme 180-foot measurement is well beyond the range defined by \\(Q_3 + 3 \\times \\text{IQR}\\):\n\nmax_height &lt;- quantile(outlier_example, 0.75) + 3*IQR(outlier_example)\nmax_height\n#&gt;  75% \n#&gt; 6.91\n\nIf we remove this value, the remaining data follow an approximately normal distribution, as expected:\n\nx &lt;- outlier_example[outlier_example &lt; max_height]\nhist(x, breaks = seq(5, 7, 2/12))\n\n\n\n\n\n\n\nThis demonstrates the usefulness of Tukey’s approach: it provides an objective, reproducible way to detect unusual observations—one that adapts to the spread of the data itself.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "summaries/numerical-summaries.html#boxplots",
    "href": "summaries/numerical-summaries.html#boxplots",
    "title": "2  Numerical Summaries",
    "section": "\n2.5 Boxplots",
    "text": "2.5 Boxplots\nTo introduce boxplots, we will use a dataset of U.S. murders by state. Suppose we want to summarize the distribution of murder rates. A histogram quickly reveals that the data are right-skewed, indicating that the normal approximation does not apply:\n\n\n\n\n\n\n\n\nWhile a histogram provides valuable detail, people often prefer a more concise numerical summary, one that highlights the main features of the distribution without showing every data point.\nThe boxplot offers such a summary. It condenses a dataset into five numbers: the minimum, first quartile (\\(Q_1\\)), median (\\(Q_2\\)), third quartile (\\(Q_3\\)), and maximum. Outliers, values that fall outside Tukey’s range, defined in the previous section, are not included in the computation of the range, but are instead plotted as individual points.\nHere’s the boxplot for the murder rate data:\n\n\n\n\n\n\n\n\nThe box spans from \\(Q_1\\) to \\(Q_3\\), with a horizontal line marking the median. The whiskers extend to the smallest and largest observations that are not outliers, while outliers are shown as separate points. The height of the box corresponds to the IQR, representing the middle 50% of the data.\nBoxplots are especially useful when comparing distributions across groups, for example, comparing murder rates across regions or male and female height distributions. They provide a quick visual summary of differences in center, spread, and the presence of outliers. In the next chapter, we will use boxplots in exactly this way to compare distributions between groups.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "summaries/numerical-summaries.html#exercises",
    "href": "summaries/numerical-summaries.html#exercises",
    "title": "2  Numerical Summaries",
    "section": "\n2.6 Exercises",
    "text": "2.6 Exercises\n1. Load the height dataset and create a vector x with just the male heights:\n\nlibrary(dslabs)\nx &lt;- heights$height[heights$sex==\"Male\"]\n\nWhat proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean.\n2. Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the pnorm function to predict the proportions.\n3. Notice that the approximation calculated in question 1 is very close to the exact calculation in question 2. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81]. How many times larger is the actual proportion than the approximation?\n4. Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as seven-footers. Hint: use the pnorm function.\n5. There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many of these men (18-40 year olds) are seven feet tall or taller in the world?\n6. There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world’s 18-to-40-year-old seven-footers are in the NBA?\n7. Repeat the calculations performed in the previous question for Lebron James’ height: 6 feet 8 inches. There are about 150 players that are at least that tall.\n8. In answering the previous questions, we found that it is not uncommon for a seven-footer to become an NBA player. What would be a fair critique of our calculations:\n\nPractice and talent are what make a great basketball player, not height.\nThe normal approximation is not appropriate for heights.\nThe normal approximation tends to underestimate the extreme values. It’s possible that there are more seven-footers than we predicted.\nThe normal approximation tends to overestimate the extreme values. It’s possible that there are fewer seven-footers than we predicted.\n\nWe are going to use the HistData package. Load the Galton data and create a vector x, consisting solely of the child heights:\n\nlibrary(HistData)\nx &lt;- Galton$child\n\n9. Compute the average and SD of these data.\n10. Compute the median and median absolute deviation of these data.\n11. Now suppose Galton made a mistake when entering the first value and forgot to use the decimal point. You can imitate this error by typing:\n\nx_with_error &lt;- x\nx_with_error[1] &lt;- x_with_error[1]*10\n\nHow many inches does the average grow after this mistake?\n12. How many inches does the SD grow after this mistake?\n13. How many inches does the median grow after this mistake?\n14. How many inches does the MAD grow after this mistake?\n15. How could you use exploratory data analysis to detect that an error was made?\n\nSince it is only one value out of many, we will not be able to detect this.\nWe would see an obvious shift in the distribution.\nA boxplot, histogram, or qqplot would reveal a clear outlier.\nA scatterplot would show high levels of measurement error.\n\n16. How much can the average accidentally grow with mistakes like this? Write a function called error_avg that takes a value k and returns the average of the vector x after the first entry changed to k. Show the results for k=10000 and k=-10000.\n17. Using the murders dataset in the dslabs package. Compute the murder rate for each state. Make a boxplot comparing the murder rates for each region of the United States.\n18. For the same dataset, compute the median and IQR murder rate for each region.\n19. The heights we have been looking at in the heights data frame are not the original heights reported by students. The original reported heights are also included in the dslabs package in the object reported_heights. Note that the height column in this data frame is a character, and if we try to create a new column with the numeric version:\n\nlibrary(tidyverse)  \nreported_heights &lt;- reported_heights |&gt;\n  mutate(original_heights = height, height = as.numeric(height))\n\nwe get a warnings about NAs. Examine the rows that result in NAs and describe why this is happening.\n20. Add a column to the reported_heights with the year the height was entered. You can use the year function in the lubridate package to extract the year from reported_heights$time_stamp. Change the height column from characters to numbers using as.numeric. Some of the heights will be converted to NA because they were incorrectly entered and include characters, for example 165cm. These heights were supposed to be reported in inches, but many clearly did not. Convert any entry with highly unlikely heights, below 54 or above 72, to NA using the na_if function from dplyr. Once you do this, stratify by sex and year and report the percentage of incorrectly entered heights, represented by the NA.\n21. Remove the entries that result in NAs when attempting to convert heights to numbers. Compute the mean, standard deviation, median, and MAD by sex. What do you notice?\n22. Generate boxplots summarizing the heights for males and females and describe what you see.\n23. Look at the largest 10 heights and provide a hypothesis for what you think is happening.\n24. Review all the nonsensical answers by looking at the data considered to be far out by Tukey and comment on the type of errors you see.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Summaries</span>"
    ]
  },
  {
    "objectID": "summaries/comparing-groups.html",
    "href": "summaries/comparing-groups.html",
    "title": "\n3  Comparing Groups\n",
    "section": "",
    "text": "3.1 Quantile-quantile plots\nSo far, we have focused on summarizing the distribution of a single variable, describing its center, spread, and shape. In practice, however, one of the most common goals of data analysis is comparison: we want to know how one group differs from another, how a treatment compares to a control, or how outcomes vary across conditions. Comparing groups is often the first step toward uncovering relationships and understanding sources of variation.\nThe tools we developed for describing distributions, histograms, boxplots, and numerical summaries such as the mean and median, extend naturally to comparisons. By examining these summaries within subgroups or by comparing quantiles across distributions, we can reveal patterns that a single summary might conceal.\nIn this chapter, we explore two key techniques for comparing distributions:\nTogether, these methods complete our descriptive toolkit, bridging the transition from exploring single variables to analyzing relationships between variables.\nA systematic way to compare two distributions is to compare their cumulative distribution functions (CDFs). A practical way to do this is through their quantiles.\nFor a proportion \\(0 \\leq p \\leq 1\\), the \\(p\\)-th quantile of a distribution \\(F(x)\\) is the value \\(q\\) such that \\(F(q) = p\\), or \\(q = F^{-1}(p)\\). For example, \\(p = 0\\) gives the minimum, \\(p = 0.5\\) gives the median, and \\(p = 1\\) gives the maximum.\nThe notation \\(Q(p) = F^{-1}(p)\\) or equivalently \\(q_p = F^{-1}(p)\\) is commonly used in textbooks to denote the \\(p\\)-th quantile of a distribution.\nA quantile–quantile plot (qqplot) provides a visual way to compare two distributions, say \\(F_1(x)\\) and \\(F_2(x)\\), by plotting the quantiles of one distribution against the corresponding quantiles of the other. Specifically, if we define \\[\nQ_1(p_i) = F_1^{-1}(p_i)\n\\quad \\text{and} \\quad\nQ_2(p_i) = F_2^{-1}(p_i)\n\\] for a set of probabilities \\(p_1, \\dots, p_m\\), then the qqplot is created by plotting the pairs \\[\n\\{Q_1(p_i),\\, Q_2(p_i)\\}, \\quad i = 1, \\dots, m.\n\\] A common choice for these probabilities is \\[\np_i = \\frac{i - 0.5}{n}, \\quad i = 1, \\dots, n,\n\\] where \\(n\\) is the sample size of the smaller dataset. Subtracting \\(0.5\\) prevents evaluating quantiles at \\(p = 0\\) or \\(p = 1\\), which for some theoretical distributions, such as the normal, would correspond to \\(-\\infty\\) or \\(+\\infty\\).\nThe most common use of qqplots is to assess whether two distributions, \\(F_1(x)\\) and \\(F_2(x)\\), are similar. If the points in the qqplot fall approximately along the identity line, it suggests that the two distributions have a similar shape.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "summaries/comparing-groups.html#quantile-quantile-plots",
    "href": "summaries/comparing-groups.html#quantile-quantile-plots",
    "title": "\n3  Comparing Groups\n",
    "section": "",
    "text": "Example\nUp to now we have only examined the distribution of heights for males. We expect that the distribution for females to be similar except for 2-3 inch shift due to males being taller than females, on average. A qqplot confirms this:\n\nlibrary(dslabs)\nf &lt;- with(heights, height[sex == \"Female\"])\nm &lt;- with(heights, height[sex == \"Male\"])\nqqplot(f - mean(f), m - mean(m))\nabline(0,1)\n\n\n\n\n\n\n\nWe see that in the middle of the plot the points are on the identiy line, confirming the distirbutiosn are similar except for the shift. However, for the smaller and larger quantiles we see a deviation from the line. We come back to this observation in Section 3.3.\nPercentiles\nPercentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). For example, we refer to the case of \\(p = 0.25\\) as the 25th percentile, representing a value below which 25% of the data falls. The most famous percentile is the 50th, also known as the median. Another special case that receives a name are the quartiles, which are obtained when setting \\(p = 0.25,0.50\\), and \\(0.75\\).",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "summaries/comparing-groups.html#sec-qqplots",
    "href": "summaries/comparing-groups.html#sec-qqplots",
    "title": "\n3  Comparing Groups\n",
    "section": "\n3.2 Assessing normality with QQ plots",
    "text": "3.2 Assessing normality with QQ plots\nA common use of qqplots is to assess whether data follow a normal distribution.\nThe cumulative distribution function (CDF) of the standard normal is written as \\(\\Phi(x)\\), giving the probability that a normal random variable is less than \\(x\\). For example, \\(\\Phi(-1.96) = 0.025\\) and \\(\\Phi(1.96) = 0.975\\). In R, this is computed with pnorm:\n\npnorm(-1.96)\n#&gt; [1] 0.025\n\nThe inverse of \\(\\Phi\\), denoted \\(\\Phi^{-1}(p)\\), provides the theoretical quantiles. For instance, \\(\\Phi^{-1}(0.975) = 1.96\\) which we compute with qnorm:\n\nqnorm(0.975)\n#&gt; [1] 1.96\n\nBy default, pnorm and qnorm assume the standard normal distribution (mean 0, SD 1). You can specify different parameters with the mean and sd arguments:\n\nqnorm(0.975, mean = 69, sd = 3)\n#&gt; [1] 74.9\n\nFor observed data, we can compute sample quantiles using the quantile function. For example, here are the quartiles of male heights:\n\nx &lt;- with(heights, height[sex == \"Male\"])\nquantile(x, c(0.25, 0.50, 0.75))\n#&gt; 25% 50% 75% \n#&gt;  67  69  72\n\nTo check whether these data are approximately normal, we can construct a QQ plot following these steps:\n\nDefine a vector of proportions \\(p_1, \\dots, p_m\\).\nCompute sample quantiles from the data.\nCompute theoretical quantiles from a normal distribution with the same mean and SD as the data.\nPlot the two sets of quantiles against each other.\n\nExample:\n\np &lt;- seq(0.05, 0.95, 0.05)\nsample_quantiles &lt;- quantile(x, p)\ntheoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x))\nplot(theoretical_quantiles, sample_quantiles)\nabline(0, 1)\n\n\n\n\n\n\n\nBecause the points fall close to the identity line, we conclude that the normal distribution provides a good approximation for this dataset.\nThe code becomes simpler if we standardize the data first:\n\nplot(quantile(scale(x), p), qnorm(p))\n\nR provides built-in functions for generating qqplots that compare observed data to a normal distribution:\n\nqqnorm(x)\nqqline(x)\n\nThese functions use the average and standard deviation of the data x to define the theoretical normal distribution used for the x-axis values. In other words, the plotted line represents what we would expect if the data were normally distributed with the same mean and spread as the observed sample.\nAnd equivalently with ggplot2:\n\nlibrary(ggplot2)\nheights |&gt; filter(sex == \"Male\") |&gt;\n  ggplot(aes(sample = scale(height))) + \n  geom_qq() +\n  geom_abline()\n\n\n\n\n\n\n\nBy default, both qqnorm and geom_qq use all \\(n\\) quantiles, with \\(p_i = (i - 0.5)/n\\). Because of this default behavior, large datasets can produce thousands of overlapping points that appear as a solid line. In such cases, it’s better to compute and plot a smaller, representative set of quantiles manually rather than relying on the defaults in qqplot or qqnorm.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "summaries/comparing-groups.html#sec-stratification",
    "href": "summaries/comparing-groups.html#sec-stratification",
    "title": "\n3  Comparing Groups\n",
    "section": "\n3.3 Stratification",
    "text": "3.3 Stratification\nIn data analysis, we often divide observations into groups based on the values of one or more variables associated with those observations. For example, we can divide the height values into groups based on a sex variable: females and males. We call this procedure stratification and refer to the resulting groups as strata.\nStratification is common in data visualization because we are often interested in how the distribution of variables differs across different subgroups.\nUsing the histogram, density plots, and qqplots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.3 inches and a SD of 3.6 inches. With this information, ET will have a good idea of what to expect when he meets our male students. However, to provide a complete picture we need to also provide a summary of the female heights.\nWe learned that boxplots are useful when we want to quickly compare two or more distributions. Here are the heights for females and males:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nheights |&gt; ggplot(aes(sex, height, fill = sex)) + geom_boxplot()\n\n\n\n\n\n\n\nThe plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. But does the normal approximation also work for the female height data collected by the survey? We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful:\n\n\n\n\n\n\n\n\nWe see something we did not see for the males: the density plot has a second bump. Also, the qqplot shows that the highest points tend to be taller than expected by the normal distribution. Finally, we also see five points in the qqplot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights.\nWe have noticed what we didn’t expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, Female was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data.\nRegarding the five smallest values, note that these are:\n\nheights |&gt; filter(sex == \"Female\") |&gt; \n  top_n(5, desc(height)) |&gt;\n  pull(height)\n#&gt; [1] 51 53 55 52 52\n\nBecause these are reported heights, a possibility is that the student meant to enter 5'1\", 5'2\", 5'3\" or 5'5\".",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "summaries/comparing-groups.html#exercises",
    "href": "summaries/comparing-groups.html#exercises",
    "title": "\n3  Comparing Groups\n",
    "section": "\n3.4 Exercises",
    "text": "3.4 Exercises\n1. Study the following boxplots showing population sizes by country:\n\n\n\n\n\n\n\n\nWhich continent has the country with the biggest population size?\n2. Which continent has the largest median population size?\n3. What is median population size for Africa to the nearest million?\n4. What proportion of countries in Europe have populations below 14 million?\n\n0.99\n0.75\n0.50\n0.25\n\n5. If we use a log transformation, which continent shown above has the largest interquartile range?\n6. Define variables containing the heights of males and females as follows:\n\nlibrary(dslabs)\nmale &lt;- heights$height[heights$sex == \"Male\"]\nfemale &lt;- heights$height[heights$sex == \"Female\"]\n\nHow many measurements do we have for each?\n7. Suppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, 70th, & 90th percentiles for each sex. Then create a data frame with these two as columns.\n8. Use a qqplot to demonstrate that murder rates\n\nRemove DC, then generate qqplot to see if the rate follow a normal distribution within each of the four region.",
    "crumbs": [
      "Summary Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comparing Groups</span>"
    ]
  },
  {
    "objectID": "summaries/reading-summaries.html",
    "href": "summaries/reading-summaries.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "Freedman, D., Pisani, R., & Purves, R. (2007). Statistics (4th ed.). A clear, concept-first introduction to distributions, variability, and summaries, excellent for intuition and statistical thinking.\nTukey, J. W. (1977). Exploratory Data Analysis. The classic source for boxplots, robust summaries, and EDA philosophy, foundational for this chapter’s emphasis on describing data.\nCleveland, W. S. (1993). Visualizing Data. A practical, graphics-first treatment of distributions and patterns; pairs well with histograms, density plots, and eCDFs.\nHoaglin, D. C., Mosteller, F., & Tukey, J. W. (1983). Understanding Robust and Exploratory Data Analysis. Wiley. A classic edited volume aimed at explaining robust methods in an accessible style; many chapters are practical.\nWilcox, R. R. (2017). Introduction to Robust Estimation and Hypothesis Testing (4th ed.). Academic Press. A practical, applied text with many worked examples. It goes well beyond what we convered in this chapter but starts with chapters on medians, trimmed means, MAD, and robust alternatives to SD. The later chapters require knowledge of hypothesis testing which we cover in the statistical inference part of the book.\nHuber, P. J., & Ronchetti, E. M. (2009). Robust Statistics (2nd ed.). Wiley. More advanced, but the opening chapters give a rigorous yet readable account of why robustness matters. Useful for readers that want to go a deeper. Requires knowledge we topics we cover in the statistical inference and linear model parts of the book.",
    "crumbs": [
      "Summary Statistics",
      "Recommended Reading"
    ]
  },
  {
    "objectID": "prob/intro-to-prob.html",
    "href": "prob/intro-to-prob.html",
    "title": "Probability",
    "section": "",
    "text": "Introduction\nThe first part of the book focused on describing data we have already collected. But in most data analysis projects, what we observe is only one possible outcome among many. If we were to collect the same data again, another survey, another experiment, we would not obtain exactly the same results. This variation is not to be ignored as it reflects the fundamental uncertainty that accompanies all real data.\nProbability is the mathematical language we use to describe that uncertainty. It provides the foundation for reasoning about variation, quantifying how likely different outcomes are, and ultimately, for drawing conclusions from data. Every inferential method we will learn, from confidence intervals to regression and machine learning, rests on these ideas.\nThe study of probability began with games of chance, where uncertainty is both concrete and controlled: tossing coins, drawing cards, or betting on outcomes. Mathematicians such as Cardano, Fermat, and Pascal developed the first formal methods for computing odds in these settings, laying the groundwork for what would become modern probability theory.\nIn this part of the book, we use these games of chance as a pedagogical tool. They provide clear, well-defined examples that make abstract ideas intuitive. By working with them, we can focus on the logic of probability without the complications of real-world data collection. In later parts of the book, we return to data analysis and show how these same principles apply to real-world uncertainty, polls, experiments, and prediction problems.\nWe will introduce the essential concepts of probability through simple, concrete examples: defining events, computing probabilities, and describing random variables and their distributions. Our emphasis is on intuition and computation rather than mathematical derivation.\nThroughout this part of the book, we also connect probability theory to computer simulations. Using R code and Monte Carlo methods, we will learn how to estimate probabilities, explore random behavior, and develop intuition for uncertainty through simulation. This practical, code-based approach will prepare you to see how probability connects directly to real data in the chapters that follow.",
    "crumbs": [
      "Probability"
    ]
  },
  {
    "objectID": "prob/connecting-data-and-probability.html",
    "href": "prob/connecting-data-and-probability.html",
    "title": "4  Connecting Data and Probability",
    "section": "",
    "text": "4.1 Probability models in the real world\nIn this part of the book, we use games of chance, tossing coins, drawing beads, spinning a roulette wheel, to illustrate the principles of probability. These examples are valuable because the probabilistic mechanism is perfectly known. We can compute exact probabilities, repeat experiments as many times as we like, and verify the results mathematically or by simulation.\nIn real-world data analysis, the connection between probability and observation is rarely so clear. The systems we study, such as, people, hospitals, weather, economies, and biological processes, are complex. We use probability not because these systems are necessarily truly random, but because probabilistic models provide a practical way to describe and reason about unexplained variability and uncertainty. In this chapter, we introduce some of the ways real-world problems can be connected to probabilistic models, so you can begin to recognize these connections as you learn about probability theory in the following chapters.\nToday, probability underlies far more than gambling. We use it to describe the likelihood of rain, the risk of disease, or the uncertainty in an election forecast. Yet, outside of games of chance, the meaning of probability is often less obvious. In practice, when we use probability to analyze data, we are almost always assuming that a probabilistic model applies. Sometimes this assumption is well motivated; other times, it is simply a convenient approximation.\nIt is important to keep this in mind: when our probability model is wrong, our conclusions may be wrong as well.\nBelow we go over the five main ways we connect probability to data.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Connecting Data and Probability</span>"
    ]
  },
  {
    "objectID": "prob/connecting-data-and-probability.html#probability-models-in-the-real-world",
    "href": "prob/connecting-data-and-probability.html#probability-models-in-the-real-world",
    "title": "4  Connecting Data and Probability",
    "section": "",
    "text": "Random sampling\nIn polling, we often assume that the people we answer our questions are a random sample from the population of interest. For example, imagine putting the phone numbers of all eligible voters into a large bag and drawing 1,000 at random, then calling them and asking who they support. If this process were truly random, the statistical methods we will introduce will apply directly.\nThis assumption is also common in animal or laboratory studies. When a researcher orders mice from a breeding facility, the supplier’s shipment is often treated as a random sample from all possible mice in that strain. This makes it possible to apply probability models in a way very similar to our games of chance.\n\n\nRandom assignment\nA second setting where probability connects directly to data is randomized experiments. In clinical trials, for instance, patients are assigned to a treatment or control group by chance, often by flipping a coin. This ensures that, on average, the groups are comparable, and that any difference in outcomes can be attributed to the treatment itself. The same logic underlies what is today called A/B testing, widely used in technology and business settings to compare two versions of a product, website, or algorithm. Whether in medicine or online platforms, these experiments rely on the same fundamental idea: random assignment creates conditions where probability can be used to infer causation. In these examples, as in urn or coin examples, the link between probability and data is clear.\n\n\nWhen randomness is assumed\nIn many real-world analyses, data are not collected through random sampling or controlled experiments. Instead, most datasets we work with are convenience samples, collected from individuals who are easy to reach rather than randomly selected from a well-defined population. Examples include participants in online surveys, patients from a particular hospital, students in a specific class, or volunteers in a research study.\nWhen we fit probability models to such data, we implicitly make an assumption that the sample behaves as if it were random. This assumption is rarely stated explicitly, yet it underlies many analyses that report results such as \\(p\\)-values, confidence intervals, or predicted probabilities. These quantities all rely on probabilistic reasoning, which presumes that the data can be treated as draws from a random process.\nThe same assumption appears in machine learning. When we train a model, we use only a subset of available data and expect the model to generalize to unseen cases. This expectation depends on the idea that the training data are, for all practical purposes, a random sample from the same population as the future data. In practice, however, these training sets are often convenience samples as well.\nDespite this, such models and analyses often work remarkably well and have provided valuable insights. For example, studies using observational data have convincingly shown that smoking harms health and that regular exercise and good diet extend life. But when data come from convenience samples, extra care is needed. The assumptions behind our probability models become especially important. Treating a convenience sample as if it were a simple random sample can lead to biased and misleading conclusions.\nThese biases can sometimes be corrected with statistical techniques, but doing so requires making additional assumptions. Understanding what those assumptions are, and thinking critically about whether they are reasonable, is essential for sound data analysis. Later in the book (see Chapter 19), we will revisit this topic and show concrete examples of how unexamined assumptions about randomness can distort results.\n\n\nNatural variation\nFinally, many natural processes appear random even when no measurement error or sampling is involved. Which genes are inherited, small biometric differences between genetically identical organisms, variation in the stock market, the number of earthquakes in a year, whether a particular cell becomes cancerous, or the number of photons striking a telescope’s detector, all vary in ways that seem unpredictable. In these situations, probability serves as a modeling tool: a concise way to describe complex systems whose underlying mechanisms are too intricate, chaotic, or numerous to predict exactly.\nIn such cases, probabilistic models do not imply that nature itself is random. Rather, they acknowledge our limited ability to predict and give us a useful way to summarize uncertainty.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Connecting Data and Probability</span>"
    ]
  },
  {
    "objectID": "prob/connecting-data-and-probability.html#the-role-of-design",
    "href": "prob/connecting-data-and-probability.html#the-role-of-design",
    "title": "4  Connecting Data and Probability",
    "section": "4.2 The role of design",
    "text": "4.2 The role of design\nBecause most real-world data do not arise from ideal random processes, our ability to draw valid conclusions depends critically on the design of studies and experiments. As the statistician Ronald Fisher famously remarked, “To consult the statistician after an experiment is finished is often merely to ask him to perform a post-mortem examination.” Design is what connects the mathematics of probability to the messy realities of data collection.\nA well-designed study, one that uses randomization, proper sampling, and consistent measurement, creates conditions under which probabilistic reasoning is meaningful. Without this foundation, even the most sophisticated analyses can produce misleading results. Random assignment helps isolate cause and effect. Random sampling helps ensure representativeness. Standardized procedures reduce bias and make uncertainty quantifiable.\nThis book does not cover experimental design in detail, but in the Further Reading section we recommend several excellent introductions. For now, the key idea is this: when we use probability to reason about data, we are implicitly assuming a connection between a mathematical model and a real process. The stronger that connection, through careful design, random sampling, or random assignment, the more trustworthy our conclusions will be. Poor design, on the other hand, weakens this bridge and makes uncertainty far harder to interpret.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Connecting Data and Probability</span>"
    ]
  },
  {
    "objectID": "prob/connecting-data-and-probability.html#exercises",
    "href": "prob/connecting-data-and-probability.html#exercises",
    "title": "4  Connecting Data and Probability",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\n1. Suppose a national poll reports that 52% of voters support a candidate, based on a random sample of 1,000 people. a. Explain what “random sample” means in this context. b. Describe at least two ways the real sampling process might deviate from true randomness.\n2. A clinical trial randomly assigns 500 participants to treatment and 500 to control. a. Is this an example of random sampling or random assignment? b. How does random assignment help us draw causal conclusions? c. What assumptions must still hold for the trial’s conclusions to generalize to the broader population?\n3. A researcher records each patient’s blood pressure twice and finds that the two measurements differ by about 5 mmHg on average. Explain how measurement error can be viewed as a source of randomness.\n4. You are studying caffeine consumption among college students, but your survey responses come primarily from biology majors. a. Why might treating this sample as “random” lead to misleading conclusions? b. Suggest at least one way to improve the representativeness of the data collection process. c. Describe a real-world scenario where improving representativeness might be difficult or impossible.\n5. For each of the following, identify the primary source of randomness being modeled, sampling variation, measurement error, natural variation, or random assignment: a. Estimating unemployment from a household survey. b. Measuring air temperature every hour for a week. c. Testing whether a fertilizer increases crop yield. d. Comparing test scores from different schools. e. Predicting how many hurricanes will form next year.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Connecting Data and Probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html",
    "href": "prob/discrete-probability.html",
    "title": "\n5  Discrete Probability\n",
    "section": "",
    "text": "5.1 Relative frequency\nWe begin by covering some basic principles related to categorical data. The specific area of probability which deals with categorical data is referred to as discrete probability. Understanding this topic, will help us comprehend the probability theory we will later introduce for numeric and continuous data, which is much more common in data analysis. Since discrete probability is invaluable in card games, we will use these as illustrative examples.\nThe term probability is used in everyday language. Yet answering questions about probability is often hard, if not impossible. In this section, we discuss a mathematical definition of probability that allows us to give precise answers to certain questions.\nFor example, if I have 2 red beads and 3 blue beads inside an urn1 (most probability books use this archaic term, so we do too) and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes, of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has an equal chance of occurring, we conclude that the probability is .4 for red and .6 for blue.\nA more intuitive way to think about the probability of an event is as the long-run proportion of times the event occurs when an experiment is repeated independently and under identical conditions, an infinite number of times. This interpretation naturally leads to one of the most powerful techniques in data science, Monte Carlo simulations, which we will explore later in this chapter.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#probability-distributions",
    "href": "prob/discrete-probability.html#probability-distributions",
    "title": "\n5  Discrete Probability\n",
    "section": "\n5.2 Probability distributions",
    "text": "5.2 Probability distributions\nProbability distributions are the foundation of most statistical models. They describe how likely different outcomes are, providing a mathematical summary of uncertainty. Every model we use in data analysis, from simple polling examples to complex multivariate models, is built on assumptions about underlying probability distributions. As we move to continuous and multidimensional settings, these distributions become more complex and abstract. For discrete cases, however, the idea is quite intuitive: each possible outcome, or category, is assigned a probability based on its relative frequency.\nIf we know the relative frequency of the different categories, defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases analogous to beads in an urn, for each bead type, their proportion defines the distribution.\nFor example, if we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% Undecided, and 2% Green Party, these proportions define the probability for each group in any given call. The probability distribution for each call is defined by four numbers: Pr(calling a Republican) = 0.44, Pr(calling a Democrat) = 0.44, Pr(callin an Undecided) = 10%, and Pr(calling a Green) = 0.02.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#sec-monte-carlo",
    "href": "prob/discrete-probability.html#sec-monte-carlo",
    "title": "\n5  Discrete Probability\n",
    "section": "\n5.3 Monte Carlo",
    "text": "5.3 Monte Carlo\nComputers provide a way to actually perform the simple random long-run experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.\nAn example is the sample function in R. We demonstrate its use in the code below. First, we use the function rep to generate the urn:\n\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2,3))\n\nand then use sample to pick a bead at random:\n\nsample(beads, 1)\n#&gt; [1] \"red\"\n\nThis line of code produces a single random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat indefinitely. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating indefinitely. This is an example of a Monte Carlo simulation.\nMuch of what mathematical and theoretical statisticians study, topics that we do not cover in this book, relates to providing rigorous definitions of “practically equivalent”. Additionally, they explore how close a large number of experiments brings us to what happens in the limit. Later in this chapter, we provide a practical approach to determining what is “large enough”.\nTo perform our first Monte Carlo simulation, we use the replicate function, which allows us to repeat the same task any number of times. Here, we repeat the random event 10,000 times:\n\nset.seed(1986) \nevents &lt;- replicate(10000, sample(beads, 1))\n\n\n\n\n\n\n\nHere we use the replicate function for educational purposes, but there is a faster way to generate this simulation using:\n\nsample(beads, 10000, replace = TRUE)\n\nwhich we explain in the following section.\n\n\n\nWe can now verify if our definition actually is in agreement with this Monte Carlo simulation approximation. We use table to count the outcomes and and prop.table gives us the proportions:\n\nprop.table(table(events))\n#&gt; events\n#&gt;  blue   red \n#&gt; 0.601 0.399\n\nThe numbers above represent the estimated probabilities obtained by this Monte Carlo simulation. Statistical theory tells us that as \\(B\\) gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.\nThis is a simple and not very useful example, since we can easily compute the probabilities mathematically. Monte Carlo simulations are useful when it is hard, or impossible, to compute the exact probabilities mathematically. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.\nSetting the random seed\nBefore we continue, we will briefly explain the following important line of code:\n\nset.seed(1986) \n\nThroughout this book, we use random number generators. This implies that many of the results presented can potentially change by chance, indicating that a static version of the book may show a different result than what you obtain when following the code as presented. This is actually fine, given that the results are random and change by chance. However, if you want to ensure that results are consistent with each run, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed every time and we used a popular way to pick the seed which is year - month - day. For example, we chose 1986 on December 20, 2018: \\(2018 - 12 - 20 = 1986\\).\nYou can learn more about setting the seed by looking at the documentation:\n\n?set.seed\n\nIn the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.\nWith and without replacement\nThe function sample has an argument that allows us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not returned to the bag. Notice what happens when we ask to randomly select five beads:\n\nsample(beads, 5)\n#&gt; [1] \"red\"  \"blue\" \"blue\" \"blue\" \"red\"\nsample(beads, 5)\n#&gt; [1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\nThis results in rearrangements that consistently comprise three blue and two red beads. If we ask that six beads be selected, we get an error.\nHowever, the sample function can be used directly, without the use of replicate, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample with replacement: return the bead back to the urn after selecting it. We can tell sample to do this by changing the replace argument, which defaults to FALSE, to replace = TRUE:\n\nevents &lt;- sample(beads, 10000, replace = TRUE)",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#combinations-and-permutations",
    "href": "prob/discrete-probability.html#combinations-and-permutations",
    "title": "\n5  Discrete Probability\n",
    "section": "\n5.4 Combinations and permutations",
    "text": "5.4 Combinations and permutations\nMost university-level statistics courses begin with combinatorics, which provides the foundation for many probability calculations. These techniques let us count the number of outcomes that satisfy a condition, typically by computing the number of permutations (when order matters) or combinations (when order does not matter).\nTo make these ideas concrete, let’s consider a familiar setting: card games. Suppose we want to understand the probability of drawing specific hands from a deck. This example helps demonstrate how combinatorics works and how we can use R to perform these calculations.\nFirst, let’s build a deck of cards:\n\nnumbers &lt;- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\",\n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\nsuits &lt;- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\")\ndeck &lt;- expand.grid(number = numbers, suit = suits)\ndeck &lt;- paste(deck$number, deck$suit)\n\nIf we draw one card at random, the probability of getting a King is 1/13:\n\nmean(deck %in% paste(\"King\", suits))\n#&gt; [1] 0.0769\n\nBecause the deck includes all possible outcomes, this calculation gives the exact probability. Since mean converts logical values to 0s and 1s, the result represents the fraction of Kings in the deck, four out of fifty-two cards, which corresponds exactly to the theoretical probability of \\(1/13\\).\nNow let’s introduce two key functions: permutations() and combinations() from the gtools package. These functions let us enumerate all possible ways to select items from a list, depending on whether order matters (permutations) or not (combinations). Such calculations are fundamental in probability because they allow us to count the number of possible outcomes in complex settings, like determining the odds of drawing a particular poker hand or arranging cards in a specific order. Card games, lotteries, and many other probability problems rely on these principles to compute exact probabilities. For readers interested in the mathematical foundations, the Recommended Reading section lists probability textbooks that explore permutations and combinations in greater detail.\nHere is a simple example enumerating all the permutations of three items taken two at a time:\n\nlibrary(gtools)\npermutations(3, 2)\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    1    3\n#&gt; [3,]    2    1\n#&gt; [4,]    2    3\n#&gt; [5,]    3    1\n#&gt; [6,]    3    2\n\nThis lists all ordered pairs from the numbers 1, 2, and 3. Notice that (1,2) and (2,1) are treated as distinct. If order does not matter, we use combinations():\n\ncombinations(3, 2)\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    1    3\n#&gt; [3,]    2    3\n\nNow (1,2) and (2,1) are treated as the same outcome.\nLet’s use these tools to compute the probability of a Natural 21 in Blackjack, which occurs when a player’s first two cards are an Ace and a face card (Jack, Queen, King, or Ten). Since order does not matter, we use combinations():\n\naces &lt;- paste(\"Ace\", suits)\nfacecard &lt;- expand.grid(number = c(\"King\", \"Queen\", \"Jack\", \"Ten\"), suit = suits)\nfacecard &lt;- paste(facecard$number, facecard$suit)\nhands &lt;- combinations(52, 2, v = deck)\nmean((hands[,1] %in% aces & hands[,2] %in% facecard) |\n       (hands[,2] %in% aces & hands[,1] %in% facecard))\n#&gt; [1] 0.0483\n\nMonte Carlo approach\nWe can obtain the same result through simulation. Instead of enumerating all combinations, we repeatedly draw two cards from the deck and check whether we have a Natural 21. This is a Monte Carlo approximation to the same probability.\n\nblackjack &lt;- function(){\n   x &lt;- sample(deck, 2)\n   (x[1] %in% aces & x[2] %in% facecard) | (x[2] %in% aces & x[1] %in% facecard)\n}\n\nresults &lt;- replicate(10000, blackjack())\nmean(results)\n#&gt; [1] 0.0453\n\nBoth approaches yield nearly the same result. The combinatoric functions give the exact probability, while the Monte Carlo simulation provides an approximation that converges to the same value as the number of repetitions increases.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#examples",
    "href": "prob/discrete-probability.html#examples",
    "title": "\n5  Discrete Probability\n",
    "section": "\n5.5 Examples",
    "text": "5.5 Examples\nIn this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We use R to help illustrate the mathematical concepts.\nMonty Hall problem\nIn the 1970s, there was a game show called “Let’s Make a Deal,” with Monty Hall as the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door, there was a prize, while the other doors had a goat behind them to show the contestant had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and reveal to the contestant that there was no prize behind that door. Then, he would ask, “Do you want to switch doors?” What would you do?\nWe can use probability to demonstrate that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This might seem counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy2 or read one on Wikipedia3. Below, we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.\n\nmonty_hall &lt;- function(strategy = c(\"stick\", \"switch\")) {\n  strategy &lt;- match.arg(strategy)\n  doors &lt;- c(\"1\", \"2\", \"3\")\n  prize_door &lt;- sample(doors, 1)\n  my_pick &lt;- sample(doors, 1)\n  show &lt;- sample(setdiff(doors, c(my_pick, prize_door)), 1)\n  final &lt;- if (strategy == \"stick\") my_pick else setdiff(doors, c(my_pick, show))\n  final == prize_door\n}\nB &lt;- 10000\nmean(replicate(B, monty_hall(\"stick\")))\n#&gt; [1] 0.326\nmean(replicate(B, monty_hall(\"switch\")))\n#&gt; [1] 0.669\n\nAs we examine the code, we notice that the lines starting with my_pick and show do not affect the final outcome when we stick with our original choice. This confirms that our winning probability in that case is 1 in 3, as expected. When we switch, the Monte Carlo simulation verifies the 2/3 probability. This illustrates why switching improves our odds: the show line removes a door that is guaranteed not to be the winner, effectively transferring its probability to the remaining unopened door. Thus, unless our initial choice was correct, which happens only one-third of the time, we win by switching, which occurs two-thirds of the time.\nBirthday problem\nSuppose you are in a classroom with 50 people. If we assume this is a randomly selected group, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do that later. Here, we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29, which doesn’t significantly change the answer.\nFirst, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained as follows:\n\nn &lt;- 50\nbdays &lt;- sample(1:365, n, replace = TRUE)\n\nTo check if there are at least two people with the same birthday in this particular set of 50 people, we can use the duplicated function, which returns TRUE whenever an element of a vector is a duplicate:\n\nany(duplicated(bdays))\n#&gt; [1] TRUE\n\nIn this case, we see that it did happen; there were at least two people who had the same birthday.\nTo estimate the probability of a shared birthday in the group, we repeat this experiment by repeatedly sampling sets of 50 birthdays:\n\nsame_birthday &lt;- function(n) any(duplicated(sample(1:365, n, replace = TRUE)))\nresults &lt;- replicate(10000, same_birthday(50))\nmean(results)\n#&gt; [1] 0.969\n\nPeople tend to underestimate these probabilities so let’s say we want to use this knowledge to make bet with friends about the likelihood of two people sharing the same birthday in a group. At what group size do the chances become greater than 50%? Greater than 75%?\nLet’s create a look-up table. We can quickly create a function to compute this for any group size:\n\ncompute_prob &lt;- function(n, B = 10000) mean(replicate(B, same_birthday(n)))\nn &lt;- seq(1,60)\nprob &lt;- sapply(n, compute_prob)\n\nWe can now make a plot of the estimated probabilities of two people having the same birthday in a group of size \\(n\\):\n\nplot(n, prob)\n\n\n\n\n\n\n\nNow let’s compute the exact probabilities rather than relying on Monte Carlo approximations. Using math gives the precise answer and is much faster since we don’t need to simulate experiments. We apply a common probability trick: instead of finding the probability that at least two people share a birthday, we compute the probability that no one does. By the multiplication rule, person 1 can have any birthday, person 2 must choose from the remaining 364 out of 365 days, and so on, giving the probability that all \\(n\\) birthdays are unique as:\n\\[\n1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365}\n\\]\nWe can write a function that does this for any number and plot this as a red curve:\n\nexact_prob &lt;- function(n) 1 - prod(seq(365, 365 - n + 1)/365)\neprob &lt;- sapply(n, exact_prob)\nplot(n, prob)\nlines(n, eprob, col = \"red\")\n\n\n\n\n\n\n\nThis plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had we not been able to compute the exact probabilities, we could still accurately estimate them.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#infinity-in-practice",
    "href": "prob/discrete-probability.html#infinity-in-practice",
    "title": "\n5  Discrete Probability\n",
    "section": "\n5.6 Infinity in practice",
    "text": "5.6 Infinity in practice\nThe theory described here requires repeating experiments over and over indefinitely. In practice, we can’t do this. In the examples above, we used \\(B=10,000\\) Monte Carlo experiments, yielding accurate estimates. The larger this number, the more accurate the estimate becomes, until the approximation is so good that your computer can’t tell the difference. However, in more complex calculations, 10,000 may not be nearly enough. Moreover, for some calculations, 10,000 experiments might not be computationally feasible.\nAlthought in this case we know the exact probability is 0.5686997, in practical scenarios, we won’t know what the answer is beforehand, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger the \\(B\\), the better the approximation. But how large do we need it to be? This is actually a challenging question, and answering it often requires advanced theoretical statistics training.\nOne practical approach is to check for the stability of the estimate. The following example illustrates the birthday problem for a group of 25 people.\n\nB &lt;- 10^seq(1, 5, len = 100)\ncompute_prob &lt;- function(B, n = 25) mean(replicate(B, same_birthday(n)))\nprob &lt;- sapply(B, compute_prob)\nplot(log10(B), prob)\nabline(h = eprob[25], lty = 2)\n\n\n\n\n\n\n\nIn this plot, we can see that the values start to stabilize at around 1,000.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#exercises",
    "href": "prob/discrete-probability.html#exercises",
    "title": "\n5  Discrete Probability\n",
    "section": "\n5.7 Exercises",
    "text": "5.7 Exercises\n1. A box contains 3 cyan balls, 5 magenta balls, and 7 yellow balls. One ball is drawn at random. Write a Monte Carlo simulation to estimate and confirm the probability that the ball drawn is cyan.\n2. Do the same for the probability that the ball will not be cyan?\n3. Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. Write a Monte Carlo simulation to estimate and confirm the probability of the first draw being cyan and the second draw not being cyan?\n4. Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. Write a Monte Carlo simulation to estimate and confirm the probability of the first draw being cyan and the second draw not being cyan?\n5. Let’s say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one will be yellow?\n6. If you roll a 6-sided die six times, write a Monte Carlo simulation to estimate and confirm the probability of not seeing a 6?\n7. Two teams, let’s say the Celtics and the Cavaliers, are playing a seven game series. The Cavaliers are a better team and have a 60% chance of winning each game. What is the probability that the Celtics will win at least one game?\n8. Create a Monte Carlo simulation to confirm your answer to the previous problem. Use B &lt;- 10000 simulations. Hint: use the following code to generate the results of the first four games:\n\nceltic_wins &lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))\n\nThe Celtics must win one of these 4 games.\n9. Two teams, say the Cavaliers and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavaliers lose the first game, what is the probability that they win the series?\n10. Confirm the results of the previous question with a Monte Carlo simulation.\n11. Two teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p&gt;0.5\\) chance of winning each game. Given a value \\(p\\), the probability of winning the series for the underdog team \\(B\\) can be computed with the following function based on a Monte Carlo simulation:\n\nprob_win &lt;- function(p, B =10000){\n  result &lt;- replicate(B, {\n    b_win &lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1 - p, p))\n    sum(b_win) &gt;= 4\n  })\n  mean(result)\n}\n\nUse the function sapply to compute the probability, call it ws, of winning for p &lt;- seq(0.5, 0.95, 0.025). Then plot the result.\n12. Repeat the exercise above, but now keep the probability fixed at p &lt;- 0.75 and compute the probability for different series lengths: best of 1 game, best of 3 games, best of 5 games, and so on. Specifically, ns &lt;- seq(1, 25, 2). Hint: use the function below.\n\nprob_win &lt;- function(n, p = 0.75, B = 10000){\n  result &lt;- replicate(B, {\n    b_win &lt;- sample(c(1,0), n, replace = TRUE, prob = c(1 - p, p))\n    sum(b_win) &gt;= (n + 1)/2\n  })\n  mean(result)\n}",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#footnotes",
    "href": "prob/discrete-probability.html#footnotes",
    "title": "\n5  Discrete Probability\n",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Urn_problem↩︎\nhttps://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem↩︎\nhttps://en.wikipedia.org/wiki/Monty_Hall_problem↩︎",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html",
    "href": "prob/continuous-probability.html",
    "title": "\n6  Continuous Probability\n",
    "section": "",
    "text": "6.1 Cumulative distribution functions\nIn Section 1.3, we discussed why it is not practical to assign a frequency to every possible continuous outcome, such as an exact height, since there are infinitely many possible values. The same idea extends to random outcomes that take values on a continuous scale: each individual value has probability zero. Instead, we describe their behavior through probability density functions, which let us compute probabilities for intervals of values rather than single points.\nIn this chapter, we introduce the mathematical framework for continuous probability distributions and present several useful approximations that frequently appear in data analysis.\nWe return to our example using the heights of adult male students:\nlibrary(tidyverse)\nlibrary(dslabs)\nx &lt;- heights |&gt; filter(sex == \"Male\") |&gt; pull(height)\nWe previously defined the empirical cumulative distribution function (eCDF) as\nF &lt;- function(a) mean(x &lt;= a)\nwhich, for any value a, gives the proportion of values in the list x that are less than or equal to a.\nTo connect the eCDF to probability, imagine randomly selecting one of the male students. What is the chance that he is taller than 70.5 inches? Because each student is equally likely to be chosen, this probability is simply the proportion of students taller than 70.5 inches. Using the eCDF, we can compute it as:\n1 - F(70.5)\n#&gt; [1] 0.363\nThe cumulative distribution function (CDF) is the theoretical counterpart of the eCDF. Rather than relying on observed data, it assigns probabilities to ranges of values for a random outcome \\(X\\). Specifically, the CDF gives, for any number \\(a\\), the probability that \\(X\\) is less than or equal to \\(a\\):\n\\[\nF(a) = \\Pr(X \\leq a)\n\\]\nOnce the CDF is defined, we can compute the probability that \\(X\\) falls within any interval. For example, the probability that a student’s height is between \\(a\\) and \\(b\\) is:\n\\[\n\\Pr(a &lt; X \\leq b) = F(b) - F(a)\n\\]\nBecause we can determine the probability of any event from the CDF, it fully defines the probability distribution of a continuous outcome.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#probability-density-function",
    "href": "prob/continuous-probability.html#probability-density-function",
    "title": "\n6  Continuous Probability\n",
    "section": "\n6.2 Probability density function",
    "text": "6.2 Probability density function\nFor most continuous distributions, we can describe the cumulative distribution function (CDF) in terms of another function, \\(f(x)\\), such that\n\\[\nF(b) - F(a) = \\int_a^b f(x)\\,dx\n\\]\nThis function \\(f(x)\\) is called the probability density function (PDF).\nThe PDF plays a role similar to the relative frequency distribution for discrete data. The key difference is that for a continuous variable, individual values have probability zero, so instead of assigning probability to specific outcomes, the PDF shows how probability is spread across the range of possible values of \\(x\\). The probability that the variable falls within an interval, say \\([a,b]\\), is given by the area under the curve between \\(a\\) and \\(b\\). Thus, the shape of the PDF shows where values are more or less likely to occur, with wider or higher regions corresponding to ranges that contain more probability mass.\nTo build intuition for why we use a continuous function and an integral to describe probabilities, consider a situation where an outcome can be measured with very high precision. In this case, we can think of the relative frequency as being proportional to the height of the bars in the plot below:\n\n\n\n\n\n\n\n\nThe probability of the outcome falling within an interval, for example, \\(x \\in [0.5, 1.5]\\), can be approximated by adding the heights of the bars within that range and dividing by the total height of all bars. As the measurement becomes more precise and the bar widths become narrower, this sum approaches the area under the curve of the continuous function \\(f(x)\\).\nIn the limit where the bar widths shrink to zero, the sum and the integral are identical: the probability of \\(x\\) falling in an interval is exactly the area under \\(f(x)\\) over that interval. To make this work, we define \\(f(x)\\) so that the total area under the curve equals 1:\n\\[\n\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\n\\]\nThis ensures that \\(f(x)\\) represents a valid PDF.\nAn important example of a PDF is the normal distribution, introduced in Section 2.1. Its probability density function is\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2\\right)\n\\]\nand as we previous saw has a bell shaped centered at \\(\\mu\\) and 95% of the area with \\(2\\sigma\\) from \\(\\mu\\):\n\n\n\n\n\n\n\n\nIn R, this function this PDF is given by dnorm and the corresponding CDF by pnorm.\nA random outcome is said to be normally distributed with mean m and standard deviation s if its CDF is defined by\n\nF(a) &lt;- pnorm(a, mean = m, sd = s)\n\nThis is particularly useful in practice. If we are willing to assume that a variable, such as height follows a normal distribution, we can answer probability questions without needing the full dataset. For example, to find the probability that a randomly selected student is taller than 70.5 inches, we only need the sample mean and standard deviation:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n1 - pnorm(70.5, m, s)\n#&gt; [1] 0.371\n\nGood — your existing “Theoretical distributions as approximations” section is clear and conceptually sound, but it overlaps heavily with what you already explain in “The normal distribution” (particularly where you discuss approximating discrete data with a continuous normal curve).",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#theoretical-distributions-as-practical-models",
    "href": "prob/continuous-probability.html#theoretical-distributions-as-practical-models",
    "title": "\n6  Continuous Probability\n",
    "section": "\n6.3 Theoretical distributions as practical models",
    "text": "6.3 Theoretical distributions as practical models\nTheoretical distributions, such as the normal distribution, are defined mathematically rather than derived directly from data. In practice, we use them to approximate the behavior of real data that arise from complex or unknown processes. Almost all datasets we analyze consist of discrete observations, yet many of these quantities, such as height, weight, or blood pressure, are better understood as measurements of underlying continuous variables.\nFor example, our height data appear discrete because values are typically rounded to the nearest inch. A few individuals report more precise metric measurements, while most round to whole inches. It is therefore more realistic to think of height as a continuous variable whose apparent discreteness comes from rounding.\nWhen using a continuous distribution such as the normal, we no longer assign probabilities to individual points, each exact value has probability zero, but rather to intervals. In Chapter 2 we showed how this theoretical distributions can approximate.\nThe key idea is that theoretical distributions serve as useful approximation. They provide smooth mathematical descriptions that let us compute probabilities and reason about uncertainty. Even though real measurements are discrete, the continuous approximations that apply, such as the normal distribution, allow us to work with data analytically and build models that generalize well beyond a specific dataset.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#monte-carlo-simulations",
    "href": "prob/continuous-probability.html#monte-carlo-simulations",
    "title": "\n6  Continuous Probability\n",
    "section": "\n6.4 Monte Carlo simulations",
    "text": "6.4 Monte Carlo simulations\nSimulation is a powerful way to understand randomness, approximate probabilities, and explore how theoretical models behave in practice. Many of the probability models used in data analysis are continuous distributions, which describe outcomes that can take any value within a range. In R, all random-number–generating functions that produce simulated data from a continuous distribution, begin with the letter r. These functions form part of a consistent family that allows us to simulate from nearly any probability distribution. In this section, we illustrate Monte Carlo techniques using continuous distributions, starting with the normal distribution and extending to other commonly used models.\nNormal distribution\nR provides the rnorm function to generate normally distributed outcomes. It takes three arguments, sample size, mean (default 0), and standard deviation (default 1), and produces random numbers that follow a normal distribution.\nHere is an example of how we could generate data that resemble our reported heights:\n\nn &lt;- length(x)\nm &lt;- mean(x)\ns &lt;- sd(x)\nsimulated_heights &lt;- rnorm(n, m, s)\n\nNot surprisingly, the distribution of the simulated heights looks normal:\n\n\n\n\n\n\n\n\nThis is one of the most useful functions in R because it lets us generate data that mimic natural variation and explore what outcomes might occur by chance through Monte Carlo simulations.\nExample: extreme values\nHow rare is it to find a seven-footer among 800 men? Suppose we repeatedly sample 800 male students at random and record the tallest in each group. What does the distribution of these tallest height look like? The following Monte Carlo simulation helps us find out:\n\ntallest &lt;- replicate(10000, max(rnorm(800, m, s)))\n\nHaving a seven-footer is quite rare:\n\nmean(tallest &gt;= 7*12)\n#&gt; [1] 0.0187\n\nHere is the resulting distribution of the tallest person’s height:\n\n\n\n\n\n\n\n\nAlthough deriving this distribution analytically is possible, it is not straightforward. Once derived, it provides a faster way to evaluate probabilities than simulation. However, when analytic derivations are too complex or infeasible, Monte Carlo simulation offers a practical alternative. By repeatedly generating random samples, we can approximate the distribution of almost any statistic and obtain reliable estimates even when theoretical results are unavailable.\nOther continuous distributions\nThe normal distribution is not the only useful theoretical model. Other continuous distributions that often appear in data analysis include the Student’s t, chi-square, exponential, gamma, and beta distributions. Their corresponding shorthand names in R are t, chisq, exp, gamma, and beta.\nEach of these distributions has an associated r function, such as rt, rchisq, rexp, rgamma, and rbeta, that allows you to generate random samples for simulation. This consistency makes it easy to apply Monte Carlo methods to a wide variety of data-generating processes.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#calculating-densities-probabilities-and-quantiles",
    "href": "prob/continuous-probability.html#calculating-densities-probabilities-and-quantiles",
    "title": "\n6  Continuous Probability\n",
    "section": "\n6.5 Calculating densities, probabilities, and quantiles",
    "text": "6.5 Calculating densities, probabilities, and quantiles\nBeyond simulation, R provides companion functions to evaluate and summarize continuous distributions. Each distribution in base R follows a simple naming convention:\n\n\nd - density function\n\np - cumulative distribution function (CDF)\n\nq - quantile function\n\nr - random number generation\n\nFor example, the Student’s t distribution uses dt, pt, qt, and rt. Base R includes the most common continuous distributions, normal, t, chi-square, exponential, gamma, and beta, but many additional distributions are available in specialized packages such as extraDistr and actuar. While there are dozens of continuous distributions in use, the ones provided by base R cover the majority of applications encountered in practice.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#exercises",
    "href": "prob/continuous-probability.html#exercises",
    "title": "\n6  Continuous Probability\n",
    "section": "\n6.6 Exercises",
    "text": "6.6 Exercises\n1. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?\n2. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?\n3. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?\n4. Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?\n5. Notice that the answer to the question does not change when you change units. This makes sense since the standard deviations from the average for an entry in a list are not affected by what units we use. In fact, if you look closely, you notice that 61 and 67 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.\n6. To understand the mathematical rationale that explains why the answers to exercises 3, 4, and 5, suppose we have a random variable \\(X\\) with average \\(\\mu\\) and standard error \\(\\sigma\\) and that we ask what is the probability of \\(X\\) being smaller or equal to \\(a\\). The probability is:\n\\[\n\\mathrm{Pr}(X \\leq a)\n\\] Remember that, by definition, \\(a\\) is \\((a - \\mu)/\\sigma\\) standard deviations \\(s\\) away from the average \\(\\mu\\). Now we subtract \\(\\mu\\) to both sides and then divide both sides by \\(\\sigma\\):\n\\[\n\\mathrm{Pr}\\left(\\frac{X-\\mu}{\\sigma} \\leq \\frac{a-\\mu}{\\sigma} \\right)\n\\]\nThe quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it \\(Z\\):\n\\[\n\\mathrm{Pr}\\left(Z \\leq \\frac{a-\\mu}{\\sigma} \\right)\n\\]\nSo, no matter the units, the probability of \\(X\\leq a\\) is the same as the probability of a standard normal variable being less than \\((a - \\mu)/s\\).\nIf m is the average and s the standard error, which of the following R code would give us the right answer in every situation?\n\nmean(X &lt;= a)\npnorm((a - m)/s)\npnorm((a - m)/s, m, s)\npnorm(a)\n\n7. Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm.\n8. IQ scores are approximately normally distributed, with a mean of 100 and a standard deviation of 15. Suppose we want to know what the distribution of the highest IQ would look like across all high school graduating classes if 10,000 people are born each year in a school district. Run a Monte Carlo simulation with B = 1000, where each iteration generates 10,000 IQ scores and records the highest value. Then, create a histogram to visualize the distribution of the maximum IQ in the district.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability</span>"
    ]
  },
  {
    "objectID": "prob/random-variables.html",
    "href": "prob/random-variables.html",
    "title": "7  Random Variables",
    "section": "",
    "text": "7.1 Definition\nTo study uncertainty rigorously, we need mathematical tools that describe how data behave under randomness. Statistical inference, the topic of the next part of this book, builds on these tools to quantify uncertainty and make predictions. In this chapter, we introduce the concept of random variables and explore their main properties through simple examples.\nStatistical inference begins with the concept of a random variable, a numerical quantity whose value depends on the outcome of a random process. Random variables connect probability theory to data analysis by giving us a way to describe uncertainty mathematically. Once we define a random variable, we can study its distribution, compute its expected value and variability, and use it to make probability-based statements about future or unseen outcomes.\nIn this chapter, we focus on cases in which the probability distribution of the random variable is completely known. These are idealized settings, such as games of chance, where probabilities are determined by the rules of the game rather than by data. Working with these simple, controlled examples allows us to understand the mathematical foundations of probability, expected values, standard errors, and sampling distributions.\nIn later chapters, we will turn to real-world data problems, where the underlying distributions are not known. In those cases, we use statistical inference to estimate or approximate these distributions from data. The probability concepts introduced here provide the theoretical foundation for those inferential methods.\nTo start, consider a simple discrete example. Suppose we draw one bead at random from an urn containing red and blue beads. Define the random variable:\n\\[\nX =\n\\begin{cases}\n1 & \\text{if the bead is blue},\\\\\n0 & \\text{if the bead is red.}\n\\end{cases}\n\\]\nIn R, we can simulate this process using the code we introduced in Section 5.3:\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2, 3))\nx &lt;- ifelse(sample(beads, 1) == \"blue\", 1, 0)\nEach time we draw a bead, the value of \\(X\\) may change because the outcome is random. A variable like this, which takes only the values 0 and 1, is called a Bernoulli random variable in statistical textbooks. Bernoulli trials are the building blocks for many statistical models, since many outcomes, such as success/failure, yes/no, or heads/tails, can be represented this way.\nA classic example is the number of heads, call it \\(S_n\\), observed when tossing \\(n\\) fair coins. We can generate one observation of this random variable using a simulation:\ns &lt;- sum(sample(c(0,1), n, replace = TRUE))\nWe will use this example in later sections.\nNot all random variables are discrete. Some can take on a continuum of values. For example, the height of a randomly selected person or the result of a physical measurement can be viewed as a continuous random variable. As an example, we simulate such a variable using the normal distribution:\nx &lt;- rnorm(10, mean = 70, sd = 3)\nHere each value of x represents one realization of a random variable drawn from a normal distribution with mean 70 and standard deviation 3. Repeating the simulation produces slightly different numbers each time, reflecting the inherent randomness of the process.\nThese examples, discrete Bernoulli variables and a continuous normal variable, illustrate the main types of random variables we will study in this chapter. Understanding their behavior and summarizing their distributions are the key steps that lead us toward the ideas of expected value, variability, and, ultimately, statistical inference.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob/random-variables.html#notation-for-random-variables",
    "href": "prob/random-variables.html#notation-for-random-variables",
    "title": "7  Random Variables",
    "section": "\n7.2 Notation for random variables",
    "text": "7.2 Notation for random variables\nIn statistical notation, uppercase letters denote random variables, while lowercase letters represent observed values. Sometimes both appear together, as in \\(X = x\\), where \\(X\\) is random and \\(x\\) is a fixed value. For example, \\(X\\) might represent the number shown on a die roll, and \\(x\\) an observed outcome, 1, 2, 3, 4, 5, or 6:\n\\[\n\\mathrm{Pr}(X = x) = 1/6 \\quad \\text{for } x = 1, \\dots, 6.\n\\]\nThis notation is convenient because it lets us express probability statements compactly. It may seem odd at first, since \\(X\\) represents an outcome not yet observed, we can discuss the likelihood of its possible values, but not its realized value. Once data are collected, we observe one realization of \\(X\\), and then reason about what could have occurred, given what did occur.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob/random-variables.html#the-probability-distribution-of-a-random-variable",
    "href": "prob/random-variables.html#the-probability-distribution-of-a-random-variable",
    "title": "7  Random Variables",
    "section": "\n7.3 The probability distribution of a random variable",
    "text": "7.3 The probability distribution of a random variable\nThe probability distribution of a random variable tells us how likely each possible outcome is. For example, if we toss \\(n\\) fair coins, we might ask for the probability of observing 40 or fewer heads. This is equivalent to asking for \\(\\mathrm{Pr}(S_n \\leq 40)\\).\nIf we can define a cumulative distribution function (CDF) \\(F(a) = \\mathrm{Pr}(S_n \\leq a)\\), we can answer any probability question involving \\(S_n\\), such as the probability of falling within an interval \\((a, b]\\). We call \\(F\\) the distribution function of the random variable.\nWe can approximate this distribution function by Monte Carlo simulation. For example, the following code simulates tossing 100 fair coins 100,000 times:\n\nn &lt;- 100\ns &lt;- replicate(100000, sum(sample(c(0,1), n, replace = TRUE)))\n\nWe can then estimate \\(\\mathrm{Pr}(S_n \\leq a)\\) by computing the proportion of simulated values less than or equal to a:\n\nmean(s &lt;= a)\n\nFor instance, the probability of observing 40 or fewer heads out of 100 is:\n\nmean(s &lt;= 40)\n#&gt; [1] 0.029\n\nWe can visualize the distribution of \\(S_n\\) with a histogram, overlaying the corresponding normal density curve for comparison:\n\n\n\n\n\n\n\n\nWe see that the distribution is approximately normal. A qqplot will confirm that the normal approximation provides an excellent fit. In the next chapter (Section 8.3), we will learn the theory that explains why this approximation works so well. In fact, this result extends far beyond coin flips, it applies broadly to averages and sums of many kinds of random variables.\nIf the distribution is normal, it is fully characterized by its average and standard deviation. These two quantities have special names in probability theory: the expected value and the standard error of the random variable. We discuss these next.\n\n\n\n\n\n\nStatistical theory lets us derive the exact distribution of random variables defined as sums of independent draws from an urn. In our coin tossing example, the number of heads (successes), \\(S_n\\), follows a binomial distribution. Thus, Monte Carlo simulations were used only for illustration.\nWe can compute probabilities directly with dbinom or pbinom. For example, to find \\(\\mathrm{Pr}(S_n \\leq 40)\\):\n\n\npbinom(40, size = n, prob = 1/2)\n#&gt; [1] 0.0284\n\nWe can also simnulate the number of heads using a rbinom instead of replicate and sample:\n\nrbinom(100000, n, 1/2)",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob/random-variables.html#sec-mean-var-eqs",
    "href": "prob/random-variables.html#sec-mean-var-eqs",
    "title": "7  Random Variables",
    "section": "\n7.4 The expected value and standard error",
    "text": "7.4 The expected value and standard error\nIn statistics textbooks, the expected value of a random variable \\(X\\) is commonly denoted as \\(\\mu_X\\) or \\(\\mathrm{E}[X]\\), both meaning “the expected value of \\(X\\).”\nIntuitively, the expected value represents the long-run average outcome if the random process were repeated many times. The more repetitions, the closer the average of the observed values will be to \\(\\mathrm{E}[X]\\).\nFor a discrete random variable with possible outcomes \\(x_1,\\dots,x_n\\), the expected value is defined as:\n\\[\n\\mathrm{E}[X] = \\sum_{i=1}^n x_i ,\\mathrm{Pr}(X = x_i)\n\\]\nFor a continuous random variable with probability density function \\(f(x)\\) and range \\((a, b)\\), the sum becomes an integral:\n\\[\n\\mathrm{E}[X] = \\int_a^b x f(x), dx\n\\]\nIf all \\(x_i\\) have equal probability \\(1/n\\), as when drawing uniformly from an urn, the expected value reduces to the arithmetic mean:\n\\[\n\\mathrm{E}[X] = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\]\nFor example, in a coin toss where \\(X = 1\\) for heads and \\(0\\) for tails,\n\\[\n\\mathrm{E}[X] = 1 \\times \\Pr(X=1) + 0 \\times \\Pr(X=0) = 1/2.\n\\]\nAlthough \\(X\\) only takes the values 0 or 1, its expected value of 0.5 represents the long-run proportion of heads if the experiment were repeated many times:\n\nB &lt;- 10^6\nx &lt;- sample(c(0, 1), B, replace = TRUE)\nmean(x)\n#&gt; [1] 0.5\n\nThe standard error (SE) describes the typical deviation of \\(X\\) from its expected value. It is defined as:\n\\[\n\\mathrm{SE}[X] = \\sqrt{\\sum_{i=1}^n (x_i - \\mathrm{E}[X])^2 ,\\mathrm{Pr}(X = x_i)}.\n\\]\nFor a continuous random variable with density \\(f(x)\\), this becomes:\n\\[\n\\mathrm{SE}[X] = \\sqrt{\\int_a^b (x - \\mathrm{E}[X])^2 f(x), dx}.\n\\]\n\n\n\n\n\n\nMany textbooks introduce the variance before the standard error. The variance is simply the square of the standard error:\n\\[\n\\mathrm{Var}[X] = \\mathrm{SE}[X]^2.\n\\]\nVariance is useful in derivations because it avoids square roots, but the standard error is easier to interpret, since it uses the same units as the data.\n\n\n\nWhen all outcomes \\(x_i\\) are equally likely, the standard error reduces to the standard deviation:\n\\[\n\\mathrm{SE}[X] = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\mathrm{E}[X])^2}, \\quad \\mathrm{E}[X] = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\]\nFor a coin toss:\n\\[\n\\mathrm{SE}[X] = \\sqrt{(1-0.5)^2 \\times 0.5 + (0-0.5)^2 \\times 0.5} = 0.5.\n\\]\nThus, one coin has an expected value of 0.5 and a standard error of 0.5—reasonable, since the possible outcomes are 0 or 1.\n\n\n\n\n\n\nWe use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error, respectively. This convention reflects their connection to the mean (\\(m\\)) and standard deviation (\\(s\\)). However, we often prefer \\(\\mathrm{E}[X]\\) and \\(\\mathrm{SE}[X]\\) notation because it generalizes more clearly to mathematical expressions involving sums or transformations of random variables.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob/random-variables.html#sec-key-properties",
    "href": "prob/random-variables.html#sec-key-properties",
    "title": "7  Random Variables",
    "section": "\n7.5 Key properties of expectations and standard errors",
    "text": "7.5 Key properties of expectations and standard errors\nThere are several mathematical properties of expectations and standard errors that we will use frequently when working with data.\n1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:\n\\[\n\\mathrm{E}[X_1+X_2+\\dots+X_n] =  \\mathrm{E}[X_1] + \\mathrm{E}[X_2]+\\dots+\\mathrm{E}[X_n]\n\\]\nIf \\(X\\) represents independent draws from the urn, then they all have the same expected value. Let’s denote the expected value with \\(\\mu_X\\) and rewrite the equation as:\n\\[\n\\mathrm{E}[X_1+X_2+\\dots+X_n]=  n\\mu_X\n\\]\nwhich is another way of writing the result we show above for the sum of draws.\n2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. If we shift the random variable, the expected value shifts by that amount. This is easier to explain with symbols:\n\\[\n\\mathrm{E}[aX+b] =  a\\times\\mathrm{E}[X] + b\n\\]\nTo understand why this is intuitive, consider changing units. If we change the units of a random variable, such as from dollars to cents, the expectation should change in the same way.\nA consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, denoted as \\(\\mu_X\\) again:\n\\[\n\\mathrm{E}[(X_1+X_2+\\dots+X_n) / n]=   \\mathrm{E}[X_1+X_2+\\dots+X_n] / n = n\\mu_X/n = \\mu_X\n\\]\n3. The variance of the sum of independent random variables is the sum of variances of each random variable:\n\\[\n\\mathrm{Var}[X_1+X_2+\\dots+X_n] =\\mathrm{Var}[X_1] + \\mathrm{Var}[X_2]+\\dots+\\mathrm{Var}[X_n]  \n\\] This implies that the following property for the standard error of the sum of independent random variables:\n\\[\n\\mathrm{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mathrm{SE}[X_1]^2 + \\mathrm{SE}[X_2]^2+\\dots+\\mathrm{SE}[X_n]^2  }\n\\]\nNote that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.\n4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation:\n\\[\n\\mathrm{SE}[aX] =  a \\times \\mathrm{SE}[X]\n\\]\nTo see why this is intuitive, again think of units.\nIf we shift the random variable by a constant the variability does not change. So it has not effect on the standard error:\n\\[\n\\mathrm{SE}[aX+b] =  a \\times \\mathrm{SE}[X]\n\\]\nA consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma_X\\):\n\\[\n\\begin{aligned}\n\\mathrm{SE}[\\bar{X}] = \\mathrm{SE}[(X_1+X_2+\\dots+X_n) / n] &=   \\mathrm{SE}[X_1+X_2+\\dots+X_n]/n \\\\\n&= \\sqrt{\\mathrm{SE}[X_1]^2+\\mathrm{SE}[X_2]^2+\\dots+\\mathrm{SE}[X_n]^2}/n \\\\\n&= \\sqrt{\\sigma_X^2+\\sigma_X^2+\\dots+\\sigma_X^2}/n\\\\\n&= \\sqrt{n\\sigma_X^2}/n\\\\\n&= \\sigma_X / \\sqrt{n}    \n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe assumption of independence is important\n\n\n\nThe given equation reveals crucial insights for practical scenarios. Specifically, it suggests that the standard error can be minimized by increasing the sample size, \\(n\\), and we can quantify this reduction. However, this principle holds true only when the variables \\(X_1, X_2, ... X_n\\) are independent. If they are not, the estimated standard error can be significantly off.\nIn Section 15.2, we introduce the concept of correlation, which quantifies the degree to which variables are interdependent. If the correlation coefficient among the \\(X\\) variables is \\(\\rho\\), the standard error of their average is:\n\\[\n\\mathrm{SE}\\left[\\bar{X}\\right] = \\sigma_X \\sqrt{\\frac{1 + (n-1) \\rho}{n}}\n\\]\nThe key observation here is that as \\(\\rho\\) approaches its upper limit of 1, the standard error increases. Notably, in the situation where \\(\\rho = 1\\), the standard error, \\(\\mathrm{SE}[\\bar{X}]\\), equals \\(\\sigma_X\\), and it becomes unaffected by the sample size \\(n\\).\n\n\n5. If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\).",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob/random-variables.html#law-of-large-numbers",
    "href": "prob/random-variables.html#law-of-large-numbers",
    "title": "7  Random Variables",
    "section": "\n7.6 Law of Large Numbers",
    "text": "7.6 Law of Large Numbers\nAn important implication of result 4 above is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.\n\n\n\n\n\n\nMisinterpretation of the law of averages\n\n\n\nThe law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. Yet these events are independent so the chance of a coin landing heads is 50%, regardless of the previous 5. The same principle applies to the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses. Another funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob/random-variables.html#data-distributions-versus-probability-distributions",
    "href": "prob/random-variables.html#data-distributions-versus-probability-distributions",
    "title": "7  Random Variables",
    "section": "\n7.7 Data distributions versus probability distributions",
    "text": "7.7 Data distributions versus probability distributions\nBefore moving on, it is important to distinguish between the distribution of a dataset and a probability distribution.\nAny list of numbers \\(x_1, \\dots, x_n\\), or any dataset, has a distribution that describes how the observed values are spread out. We can summarize it with simple statistics such as the mean and standard deviation:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n\nA probability distribution, on the other hand, is a theoretical construct that describes the possible values of a random variable and their likelihoods. It does not depend on data.\nWhen \\(X\\) represents drawing a number at random from an urn, the list of numbers in the urn defines the possible outcomes, and their relative frequencies define the probability distribution of \\(X\\). The average and standard deviation of the numbers in the urn correspond to the expected value and standard error of the random variable.\nThis connection can cause confusion: every list of numbers has a standard deviation, and every random variable has a standard error, but the standard error of a random variable is the standard deviation of its probability distribution, not of a particular dataset.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob/random-variables.html#exercises",
    "href": "prob/random-variables.html#exercises",
    "title": "7  Random Variables",
    "section": "\n7.8 Exercises",
    "text": "7.8 Exercises\n1. In American Roulette, you can bet on black or red. There are 18 reds, 18 blacks, and 2 greens (0 and 00). What are the chances that black comes out?\n2. The payout for winning on black is $1 dollar. This means that if you bet a dollar and it lands on black, you get $1, otherwise you lose $1. Create a sampling model using sample to simulate the random variable \\(X\\) for your winnings.\n3. Compute the expected value of \\(X\\).\n4. Compute the standard error of \\(X\\).\n5. Now create a random variable \\(S_n\\) that is the sum of your winnings after betting on black n = 100 times. Simulate 100,000 outcomes of \\(S_n\\). Start your code by setting the seed to 1 with set.seed(1).\n6. What is the expected value of \\(S_n\\)?\n7. What is the standard error of \\(S_n\\)?\n8. What is the probability that you end up winning money?\n10. In American Roulette, you can also bet on green. There are 18 reds, 18 blacks, and 2 greens (0 and 00). What are the chances the green comes out?\n11. The payout for winning on green is $17 dollars. This means that if you bet a dollar and it lands on green, you get $17, otherwise you lose $1. Create a sampling model using sample to simulate the random variable \\(X\\) for your winnings.\n12. Compute the expected value of \\(X\\).\n13. Compute the standard error of \\(X\\).\n14. Now create a random variable \\(S_n\\) that is the sum of your winnings after betting on green n = 100 times. Simulate 100,000 outcomes of \\(S_n\\). Start your code by setting the seed to 1 with set.seed(1).\n15. What is the expected value of \\(S_n\\)?\n16. What is the standard error of \\(S_n\\)?\n17. What is the probability that you end up winning money?",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob/sampling-models-and-clt.html",
    "href": "prob/sampling-models-and-clt.html",
    "title": "\n8  Sampling Models and the Central Limit Theorem\n",
    "section": "",
    "text": "8.1 Sampling models\nIn the previous chapter, we introduced random variables and saw how their distributions, expected values, and standard errors describe uncertainty. In this chapter, we build on those ideas by introducing sampling models, which are mathematical abstractions of how data are generated. We then use the Central Limit Theorem (CLT) to approximate the distribution of sums and averages of independent draws. Through a concrete example based on playing roulette in a casino, we show how the CLT can be used to answer practical questions such as “What is the chance of losing money?”\nMany data-generating processes can be effectively modeled as draws from an urn. For example, polling likely voters can be viewed as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the preferences of the entire population. In epidemiology, we often assume that individuals in a study are random draws from a larger population, where each draw corresponds to an individual’s outcome. Similarly, in experimental research, we treat subjects (worms, flies, mice, or humans, for exmaple) as random draws from a broader population. Randomized experiments follow the same logic: each participant’s assignment to treatment or control can be seen as a random draw from an urn.\nBecause random sampling and random assignment are fundamental to data analysis, sampling models appear everywhere in statistics. To build intuition, we start with games of chance, where randomness is explicit and the link between probability and real outcomes is perfectly clear. We now define a specific example.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Models and the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob/sampling-models-and-clt.html#sec-sampling-models",
    "href": "prob/sampling-models-and-clt.html#sec-sampling-models",
    "title": "\n8  Sampling Models and the Central Limit Theorem\n",
    "section": "",
    "text": "Roullete example\nSuppose a small casino hires you to decide whether it should install roulette wheels. To keep things simple, imagine 1,000 people each place a single bet on black. The casino wants you to estimate how much money it can expect to make, or lose, and, most importantly, the probability of losing money. If that probability is too high, they’ll skip installing the wheels.\nWe define a random variable \\(S_n\\) to represent the casino’s total winnings after \\(n\\) games. A roulette wheel has 18 red pockets, 18 black pockets, and 2 green ones. The outcome is equivalent to drawing a ball from this urn:\n\ncolor &lt;- rep(c(\"Black\", \"Red\", \"Green\"), times = c(18, 18, 2))\n\nEach of the 1,000 outcomes is an independent draw from this urn. If black comes up, the gambler wins and the casino loses $1 (recorded as −1). Otherwise, the casino wins $1.\n\nn &lt;- 1000\nx &lt;- sample(ifelse(color == \"Black\", -1, 1), n, replace = TRUE)\n\nSince we already know the proportions of 1s and −1s, we can simplify:\n\nx &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))\n\nWe call this a sampling model because it represents random behavior through repeated sampling from an urn. The casino’s total winnings, define it as \\(S_n\\), are simply the sum of these independent draws:\n\ns &lt;- sum(x)\n\nBefore deriving the distribution of total winnings \\(S_n\\), it’s worth pausing to ask why we study problems like roulette in the first place.\nConnecting sampling models to real data\nThinking in terms of a sampling model is central to data analysis under uncertainty. It connects the random variables we study in probability to the datasets we analyze in practice. By imagining data as random draws, from an urn, a population, or a randomization procedure, we clarify what our models represent and what assumptions they rely on.\nThis perspective also underpins much of modern statistics. Tools like confidence intervals, hypothesis tests, and regression all depend on assumptions derived from sampling models. Understanding these assumptions helps us apply these methods thoughtfully and recognize their limits.\nCasino games make these ideas tangible because their sampling models are fully known. The probabilities, such as the number of red, black, and green pockets, are explicit and easy to simulate. In contrast, real-world data come from more complex “urns”, such as populations of individuals with varying heights, incomes, or health outcomes. When we collect data, we are effectively drawing a random sample from such a population.\nThis way of thinking builds statistical intuition: every dataset represents a sample from some underlying process, and the validity of our conclusions depends on how well our assumptions about that process hold.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Models and the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob/sampling-models-and-clt.html#a-monte-carlo-simulation",
    "href": "prob/sampling-models-and-clt.html#a-monte-carlo-simulation",
    "title": "\n8  Sampling Models and the Central Limit Theorem\n",
    "section": "\n8.2 A Monte Carlo Simulation",
    "text": "8.2 A Monte Carlo Simulation\nTo find the probability that the casino loses money, we need to estimate the distribution of \\(S_n\\). In the language of probability, this is \\(\\Pr(S_n &lt; 0)\\). If we define the distribution function \\(F(a) = \\Pr(S_n \\leq a)\\), we can answer any question about probabilities involving \\(S_n\\).\nWe can estimate \\(F(a)\\) with a Monte Carlo simulation, generating many realizations of \\(S_n\\). The following code simulates 1,000 people playing roulette and repeats the process 100,000 times:\n\nn &lt;- 1000\nroulette_winnings &lt;- function(n){\n  x &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))\n  sum(x)\n}\ns &lt;- replicate(100000, roulette_winnings(n))\n\nThe fraction of simulations in which \\(S_n \\leq a\\) approximates \\(F(a)\\):\n\nmean(s &lt;= a)\n\nFor instance, the probability of losing money is:\n\nmean(s &lt; 0)\n#&gt; [1] 0.0458\n\nA histogram and qqplot of \\(S_n\\) shows that the distribution is approximately normal, an observation that the Central Limit Theorem will soon explain.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Models and the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob/sampling-models-and-clt.html#sec-clt-prob",
    "href": "prob/sampling-models-and-clt.html#sec-clt-prob",
    "title": "\n8  Sampling Models and the Central Limit Theorem\n",
    "section": "\n8.3 The Central Limit Theorem",
    "text": "8.3 The Central Limit Theorem\nThe Central Limit Theorem (CLT) states that when the number of draws (the sample size) is large, the distribution of the sum of independent draws is approximately normal. Because sampling models describe so many data-generating processes, the CLT is one of the most important results in statistics.\nIn the roulette example, the urn contains 20 $1 gains and 18 $1 losses, giving an expected value of:\n\\[\n\\mathrm{E}[X] = (20 -18)/38 = 1/19 \\approx 0.05.\n\\]\nIn general, for two possible outcomes \\(a\\) and \\(b\\) with probabilities \\(p\\) and \\((1-p)\\),\n\\[\n\\mathrm{E}[X] = ap + b(1-p).\n\\]\nBy the properties introduced in Section 7.5, the expected total winnings are:\n\\[\n\\mathrm{E}[S_n] = n\\,\\mathrm{E}[X].\n\\]\nSo if 1,000 people play, the casino expects to win about $50 on average.\nTo quantify uncertainty, we compute the standard error, which measures how much \\(S_n\\) varies around its expected value. For an urn with two outcomes \\(a\\) and \\(b\\), the standard deviation can be shown to be:\n\\[\n\\mathrm{SE}[X] = |b - a|\\sqrt{p(1-p)}.\n\\]\nIn the roulette example we obtain:\n\n2*sqrt(90)/19\n#&gt; [1] 0.999\n\nThus, the standard error of the sum is:\n\\[\n\\mathrm{SE}[S_n] = \\sqrt{n}\\,\\mathrm{SD}[X],\n\\]\nwhich in this case is:\n\nsqrt(n)*2*sqrt(90)/19\n#&gt; [1] 31.6\n\nThese theoretical values agree closely with the Monte Carlo results:\n\nmean(s)\n#&gt; [1] 52.5\nsd(s)\n#&gt; [1] 31.5\n\nUsing the CLT, we can now compute the probability that the casino loses money without simulation:\n\npnorm(0, n*(20 - 18)/38, sqrt(n)*2*sqrt(90)/19)\n#&gt; [1] 0.0478\n\nwhich matches our earlier estimate:\n\nmean(s &lt; 0)\n#&gt; [1] 0.0458\n\n\n\n\n\n\n\nThe quantity \\((S_n + n)/2\\) represents the number of wins for the casino and follows a binomial distribution. Thus, the probability \\(\\Pr(S_n &lt; 0)\\) can also be computed directly:\n\npbinom(n/2 - 1, size = n, prob = 10/19)\n#&gt; [1] 0.0448\n\nHowever, the binomial result applies only to Bernoulli trials. In many real-world settings, data are not binomial, yet we still want to approximate the distribution of sums or averages. For those cases, the CLT provides a general framework that extends far beyond Bernoulli trials.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Models and the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob/sampling-models-and-clt.html#how-large-is-large",
    "href": "prob/sampling-models-and-clt.html#how-large-is-large",
    "title": "\n8  Sampling Models and the Central Limit Theorem\n",
    "section": "\n8.4 How large is large?",
    "text": "8.4 How large is large?\nThe CLT applies when the number of draws is “large”, but large depends on context. In many cases, 30 draws are enough for a good approximation, sometimes even fewer. In the roullette example, when the probabilities of success is very small, much larger samples may be required.\nFor example, in a lottery where the chance of winning is less than one in a million, even millions of tickets result in only a few winners. In such highly skewed cases, the Poisson distribution offers a better approximation than the normal. While we do not cover its theory here, it is discussed in most probability textbooks and on Wikipedia. You can explore the Poisson distribution in R with dpois, ppois, and rpois.\nIn summary, the CLT explains why the normal distribution appears so often in statistics and why many methods work well in practice. But it has limits: when probabilities are extremely small or distributions are highly skewed, other models, such as the Poisson or binomial, are more appropriate. Developing intuition for when the CLT applies is essential for principled, thoughtful data analysis.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Models and the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob/sampling-models-and-clt.html#exercises",
    "href": "prob/sampling-models-and-clt.html#exercises",
    "title": "\n8  Sampling Models and the Central Limit Theorem\n",
    "section": "\n8.5 Exercises",
    "text": "8.5 Exercises\n1. In American Roulette, you can also bet on green. There are 18 reds, 18 blacks, and 2 greens (0 and 00). Create a random variable \\(S_n\\) that is the sum of your winnings after betting on green 1,000 times.\n2. Compute the expected value of \\(S_n\\)?\n3. Compute the standard error of \\(S_n\\)?\n4. Use the CLT to estimate the probability that you end up winning money? Hint: Use the CLT.\n5. Create a Monte Carlo simulation that generates 10,000 outcomes of \\(S_n\\). Compute the average and standard deviation of the resulting list to confirm the results of 2 and 3. Start your code by setting the seed to 1 with set.seed(1).\n6. Now check your answer to 4 using the Monte Carlo result.\n7. The Monte Carlo result and the CLT approximation are close, but not that close. What could account for this?\n\n10,000 simulations is not enough. If we do more, they match.\nThe CLT does not work as well when the probability of success is small. In this case, it was 1/19. If we make the number of roulette plays bigger, they will match better.\nThe difference is within rounding error.\nThe CLT only works for averages.\n\nNow create a random variable \\(\\bar{X}_n\\) that is your average winnings per bet defined as \\(X_n = S_n/n\\). Keep \\(n\\) = 1,000.\n8. What is the expected value of \\(\\bar{X}_n\\)?\n9. What is the standard error of \\(\\bar{X}_n\\)?\n10. What is the probability that you end up with winnings per game that are positive? Hint: Use the CLT.\n11. Create a Monte Carlo simulation that generates 25,000 outcomes of \\(\\bar{X}_n\\), instead of \\(1,000\\). Compute the average and standard deviation of the resulting list to confirm the results of 13 and 14. Start your code by setting the seed to 1 with set.seed(1).\n12. Now compare your answer to 14 using the Monte Carlo result. What can you say about the CLT approximation for \\(\\mathrm{Pr}(\\bar{X}_n&gt;0)\\) compared for \\(\\mathrm{Pr}(S_n&gt;0)\\).\n\nWe are now computing averages instead of sums so they are very different.\n25,000 Monte Carlo simulations is not better than 10,000 and provides a much closer estimate.\nThe CLT works better when the sample size is larger. We increased from 10,000 to 25,000.\nThe difference is within rounding error.\n\nThe following exercises are inspired by the events surrounding the financial crisis of 2007-20081. This financial crisis was in part caused by underestimating the risk of certain securities2 sold by financial institutions. Specifically, the risks of mortgage-backed securities (MBS) and collateralized debt obligations (CDO) were grossly underestimated. These assets were sold at prices that assumed most homeowners would make their monthly payments, and the probability of this not occurring was calculated as being low. A combination of factors resulted in many more defaults than were expected, which led to a price crash of these securities. As a consequence, banks lost so much money that they required government bailouts to avoid complete closure.\n13. More complex versions of the sampling models we have discussed are also used by banks to determine interest rates and insurance companies to determine premiums. To understand this, suppose you run a small bank that has a history of identifying potential homeowners that can be trusted to make payments. In fact, historically, only 2% of your customers default in a given year, meaning that they don’t pay back the money that you lent them. Suppose your bank will give out \\(n\\) = 1,000 loans for $180,000 this year. Also, after adding up all costs, suppose your bank loses \\(l\\)= $200,000 per foreclosure. For simplicity, we assume this includes all operational costs. What is the expected profit \\(S_n\\) for you bank under this scenario?\n14. Note that the total loss defined by the final sum in the previous exercise is a random variable. Every time you run the sampling model code, you obtain a different number of people defaulting which results in a different loss. Code a sampling model for the random variable representing your banks profit \\(S_n\\) under scenario described in 13.\n15. The previous exercise demonstrates that if you simply loan money to everybody without interest, you will end up losing money due to the 2% that defaults. Although you know 2% of your clients will probably default, you don’t know which ones, so you can’t remove them. Yet by charging everybody just a bit extra in interest, you can make up the losses incurred due to that 2%, and also cover your operating costs. What quantity \\(x\\) would you have to charge each borrower so that your bank’s expected profit is 0? Assume that you don’t get \\(x\\) from the borrowers that default. Also, note \\(x\\) is not the interest rate, but the total you add meaning \\(x/180000\\) is the interest rate.\n16. Rewrite the sample model from exercise 14 to account for the interest you are charging and run a Monte Carlo simulation to get an idea of the distribution of your profit when you charge interest rates.\n17. We don’t actually need a Monte Carlo simulation. Based on what we have learned, the CLT informs us that, since our losses are a sum of independent draws, its distribution is approximately normal. What are the expected value and standard errors of the profit \\(S_n\\)? Write these as functions of the probability of foreclosure \\(p\\), the number of loans \\(n\\), the loss per foreclosure \\(l\\), and the quantity you charge each borrower \\(x\\).\n18. If you set \\(x\\) to assure your bank breaks even (expected profit is 0), what is the probability that your bank loses money?\n19. Suppose that if your bank has negative profit, it has to close. Therefore, you need to increase \\(x\\) to minimize this risk. However, setting the interest rates too high may lead your clients to choose another bank. So, let’s say that we want our chances of losing money to be 1 in 100. What does the \\(x\\) quantity need to be now? Hint: We want \\(\\mathrm{Pr}(S_n&lt;0) = 0.01\\). Note that you can add subtract constants to both side of an inequality, and the probability does not change: \\(\\mathrm{Pr}(S_n&lt;0) = \\mathrm{Pr}(S_n+k&lt;0+k)\\), Similarly, with division of positive constants: \\(\\mathrm{Pr}(S_n+k&lt;0+k) = \\mathrm{Pr}((S_n+k)/m &lt;k/m)\\). Use this fact and the CLT to transform the left side of the inequality in \\(\\mathrm{Pr}(S_n&lt;0)\\) into a standard normal.\n20. Our interest rate now increases. But it is still a very competitive interest rate. For the \\(x\\) you obtained in exercise 20, what is expected profit per loan and the expected total profit?\n21. Run run a Monte Carlo simulation to double check the theoretical approximation used in 19 and 20.\n22. One of your employees points out that, since the bank is making a profit per loan, the bank should give out more loans! Why limit it to just \\(n\\)? You explain that finding those \\(n\\) clients was work. You need a group that is predictable and that keeps the chances of defaults low. The employee then points out that even if the probability of default is higher, as long as our expected value is positive, you can minimize your chances of losses by increasing \\(n\\) and relying on the law of large numbers. Suppose the default probability is twice as high, or 4%, and you set the interest rate to 5%, or \\(x\\) = $9,000, what is your expected profit per loan?\n23. How much do we have to increase \\(n\\) by to assure the probability of losing money is still less than 0.01?\n24. Confirm the result in exercise 23 with a Monte Carlo simulation.\n25. According to this equation, giving out more loans increases your expected profit and lowers the chances of losing money! Giving out more loans seems like a no-brainier. As a result, your colleague decides to leave your bank and start his own high-risk mortgage company. A few months later, your colleague’s bank has gone bankrupt. A book is written, and eventually, the movies “The Big Short” and “Margin Call” are made, recounting the mistake your friend, and many others, made. What happened?\nYour colleague’s scheme was mainly based on this mathematical formula \\(\\mathrm{SE}[\\bar{X}]= \\sigma_X / \\sqrt{n}\\). By making \\(n\\) large, we minimize the standard error of our per-loan profit. However, for this rule to hold, the \\(X\\)s must be independent draws: one person defaulting must be independent of others defaulting.\nTo construct a more realistic simulation than the original one your colleague ran, let’s assume there is a global event affecting everybody with high-risk mortgages and altering their probability simultaneously. We will assume that with a 50-50 chance all the default probabilities slightly increase or decrease to somewhere between 0.03 and 0.05. However, this change occurs universally, impacting everybody at once, not just one person. As these draws are no longer independent, our equation for the standard error of the sum of random variables does not apply. Write a Monte Carlo simulation for your total profit with this model.\n26. Use the simulation results of exercise 25 to report the expected profit, the probability of losing money, and the probability of losing more than $10,000,000. Study the distribution of profit and discuss how making the wrong assumption lead to a catastrophic result.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Models and the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob/sampling-models-and-clt.html#footnotes",
    "href": "prob/sampling-models-and-clt.html#footnotes",
    "title": "\n8  Sampling Models and the Central Limit Theorem\n",
    "section": "",
    "text": "https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008↩︎\nhttps://en.wikipedia.org/w/index.php?title=Security_(finance)↩︎",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sampling Models and the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob/reading-prob.html",
    "href": "prob/reading-prob.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "Fisher, R. A. (1935). The Design of Experiments. Edinburgh: Oliver & Boyd.\nA foundational text that introduced the principles of randomization and experimental control, forming the basis of modern statistical design.\nMontgomery, D. C. (2017). Design and Analysis of Experiments (9th ed.). Wiley.\nA comprehensive and accessible modern treatment of experimental design, emphasizing applications and interpretation.\nDavid Freedman, Robert Pisani & Roger Purves, Statistics.\nThis book emphasizes clear reasoning, real examples, and conceptual insight rather than formula memorization.\nA First Course in Probability by Sheldon Ross - a widely used undergraduate text that builds a solid foundation through clear explanations and a broad range of exercises.\nIntroduction to Probability by Joseph K. Blitzstein and Jessica Hwang - an engaging, example-driven introduction that emphasizes problem solving and intuition.\nAn Introduction to Probability Theory and Its Applications, Volume 1 by William Feller - a rigorous classic that provides a deeper and more formal treatment of the subject.",
    "crumbs": [
      "Probability",
      "Recommended Reading"
    ]
  },
  {
    "objectID": "inference/intro-inference.html",
    "href": "inference/intro-inference.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Statistical Inference is the branch of statistics dedicated to distinguishing patterns arising from signal versus those arising from chance. It is a broad topic and, in this section, we review the basics using polls as a motivating example. To illustrate the concepts, we supplement mathematical formulas with Monte Carlo simulations and R code. We demonstrate the practical value of these concepts by using election forecasting as a case study. Readers already familiar with statisical inference theory and interested in the case study you can focus on 12  Hierarchical Models.\nThe day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. Political commentator Joe Scarborough said during his show1:\nTo which Nate Silver responded via Twitter:\nObama won the election.\nIn 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. In contrast, many other forecasters were almost certain she would win. She lost. But 71% is still more than 50%, so was Mr. Silver wrong? And what does probability mean in this context anyway?\nIn this part of the book, we show how the probability concepts from the previous part can be applied to develop statistical methods that make polls effective tools for understanding public opinion. Although in the United States the popular vote does not determine the presidential outcome, we use it as a simple and illustrative example to introduce the core ideas of statistical inference.\nWe begin by learning how to define estimates and margins of error for the popular vote and how these lead naturally to confidence intervals. We then extend this framework by aggregating data from multiple pollsters to examine the limitations of traditional models and explore improvements. To interpret probabilistic statements about the likelihood of a candidate winning, we introduce Bayesian modeling, and finally, we combine these ideas through hierarchical models to build a simplified version of the FiveThirtyEight election model and apply it to the 2016 election.\nWe conclude with two widely taught topics that, while not required for the case study, are central to statistical practice: statistical power and p-values. The part ends with a brief introduction to the bootstrap, an inferential method we will revisit in the machine learning part.",
    "crumbs": [
      "Statistical Inference"
    ]
  },
  {
    "objectID": "inference/intro-inference.html#footnotes",
    "href": "inference/intro-inference.html#footnotes",
    "title": "Statistical Inference",
    "section": "",
    "text": "https://www.youtube.com/watch?v=TbKkjm-gheY↩︎",
    "crumbs": [
      "Statistical Inference"
    ]
  },
  {
    "objectID": "inference/estimates-confidence-intervals.html",
    "href": "inference/estimates-confidence-intervals.html",
    "title": "9  Estimates and Confidence Intervals",
    "section": "",
    "text": "9.1 The sampling model for polls\nOpinion polling has been conducted since the 19th century. The general aim is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have become especially common in the United States during presidential elections. Polls are useful when interviewing every member of a population is logistically impossible. The general strategy involves interviewing a smaller, randomly chosen group and then inferring the opinions of the entire population from those of this subset.\nStatistical theory, known as inference, provides the framework that justifies this process. In this part of the book, we describe the foundations of statistical inference and show how they are applied in opinion polling. In particular, we explain how pollsters use confidence intervals and the margin of error to quantify the uncertainty in their estimates and to report results that reflect the limits of what the data can reveal.\nPerhaps the best-known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists extensively use such polls to decide where to allocate resources, such as determining which regions to target in their efforts to mobilize voters.\nElections are especially interesting examples of opinion polls because the true opinions of the entire population are revealed on election day. Of course, it costs millions of dollars to conduct a real election, which makes polling a cost-effective alternative for those seeking to forecast results. News organizations are also deeply invested in these forecasts, given the public’s appetite for early insights into election outcomes.\nWe start by connecting probability theory to the task of using polls to learn about a population.\nAlthough typically the results of polls run by political candidates are kept private, polls are also conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at these public datasets.\nReal Clear Politics1 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results, reporting estimates of the popular vote for the 2016 presidential election2:\nPoll\nDate\nSample\nMoE\nClinton\nTrump\nSpread\n\n\n\nRCP Average\n10/31 - 11/7\n--\n--\n47.2\n44.3\nClinton +2.9\n\n\nBloomberg\n11/4 - 11/6\n799 LV\n3.5\n46.0\n43.0\nClinton +3\n\n\nEconomist\n11/4 - 11/7\n3669 LV\n--\n49.0\n45.0\nClinton +4\n\n\nIBD\n11/3 - 11/6\n1026 LV\n3.1\n43.0\n42.0\nClinton +1\n\n\nABC\n11/3 - 11/6\n2220 LV\n2.5\n49.0\n46.0\nClinton +3\n\n\nFOX News\n11/3 - 11/6\n1295 LV\n2.5\n48.0\n44.0\nClinton +4\n\n\nMonmouth\n11/3 - 11/6\n748 LV\n3.6\n50.0\n44.0\nClinton +6\n\n\nCBS News\n11/2 - 11/6\n1426 LV\n3.0\n47.0\n43.0\nClinton +4\n\n\nLA Times\n10/31 - 11/6\n2935 LV\n4.5\n43.0\n48.0\nTrump +5\n\n\nNBC News\n11/3 - 11/5\n1282 LV\n2.7\n48.0\n43.0\nClinton +5\n\n\nNBC News\n10/31 - 11/6\n30145 LV\n1.0\n51.0\n44.0\nClinton +7\n\n\nMcClatchy\n11/1 - 11/3\n940 LV\n3.2\n46.0\n44.0\nClinton +2\n\n\nReuters\n10/31 - 11/4\n2244 LV\n2.2\n44.0\n40.0\nClinton +4\n\n\nGravisGravis\n10/31 - 10/31\n5360 RV\n1.3\n50.0\n50.0\nTie\nLet’s make some observations about the table above. First, observe that different polls, all conducted days before the election, report different spreads: the estimated difference between support for the two candidates. Notice that the reported spreads hover around what eventually became the actual result: Clinton won the popular vote by 2.1%. Additionally, we see a column titled MoE which stands for margin of error. We will learn what this means.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimates and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "inference/estimates-confidence-intervals.html#sec-poll-sampling-models",
    "href": "inference/estimates-confidence-intervals.html#sec-poll-sampling-models",
    "title": "9  Estimates and Confidence Intervals",
    "section": "",
    "text": "A Motivating Example: The Polling Competition\nTo help us understand the connection between polls and what we have learned, let’s construct a situation similar to what pollsters face. To simulate the challenge pollsters encounter in terms of competing with other pollsters for media attention, we will use an urn filled with beads to represent voters, and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar):\n\n\n\n\n\n\n\n\nBefore making a prediction, you can take a sample (with replacement) from the urn. To reflect the fact that running polls is expensive, it costs you $0.10 for each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you would have paid $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you receive half what you paid and proceed to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ntake_poll(25)\n\n\n\n\n\n\n\nThink about how you would construct your interval based on the data shown above.\nWe have just described a simple sampling model for opinion polls. In this model, the beads inside the urn represent individuals who will vote on election day. The red beads represent those voting for the Republican candidate, while the blue beads represent the Democrats. For simplicity, let’s assume there are no other colors; that is, that there are just two parties: Republican and Democratic.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimates and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "inference/estimates-confidence-intervals.html#populations-samples-parameters-and-estimates",
    "href": "inference/estimates-confidence-intervals.html#populations-samples-parameters-and-estimates",
    "title": "9  Estimates and Confidence Intervals",
    "section": "\n9.2 Populations, samples, parameters, and estimates",
    "text": "9.2 Populations, samples, parameters, and estimates\nWe want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The goal of statistical inference is to predict the parameter \\(p\\) based on the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it seems unlikely that \\(p\\) &gt; .9 or \\(p\\) &lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?\nWe want to construct an estimate of \\(p\\) using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.\n\n\n\n\n\n\n\n\nObserve that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can improve it.\nThe sample average as an estimate\nConducting an opinion poll is being modeled as taking a random sample from an urn. We propose using the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can easily report an estimate for the spread \\(2p-1\\). However, for simplicity, we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to justify our use of the sample proportion and to quantify its proximity to the population proportion \\(p\\).\nWe start by defining the random variable \\(X\\) as \\(X=1\\), if we pick a blue bead at random, and \\(X=0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads, and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In statistics textbooks, a bar on top of a symbol typically denotes the average.\nThe theory we just covered about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\\[\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i\n\\]\nFor simplicity, let’s assume that the draws are independent; after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nHere, we encounter an important difference compared to what we did in the probability part of the book: we don’t know the composition of the urn. While we know there are blue and red beads, we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\).\nParameters\nJust as we use variables to define unknowns in systems of equations, in statistical inference, we define parameters to represent unknown parts of our models. In the urn model, which we are using to simulate an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. Since our main goal is determining \\(p\\), we are going to estimate this parameter.\nThe concepts presented here on how we estimate parameters, and provide insights into how good these estimates are, extend to many data analysis tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group, investigate the health effects of smoking on a population, analyze the differences in racial groups of fatal shootings by police, or assess the rate of change in life expectancy in the US during the last 10 years. All these questions can be framed as a task of estimating a parameter from a sample.\n\n\n\n\n\n\nIntroductory statistics textbooks typically begin by introducing the population average as the first example of a parameter. In our case, the parameter of interest \\(p\\) is defined as the proportion of 1s (blue) in the urn. Notice that this proportion is also equal to the average of all the numbers in the urn, since the 1s and 0s can be treated as numeric values.\nThis means that our parameter \\(p\\) can be interpreted as a population average. For this reason, we will use \\(\\bar{X}\\) to denote its estimate, the average computed from a sample of draws from the urn. Although many textbooks use the notation \\(\\hat{p}\\) for this estimate, the symbol \\(\\bar{X}\\) better emphasizes the connection between sample averages and population means, a concept that extends naturally to situations beyond binary data.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimates and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "inference/estimates-confidence-intervals.html#estimate-properties-expected-value-and-standard-error",
    "href": "inference/estimates-confidence-intervals.html#estimate-properties-expected-value-and-standard-error",
    "title": "9  Estimates and Confidence Intervals",
    "section": "\n9.3 Estimate properties: expected value and standard error",
    "text": "9.3 Estimate properties: expected value and standard error\nTo understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws divided by a non-random constant so the rules we covered Section 7.4 apply.\nApplying the concepts we have learned we can show that:\n\\[\n\\mathrm{E}[\\bar{X}] = p\n\\]\nWe can also use what we learned to determine the standard error: \\[\n\\mathrm{SE}[\\bar{X}] = \\sqrt{p(1-p)/N}\n\\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\), and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough sample, our estimate converges to \\(p\\).\nIf we take a large enough sample to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t actually compute the standard error. However, for illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):\n\n\n\n\n\n\n\n\nThe plot shows that we would need a poll of over 10,000 people to achieve a standard error that low. We rarely see polls of this size due in part to the associated costs. According to the Real Clear Politics table, sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is:\n\nsqrt(p*(1 - p))/sqrt(1000)\n#&gt; [1] 0.0158\n\nor 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get to \\(p\\) and we do that in Section 9.4.1.\n\n\n\n\n\n\nPolling versus forecasting\n\n\n\nBefore we continue, it’s important to clarify a practical issue related to forecasting an election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment, and not for election day. The \\(p\\) for election night might be different, as people’s opinions tend to fluctuate through time. Generally, the polls conducted the night before the election tend to be the most accurate, since opinions do not change significantly in a day. However, forecasters try to develop tools that model how opinions vary over time and aim to predict the election night results by taking into consideration these fluctuations. We will explore some approaches for doing this in Chapter 12.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimates and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "inference/estimates-confidence-intervals.html#confidence-intervals",
    "href": "inference/estimates-confidence-intervals.html#confidence-intervals",
    "title": "9  Estimates and Confidence Intervals",
    "section": "\n9.4 Confidence Intervals",
    "text": "9.4 Confidence Intervals\nPollsters summarize uncertainty using a single, easy-to-interpret number called the margin of error. The margin of error, together with the estimate of the parameter, defines an interval that they are confident contains the true value. But what does confident actually mean?\nTo connect this idea to a familiar challenge, recall the competition described in Section 9.1, where you were asked to report an interval for the true proportion \\(p\\). If your interval included the actual \\(p\\), you recovered half the cost of your poll and advanced to the next round. A guaranteed way to move on would be to report a very wide interval, say \\([0,1]\\), which is certain to contain \\(p\\). But such an interval is useless because it conveys no information, and you would lose to someone submitting a narrower one. Similarly, an election forecaster who predicts that the spread will be between −100% and 100% would not be taken seriously. Even an interval between −10% and 10% would be too vague to be meaningful.\nOn the other hand, a very narrow interval is risky. A pollster who reports extremely tight intervals but frequently misses the true value will quickly lose credibility. The goal is to find a balance: intervals that are narrow enough to be informative but wide enough to be reliable.\nStatistical theory provides a way to quantify precisely what we mean by confidence, using the probability framework developed earlier. Specifically, we can construct intervals for which we can compute the probability of containing the true parameter \\(p\\). When a pollster reports an estimate along with a margin of error, they are effectively reporting an interval with a 95% probability of containing the true parameter, known as a 95% confidence interval.\nThe Central Limit Theorem\nIn Chapter 9, we introduced the sample average \\(\\bar{X}\\) as an estimate of the parameter \\(p\\), and we showed how to compute its standard error. However, to calculate probabilities, we need the distribution of \\(\\bar{X}\\).\nIn the Probability part of the book (Section 8.3), we learned that when the sample size is large, the average of independent draws from a population is approximately normally distributed, regardless of the shape of the population. This is the Central Limit Theorem (CLT). Because \\(\\bar{X}\\) is the average of independent draws, the CLT applies directly here, and it is the main tool used to construct confidence intervals.\nLet’s use it to answer a concrete question:\n\nHow likely is it that our sample estimate \\(\\bar{X}\\) is within 2% of the true population proportion \\(p\\)?\n\nWe can express this as:\n\\[\n\\Pr(|\\bar{X} - p| \\leq 0.02)\n= \\Pr(\\bar{X} \\leq p + 0.02) - \\Pr(\\bar{X} \\leq p - 0.02).\n\\]\nIf we standardize \\(\\bar{X}\\), subtracting its expected value and dividing by its standard error, we obtain a standard normal random variable \\(Z\\):\n\\[\nZ = \\frac{\\bar{X} - \\mathrm{E}[\\bar{X}]}{\\mathrm{SE}[\\bar{X}]}.\n\\]\nSince \\(\\mathrm{E}[\\bar{X}] = p\\) and \\(\\mathrm{SE}[\\bar{X}] = \\sqrt{p(1-p)/N}\\), the probability above becomes:\n\\[\n\\Pr!\\left(Z \\leq \\frac{0.02}{\\sqrt{p(1-p)/N}}\\right) -\n\\Pr!\\left(Z \\leq -\\frac{0.02}{\\sqrt{p(1-p)/N}}\\right).\n\\]\nTo compute this, we need \\(\\sqrt{p(1-p)/N}\\), but \\(p\\) is unknown. Fortunately, the CLT still holds if we use a plug-in estimate, replacing \\(p\\) with our observed \\(\\bar{X}\\):\n\\[\n\\widehat{\\mathrm{SE}}[\\bar{X}] = \\sqrt{\\bar{X}(1 - \\bar{X})/N}.\n\\]\n\n\n\n\n\n\nIn statistics, hats indicate estimates. For example, \\(\\hat{p}\\) is often used instead of \\(\\bar{X}\\) to denote the estimate of \\(p\\). Similarly, \\(\\widehat{\\mathrm{SE}}\\) means we are estimating the standard error rather than computing it exactly.\n\n\n\nUsing our earlier poll with \\(\\bar{X} = 0.48\\) and \\(N = 25\\), we obtain:\n\nx_hat &lt;- 0.48\nse &lt;- sqrt(x_hat*(1 - x_hat)/25)\nse\n#&gt; [1] 0.0999\n\nNow we can compute the probability that our estimate is within 2% of the true value:\n\npnorm(0.02/se) - pnorm(-0.02/se)\n#&gt; [1] 0.159\n\nThere is only a small chance of being this close when \\(N = 25\\).\nThe same reasoning applies for any desired error. To find the error \\(\\epsilon\\) that gives a 95% probability that \\(\\bar{X}\\) is within that range of \\(p\\), we solve:\n\\[\n\\Pr(|\\bar{X} - p| \\leq \\epsilon) = 0.95.\n\\]\nFrom the standard normal distribution, we know that\n\\[\n\\Pr(-1.96 \\leq Z \\leq 1.96) = 0.95.\n\\]\nTherefore, \\(\\epsilon = 1.96 \\times \\widehat{\\mathrm{SE}}[\\bar{X}]\\).\nThis result gives us a way to construct confidence intervals that, in repeated samples, contain the true parameter 95% of the time.\nConstructing Confidence Intervals\nUsing this result, we can write the 95% confidence interval for \\(p\\) as:\n\\[\n\\left[\\bar{X} - 1.96,\\widehat{\\mathrm{SE}}[\\bar{X}], , \\bar{X} + 1.96,\\widehat{\\mathrm{SE}}[\\bar{X}]\\right].\n\\]\nThe endpoints of this interval are not fixed numbers—they depend on the data. Each time we take a new sample, we obtain a new \\(\\bar{X}\\) and therefore a new confidence interval. To see this, we can repeat the sampling process using the same parameters as before:\n\np &lt;- 0.45\nN &lt;- 1000\nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\nx_hat &lt;- mean(x)\nse_hat &lt;- sqrt(x_hat*(1 - x_hat)/N)\nc(x_hat - 1.96*se_hat, x_hat + 1.96*se_hat)\n#&gt; [1] 0.400 0.462\n\nIf you run this code several times, you’ll see that the interval changes from run to run due to random sampling variation.\nThe definition of a 95% confidence interval is that, over many repetitions of this process, about 95% of the intervals constructed in this way will contain the true value of \\(p\\). Mathematically:\n\\[\n\\Pr!\\left(p \\in \\left[\\bar{X} - 1.96 ,\\widehat{\\mathrm{SE}}[\\bar{X}], \\bar{X} + 1.96,\\widehat{\\mathrm{SE}}[\\bar{X}]\\right]\\right) = 0.95.\n\\]\n\n\n\n\n\n\nConfidence intervals are not unique. We can construct other intervals that also have 95 percent coverage for \\(p\\). For example, approaches that avoid going past 0 or 1 when \\(\\bar{X}\\) is close to 0 or 1.\nThe 95 percent confidence interval we derived is symmetric around \\(\\bar{X}\\) and, under standard assumptions, is the shortest interval for which the probability of containing \\(p\\) is 95 percent. For a discussion of alternative intervals and their properties, see the recommended reading section.\n\n\n\nIf we want a different confidence level, say 99%, we adjust the multiplier. The value \\(z\\) is determined by\n\\[\n\\Pr(-z \\leq Z \\leq z) = 1 - \\alpha,\n\\]\nwhich in R can be computed as:\n\nalpha &lt;- 0.01\nz &lt;- qnorm(1 - alpha/2)\n\nFor example, qnorm(0.975) gives 1.96 for a 95% confidence interval, and qnorm(0.995) gives 2.58 for a 99% interval.\nA Monte Carlo Simulation\nWe can verify that the intervals we construct have the desired probability of including \\(p\\) using a Monte Carlo simulation. By repeatedly drawing samples and constructing confidence intervals, we can check how often the intervals actually contain the true value of \\(p\\).\n\nset.seed(1)\nN &lt;- 1000\nB &lt;- 10000\np &lt;- 0.45\ninside &lt;- replicate(B, {\n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p))\n  x_hat &lt;- mean(x)\n  se_hat &lt;- sqrt(x_hat*(1 - x_hat)/N)\n  p &gt;= x_hat - 1.96*se_hat & p &lt;= x_hat + 1.96*se_hat\n})\nmean(inside)\n#&gt; [1] 0.948\n\nAs expected, the interval covers \\(p\\) about 95% of the time.\nThe plot below shows the first 100 simulated confidence intervals. Each horizontal line represents one interval, and the black vertical line marks the true proportion \\(p\\). Roughly 95% of the intervals overlap the line, while about 5% miss it, exactly as predicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen applying this theory, remember that the intervals are random, not \\(p\\). The confidence level refers to the procedure, not to a probability about \\(p\\) itself.\n\n\n\nMargin of Error\nMargins of error are closely related to confidence intervals. The connection is simple: the margin of error is the amount added to and subtracted from the estimate to form a confidence interval.\nFor polls, the margin of error is given by\n\\[\nz \\times \\sqrt{\\bar{X}(1-\\bar{X}) / N}.\n\\]\nFor a 95 percent confidence interval, this corresponds to using \\(z = 1.96\\). Increasing the confidence level or decreasing the sample size both make the margin of error larger, while larger samples produce smaller margins of error.\nIn practice, pollsters almost always report 95 percent confidence intervals. Unless otherwise stated, you should assume that the reported margin of error is based on this level of confidence. Results are typically presented as “estimate ± margin of error.” For example, if \\(\\bar{X} = 0.52\\) with a margin of error of \\(0.03\\), the result would be reported as “52% ± 3%”\nWhy Not Just Run a Huge Poll?\nIf we surveyed 100,000 people, the margin of error would shrink to less than 0.3%. In principle, we can make the margin of error as small as we want by increasing \\(N\\).\nIn practice, however, polling is limited not only by cost but also by bias. Real-world polls are rarely simple random samples. Some people do not respond, others misreport their preferences, and defining the population (for example, registered voters versus likely voters) is not straightforward.\nThese imperfections introduce systematic errors that the margin of error does not capture. Historically, U.S. popular-vote polls have shown biases of about 2–3%. Understanding and modeling these biases is an essential part of modern election forecasting, a topic we return to in Chapter 10.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimates and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "inference/estimates-confidence-intervals.html#exercises",
    "href": "inference/estimates-confidence-intervals.html#exercises",
    "title": "9  Estimates and Confidence Intervals",
    "section": "\n9.5 Exercises",
    "text": "9.5 Exercises\n1. Suppose you poll a population in which a proportion \\(p\\) of voters are Democrats and \\(1-p\\) are Republicans. Your sample size is \\(N=25\\). Consider the random variable \\(S\\), which is the total number of Democrats in your sample. What is the expected value of this random variable? Hint: It’s a function of \\(p\\).\n2. What is the standard error of \\(S\\) ? Hint: It’s a function of \\(p\\).\n3. Consider the random variable \\(S/N\\). This is equivalent to the sample average, which we have been denoting as \\(\\bar{X}\\). What is the expected value of the \\(\\bar{X}\\)? Hint: It’s a function of \\(p\\).\n4. What is the standard error of \\(\\bar{X}\\)? Hint: It’s a function of \\(p\\).\n5. Write a line of code that gives you the standard error \\(\\mathrm{SE}[\\bar{X}]\\) for problem 4 for several values of \\(p\\), specifically for p &lt;- seq(0, 1, length = 100). Make a plot of \\(\\mathrm{SE}[\\bar{X}]\\) versus p.\n6. Copy the code from your solution to 5 and put it inside a for-loop to make three plots, one for \\(N=25\\), \\(N=100\\), and \\(N=1,000\\).\n7. If we are interested in the difference in proportions, \\(\\theta = p - (1-p)\\), our estimate is \\(\\hat{\\theta} = \\bar{X} - (1-\\bar{X}) = 2\\bar{X}-1\\). Use the rules we learned about scaled random variables to derive the expected value of \\(\\hat{\\theta}\\).\n8. What is the standard error of \\(\\hat{\\theta}\\)?\n9. If \\(p=.45\\), it means the Republicans are winning by a relatively large margin, since \\(\\theta = -.1\\), which is a 10% margin of victory. In this case, what is the standard error of \\(\\hat{\\theta} = 2\\bar{X}-1\\) if we take a sample of \\(N=25\\)?\n10. Given the answer to exercise 9, which of the following best describes your strategy of using a sample size of \\(N=25\\)?\n\nThe expected value of our estimate \\(2\\bar{X}-1\\) is \\(\\theta\\), so our prediction will be accurate.\nOur standard error is almost as large as the observed difference, so the chances of \\(2\\bar{X}-1\\) representing a large margin are not small even when \\(p=0.5\\). We should use a larger sample size.\nThe difference is 10% and the standard error is about 0.1, therefore much smaller than the difference.\nBecause we don’t know \\(p\\), we have no way of knowing that making \\(N\\) larger would actually improve our standard error.\n\n11. Write an urn model function that takes the proportion of Democrats \\(p\\) and the sample size \\(N\\) as arguments, and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function take_sample.\n12. Now assume p &lt;- 0.45 and that your sample size is \\(N=100\\). Take a sample 10,000 times and save the vector of mean(X) - p into an object called errors. Hint: Use the function you wrote for exercise 11 to write this in one line of code.\n13. The vector errors contains, for each simulated sample, the difference between the our estimate \\(\\bar{X}\\) and the actual \\(p\\). We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation,\n\nmean(errors)\nhist(errors)\n\nand select which of the following best describes their distributions:\n\nThe errors are all about 0.05.\nThe errors are all about -0.05.\nThe errors are symmetrically distributed around 0.\nThe errors range from -1 to 1.\n\n14. Note that the error \\(\\bar{X}-p\\) is a random variable. In practice, the error is not observed because we do not know \\(p\\). Here, we observe it since we constructed the simulation. What is the average size of the error if we define the size by taking the absolute value \\(\\mid \\bar{X} - p \\mid\\)?\n15. The standard error is related to the typical size of the error we make when predicting. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of errors, rather than the average of the absolute values, to quantify the typical size. What is this standard deviation of the errors?\n16. The theory we just learned tells us what this standard deviation is going to be because \\(\\mathrm{SE}[\\bar{X}-p]=\\mathrm{SE}[\\bar{X}]\\) and we showed how to compute this. What does theory tell us is the standard error \\(\\mathrm{SE}[\\bar{X}-p]\\) is for a sample size of 100?\n17. In practice, we don’t know \\(p\\), so we construct an estimate of the theoretical prediction by plugging in \\(\\bar{X}\\) for \\(p\\). Compute this estimate. Set the seed at 1 with set.seed(1).\n18. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 15), the theoretical prediction (exercise 16), and the estimate of the theoretical prediction (exercise 17) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict \\(p\\) with \\(\\bar{X}\\). Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier, we learned that the largest standard errors occur for \\(p=0.5\\). Create a plot of the largest standard error for \\(N\\) ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?\n\n100\n500\n2,500\n4,000\n\n19. For sample size \\(N=100\\), the Central Limit Theorem tells us that the distribution of \\(\\bar{X}\\) is:\n\npractically equal to \\(p\\).\napproximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\napproximately normal with expected value \\(\\bar{X}\\) and standard error \\(\\sqrt{\\bar{X}(1-\\bar{X})/N}\\).\nnot a random variable.\n\n20. Based on the answer from exercise 18, the error \\(\\bar{X} - p\\) is:\n\npractically equal to 0.\napproximately normal with expected value \\(0\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\napproximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\nnot a random variable.\n\n21. To corroborate your answer to exercise 19, make a qqplot of the errors you generated in exercise 12 to see if they follow a normal distribution.\n22. Define \\(p=0.45\\) and \\(N=100\\) as in exercise 12. Then use the CLT to estimate the probability that \\(\\bar{X}&gt;0.5\\). Assume you know \\(p=0.45\\) for this calculation.\n23. Assume you are in a practical situation and you don’t know \\(p\\). Take a sample of size \\(N=100\\) and obtain a sample average of \\(\\bar{X} = 0.51\\). What is the CLT approximation for the probability that your error is equal to or larger than 0.01?\nFor the next exercises, we will use actual polls from the 2016 election included in the dslabs package. Specifically, we will use all the national polls that ended within one week prior to the election.\n\nlibrary(dslabs)\nlibrary(tidyverse)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\") \n\n24. For the first poll, you can obtain the samples size and estimated Clinton percentage with:\n\nN &lt;- polls$samplesize[1]\nx_hat &lt;- polls$rawpoll_clinton[1]/100\n\nAssume there are only two candidates and construct a 95% confidence interval for the election night proportion \\(p\\) of Clinton voters.\n25. Add a confidence interval as two columns, call them lower and upper, to the object polls. Then show the pollster, enddate, x_hat,lower, upper variables. Hint: Define temporary columns x_hat and se_hat.\n26. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion \\(p=0.482\\) or not.\n27. For the table you just created, what proportion of confidence intervals included \\(p\\)?\n28. If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include \\(p\\)?\n29. A much smaller proportion of the polls than expected produce confidence intervals containing \\(p\\). If you look closely at the table, you will see that most polls that fail to include \\(p\\) are underestimating. The main reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide close to evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates, we will denote tit with \\(\\theta\\), which in this election was \\(0. 482 - 0.461 = 0.021\\). Assume that there are only two parties and that \\(\\theta = 2p - 1\\). Redefine polls as below and re-do exercise 15, but for the difference.\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\")  |&gt;\n  mutate(theta_hat = rawpoll_clinton/100 - rawpoll_trump/100)\n\n30. Now repeat exercise 26, but for the difference.\n31. Now repeat exercise 27, but for the difference.\n32. Although the proportion of confidence intervals increases substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual \\(\\theta=0.021\\). Stratify by pollster.\n33. Redo the plot that you made for exercise 32, but only for pollsters that took five or more polls.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimates and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "inference/estimates-confidence-intervals.html#footnotes",
    "href": "inference/estimates-confidence-intervals.html#footnotes",
    "title": "9  Estimates and Confidence Intervals",
    "section": "",
    "text": "http://www.realclearpolitics.com↩︎\nhttp://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html↩︎",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimates and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "inference/models.html",
    "href": "inference/models.html",
    "title": "10  Data-Driven Models",
    "section": "",
    "text": "10.1 Case study: poll aggregators\nSo far, our analysis of poll-related results has been based on a simple sampling model. This model assumes that each voter has an equal chance of being selected for the poll, similar to picking beads from an urn with two colors. However, in this section, we explore real-world data and discover that this model is oversimplified. Instead, we propose a more effective approach in which we directly model the outcomes of pollsters rather than the individual polls.\nA more recent development since the original invention of opinion polls, is the use of computers to aggregate publicly available data from different sources and develop data-driven forecasting models. Here, we explore how poll aggregators collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the statistical models used to improve election forecasts beyond the power of individual polls. Specifically, we introduce a useful model for constructing a confidence interval for the popular vote difference.\nIt is important to emphasize that this chapter offers only a brief introduction to the vast field of statistical modeling. The model presented here, for example, does not allow us to assign probabilities to outcomes such as a particular candidate winning the popular vote, as is done by poll aggregators like FiveThirtyEight. In the next chapter, we introduce Bayesian models, which provide the mathematical framework for such probabilistic statements. Later, in the Linear Models part of the book, we explore widely used approaches for analyzing data in practice. Still, this introduction only scratches the surface. Readers interested in statistical modeling are encouraged to consult the Recommended Reading section for additional references that provide a deeper and broader perspective.\nA few weeks before the 2012 election, Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had, and which others missed.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data-Driven Models</span>"
    ]
  },
  {
    "objectID": "inference/models.html#case-study-poll-aggregators",
    "href": "inference/models.html#case-study-poll-aggregators",
    "title": "10  Data-Driven Models",
    "section": "",
    "text": "A Monte Carlo simulation\nTo do this, we simulate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls, construct, and report 95% confidence intervals for the spread for each of the 12 polls:\n\n\n\n\n\n\n\n\nNot surprisingly, all 12 polls report confidence intervals that include the result we eventually saw on election night (vertical dashed line). However, all 12 polls also include 0 (vertical solid line). Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up.\nPoll aggregators recognized that combining results from multiple polls can greatly improve precision. One straightforward approach is to reverse engineer the original data from each poll using the reported estimates and sample sizes. From these, we can reconstruct the total number of successes and failures across all polls, effectively combining them into a single, larger dataset. We then compute a new overall estimate and its corresponding standard error using the combined sample size, which is much larger than that of any individual poll.\nWhen we do this, the resulting margin of error is 0.018.\nOur combined estimate predicts a spread of 3.1 percentage points, plus or minus 1.8. This interval not only includes the actual result observed on election night but is also far from including zero. By aggregating the twelve polls, assuming no systematic bias, we obtain a much more precise estimate and can be quite confident that Obama will win the popular vote.\n\n\n\n\n\n\n\n\nHowever, this example was just a simplified simulation to illustrate the basic idea. In real-world settings, combining data from multiple polls is more complicated than treating them as one large random sample. Differences in methodology, timing, question wording, and even sampling biases can all affect the results. We will later explore how these factors make real poll aggregation more challenging and how statistical models can account for them.\nReal data from the 2016 presidential election\nThe following subset of the polls_us_election_2016 data in dslabs includes results for national polls as well as state polls taken during the year prior to the election and organized by FiveThirtyEight. In this first example, we will filter the data to include national polls conducted on likely voters (lv) within the last week leading up to the election and consider their estimate of the spread:\n\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-11-02\" & population == \"lv\") |&gt;\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) \n\nNote we will focus on predicting the spread, not the proportion \\(p\\). Since we are assuming there are only two parties, we know that the spread is \\(\\theta = p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(\\theta\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\widehat{\\mathrm{SE}}[\\bar{X}]\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\widehat{\\mathrm{SE}}[\\bar{X}]\\). Remember that subtracting 1 does not add any variability, so it does not affect the standard error. Also, the CLT applies here because our estimate is a linear combination of a sample average, which itself follows approximately normal distributions.\n\n\n\n\n\n\nWe use \\(\\theta\\) to denote the spread here because this is a common notation used in statistical textbooks for the parameter of interest.\n\n\n\nWe have 50 estimates of the spread. The theory we learned from sampling models tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(\\theta\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\).\nAssuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:\n\ntheta_hat &lt;- with(polls, sum(spread*samplesize)/sum(samplesize)) \n\nand the standard error is:\n\np_hat &lt;- (theta_hat + 1)/2 \nmoe &lt;- 2*1.96*sqrt(p_hat*(1 - p_hat)/sum(polls$samplesize))\n\nSo we report a spread of 3.59% with a margin of error of 0.4%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?\nA histogram of the reported spreads reveals a problem:\n\npolls |&gt; ggplot(aes(spread)) + geom_histogram(color = \"black\", binwidth = .01)\n\n\n\n\n\n\n\nThe estimates do not appear to be normally distributed, and the standard error appears to be larger than 0.0038. The theory is not working here.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data-Driven Models</span>"
    ]
  },
  {
    "objectID": "inference/models.html#sample-avg-model",
    "href": "inference/models.html#sample-avg-model",
    "title": "10  Data-Driven Models",
    "section": "\n10.2 Beyond the simple sampling model",
    "text": "10.2 Beyond the simple sampling model\nNotice that data come from various pollsters, and some are taking several polls a week:\n\npolls |&gt; count(pollster) |&gt; arrange(desc(n)) |&gt; head(5)\n#&gt;                   pollster n\n#&gt; 1                 IBD/TIPP 6\n#&gt; 2 The Times-Picayune/Lucid 6\n#&gt; 3    USC Dornsife/LA Times 6\n#&gt; 4 ABC News/Washington Post 5\n#&gt; 5     CVOTER International 5\n\nLet’s visualize the data for the pollsters that are regularly polling:\n\n\n\n\n\n\n\n\nThis plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll is between 0.018 and 0.033:\n\npolls |&gt; group_by(pollster) |&gt; filter(n() &gt;= 5) |&gt;\n  summarize(se = 2*sqrt(p_hat*(1 - p_hat)/median(samplesize)))\n#&gt; # A tibble: 5 × 2\n#&gt;   pollster                     se\n#&gt;   &lt;fct&gt;                     &lt;dbl&gt;\n#&gt; 1 ABC News/Washington Post 0.0243\n#&gt; 2 CVOTER International     0.0260\n#&gt; 3 IBD/TIPP                 0.0333\n#&gt; 4 The Times-Picayune/Lucid 0.0197\n#&gt; 5 USC Dornsife/LA Times    0.0183\n\nThis agrees with the within-poll variation we see. However, there appear to be differences across the polls. Observe, for example, how the USC Dornsife/LA Times pollster is predicting a 4% lead for Trump, while Ipsos is predicting a lead larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values, instead it assumes all the polls have the same expected value. FiveThirtyEight refers to these differences as house effects. We also call them pollster bias. Nothing in our simple urn model provides an explanation for these pollster-to-pollster differences.\nThis model misspecification led to an overconfident interval that ended up not including the election night result. So, rather than modeling the process generating these values with an urn model, we instead model the pollster results directly. To do this, we collect data. Specifically, for each pollster, we look at the last reported result before the election:\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt; \n  filter(enddate == max(enddate)) |&gt; ungroup()\n\nHere is a histogram of the calculated spread for these 20 pollsters’ final polls:\n\nhist(one_poll_per_pollster$spread, breaks = 10)\n\n\n\n\n\n\n\nAlthough we are no longer using a model with red (Republicans) and blue (Democrats) beads in an urn, our new model can also be thought of as an urn model, but containing poll results from all possible pollsters. Think of our \\(N=\\) 20 data points \\(Y_1,\\dots Y_N\\) as a random sample from this urn. To develop a useful model, we assume that the expected value of our urn is the actual spread \\(\\theta = 2p - 1\\), which implies that the sample average has expected value \\(\\theta\\).\nNow, instead of 0s and 1s, our urn contains continuous numbers representing poll results. The standard deviation of this urn is therefore no longer \\(2\\sqrt{p(1-p)}\\). In this setting, the variability we observe arises not only from random sampling of voters but also from differences across pollsters. Our new urn captures both sources of variation. The overall standard deviation of this distribution is unknown, and we denote it by \\(\\sigma\\).\nSo our new statistical model is that \\(Y_1, \\dots, Y_N\\) are a random sample with expected \\(\\theta\\) and standard deviation \\(\\sigma\\). The distribution, for now, is unspecified. But we consider \\(N\\) to be large enough to assume that the sample average \\(\\bar{Y} = \\sum_{i=1}^N Y_i\\) follows a normal distribution with expected value \\(\\theta\\) and standard error \\(\\sigma / \\sqrt{N}\\). We write:\n\\[\n\\bar{Y} \\sim \\mbox{N}(\\theta, \\sigma / \\sqrt{N})\n\\]\nHere the \\(\\sim\\) symbol tells us that the random variable on the left of the symbol follows the distribution on the right. We use the notation \\(N(a,b)\\) to represent the normal distribution with mean \\(a\\) and standard deviation \\(b\\).\n\nThis model for the sample average will be used again the next chapter.\n\nEstimating the standard deviation\nThe model we have specified has two unknown parameters: the expected value \\(\\theta\\) and the standard deviation \\(\\sigma\\). We know that the sample average \\(\\bar{Y}\\) will be our estimate of \\(\\theta\\). But what about \\(\\sigma\\)?\nOur task is to estimate \\(\\theta\\). Given that we model the observed values \\(Y_1,\\dots Y_N\\) as a random sample from the urn, for a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{Y}\\) is approximately normal with expected value \\(\\theta\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider \\(N=\\) 20 large enough, we can use this to construct confidence intervals.\nTheory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as:\n\\[\ns = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N (Y_i - \\bar{Y})^2 }\n\\]\nKeep in mind that, unlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we do not cover it here.\nThe sd function in R computes the sample standard deviation:\n\nsd(one_poll_per_pollster$spread)\n#&gt; [1] 0.0222\n\nComputing a confidence interval\nWe are now ready to form a new confidence interval based on our new data-driven model:\n\nresults &lt;- one_poll_per_pollster |&gt; \n  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) |&gt; \n  mutate(start = avg - 1.96*se, end = avg + 1.96*se) \nround(results*100, 2)\n#&gt;    avg  se start end\n#&gt; 1 3.03 0.5  2.06   4\n\nOur confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.\nThe t-distribution\nAbove, we made use of the CLT with a sample size of 20. Because we are estimating a second parameter \\(\\sigma\\), further variability is introduced into our confidence interval, which results in intervals that are too small. For very large sample sizes, this extra variability is negligible, but in general, especially for \\(N\\) smaller than 30, we need to be cautious about using the CLT.\n\n\n\n\n\n\nNote that 30 is a very general rule of thumb based on the case when the data come from a normal distribution. There are cases when a large sample size is needed as well as cases when smaller sample sizes are good enough.\n\n\n\nHowever, if the data in the urn is known to follow a normal distribution, then we actually have mathematical theory that tells us how much bigger we need to make the intervals to account for the estimation of \\(\\sigma\\). Applying this theory, we can construct confidence intervals for any \\(N\\). But again, this works only if the data in the urn is known to follow a normal distribution. So for the 0, 1 data of our previous urn model, this theory definitely does not apply.\nThe statistic on which confidence intervals for \\(\\theta\\) are based is:\n\\[\nZ = \\frac{\\bar{Y} - \\theta}{\\sigma/\\sqrt{N}}\n\\] Here, \\(\\theta\\) is the true population spread, and \\(\\sigma\\) is the standard deviation of the pollster-level urn.\nCLT tells us that Z is approximately normally distributed with expected value 0 and standard error 1. But in practice we don’t know \\(\\sigma\\), so we use:\n\\[\nt = \\frac{\\bar{Y} - \\theta}{s/\\sqrt{N}}\n\\]\nThis is referred to as a t-statistic. By substituting \\(\\sigma\\) with \\(s\\), we introduce some variability. The theory tells us that \\(t\\) follows a student t-distribution with \\(N-1\\) degrees of freedom. The degrees of freedom is a parameter that controls the variability via fatter tails. Here are three examples of t-distributions with three different degrees of freedom:\n\n\n\n\n\n\n\n\nIf we are willing to assume the pollster effect data is normally distributed, based on the sample data \\(Y_1, \\dots, Y_N\\),\n\none_poll_per_pollster |&gt; ggplot(aes(sample = spread)) + stat_qq()\n\n\n\n\n\n\n\nthen \\(t\\) follows a t-distribution with \\(N-1\\) degrees of freedom. To construct a 95% confidence interval we simply use qt instead of qnorm: This results in a slightly larger confidence interval than we obtained before:\n\nn &lt;- length(one_poll_per_pollster$spread)\nttest_ci &lt;- one_poll_per_pollster |&gt; \n  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) |&gt; \n  mutate(start = avg - qt(0.975, n - 1)*se, end = avg + qt(0.975, n - 1)*se) |&gt;\n  select(start, end)\nround(ttest_ci*100, 2)\n#&gt;   start  end\n#&gt; 1  1.99 4.07\n\nThis interval is a bit larger than the one using normal because the t-distribution has large tails, as seen in the densities. Specifically, the 97.5-th quantile value for the t distribution\n\nqt(0.975, n - 1)\n#&gt; [1] 2.09\n\nis larger than that for the normal distribution\n\nqnorm(0.975)\n#&gt; [1] 1.96\n\n\n\n\n\n\n\nUsing the t-distribution and the t-statistic is the basis for t-tests, a widely used approach for computing p-values. To learn more about t-tests, you can consult any statistics textbook.\n\n\n\nThe t-distribution can also be used to model errors in bigger deviations that are more likely than with the normal distribution. In their Monte Carlo simulations, FiveThirtyEight uses the t-distribution to generate errors that better model the deviations we see in election data. For example, in Wisconsin, the average of six high quality polls was 7% in favor of Clinton with a standard deviation of 3%, but Trump won by 0.8%. Even after taking into account the overall bias, this 7.7% residual is more in line with t-distributed data than the normal distribution.\n\npolls_us_election_2016 |&gt;\n  filter(state == \"Wisconsin\" & enddate &gt; \"2016-11-01\" & \n            population == \"lv\" & grade %in% c(\"A+\",\"A\",\"A-\",\"B+\",\"B\")) |&gt;\n  left_join(results_us_election_2016, by = \"state\") |&gt;\n  mutate(spread = rawpoll_clinton - rawpoll_trump, actual = clinton - trump) |&gt;\n  summarize(actual = first(actual), estimate = mean(spread), sd = sd(spread)) \n#&gt;   actual estimate sd\n#&gt; 1 -0.764     7.04  3",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data-Driven Models</span>"
    ]
  },
  {
    "objectID": "inference/models.html#exercises",
    "href": "inference/models.html#exercises",
    "title": "10  Data-Driven Models",
    "section": "\n10.3 Exercises",
    "text": "10.3 Exercises\nWe have been using urn models to motivate the use of probability models. Yet, most data science applications are not related to data obtained from urns. More common are data that come from individuals. The reason probability plays a role here is because the data come from a random sample. The random sample is taken from a population, and the urn serves as an analogy for the population.\nDefine the males that replied to the height survey as the population\n\nlibrary(dslabs)\nx &lt;- heights |&gt; filter(sex == \"Male\") |&gt;\n  pull(height)\n\nto answer the following questions.\n1. Mathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it. What are the mean and standard deviation of our population?\n2. Call the population mean computed above \\(\\mu\\) and the standard deviation \\(\\sigma\\). Now take a sample of size 50, with replacement, and construct an estimate for \\(\\mu\\) and \\(\\sigma\\).\n3. What does the theory tell us about the sample average \\(\\bar{X}\\) and how it is related to \\(\\mu\\)?\n\nIt is practically identical to \\(\\mu\\).\nIt is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\).\nIt is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma\\).\nContains no information.\n\n4. So, how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we can only measure 50 of the 708. We will use \\(\\bar{X}\\) as our estimate. We know from the answer to exercise 3 that the standard error of our estimate \\(\\bar{X}-\\mu\\) is \\(\\sigma/\\sqrt{N}\\). We want to compute this, but we don’t know \\(\\sigma\\). Based on what is described in this chapter, show your estimate of \\(\\sigma\\).\n5. Construct a 95% confidence interval for \\(\\mu\\) using our estimate of \\(\\sigma\\).\n6. Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you did in exercise 5. What proportion of these intervals include \\(\\mu\\)?\n7. Use the qnorm and qt functions to generate quantiles. Compare these quantiles for different degrees of freedom for the t-distribution. Use this to motivate the sample size of 30 rule of thumb.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data-Driven Models</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html",
    "href": "inference/bayes.html",
    "title": "11  Bayesian Statistics",
    "section": "",
    "text": "11.1 Bayes theorem\nIn 2016, FiveThirtyEight showed this chart depicting distributions for the percent of the popular vote for each candidate:\nBut what does this mean in the context of the theory we have previously covered, in which these percentages are considered fixed? Furthermore, election forecasters make probabilistic statements such as “Obama has a 90% chance of winning the election.” Note that in the context of an urn model, this would be equivalent to stating that the probability of \\(p&gt;0.5\\) is 90%. However, in the urn model \\(p\\) is a fixed parameter and it does not make sense to talk about probability. With Bayesian statistics, we assume \\(p\\) is a random variable, and thus, a statement such as “90% chance of winning” is consistent with the mathematical approach. Forecasters also use models to describe variability at different levels. For example, sampling variability, pollster to pollster variability, day to day variability, and election to election variability. One of the most successful approaches used for this are hierarchical models, which can be explained in the context of Bayesian statistics.\nIn this chapter, we will briefly describe Bayesian statistics. We use three cases studies: 1) interpreting diagnostic tests for a rare disease, and 2) estimating the probability of Hillary Clinton winning the popular vote in 2016 using pre-election poll data.\nWe start by describing Bayes theorem, using a hypothetical cystic fibrosis test as an example.\nSuppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation:\n\\[\n\\mathrm{Pr}(+ \\mid D=1)=0.99, \\mathrm{Pr}(- \\mid D=0)=0.99\n\\]\nwith \\(+\\) meaning a positive test and \\(D\\) representing if you actually have the disease (1) or not (0).\nImagine we select a random person and they test positive. What is the probability that they have the disease? We write this probability as \\(\\mathrm{Pr}(D=1 \\mid +)\\).\nTo answer this question, we will use Bayes theorem, which tells us that:\n\\[\n\\mathrm{Pr}(A \\mid B)  =  \\frac{\\mathrm{Pr}(B \\mid A)\\mathrm{Pr}(A)}{\\mathrm{Pr}(B)}\n\\]\nThis equation, when applied to our problem, becomes:\n\\[\n\\begin{aligned}\n\\mathrm{Pr}(D=1 \\mid +) & =  \\frac{ \\mathrm{Pr}(+ \\mid D=1) \\, \\mathrm{Pr}(D=1)} {\\mathrm{Pr}(+)} \\\\\n& =  \\frac{\\mathrm{Pr}(+ \\mid D=1) \\, \\mathrm{Pr}(D=1)} {\\mathrm{Pr}(+ \\mid D=1) \\, \\mathrm{Pr}(D=1) + \\mathrm{Pr}(+ \\mid D=0) \\mathrm{Pr}( D=0)}\n\\end{aligned}\n\\]\nThe cystic fibrosis rate is 1 in 3,900, which implies that \\(\\mathrm{Pr}(D=1)\\approx0.00025\\). Plugging in the numbers, we get:\n\\[\n\\mathrm{Pr}(D=1 \\mid +)  = \\frac{0.99 \\cdot 0.00025}{0.99 \\cdot 0.00025 + 0.01 \\cdot (.99975)}  \\approx  0.02\n\\]\nAccording to the above, despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02. This might seem counter-intuitive to some, but it is because we must factor in the very rare probability that a randomly chosen person has the disease. To illustrate this, we run a Monte Carlo simulation.\nWe start by randomly selecting 100,000 people from a population in which the disease in question has a 1 in 4,000 prevalence.\np &lt;- 0.00025\nN &lt;- 100000\nD &lt;- sample(c(1, 0), N, replace = TRUE, prob = c(p, 1 - p))\nNote that, as expected, there are very few people with the disease and many people without the disease,\nN_1 &lt;- sum(D == 1)\nN_0 &lt;- sum(D == 0)\ncat(N_1, \"with disease, and\", N_0, \"without.\")\n#&gt; 23 with disease, and 99977 without.\nThis makes it more probable that we will see some false positives given that the test is not perfect. Now, each person gets the test, which is correct 99% of the time:\nacc &lt;- 0.99\ntest &lt;- vector(\"character\", N)\ntest[D == 1] &lt;- sample(c(\"+\", \"-\"), N_1, replace = TRUE, prob = c(acc, 1 - acc))\ntest[D == 0] &lt;- sample(c(\"-\", \"+\"), N_0, replace = TRUE, prob = c(acc, 1 - acc))\nSince the number of healthy individuals is much larger than the number of individuals with the disease, even a low false positive rate results in more healthy individuals testing positive than actual cases.\ntable(D, test)\n#&gt;    test\n#&gt; D       -     +\n#&gt;   0 99012   965\n#&gt;   1     0    23\nFrom this table, we see that the proportion of positive tests that have the disease is 23 out of 988. We can run this over and over again to see that, in fact, the probability converges to about 0.02.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html#priors-and-posteriors",
    "href": "inference/bayes.html#priors-and-posteriors",
    "title": "11  Bayesian Statistics",
    "section": "\n11.2 Priors and posteriors",
    "text": "11.2 Priors and posteriors\nIn the previous chapter, we computed an estimate and margin of error for the difference in popular votes between Hillary Clinton and Donald Trump. We denoted the parameter, the difference in popular votes, with \\(\\theta\\). The estimate was between 2 and 4 percent, and the confidence interval did not include 0. A forecaster would use this to predict Hillary Clinton would win the popular vote. But to make a probabilistic statement about winning the election, we need to use a Bayesian approach.\nWe start the Bayesian approach by quantifying our knowledge before seeing any data. This is done using a probability distribution referred to as a prior. For our example, we could write:\n\\[\n\\theta \\sim N(\\theta_0, \\tau)\n\\]\nWe can think of \\(\\theta_0\\) as our best guess for the popular vote difference had we not seen any polling data, and we can think of \\(\\tau\\) as quantifying how certain we feel about this guess. Generally, if we have expert knowledge related to \\(\\theta\\), we can try to quantify it with the prior distribution. In the case of election polls, experts use fundamentals, which include, for example, the state of the economy, to develop prior distributions.\nThe data is used to update our initial guess or prior belief. This can be done mathematically if we define the distribution for the observed data for any given \\(\\theta\\). In our particular example, we would write down a model for the average of our polls. If we fixed \\(\\theta\\), this model is the same we used in the previous chapter:\n\\[\n\\bar{Y} \\mid \\theta \\sim N(\\theta, \\sigma/\\sqrt{N})\n\\]\nAs before, \\(\\sigma\\) describes randomness due to sampling and the pollster effects. In the Bayesian contexts, this is referred to as the sampling distribution. Note that we write the conditional \\(\\bar{Y} \\mid \\theta\\) because \\(\\theta\\) is now considered a random variable.\nWe do not show the derivations here, but we can now use calculus and a version of Bayes’ Theorem to derive the conditional distribution of \\(\\theta\\) given the observed data, referred to as the posterior distribution. Specifically, we can show that \\(\\theta \\mid \\bar{Y}\\) follows a normal distribution with expected value:\n\\[\n\\mathrm{E}[\\theta \\mid \\bar{Y}] = B \\theta_0 + (1-B) \\,\\bar{Y} \\mbox{ with } B = \\frac{\\sigma^2/N}{\\sigma^2/N+\\tau^2}\n\\]\nand standard error :\n\\[\n\\mathrm{SE}[\\mu \\mid \\bar{Y}] = \\sqrt{\\frac{1}{N/\\sigma^2+1/\\tau^2}}.\n\\]\nNote that the expected value is a weighted average of our prior guess \\(\\theta_0\\) and the observed data \\(\\bar{Y}\\). The weight depends on how certain we are about our prior belief, quantified by \\(\\tau^2\\), and the variance \\(\\sigma^2/N\\) of the summary of our observed data.\nThis weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior value. To see this note that we can rewrite the weighted average as:\n\\[\nB \\theta_0 + (1-B) \\bar{Y}= \\theta_0 + (1-B)(\\bar{Y}-\\theta_0)\\\\\n\\] The closer \\(B\\) is to 1, the more we shrink our estimate toward \\(\\theta_0\\).\nThese formulas are useful way quantifying how we update our beliefs.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html#credible-intervals",
    "href": "inference/bayes.html#credible-intervals",
    "title": "11  Bayesian Statistics",
    "section": "\n11.3 Credible intervals",
    "text": "11.3 Credible intervals\nWe can also report intervals with high probability of occurring given our model. Specifically, for any probability value \\(\\alpha\\) we can use the posterior distribution to construct intervals centered at our posterior mean and with \\(\\alpha\\) chance of occurring. These are called credible intervals.\nAs an example, we compute a posterior distribution and construct a credible interval for the popular vote difference, after defining a prior distribution with mean 0% and standard error 5%. This prior distribution can be interpreted as follows: before seeing polling data, we don’t think any candidate has the advantage, and a difference of up to 10% either way is possible.\n\ntheta_0 &lt;- 0\ntau &lt;- 0.05\n\nWe can then compute the posterior distribution by applying the equations above to the one_poll_per_pollster data defined in Chapter 10:\n\nres &lt;- one_poll_per_pollster |&gt; \n  summarise(y_bar = mean(spread), sigma = sd(spread), n = n())\nB &lt;- with(res, sigma^2/n / (sigma^2/n + tau^2))\nposterior_mean &lt;- B*theta_0 + (1 - B)*res$y_bar\nposterior_se &lt;- with(res, sqrt(1/(n/sigma^2 + 1/tau^2)))\nposterior_mean + c(-1, 1)*qnorm(0.975)*posterior_se\n#&gt; [1] 0.0203 0.0397\n\nFurthermore, we can now make the probabilistic statement that we could not make with the frequentists approach. Specifically, \\(\\mathrm{Pr}(\\mu&gt;0 \\mid \\bar{X})\\) can be computed as follows:\n\n1 - pnorm(0, posterior_mean, posterior_se)\n#&gt; [1] 1\n\nAccording to the calculation above, we would be almost 100% certain that Clinton will win the popular vote, an estimate that feels overly confident. Moreover, it does not align with FiveThirtyEight’s reported probability of 81.4%. What accounts for this difference? Our current model does not yet capture all sources of uncertainty. We will revisit this issue and address the missing variability in Chapter 12.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html#exercises",
    "href": "inference/bayes.html#exercises",
    "title": "11  Bayesian Statistics",
    "section": "\n11.4 Exercises",
    "text": "11.4 Exercises\n1. In 1999, in England, Sally Clark1 was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants, so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500, and then calculating that the chance of two SIDS cases was 8,500 \\(\\times\\) 8,500 \\(\\approx\\) 73 million. Which of the following do you agree with?\n\nSir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: \\(\\mathrm{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) &gt; \\mathrm{Pr}(\\mbox{first case of SIDS})\\).\nNothing. The multiplication rule always applies in this way: \\(\\mathrm{Pr}(A \\mbox{ and } B) =\\mathrm{Pr}(A)\\mathrm{Pr}(B)\\)\n\nSir Meadow is an expert and we should trust his calculations.\nNumbers don’t lie.\n\n2. Let’s assume that there is, in fact, a genetic component to SIDS and the probability of \\(\\mathrm{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) = 1/100\\), is much higher than 1 in 8,500. What is the probability of both of her sons dying of SIDS?\n3. Many press reports stated that the expert claimed the probability of Sally Clark being innocent was 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written as the probability of a mother is a son-murdering psychopath given that two of her children are found dead with no evidence of physical harm. According to Bayes’ rule, what is this?\n4. Assume that the chance of a son-murdering psychopath finding a way to kill her children, without leaving evidence of physical harm, is:\n\\[\n\\mathrm{Pr}(A \\mid B) = 0.50\n\\]\nwith A = two of her children are found dead with no evidence of physical harm, and B = a mother is a son-murdering psychopath = 0.50. Assume that the rate of son-murdering psychopath mothers is 1 in 1,000,000. According to Bayes’ Theorem, what is the probability of \\(\\mathrm{Pr}(B \\mid A)\\) ?\n5. After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clark was acquitted in June 2003. What did the expert miss?\n\nHe made an arithmetic error.\nHe made two mistakes. First, he misused the multiplication rule and did not take into account how rare it is for a mother to murder her children. After using Bayes’ rule, we found a probability closer to 0.5 than 1 in 73 million.\nHe mixed up the numerator and denominator of Bayes’ rule.\nHe did not use R.\n\n6. Florida is one of the most closely watched states in U.S. elections because it has many electoral votes, giving it a large influence on the final result. Before 2016, Florida was a swing state where both Republicans and Democrats won implying it could affect a close elections.\nCreate the following table with the polls taken during the last two weeks:\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"Florida\" & enddate &gt;= \"2016-11-04\" ) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nTake the average spread of these polls. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called results.\n7. Now assume a Bayesian model that sets the prior distribution for Florida’s election night spread \\(\\theta\\) to follow a normal distribution with expected value \\(\\theta_0\\) and standard deviation \\(\\tau\\). What are the interpretations of \\(\\theta\\) and \\(\\tau\\)?\n\n\n\\(\\theta_0\\) and \\(\\tau\\) are arbitrary numbers that let us make probability statements about \\(\\theta\\).\n\n\\(\\theta_0\\) and \\(\\tau\\) summarize what we would predict for Florida before seeing any polls. Based on past elections, we would set \\(\\theta\\) close to 0, because both Republicans and Democrats have won, and \\(\\tau\\) at about \\(0.02\\), because these elections tend to be close.\n\n\\(\\theta_0\\) and \\(\\tau\\) summarize what we want to be true. We therefore set \\(\\theta_0\\) at \\(0.10\\) and \\(\\tau\\) at \\(0.01\\).\nThe choice of prior has no effect on Bayesian analysis.\n\n8. The CLT tells us that our estimate of the spread \\(\\hat{\\theta}\\) has normal distribution with expected value \\(\\theta\\) and standard deviation \\(\\sigma\\) calculated in exercise 6. Use the formulas we provided for the posterior distribution to calculate the expected value of the posterior distribution if we set \\(\\theta_0 = 0\\) and \\(\\tau = 0.01\\).\n9. Now compute the standard deviation of the posterior distribution.\n10. Using the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals.\n11. According to this analysis, what was the probability that Trump wins Florida?\n12. Now use sapply function to change the prior variance from seq(0.005, 0.05, len = 100) and observe how the probability changes by making a plot.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html#footnotes",
    "href": "inference/bayes.html#footnotes",
    "title": "11  Bayesian Statistics",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Sally_Clark↩︎",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html",
    "href": "inference/hierarchical-models.html",
    "title": "12  Hierarchical Models",
    "section": "",
    "text": "12.1 Case study: election forecasting\nHierarchical models are useful for quantifying different levels of variability or uncertainty. One can use them using a Bayesian or Frequentist framework. However, because in the Frequentist framework they often extend a model with a fixed parameter by assuming the parameter is actually random, the model description includes two distributions that look like the prior and a sampling distribution used in the Bayesian framework. This makes the resulting summaries very similar or even equal to what is obtained with a Bayesian context. A key difference between the Bayesian and the Frequentist hierarchical model approach is that, in the latter, we use data to construct priors rather than treat priors as a quantification of prior expert knowledge. In this section, we illustrate the use of hierarchical models by describing a simplified version of the approach used by FiveThirtyEight to forecast the 2016 election.\nAfter the 2008 elections, several organizations besides FiveThirtyEight launched their own election forecasting teams that aggregated polling data and used statistical models to make predictions. However, in 2016, many forecasters greatly underestimated Trump’s chances of winning. The day before the election, the New York Times reported1 the following probabilities for Hillary Clinton winning the presidency:\nNYT\n538\nHuffPost\nPW\nPEC\nDK\nCook\nRoth\n\n\nWin Prob\n85%\n71%\n98%\n89%\n&gt;99%\n92%\nLean Dem\nLean Dem\nNote that the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, substantially higher than the others. In fact, four days before the election, FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton2.\nSo why did FiveThirtyEight’s model fair so much better than others? How could PEC and Huffington Post get it so wrong if they were using the same data? In this chapter, we describe how FiveThirtyEight used a hierarchical model to correctly account for key sources of variability and outperform all other forecasters. For illustrative purposes, we will continue examining our popular vote example. In the final section, we will describe the more complex approach used to forecast the electoral college result.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#sec-general-bias",
    "href": "inference/hierarchical-models.html#sec-general-bias",
    "title": "12  Hierarchical Models",
    "section": "\n12.2 Systematic polling error",
    "text": "12.2 Systematic polling error\nIn the previous chapter, we computed the posterior probability of Hillary Clinton winning the popular vote with a standard Bayesian analysis and found it to be very close to 100%. However, FiveThirtyEight gave her a 81.4% chance3. What explains this difference? Below, we describe the systemic polling error, another source of variability, included in the FiveThirtyEight model, that accounts for the difference.\nAfter elections are over, one can look at the difference between the polling averages and the actual results. An important observation, that our initial models did not take into account, is that it is common to see a systemic polling error that affects most pollsters in the same way. Statisticians refer to this as a bias. The cause of this bias is unclear, but historical data shows it fluctuates. In one election, polls may favor Democrats by 2%, the next Republicans by 1%, then show no bias, and later favor Republicans by 3%. In 2016, polls favored Democrats by 1-2%.\nAlthough we know this systematic polling error affects our polls, we have no way of knowing what this bias is until election night. So we can’t correct our polls accordingly. What we can do is include a term in our model that accounts for the variability.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#mathematical-representations-of-the-hierarchical-model",
    "href": "inference/hierarchical-models.html#mathematical-representations-of-the-hierarchical-model",
    "title": "12  Hierarchical Models",
    "section": "\n12.3 Mathematical representations of the hierarchical model",
    "text": "12.3 Mathematical representations of the hierarchical model\nSuppose we are collecting data from one pollster and we assume there is no systematic error. The pollster collects several polls with a sample size of \\(N\\), so we observe several measurements of the spread \\(Y_1, \\dots, Y_J\\). Suppose the real proportion for Hillary is \\(p\\) and the spread is \\(\\theta\\). The urn model theory tells us that these random variables are approximately normally distributed, with expected value \\(\\theta\\) and standard error \\(2 \\sqrt{p(1-p)/N}\\):\n\\[\nY_j \\sim \\mbox{N}\\left(\\theta, \\, 2\\sqrt{p(1-p)/N}\\right)\n\\]\nWe use \\(j\\) as an index to label polls, so that the \\(j\\)th poll corresponds to the results reported by the \\(j\\)th poll.\nBelow is a simulation for six polls assuming the spread is 2.1 and \\(N\\) is 2,000:\n\nset.seed(2012)\nJ &lt;- 6\nN &lt;- 3000\ntheta &lt;- .021\np &lt;- (theta + 1)/2\ny &lt;- rnorm(J, theta, 2*sqrt(p*(1 - p)/N))\n\nNow, suppose we have \\(J=6\\) polls from each of \\(I=5\\) different pollsters. For simplicity, let’s say all polls had the same sample size \\(N\\). The urn model tell us the distribution is the same for all pollsters, so to simulate data, we use the same model for each:\n\nI &lt;- 5\ny &lt;- sapply(1:I, function(i) rnorm(J, theta, 2*sqrt(p*(1 - p)/N)))\n\nAs expected, the simulated data does not really seem to capture the features of the actual data because it does not account for pollster-to-pollster variability:\n\n\n\n\n\n\n\n\nTo fix this, we need to represent the two levels of variability and we need two indexes, one for pollster and one for the polls each pollster takes. We use \\(Y_{ij}\\) with \\(i\\) representing the pollster and \\(j\\) representing the \\(j\\)th poll from that pollster. The model is now augmented to include pollster effects \\(h_i\\), referred to as house effects by FiveThirtyEight, with standard deviation \\(\\sigma_h\\):\n\\[\n\\begin{aligned}\nh_i &\\sim \\mbox{N}\\left(0, \\sigma_h\\right)\\\\\nY_{ij} \\mid h_i &\\sim \\mbox{N}\\left(\\theta + h_i, \\, 2\\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]\nTo simulate data from a specific pollster, we first need to draw an \\(h_i\\), and then generate individual poll data after adding this effect. In the simulation below we assume \\(\\sigma_h\\) is 0.025 and generate the \\(h\\) using rnorm:\n\nh &lt;- rnorm(I, 0, 0.025)\ny &lt;- sapply(1:I, function(i) theta + h[i] + rnorm(J, 0, 2*sqrt(p*(1 - p)/N)))\n\nThe simulated data now looks more like the observed data:\n\n\n\n\n\n\n\n\nNote that \\(h_i\\) is common to all the observed spreads from pollster \\(i\\). Different pollsters have a different \\(h_i\\), which explains why we can see the groups of points shift up and down from pollster to pollster.\nNow, in the model above, we assume the average house effect is 0: we generate it with rnorm(I, 0, 0.025). We think that for every pollster biased in favor of one party, there is another one in favor of the other in way that the error averages out when computing the average across all polls. In this case the polling average is unbiased. However, as mentioned above, systematic polling error is observed when we study past elections.\nWe can’t observe this bias with just the 2016 data, but if we collect historical data, we see that the average of polls miss the actual result by more than what is prediced by models like the one above. We don’t show the data here, but if we took the average of polls for each past election and compare it to the actual election night result, we would observe difference with a standard deviation of between 2-4%.\nAlthough we can’t observe the bias, we can define a model that accounts for its variability. We do this by adding another level to the model as follows:\n\\[\n\\begin{aligned}\nb &\\sim \\mbox{N}\\left(0, \\sigma_b\\right)\\\\\nh_j \\mid \\, b &\\sim \\mbox{N}\\left(b, \\sigma_h\\right)\\\\\nY_{ij} | \\, h_j, b &\\sim \\mbox{N}\\left(\\theta + h_j, \\, 2\\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\] This model captures three distinct sources of variability:\n\n\nSystematic error across elections, represented by the random variable \\(b\\), with variability quantified by \\(\\sigma_b\\).\n\nPollster-to-pollster variability, often called the house effect, quantified by \\(\\sigma_h\\).\n\nSampling variability within each poll which arises from the random sampling of voters and is given by \\(2\\sqrt{p(1-p)/N}\\), where \\(p = (\\theta + 1)/2\\).\n\nFailing to include a term like \\(b\\), the election-level bias, led many forecasters to be overly confident in their predictions. The key point is that \\(b\\) changes from one election to another but remains constant across all pollsters and polls within a single election. Because \\(b\\) has no index, we cannot estimate \\(\\sigma_b\\) using data from only one election. Furthermore, this shared \\(b\\) implies that all random variables \\(Y_{ij}\\) within the same election are correlated, since they are influenced by the same underlying election-level bias.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#computing-a-posterior-probability",
    "href": "inference/hierarchical-models.html#computing-a-posterior-probability",
    "title": "12  Hierarchical Models",
    "section": "\n12.4 Computing a posterior probability",
    "text": "12.4 Computing a posterior probability\nNow, let’s fit a model like the above to data. We will use one_poll_per_pollster data defined in Chapter 10:\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt; \n  filter(enddate == max(enddate)) |&gt;\n  ungroup()\n\nHere, we have just one poll per pollster, so we will drop the \\(j\\) index and represent the data as before with \\(Y_1, \\dots, Y_I\\). As a reminder, we have data from $I=$20 pollsters. Based on the model assumptions described above, we can mathematically show that the average \\(\\bar{Y}\\)\n\ny_bar &lt;- mean(one_poll_per_pollster$spread)\n\nhas expected value \\(\\theta\\); thus, in the long run, it provides an unbiased estimate of the outcome of interest. But how precise is this estimate? Can we use the observed sample standard deviation to construct an estimate of the standard error of \\(\\bar{Y}\\)?\nIt turns out that, because the \\(Y_i\\) are correlated, estimating the standard error is more complex than what we have described up to now. Specifically, we can show that the standard error can be estimated with:\n\nsigma_b &lt;- 0.03\ns2 &lt;- with(one_poll_per_pollster, sd(spread)^2/length(spread))\nse &lt;- sqrt(s2 + sigma_b^2)\n\nAs mentioned earlier, estimating \\(\\sigma_b\\) requires data from past elections. However, collecting this data is a complex process and beyond the scope of this book. To provide a practical example, we set \\(\\sigma_b\\) to 3%.\nWe can now redo the Bayesian calculation to account for this additional variability. This adjustment yields a result that closely aligns with FiveThirtyEight’s estimates:\n\ntheta_0 &lt;- 0\ntau &lt;- 0.05\nB &lt;- se^2/(se^2 + tau^2)\nposterior_mean &lt;- B*theta_0 + (1 - B)*y_bar\nposterior_se &lt;- sqrt(1/(1/se^2 + 1/tau^2))\n1 - pnorm(0, posterior_mean, posterior_se)\n#&gt; [1] 0.803",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#predicting-the-electoral-college",
    "href": "inference/hierarchical-models.html#predicting-the-electoral-college",
    "title": "12  Hierarchical Models",
    "section": "\n12.5 Predicting the electoral college",
    "text": "12.5 Predicting the electoral college\nUp to now, we have focused on the popular vote. However, in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college. Each state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state. In 2016 California, the largest state, had 55 electoral votes, while the smallest seven states and the District of Columbia had 3.\nWith the exception of two states, Maine and Nebraska, electoral votes in U.S. presidential elections are awarded on a winner-takes-all basis. This means that if a candidate wins the popular vote in a state by even a single vote, they receive all of that state’s electoral votes. For example, winning California in 2016 by just one vote would secure all 55 of its electoral votes.\nThis system can lead to scenarios where a candidate wins the national popular vote but loses the electoral college, as was the case in 1876, 1888, 2000, and 2016.\n\n\n\n\n\n\nThe electoral college was designed to balance the influence of populous states and protect the interests of smaller ones in presidential elections. As a federation, the U.S. included states wary of losing power to larger states. During the Constitutional Convention of 1787, smaller states negotiated this system to ensure their voices remained significant, receiving electors based on their senators and representatives. This compromise helped secure their place in the union.\n\n\n\nOrganizing the data\nWe are now ready to predict the electoral college result for 2016. We start by creating a data frame with the electoral votes for each state:\n\nresults &lt;- results_us_election_2016 |&gt; select(state, electoral_votes)\n\nWe then aggregate results from polls taken during the last weeks before the election and include only those taken on registered and likely voters. We define spread as the estimated difference in proportion:\n\npolls &lt;- polls_us_election_2016 |&gt;\n  filter(state != \"U.S.\" & enddate &gt;= \"2016-11-02\" &  population != \"a\") |&gt;\n  mutate(spread = (rawpoll_clinton - rawpoll_trump)/100)\n\nIf a pollster ran more than one poll in this period, we keep only the latest.\n\npolls &lt;- polls |&gt;\n  arrange(population, desc(enddate)) |&gt;\n  group_by(state, pollster) |&gt;\n  slice(1) |&gt;\n  ungroup() \n\nWeighted averages\nInstead of simply averaging polls, FiveThirtyEight assigns weights to pollsters based on a letter grade, stored in the grade column. This grade reflects the pollster’s past accuracy and reliability. We generate a table with grades and weights for future use:\n\nweights &lt;- data.frame(grade = unique(sort(polls$grade))) |&gt; \n  mutate(weight = seq(0.3, 1, length = length(grade)))\n\n\n\n\n\n\n\nFiveThirtyEight considers various factors when determining the weight of each poll, including the proximity to election day. However, for simplicity, this analysis focuses solely on pollster grades.\n\n\n\nHigher-graded pollsters are assigned more weight in the model, giving their results greater influence, while lower-graded or less reliable pollsters contribute less. This is achieved mathematically through a weighted average:\n\\[\n\\bar{Y}_w = \\frac{\\sum_{i=1}^N w_i Y_i}{\\sum_{i=1}^N w_i}\n\\]\nwhere \\(Y_i\\) represents the result from pollster \\(i\\), \\(w_i\\) is the weight assigned to pollster \\(i\\),\nand \\(N\\) is the total number of pollsters. We add the \\(w\\) substript to \\(\\bar{Y}_w\\) to denote it is a weighted aveage.\nTo understand this intuitively, think of the weights as fractions of a full data point. For example, if the maximum weight is 1, a pollster with a weight of \\(0.5\\) contributes half as much as a fully trusted pollster, while a weight of \\(0.25\\) can be interpreted as contributing a quarter of a poll.\nWe use a similar equation to estimate the standard deviation:\n\\[\ns_w = \\sqrt{\\frac{\\sum_{i=1}^N w_i (Y_i - \\bar{Y}_w)}{\\frac{N-1}{N}\\sum_{i=1}^N w_i}}\n\\]\nWe can use the formula learned in Section 7.4 to derive the standard error for our weighted estimate:\n\\[\n\\mathrm{SE}[\\bar{Y}_w] = \\frac{\\sigma_h}{\\sqrt{N_{\\mbox{eff}}}} \\mbox{ with } N_{\\mbox{eff}} = \\frac{\\left(\\sum_{i=1}^N w_i\\right)^2}{\\sum_{i=1}^N w_i^2}\n\\]\nBecause it occupies the same position in the equation for the sample average, \\(N_{\\mbox{eff}}\\) is referred to as the effective sample size. Note that it is equal to \\(N\\) when all the weights are 1 and gets smaller the closer weights get to 0.\nWe now use these equations to compute the weighted averages:\n\npolls &lt;- polls |&gt; \n  filter(!is.na(grade)) |&gt;\n  left_join(weights, by = \"grade\") |&gt; \n  group_by(state) |&gt;\n  summarize(n = n(),\n            avg = sum(weight*spread)/sum(weight), \n            sd = sqrt(sum(weight*(spread - avg)^2)/((n - 1)/n*sum(weight))),\n            n_eff = sum(weight)^2/sum(weight^2))\n\nWe assume the pollster or house effect standard deviation \\(\\sigma_h\\) is the same across states and take the median of those based on an effective sample size larger or equal to 5:\n\npolls$sd &lt;- with(polls, median(sd[n_eff &gt;= 5], na.rm = TRUE))\n\n\n\n\n\n\n\nIn the Linear Models section of the book, we explore a more statistically rigorous method for estimating standard deviations.\n\n\n\nNext we use the command left_join to combine the results data frame containing the electoral votes for each state and the polls data frame containing the summary statistics:\n\nresults &lt;- left_join(results, polls, by = \"state\")\n\nConstructing priors\nTo make probabilistic arguments, we will use a Bayesian model. To do this we need means and standard deviations for the prior distribution of each state. We will assume a normal distribution for the prior and use 2012 election results to construct priors means. Specifically we compute the difference between Democrat (Obama) and Republican (Romney) candidates:\n\nresults &lt;- results_us_election_2012 |&gt; \n  mutate(theta_0 = (obama - romney)/100) |&gt; \n  select(state, theta_0) |&gt; \n  right_join(results, by = \"state\")\n\nAs done for the popular vote prior, we assign a prior standard deviation of 5% or \\(\\tau=0.05\\).\n\nresults$tau &lt;- 0.05\n\nComputing posterior distributions\nWith the prior distribution established, we can calculate the posterior distributions for each state. In some states, the outcome is considered highly predictable, with one party (Republican or Democrat) almost certain to win. As a result, there are no polls conducted in these states. In such cases, the posterior distribution remains the same as the prior distribution.\n\nresults &lt;- results |&gt;\n  mutate(B = sd^2/n_eff/((sd^2/n_eff) + tau^2),\n         posterior_mean = if_else(is.na(avg), theta, B*theta_0 + (1 - B)*avg),\n         posterior_se = if_else(is.na(avg), tau, sqrt(1/(n_eff/sd^2 + 1/tau^2))))\n\nMonte Carlo simulation\nWe then use Monte Carlo to generate 50,000 election day results \\(\\theta\\) for each state. For each iteration, we examine \\(\\theta\\) for each iteration. If \\(\\theta&lt;0\\) Trump receives all the electoral votes for that state in that iteration. We assume that the poll results from each state are independent.\n\nset.seed(1983)\nB &lt;- 50000\nn_states &lt;- nrow(results)\ntrump_ev &lt;- replicate(B,{\n  theta &lt;- with(results, rnorm(n_states, posterior_mean, posterior_se))\n  sum(results$electoral_votes[theta &lt; 0])\n})\nmean(trump_ev &gt; 269)\n#&gt; [1] 0.00322\n\nThis model gives Trump less than a 1% chance of winning, similar to the prediction made by the Princeton Election Consortium. We now know it was quite off. What happened?\nThe model above ignores the possibility of a systematic polling error and, incorrectly, assumes the results from different states are independent. To correct this, we assume that the systematic error term has a standard deviation of 3%. For each iteration we generate systematic error at random and add it to result for all states.\n\nsigma_b &lt;- 0.03\ntrump_ev_2 &lt;- replicate(B, {\n  bias &lt;- rnorm(1, 0, sigma_b) \n  mu &lt;- with(results, rnorm(n_states, posterior_mean, posterior_se))\n  mu &lt;- mu + bias\n  sum(results$electoral_votes[mu &lt; 0])\n})\nmean(trump_ev_2 &gt; 269)\n#&gt; [1] 0.243\n\nThis gives Trump almost a 25% change of winning, which turned out to be a much more sensible estimate, and closer to the prediction made by FiveThirtyEight of 29%.\nRegional correlation\nPolling faced significant criticism after the 2016 election for “getting it wrong”. However, the polling average actually predicted the final result with notable accuracy, as is typically the case.\n\n\n\n\n\n\n\n\nIn fact, the polling average got the sign of the difference wrong in only five out of the 50 states.\n\ntmp |&gt; filter(sign(spread) != sign(avg)) |&gt;\n  select(state, avg, spread) |&gt;\n  mutate(across(-state, ~round(.,1))) |&gt;\n  setNames(c(\"State\", \"Polling average\", \"Actual result\"))\n#&gt;            State Polling average Actual result\n#&gt; 1        Florida             0.1          -1.2\n#&gt; 2   Pennsylvania             2.0          -0.7\n#&gt; 3       Michigan             3.3          -0.2\n#&gt; 4 North Carolina             0.9          -3.7\n#&gt; 5      Wisconsin             5.6          -0.8\n\nHowever, notice that the errors were all in the same direction, suggesting a systematic polling error. However, the scatter plot reveals several points above the identity line, indicating deviations from the assumption of uniform bias across all states. A closer inspection shows that the bias varies by geographical region, with some areas experiencing stronger effects than others. This pattern is further confirmed by plotting the differences across regions directly:\n\n\n\n\n\n\n\n\nAdvanced forecasting models, like FiveThirtyEight’s, recognize that systematic polling errors often vary by region. To reflect this in a model, we can group states into regions and introduce a regional error term. Since states within the same region share this error, it creates correlation between their outcomes.\nFinal results\nMore sophisticated models also account for variability by using distributions that allow for more extreme events than a normal distribution. For example, a t-distribution with small number of degrees of freedom can capture these rare but impactful outcomes.\nBy incorporating these adjustments,regional errors and heavy-tailed distributions—our model produces a probability for Trump similar to FiveThirtyEight’s reported 29%. Simulations reveal that factoring in regional polling errors and correlations increases the overall variability in the results.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#forecasting",
    "href": "inference/hierarchical-models.html#forecasting",
    "title": "12  Hierarchical Models",
    "section": "\n12.6 Forecasting",
    "text": "12.6 Forecasting\nForecasters aim to predict election outcomes well before election day, updating their projections as new polls are released. However, a key question remains: How informative are polls conducted weeks before the election about the final outcome? To address this, we examine how poll results vary over time and how this variability affects forecasting accuracy.\nTo make sure the variability we observe is not due to pollster effects, let’s study data from one pollster:\n\none_pollster &lt;- polls_us_election_2016 |&gt; \n  filter(pollster == \"Ipsos\" & state == \"U.S.\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nSince there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation. But the empirical standard deviation is higher than the highest possible theoretical estimate:\n\none_pollster |&gt; mutate(p_hat = (spread + 1)/2, N = samplesize) |&gt;\n  summarize(empirical = sd(spread), \n            theoretical = median(2*sqrt(p_hat*(1 - p_hat)/N))) \n#&gt;   empirical theoretical\n#&gt; 1    0.0403      0.0277\n\nFurthermore, the spread data does not look normal as the theory would predict:\n\n\n\n\n\n\n\n\nThe models we have described include pollster-to-pollster variability and sampling error. But this plot is for one pollster and the variability we see is certainly not explained by sampling error. Where is the extra variability coming from? The following plots make a strong case that it comes from time fluctuations not accounted for by the theory that assumes \\(p\\) is fixed:\n\n\n\n\n\n\n\n\nSome of the peaks and valleys in the data align with major events such as party conventions, which typically give candidates a temporary boost. If we generate the same plot for other pollsters, we find that these patterns are consistent across several of them. This suggests that any forecasting model should include a term to account for time effects. The variability of this term would itself depend on time, since as election day approaches, its standard deviation should decrease and approach zero.\nPollsters atry to estimate trends from these data and incorporate them into their predictions. We can model the time trend with a smooth function and then use this to improve predictions. There is a variety of methods for estimating trends which we discuss in Chapter 29 in the Machine Learning part of the book.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#exercises",
    "href": "inference/hierarchical-models.html#exercises",
    "title": "12  Hierarchical Models",
    "section": "\n12.7 Exercises",
    "text": "12.7 Exercises\n1. Create this table:\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state != \"U.S.\" & enddate &gt;= \"2016-10-31\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nNow, for each poll, use the CLT to create a 95% confidence interval for the spread reported by each poll. Call the resulting object cis with columns lower and upper for the limits of the confidence intervals. Use the select function to keep the columns state, startdate, enddate, pollster, grade, spread, lower, upper.\n2. You can add the final result to the cis table you just created using the left_join function like this:\n\nadd &lt;- results_us_election_2016 |&gt; \n  mutate(actual_spread = clinton/100 - trump/100) |&gt; \n  select(state, actual_spread)\ncis &lt;- cis |&gt; \n  mutate(state = as.character(state)) |&gt; \n  left_join(add, by = \"state\")\n\nNow, determine how often the 95% confidence interval includes the election night result stored in actual_spread.\n3. Repeat this, but show the proportion of hits for each pollster. Consider only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster. Hint: Use n=n(), grade = grade[1] in the call to summarize.\n4. Repeat exercise 3, but instead of pollster, stratify by state. Note that here we can’t show grades.\n5. Make a barplot based on the result of exercise 4. Use coord_flip.\n6. Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same. Call the object resids. Hint: Use the function sign.\n7. Create a plot like in exercise 5, but for the proportion of times the sign of the spread agreed with the election night result.\n8. In exercise 7, we see that for most states the polls had it right 100% of the time. For only 9 states did the polls miss more than 25% of the time. In particular, notice that in Wisconsin every single poll got it wrong. In Pennsylvania and Michigan, more than 90% of the polls had the signs wrong. Make a histogram of the errors. What is the median of these errors?\n9. We see that at the state level, the median error was 3% in favor of Clinton. The distribution is not centered at 0, but at 0.03. This is related to the general bias described in Section 12.2. Create a boxplot to see if the bias was general to all states or it affected some states differently. Use filter(grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) to only include pollsters with high grades.\n10. In April 2013, José Iglesias, a professional baseball player was starting his career. He was performing exceptionally well, with an excellent batting average (AVG) of .450. The batting average statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. José had 9 successes out of 20 tries. An AVG of .450 means José has been successful 45% of the times he has batted, which is rather high historically speaking. In fact, no one has finished a season with an AVG of .400 or more since Ted Williams did it in 1941! We want to predict José’s batting average at the end of the season after players have had about 500 tries or at bats. With the frequentist techniques, we have no choice but to predict that his AVG will be .450 at the end of the season. Compute a confidence interval for the success rate.\n12. Despite the frequentist prediction of \\(.450\\), not a single baseball enthusiast would make this prediction. Why is this? One reason is that they know the estimate has much uncertainty. However, the main reason is that they are implicitly using a hierarchical model that factors in information from years of following baseball. Use the following code to explore the distribution of batting averages in the three seasons prior to 2013, and describe what this tells us.\n\nlibrary(tidyverse)\nlibrary(Lahman)\nfilter(Batting, yearID %in% 2010:2012) |&gt; \n  mutate(AVG = H/AB) |&gt; \n  filter(AB &gt; 500) |&gt; \n  ggplot(aes(AVG)) +\n  geom_histogram(color = \"black\", binwidth = .01) +\n  facet_wrap( ~ yearID)\n\n13. So is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both luck and talent. But how much of each? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential. The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, \\(\\theta\\). Then, we see 20 random outcomes with success probability \\(\\theta\\). What model would you use for the first level of your hierarchical model?\n14. Describe the second level of the hierarchical model.\n15. Apply the hierarchical model to José’s data. Suppose we want to predict his innate ability in the form of his true batting average \\(\\theta\\). Write down the distributions of the hierarchical model.\n16. We now are ready to compute a the distribution of \\(\\theta\\) conditioned on the observed data \\(\\bar{Y}\\). Compute the expected value of \\(\\theta\\) given the current average \\(\\bar{Y}\\), and provide an intuitive explanation for the mathematical formula.\n17. We started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 \\(\\pm\\) 0.220. Construct a credible interval for \\(\\theta\\) based on the hierarchical model.\n18. The credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José, as we are predicting he will be just slightly above average. Interestingly, the Red Sox traded José to the Detroit Tigers in July. Here are José Iglesias’ batting averages for the next five months:\n\n\nMonth\nAt Bat\nHits\nAVG\n\n\n\nApril\n20\n9\n.450\n\n\nMay\n26\n11\n.423\n\n\nJune\n86\n34\n.395\n\n\nJuly\n83\n17\n.205\n\n\nAugust\n85\n25\n.294\n\n\nSeptember\n50\n10\n.200\n\n\nTotal w/o April\n330\n97\n.293\n\n\n\nWhich of the two approaches provided a better prediction?",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#footnotes",
    "href": "inference/hierarchical-models.html#footnotes",
    "title": "12  Hierarchical Models",
    "section": "",
    "text": "https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html↩︎\nhttps://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/↩︎\nhttps://projects.fivethirtyeight.com/2016-election-forecast/↩︎",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical Models</span>"
    ]
  },
  {
    "objectID": "inference/hypothesis-testing.html",
    "href": "inference/hypothesis-testing.html",
    "title": "13  Hypothesis Testing",
    "section": "",
    "text": "13.1 p-values\nIn scientific studies, you’ll often see phrases like “the results are statistically significant”. This points to a technique called hypothesis testing, where we use p-values, a type of probability, to test our initial assumption or hypothesis.\nIn hypothesis testing, rather than providing an estimate of the parameter we’re studying, we provide a probability that serves as evidence supporting or contradicting a specific hypothesis. The hypothesis usually involves whether a parameter is different from a predetermined value (often 0).\nHypothesis testing is used when you can phrase your research question in terms of whether a parameter differs from this predetermined value. It’s applied in various fields, asking questions such as: Does a medication extend the lives of cancer patients? Does an increase in gun sales correlate with more gun violence? Does class size affect test scores?\nTake, for instance, the previously used example with colored beads. We might not be concerned about the exact proportion of blue beads, but instead ask: Are there more blue beads than red ones? This could be rephrased as asking if the proportion of blue beads is more than 0.5.\nThe initial hypothesis that the parameter equals the predetermined value is called the null hypothesis. It’s popular because it allows us to focus on the data’s properties under this null scenario. Once data is collected, we estimate the parameter and calculate the p-value, which is the probability of the estimate being as extreme as observed if the null hypothesis is true. If the p-value is small, it indicates the null hypothesis is unlikely, providing evidence against it.\nWe will see more examples of hypothesis testing in Chapter 17.\nSuppose we take a random sample of \\(N=100\\) and we observe \\(52\\) blue beads, which gives us \\(\\bar{X} = 0.52\\). This seems to be pointing to there being more blue than red beads since 0.52 is larger than 0.5. However, we know there is chance involved in this process and we could get a 52 even when the actual proability is 0.5. We call the assumption that the probability is 0.5, \\(\\pi = 0.5\\), a null hypothesis. The null hypothesis is the skeptic’s hypothesis.\nWe have observed a random variable \\(\\bar{X} = 0.52\\), and the p-value is the answer to the question: How likely is it to see a value this large, when the null hypothesis is true? If the p-value is small enough, we reject the null hypothesis and say that the results are statistically significant.\nTo obtain a p-value for our example, we write:\n\\[\\mathrm{Pr}(\\mid \\bar{X} - 0.5 \\mid &gt; 0.02 ) \\]\nassuming the \\(\\pi=0.5\\). Under the null hypothesis we know that:\n\\[\n\\sqrt{N}\\frac{\\bar{X} - 0.5}{\\sqrt{0.5(1-0.5)}}\n\\]\nis standard normal. We, therefore, can compute the probability above, which is the p-value.\n\\[\\mathrm{Pr}\\left(\\sqrt{N}\\frac{\\mid \\bar{X} - 0.5\\mid}{\\sqrt{0.5(1-0.5)}} &gt; \\sqrt{N} \\frac{0.02}{ \\sqrt{0.5(1-0.5)}}\\right)\\]\nN &lt;- 100\nz &lt;- sqrt(N)*0.02/0.5\n1 - (pnorm(z) - pnorm(-z))\n#&gt; [1] 0.689\nIn this case, there is actually a large chance of seeing 52 or larger under the null hypothesis.\nKeep in mind that there is a close connection between p-values and confidence intervals. In our example, if a 95% confidence interval does not include 0.5, we know that the p-value must be smaller than 0.05. In general, we can show mathematically that if a \\((1-\\alpha)\\times 100\\)% confidence interval does not contain the null hypothesis value, the null hypothesis is rejected with a p-value as small or smaller than \\(\\alpha\\). So statistical significance can be determined from confidence intervals.\nTo learn more about p-values, you can consult any statistics textbook. However, in general, we prefer reporting confidence intervals over p-values because it gives us an idea of the size of the estimate. If we just report the p-value, we provide no information about the significance of the finding in the context of the problem. For this reason, we recommend avoiding p-values whenever you can compute a confidence interval.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "inference/hypothesis-testing.html#p-values",
    "href": "inference/hypothesis-testing.html#p-values",
    "title": "13  Hypothesis Testing",
    "section": "",
    "text": "We use \\(\\pi\\) to represent the probability of drawing a blue bead, instead of \\(p\\) as in previous sections, to avoid confusion between the parameter \\(p\\) and the p in p-value.\n\n\n\n\n\nThe p-value of 0.05 as a threshold for statistical significance is conventionally used in many areas of research. A cutoff of 0.01 is also used to define highly significance. The choice of 0.05 is somewhat arbitrary and was popularized by the British statistician Ronald Fisher in the 1920s. We do not recommend using these cutoff without justification and recommend avoiding the phrase statistically significant.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "inference/hypothesis-testing.html#power",
    "href": "inference/hypothesis-testing.html#power",
    "title": "13  Hypothesis Testing",
    "section": "\n13.2 Power",
    "text": "13.2 Power\nPollsters are not successful at providing correct confidence intervals, but rather at predicting who will win. When we took a 25 bead sample size, the confidence interval for the spread:\n\nN &lt;- 25\nx_hat &lt;- 0.48\n(2*x_hat - 1) + c(-1.96, 1.96)*2*sqrt(x_hat*(1 - x_hat)/N)\n#&gt; [1] -0.432  0.352\n\nincluded 0. If this were a poll and we were forced to make a declaration, we would have to say it was a “toss-up”.\nOne problem with our poll results is that, given the sample size and the value of \\(\\pi\\), we would have to sacrifice the probability of an incorrect call to create an interval that does not include 0.\nThis does not mean that the election is close. It only means that we have a small sample size. In statistical textbooks, this is called lack of power. In the context of polls, power is the probability of detecting spreads different from 0.\nBy increasing our sample size, we lower our standard error, and thus, have a much better chance of detecting the direction of the spread.\nStatistical textbooks provide much more detail on hypothesis testing, including formal definitions of p-values, statistical power, and related concepts. The Recommended Reading section lists texts that cover these topics in depth for readers interested in exploring them further.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "inference/hypothesis-testing.html#exercises",
    "href": "inference/hypothesis-testing.html#exercises",
    "title": "13  Hypothesis Testing",
    "section": "\n13.3 Exercises",
    "text": "13.3 Exercises\n1. Generate a sample of size \\(N=1000\\) from an urn model with 50% blue beads:\n\nN &lt;- 1000\npi0 &lt;- 0.5 #we use pi0 to avoid the reserved constant pi\nx &lt;- rbinom(N, 1, pi0)\n\nthen, compute a p-value if \\(\\pi=0.5\\). Repeat this 10,000 times and report how often the p-value is lower than 0.05? How often is it lower than 0.01?\n2. Make a histogram of the p-values you generated in exercise 1.\n\nThe p-values are all 0.05.\nThe p-values are normally distributed; CLT seems to hold.\nThe p-values are uniformly distributed.\nThe p-values are all less than 0.05.\n\n3. Demonstrate mathematically why we see the histogram observed in Exercise 2.\nHint: To compute a p-value, we first calculate a test statistic \\(Z\\). By the Central Limit Theorem (CLT), \\(Z\\) approximately follows a standard normal distribution under the null hypothesis.\nThe p-value is then computed as:\n\\[\np = 2\\{1 - \\Phi(|z|)\\},\n\\]\nwhere \\(z\\) is the observed value of \\(Z\\) and \\(\\Phi(z)\\) is the cumulative distribution function (CDF) of the standard normal distribution (pnorm(z) in R).\nTo understand the distribution of the p-values, consider the probability that a randomly generated p-value is less than or equal to some threshold \\(a\\) between 0 and 1:\n\\[\n\\Pr(p \\leq a) = \\Pr\\big( 2\\{1 - \\Phi(|Z|)\\} \\leq a \\big).\n\\]\nRemember, if \\(\\Pr(p \\leq a) = a\\) for all \\(a\\) in \\([0,1]\\), then \\(p\\) follows a uniform distribution. Show that this relationship holds when \\(Z\\) is standard normal.\n4. Generate a sample of size \\(N=1000\\) from an urn model with 52% blue beads:\n\nN &lt;- 1000 \npi0 &lt;- 0.52\nx &lt;- rbinom(N, 1, pi0)\n\nCompute a p-value to test if \\(\\pi=0.5\\). Repeat this 10,000 times and report how often the p-value is larger than 0.05? Note that you are computing 1 - power.\n5. Repeat exercise for but for the following values:\n\nvalues &lt;- expand.grid(N = c(25, 50, 100, 500, 1000), pi = seq(0.51 ,0.75, 0.01))\n\nPlot power as a function of \\(N\\) with a different color curve for each value of pi0.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "inference/bootstrap.html",
    "href": "inference/bootstrap.html",
    "title": "14  Bootstrap",
    "section": "",
    "text": "14.1 Example: median income\nCLT provides a useful approach to building confidence intervals and performing hypothesis testing. However, it does not always apply. Here we provide a short introduction to an alternative approach to estimating the distribution of an estimate that does not rely on CLT.\nSuppose the income distribution of your population is as follows:\nset.seed(1995)\nn &lt;- 10^6\nincome &lt;- 10^(rnorm(n, log10(45000), log10(3)))\nhist(income/10^3, nclass = 1000)\nThe population median is:\nmedian(income)\n#&gt; [1] 44939\nSuppose we don’t have access to the entire population but want to estimate the population median, denoted by \\(m\\). We take a random sample of 100 observations and use the sample median, \\(M\\), as our estimate of \\(m\\):\nN &lt;- 100\nx &lt;- sample(income, N)\nmedian(x)\n#&gt; [1] 38461\nThe question now becomes: how can we assess the uncertainty in this estimate? In other words, how do we compute a standard error and construct a confidence interval for \\(m\\) based on our sample?\nIn the following sections, we introduce the bootstrap, a powerful resampling method that allows us to estimate variability and construct confidence intervals without relying on strong distributional assumptions.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "inference/bootstrap.html#confidence-intervals-for-the-median",
    "href": "inference/bootstrap.html#confidence-intervals-for-the-median",
    "title": "14  Bootstrap",
    "section": "\n14.2 Confidence intervals for the median",
    "text": "14.2 Confidence intervals for the median\nCan we construct a confidence interval? What is the distribution of \\(M\\)?\nBecause we are simulating the data, we can use a Monte Carlo simulation to learn the actual distribution of \\(M\\).\n\nm &lt;- replicate(10^4, {\n  x &lt;- sample(income, N)\n  median(x)\n})\nhist(m, nclass = 30)\nqqnorm(scale(m)); abline(0,1)\n\n\n\n\n\n\n\n\n\nIf we know this distribution, we can construct a confidence interval. The problem here is that, as we have already described, in practice we do not have access to the distribution. In previous sections we have used CLT, but what we learned applies to averages and here we are interested in the median. We can see that the 95% confidence interval based on CLT\n\nmedian(x) + 1.96*sd(x)/sqrt(N)*c(-1, 1)\n#&gt; [1] 21018 55905\n\nis quite different from the confidence interval we would generate if we knew the actual distribution of \\(M\\):\n\nquantile(m, c(0.025, 0.975))\n#&gt;  2.5% 97.5% \n#&gt; 34438 59050\n\nThe bootstrap allows us to approximate a Monte Carlo simulation even when we do not have access to the full population distribution. The idea is straightforward: we treat the observed sample as if it were the population. From this sample, we draw new datasets of the same size, with replacement, and compute the statistic of interest, in this case, the median, for each resampled dataset. These resampled datasets are called bootstrap samples.\nIn many practical situations, the distribution of the statistics computed from bootstrap samples provides a good approximation to the sampling distribution of the original statistic. This approximation allows us to estimate variability, compute standard errors, and construct confidence intervals without knowing the true underlying distribution.\nThe following code demonstrates how to generate bootstrap samples and approximate the sampling distribution of the median:\n\nm_star &lt;- replicate(10^4, {\n  x_star &lt;- sample(x, N, replace = TRUE)\n  median(x_star)\n})\n\nNote a confidence interval constructed with the bootstrap is much closer to one constructed with the theoretical distribution:\n\nquantile(m_star, c(0.025, 0.975))\n#&gt;  2.5% 97.5% \n#&gt; 30253 56909",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "inference/bootstrap.html#exercises",
    "href": "inference/bootstrap.html#exercises",
    "title": "14  Bootstrap",
    "section": "\n14.3 Exercises",
    "text": "14.3 Exercises\n1. Generate a random dataset like this:\n\ny &lt;- rnorm(100, 0, 1)\n\nEstimate the 75th quantile, which we know is:\n\nqnorm(0.75)\n\nwith the sample quantile:\n\nquantile(y, 0.75)\n\nRun a Monte Carlo simulation to learn the expected value and standard error of this random variable.\n2. In practice, we can’t run a Monte Carlo simulation because we don’t know if rnorm is being used to simulate the data. Use the bootstrap to estimate the standard error using just the initial sample y. Use 10 bootstrap samples.\n3. Redo exercise 2, but with 10,000 bootstrap samples.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "inference/reading-inference.html",
    "href": "inference/reading-inference.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "Freedman, D., Pisani, R., & Purves, R. (2007). Statistics (4th ed.).\nEmphasizes conceptual understanding of estimation and uncertainty. Especially strong on interpreting standard errors and margins of error.\nMoore, D. S., McCabe, G. P., & Craig, B. A. (2017). Introduction to the Practice of Statistics (9th ed.).\nA classic intro text that develops inference from sampling distributions through confidence intervals and practical interpretation.\nBerger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis (2nd ed.). Springer-Verlag.\nA classic and comprehensive reference that develops Bayesian methods from first principles of decision theory. Recommended for readers who wish to explore the mathematical foundations in more depth.\nNate Silver (2016). How FiveThirtyEight’s Election Forecast Works.\nFiveThirtyEight’s official explanation of the 2016 model, including poll weighting, correlated errors, and simulation methodology.\nhttps://fivethirtyeight.com/features/how-fivethirtyeights-election-forecast-works/\nFiveThirtyEight (2020). How Our Presidential Forecast Works (2020 Edition).\nDetails updates to the 2020 model, including adjustments for state and national polling correlation, house effects, and uncertainty in turnout and the Electoral College.\nhttps://fivethirtyeight.com/features/how-our-presidential-forecast-works/\nAndrew Gelman (2020). Election Forecasting: Why We’re Not as Sure as We Think.\nA statistical perspective on model uncertainty, correlated polling errors, and the interpretation of forecast probabilities.\nhttps://statmodeling.stat.columbia.edu/2020/11/02/election-forecasting-why-were-not-as-sure-as-we-think/\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning (2nd ed.). Springer.\nSee the chapter on resampling methods for a clear, concise introduction to the bootstrap with intuitive examples and code.\nEfron, B., & Tibshirani, R. J. (1993). An Introduction to the Bootstrap. Chapman & Hall/CRC.\nThe classic, authoritative treatment. Develops the bootstrap from first principles, with theory, practical guidance, and many examples.",
    "crumbs": [
      "Statistical Inference",
      "Recommended Reading"
    ]
  },
  {
    "objectID": "linear-models/intro-to-linear-models.html",
    "href": "linear-models/intro-to-linear-models.html",
    "title": "Linear Models",
    "section": "",
    "text": "Up to this point, the book has focused mainly on datasets with a single variable. In real data analysis, however, we are often interested in the relationship between two or more variables. In this part of the book, we introduce linear models, a general framework that unifies methods for studying associations among variables, including simple and multivariable regression, treatment effect models, and association tests. We will use several case studies to illustrate these ideas. We will examine whether height is hereditary (15  Introduction to Regression), whether a high-fat diet makes mice heavier (17  Treatment Effect Models), whether there is gender bias in research funding in the Netherlands (Association Tests) and how to build a baseball team on a budget (20  Multivariable Regression). We also include a chapter on the important concept that association is not causation (19  Association Is Not Causation), with a detailed discussion and examples of the challenges that arise when interpreting relationships between variables.",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "linear-models/regression.html",
    "href": "linear-models/regression.html",
    "title": "15  Introduction to Regression",
    "section": "",
    "text": "15.1 Case study: is height hereditary?\nTo understand the concepts of correlation and simple regression, we turn to the dataset that gave birth to regression itself. The example comes from genetics. Francis Galton1 studied variation and heredity in human traits, collecting data from families to investigate how characteristics are passed from parents to children. Through this work, he introduced the ideas of correlation and regression and explored how pairs of variables that follow a normal distribution are related. Of course, when Galton collected his data, our understanding of genetics was far more limited than it is today. A central question he sought to answer was: How well can we predict a child’s height based on the parents’ height? The statistical tools he developed to address this question remain foundational and are now used in a wide range of applications. We conclude the chapter by discussing a concept known as the regression fallacy, a common misunderstanding that arises when interpreting regression results, and we illustrate it with a real data example from the world of sports.\nWe have access to Galton’s family height data through the HistData package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son from each family. We use the data.table package here because it allows for more succint code:\nlibrary(data.table)\nlibrary(HistData)\n\nset.seed(1983)\ngalton &lt;- as.data.table(GaltonFamilies)\ngalton &lt;- galton[gender == \"male\", .SD[sample(.N, 1)], by = family]\ngalton &lt;- galton[, .(father, son = childHeight)]\nSuppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:\ngalton[, .(f_avg=mean(father), f_sd=sd(father), s_avg=mean(son), s_sd=sd(son))]\n#&gt;    f_avg  f_sd s_avg  s_sd\n#&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n#&gt; 1:  69.1  2.55  69.2  2.72\nHowever, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.\ngalton |&gt; ggplot(aes(father, son)) + geom_point(alpha = 0.5)\nWe will learn that the correlation coefficient is an informative summary of how two variables move together and then motivate simple regression by noting how this can be used to predict one variable using the other.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#sec-corr-coef",
    "href": "linear-models/regression.html#sec-corr-coef",
    "title": "15  Introduction to Regression",
    "section": "\n15.2 The correlation coefficient",
    "text": "15.2 The correlation coefficient\nThe correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as the average of the product of the standardized values:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\n\\]\nwith \\(\\mu_x, \\mu_y\\) the averages of \\(x_1,\\dots, x_n\\) and \\(y_1, \\dots, y_n\\), respectively, and \\(\\sigma_x, \\sigma_y\\) the standard deviations. The Greek letter for \\(r\\), \\(\\rho\\) is commonly used in statistics books to denote the correlation. It is not a coincidence that \\(r\\) is the first letter in regression. Soon we learn about the connection between correlation and regression.\nWe can represent the formula above with R code using:\n\nrho &lt;- mean(scale(x) * scale(y))\n\nTo see why this equation captures how two variables move together, note that each term \\(\\frac{x_i - \\mu_x}{\\sigma_x}\\) represents how many standard deviations the value \\(x_i\\) is from the mean of \\(x\\), and \\(\\frac{y_i - \\mu_y}{\\sigma_y}\\) does the same for \\(y_i\\) relative to the mean of \\(y\\). If \\(x\\) and \\(y\\) are unrelated, the product of these standardized values, \\(\\left(\\frac{x_i - \\mu_x}{\\sigma_x}\\right)\\left(\\frac{y_i - \\mu_y}{\\sigma_y}\\right)\\), will be positive (positive times positive or negative times negative) about as often as it is negative (positive times negative or negative times positive). When averaged, these products cancel out, giving a correlation near zero. If \\(x\\) and \\(y\\) tend to increase or decrease together, most products will be positive, resulting in a positive correlation. If one increases when the other decreases, most products will be negative, resulting in a negative correlation.\nThe correlation coefficient is always between -1 and 1. We can show this mathematically and we include as an exercise for you to prove.\nFor other pairs, the correlation is between -1 and 1. The correlation, computed with the function cor, between father and son’s heights is about 0.5:\n\ngalton[, cor(father, son)]\n#&gt; [1] 0.43\n\n\n\n\n\n\n\nThe function cor(x, y) computes the sample correlation, which divides the sum of products by length(x)-1 rather than length(x). The rationale for this is similar to the reason we divide by length(x)-1 when computing the sample standard deviation sd(x). Namely, this adjustment helps account for the degrees of freedom in the sample, which is necessary for unbiased estimates.\n\n\n\nTo see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99:\n\n\n\n\n\n\n\n\nSample correlation is a random variable\nBefore we continue connecting correlation to regression, let’s remind ourselves about random variability.\nIn most data analysis projects, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest, but rather for a random sample. As with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.\nBy way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:\n\nr &lt;- galton[sample(.N, 25, replace = TRUE), cor(father, son)]\n\nr is a random variable. We can run a Monte Carlo simulation to see its distribution:\n\nN &lt;- 25\nr &lt;- replicate(1000, galton[sample(.N, N, replace = TRUE), cor(father, son)])\nhist(r, breaks = 20)\n\n\n\n\n\n\n\nWe see that the expected value of r is the population correlation:\n\nmean(r)\n#&gt; [1] 0.427\n\nand that it has a relatively high standard error relative to the range of values r can take:\n\nsd(r)\n#&gt; [1] 0.161\n\nSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nAlso, note that because the sample correlation is an average of independent draws, the central limit actually applies: for large enough \\(N\\), the distribution of r is approximately normal with expected value \\(\\rho\\) and standard deviation \\(\\frac{1-r^2}{\\sqrt{N}}\\). Note that the derivation of this standard deviation is complex and not shown here.\nIn our example, the qqplot below shows that \\(N=25\\) is not large enough for the normal approximation to work well. The tail of the observed distribution falls consistently below what the theoretical normal distribution predicts:\n\n\n\n\n\n\n\n\nIf you increase \\(N\\), you will see the distribution converging to normal.\nCorrelation is not always a useful summary\nCorrelation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:\n\n\n\n\n\n\n\n\nTo understand when correlation is meaningful as a summary statistic, we return to the example of predicting a son’s height using his father’s height. This example will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#sec-conditional-expectation",
    "href": "linear-models/regression.html#sec-conditional-expectation",
    "title": "15  Introduction to Regression",
    "section": "\n15.3 Conditional expectations",
    "text": "15.3 Conditional expectations\nGalton wanted to understand how well we can predict a child’s height from the parents’ height. To begin, we frame this as a prediction problem.\nSuppose we are asked to guess the height of a randomly selected son, but we are not told the father’s height. Because the distribution of sons’ heights is approximately normal, the most reasonable single-number prediction is the population mean, \\(\\mu_y\\), since this is the value around which the data are most concentrated. Later, in Chapter 28, we explain that the mean also has desirable mathematical properties for prediction: it minimizes expected squared error.\nTherefore, without any information about the father, our best prediction for the son’s height is simply the average height of sons in the population. In practice, we approximate this using the sample average, $_y = $ 69.2.\nBut what if we are told that the father is taller than average, say 72 inches tall, do we still guess \\(\\mu_y\\) = 69.2 for the son?\nIt turns out that, if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.\nIn general, we call this approach conditioning. The idea is that we divide a population into groups, or strata, based on the value of one variable, and then compute summaries of the other variable within each group.\nTo describe this mathematically, suppose we have a population of pairs \\((x_1, y_1), \\dots, (x_n, y_n)\\), such as all father and son height pairs in England. We learned that if we pick a random pair \\((X, Y)\\) from this population, the expected value and best predictor of \\(Y\\) is \\(\\mathrm{E}[Y] = \\mu_y\\).\nNow, instead of the entire population, we focus on a specific subgroup defined by a fixed value of \\(X\\). In our example, this is all pairs for which the father’s height is 72 inches. This subgroup is itself a population, so the same ideas apply. The \\(y_i\\) values in this subgroup follow a distribution called the conditional distribution, and this distribution has an expected value called the conditional expectation. In our example, the conditional expectation is the average height of all sons whose fathers are 72 inches tall.\nWe write \\(Y \\mid X = x\\) to mean a random value of \\(Y\\) selected only from the pairs where the event \\(X = x\\) holds. In other words, \\(Y \\mid X = 72\\) represents the random height of a son chosen from among those whose fathers are 72 inches tall.\nThe notation for the conditional expectation is:\n\\[\n\\mathrm{E}[Y \\mid X = x]\n\\]\nwhere \\(x\\) is the fixed value defining the subset. Similarly, we denote the standard deviation within that subset, or strata, as:\n\\[\n\\mathrm{SD}[Y \\mid X = x] = \\sqrt{\\mathrm{Var}(Y \\mid X = x)}\n\\]\nBecause the conditional expectation \\(\\mathrm{E}[Y \\mid X = x]\\) is the best predictor of \\(Y\\) for an individual in the group defined by \\(X = x\\), many data science problems can be viewed as estimating this quantity. The conditional standard deviation, \\(\\mathrm{SD}[Y \\mid X = x]\\), describes how precise that prediction is.\nSo to obtain a prediction we want to estimate \\(E[Y|X=72]\\) using the sample collected by Galton. When using continuous data we often have the challenge that there are not that many data points where \\(X = 72\\):\n\nsum(galton$father == 72)\n#&gt; [1] 8\n\nIf we change the number to 72.5, we get even fewer data points:\n\nsum(galton$father == 72.5)\n#&gt; [1] 1\n\nA practical way to improve estimates of the conditional expectations is to define strata of observations with similar value of \\(x\\). In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:\n\nmean(galton[round(father) == 72]$son)\n#&gt; [1] 70.5\n\nNote that a 72 inch father is taller than average, specifically (72.0 - 69.1)/2.5 = 1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.48 standard deviations larger than the average son. The predicted height for sons of 72 inch fathers have regressed some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be close to the sample correlation. As we will see in a later section, this is not a coincidence.\nIf we want to make a prediction of any height, not just 72 inches, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:\n\ngalton[, .(father_strata = factor(round(father)), son)] |&gt;\nggplot(aes(father_strata, son)) + \n  geom_boxplot() + \n  geom_point()\n\n\n\n\n\n\n\nNot surprisingly, the centers of the groups are increasing with height. Furthermore, these centers appear to follow a linear relationship. Below, we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:\n\n\n\n\n\n\n\n\nNote that while we divided the data into one-inch groups, the sample size in each group is still small and not the same. The groups corresponding to taller fathers are much smaller, which makes their averages more variable.\nThe fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that this line, known as the regression line, improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line, so we also describe Galton’s theoretical justification that helps us understand when to use it.\n\n\n\n\n\n\nConditional expectations are not only useful when the relationship between variables is linear. In fact, much of what we call machine learning can be understood as the task of estimating conditional expectations for complex relationships that go well beyond a straight line. We explore this idea in more detail in the machine learning part of the book, including Chapter 28, which covers conditional expectations and probabilities.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#the-regression-line",
    "href": "linear-models/regression.html#the-regression-line",
    "title": "15  Introduction to Regression",
    "section": "\n15.4 The regression line",
    "text": "15.4 The regression line\nIf we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), our prediction \\(\\hat{y}\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore:\n\\[\n\\left( \\frac{\\hat{y}-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\n\\]\nWe can rewrite it like this:\n\\[\n\\hat{y} = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y\n\\]\nIf there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use \\(x\\) at all for the prediction and simply predict the average \\(\\mu_Y\\). For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the sons regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature.\nWe can add regression lines to plots by converting the formula above to the form \\(\\hat{y} = m + bx\\), which gives us a slope \\(m = \\rho \\frac{\\sigma_y}{\\sigma_x}\\) and intercept \\(b = \\mu_y - m \\mu_x\\):\n\nparams &lt;- galton[, .(mu_x = mean(father), mu_y = mean(son),\n                             s_x  = sd(father), s_y  = sd(son),\n                             r    = cor(father, son))]\n\ngalton |&gt; ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) +\n  geom_abline(slope = with(params, r*s_y/s_x), \n              intercept = with(params, mu_y - r*s_y/s_x*mu_x))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith ggplot we don’t need to calculate all these summaries and can easily add a regression line using the geom_smooth function:\n\ngalton |&gt; ggplot(aes(father, son)) + geom_point() + geom_smooth(method = \"lm\") \n\nIn Section 16.3 we explain what lm means here.\n\n\n\nNote that the regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). You can make same plot, but using standard units like this:\n\ngalton |&gt; ggplot(aes(scale(father), scale(son))) + \n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = r) \n\nLinear regression improves precision\nWe now compare the two approaches to prediction that we have introduced:\n\nRound fathers’ heights to the nearest inch, stratify, and take the average within each group.\nCompute the regression line and use it to predict.\n\nOur goal is to estimate the expected value and the standard error of the predictions obtained with each method. We want the expected value to very close to the true population average and the standard error to be samll. To do this, we use a Monte Carlo simulation. Specifically, we repeatedly draw random samples of size \\(N = 50\\) families from the population, compute both types of predictions for each sample, and then examine the distribution of these repeated estimates. The mean of this distribution provides an estimate of the expected value of each method, and its standard deviation provides an estimate of the standard error, which reflects how variable the prediction is across random samples.\n\nB &lt;- 1000\nN &lt;- 50\n\nconditional_avg &lt;- replicate(B, {\n  dat &lt;- galton[sample(.N, N, replace = TRUE)]\n  dat[round(father) == 72, if (.N) mean(son) else NA]\n})\n\nlinear_regression_prediction &lt;- replicate(B, {\n  dat &lt;- galton[sample(.N, N, replace = TRUE)]\n  dat[, mean(son) + cor(father, son)*(72 - mean(father))/sd(son)*sd(father)]\n})\n\nAlthough the expected values from both methods are nearly the same:\n\nmean(conditional_avg, na.rm = TRUE)\n#&gt; [1] 70.5\nmean(linear_regression_prediction)\n#&gt; [1] 70.4\n\nthe standard error for the linear regression prediction is much smaller:\n\nsd(conditional_avg, na.rm = TRUE)\n#&gt; [1] 1.08\nsd(linear_regression_prediction)\n#&gt; [1] 0.534\n\nThis means that the linear-regression-based prediction is far more stable across repeated samples. The reason is intuitive: the conditional average is based on a relatively small subset of data, the fathers whose heights are about 72 inches, so its estimate is more variable. In some samples, we may even have very few or no observations in that group, which is why we use na.rm=TRUE. The regression line, on the other hand, is estimated using information from the entire dataset, which makes its predictions much more precise.\nSo why not always use the regression line for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The next section provides a justification.\nThe bivariate normal distribution justification\nCorrelation and the regression slope are widely used summary statistics, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases in which the correlation is not a useful summary. But there are many real-life examples.\nThe main way we motivate appropriate use of correlation as a summary, involves the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. As we saw in Section 15.2, they can be thin (high correlation) or circle-shaped (no correlation).\nA more technical way to define the bivariate normal distribution is the following: if\n\n\n\\(X\\) is a normally distributed random variable,\n\n\\(Y\\) is also a normally distributed random variable, and\nthe conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal,\n\nthen the pair is approximately bivariate normal.\nWhen three or more variables have the property that each pair is bivariate normal, we say the variables follow a multivariate normal distribution or that they are jointly normal.\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:\n\ngalton[, .(son, z = round((father - mean(father))/sd(father)))][z %in% -2:2] |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = son)) +\n  facet_wrap( ~ z) \n\n\n\n\n\n\n\nGalton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\\[\n\\mathrm{E}[Y | X=x] = \\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\n\\]\nThis is the regression line, with slope \\(\\rho \\frac{\\sigma_Y}{\\sigma_X}\\) and intercept \\(\\mu_y - \\rho\\mu_X\\frac{\\sigma_Y}{\\sigma_X}\\). It is equivalent to the regression equation we showed earlier which can be written like this:\n\\[\n\\frac{\\mathrm{E}[Y \\mid X=x]  - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line.\nVariance explained\nThe bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\\[\n\\mathrm{SD}[Y \\mid X=x ] = \\sigma_Y \\sqrt{1-\\rho^2}\n\\]\nTo see why this is intuitive, notice that without conditioning we are looking at the variability of all the sons: \\(\\mathrm{SD}(Y) = \\sigma_Y\\). But once we condition, we are only looking at the variability of the sons with a tall, 72 inch father. This group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement has this clear interpretation only when the data is approximated by a bivariate normal distribution.\nThere are two regression lines\nWe computed a regression line to predict the son’s height from father’s height. We used these calculations:\n\nm_1 &lt;- with(params, r*s_y/s_x)\nb_1 &lt;- with(params, mu_y - m_1*mu_x)\n\nwhich gives us the function \\(\\mathrm{E}[Y\\mid X=x] =\\) 37.5 + 0.46 \\(x\\).\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mathrm{E}[Y\\mid X=x] -\\) 37.5 \\(\\} /\\) 0.46.\nWe need to compute \\(\\mathrm{E}[X \\mid Y=y]\\). Since the data is approximately bivariate normal, the theory described earlier tells us that this conditional expectation will follow a line with slope and intercept:\n\nm_2 &lt;- with(params, r*s_x/s_y)\nb_2 &lt;- with(params, mu_x - m_2*mu_y)\n\nUsing these calculations we get \\(\\mathrm{E}[X \\mid Y=y] =\\) 41.2 + 0.4y. Again, we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nHere is a plot showing the two regression lines, with blue for the predicting son heights with father heights, and red for predicting father heights with son heights:",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#the-regression-fallacy",
    "href": "linear-models/regression.html#the-regression-fallacy",
    "title": "15  Introduction to Regression",
    "section": "\n15.5 The regression fallacy",
    "text": "15.5 The regression fallacy\nWikipedia defines the sophomore slump as:\n\nA sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).\n\nIn Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The sophomore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, a Fox Sports article asked “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”\nDoes the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for a widely used measure of success, the batting average, we see that this observation holds true for the top performing ROYs:\n\n\n\n\nnameFirst\nnameLast\nrookie_year\nrookie\nsophomore\n\n\n\nWillie\nMcCovey\n1959\n0.354\n0.238\n\n\nIchiro\nSuzuki\n2001\n0.350\n0.321\n\n\nAl\nBumbry\n1973\n0.337\n0.233\n\n\nFred\nLynn\n1975\n0.331\n0.314\n\n\nAlbert\nPujols\n2001\n0.329\n0.314\n\n\n\n\n\nSo is it “jitters” or a “jinx”? To answer this question, let’s turn our attention to all the players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).\nThe same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\nMiguel\nCabrera\n0.348\n0.313\n\n\nHanley\nRamirez\n0.345\n0.283\n\n\nMichael\nCuddyer\n0.331\n0.332\n\n\nScooter\nGennett\n0.324\n0.289\n\n\nJoe\nMauer\n0.324\n0.277\n\n\n\n\n\nBut these are not rookies! Also, look at what happens to the worst performers of 2013:\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\nDanny\nEspinosa\n0.158\n0.219\n\n\nDan\nUggla\n0.179\n0.149\n\n\nJeff\nMathis\n0.181\n0.200\n\n\nB. J.\nUpton\n0.184\n0.208\n\n\nAdam\nRosales\n0.190\n0.262\n\n\n\n\n\nTheir batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as a sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:\n\n\n\n\n\n\n\n\nThe correlation is 0.46 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(\\hat{y}\\) for any given player that had a 2013 batting average \\(x\\) with:\n\\[ \\frac{\\hat{y} - .255}{.032} = 0.46 \\left( \\frac{x - .261}{.031}\\right) \\]\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of \\(x\\), so it is expected that \\(y\\) will regress to the mean.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#exercises",
    "href": "linear-models/regression.html#exercises",
    "title": "15  Introduction to Regression",
    "section": "\n15.6 Exercises",
    "text": "15.6 Exercises\n1. Show that the correlation coefficient must be less than or equal to 1. Hint: The correlation cannot be higher than in the case of a perfect linear relationship. Consider \\(y_i = a + b x_i\\) with \\(b &gt; 0\\). Substitute \\(a + b x_i\\) for \\(y_i\\) in the correlation formula and simplify.\n2. Show that the correlation coefficient must be greater than or equal to -1. Hint: The correlation cannot be more negative than in the case of a perfectly decreasing linear relationship. Consider \\(y_i = a - b x_i\\) with \\(b &gt; 0\\). Substitute \\(a - b x_i\\) for \\(y_i\\) in the correlation formula and simplify as in Exercise 1.\n3. We performed a Monte Carlo simulation and found that with \\(N=25\\) and the CLT was not a great approximation for the distribution of the sample correlation. Generate correlated data using the mvrnorm function from the MASS package:\n\nlibrary(MASS)\nN &lt;- 25\ndat &lt;- mvrnorm(N, c(0, 0), matrix(c(1, 0.9, 0.9, 1), 2, 2))\ncor(dat[,1], dat[,2])\n\nPerform 10,000 Monte Carlo simulations for N &lt;- c(25, 50, 100, 250, 500, 1000) to determine when the CLT starts becoming a useful approximation.\n4. Repeat exercise 3 but for correlations of 0.9 instead of 0.5.\n5. Repeat exercise 3 but for correlations of 0.1 instead of 0.5.\n6. Load the GaltonFamilies data from the HistData package. The children in each family are listed by gender and then by height. Create a dataset called galton by picking a male and female at random.\n7. Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n8. Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#footnotes",
    "href": "linear-models/regression.html#footnotes",
    "title": "15  Introduction to Regression",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Francis_Galton↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/linear-model-framework.html",
    "href": "linear-models/linear-model-framework.html",
    "title": "\n16  The Linear Model Framework\n",
    "section": "",
    "text": "16.1 The linear model representation\nWe are now ready to understand the title of this part of the book, and specifically the connection between regression and linear models. In the previous chapter, we showed that if two variables follow a bivariate normal distribution, then the conditional expectation of one given the other lies on a straight line, the regression line. In that setting, the linear relationship was not an assumption, but a mathematical consequence of the probability model.\nIn practice, however, we often adopt the linear relationship as a modeling strategy, even when the underlying distributions are not exactly bivariate normal. To work with such models, we need a general mathematical approach for estimating the unknown coefficients. In this chapter, we introduce the least squares estimator (LSE), the standard method used to estimate these parameters. The LSE provides a unified way to fit linear models across a wide range of applications.\nWe end the chapter presenting one example in which the linear model was historically motivated: measurement error models. This example is different from the height data we studied earlier, but the same linear model framework, the same least squares mathematics, and even the same R functions can be used to fit it. Later chapters will introduce additional examples in which linear models play a central role.\nWe note that linear here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of variables. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, \\(3x - 4y + 5z\\) is a linear combination of \\(x\\), \\(y\\), and \\(z\\). We can also add a constant so \\(2 + 3x - 4y + 5z\\) is also a linear combination of \\(x\\), \\(y\\), and \\(z\\).\nWe previously described how if \\(X\\) and \\(Y\\) are bivariate normal, then if we look at only the pairs with \\(X=x\\), then \\(Y \\mid X=x\\) follows a normal distribution with expected value \\(\\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\\), which is a linear function of \\(x\\). Note also that the standard deviation \\(\\sigma_Y \\sqrt{1-\\rho^2}\\) does not depend on \\(x\\). This implies that we can write:\n\\[\nY = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nand assume \\(\\varepsilon\\) follows a normal distribution with expected value 0 and fixed standard deviation, then \\(Y\\) has the same properties as the regression setup gave us: \\(Y\\) follows a normal distribution, the expected value is a linear function of \\(x\\), and the standard deviation does not depend on \\(x\\).\nIf we were to specify a linear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N.\n\\]\nHere \\(x_i\\) is the father’s height, which is fixed (not random) due to the conditioning, and \\(Y_i\\) is the random son’s height that we want to predict. We can further assume that the \\(\\varepsilon_i\\) are independent from each other and all have the same standard deviation.\nIn the linear model framework the \\(x_i\\) are refereed to as the explanatory variables, covariates, or predictors.\nIn the above model we know the \\(x_i\\), but to have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\). We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height \\(x\\). We show how to do this in the next section.\nAlthough this model is exactly the same one we derived earlier by assuming bivariate normal data, a somewhat nuanced difference is that, in the first approach, we assumed the data was bivariate normal and the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)s is not necessarily specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.\nOne reason linear models are popular is that they are easily interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\). Because not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.\nGiven how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable, as it is the predicted height of a son of a father with no height. To make the slope parameter more interpretable, we can rewrite the model slightly as:\n\\[\nY_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N\n\\]\nwith \\(\\bar{x} = 1/N \\sum_{i=1}^N x_i\\) the average of the \\(x\\). In this case, \\(\\beta_0\\) represents the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father.\nLater, specifically in Chapters Chapter 17 ans Chapter 20, we will see how the linear model representation permits us to use the same mathematical frameworks in other contexts and to achieve more complicated goals than predicting one variable from another.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Linear Model Framework</span>"
    ]
  },
  {
    "objectID": "linear-models/linear-model-framework.html#the-linear-model-representation",
    "href": "linear-models/linear-model-framework.html#the-linear-model-representation",
    "title": "\n16  The Linear Model Framework\n",
    "section": "",
    "text": "In statistical textbooks, the \\(\\varepsilon\\) terms are referred to as errors. Historically, this reflected the idea that deviations from the model were due to measurement inaccuracies. Today, the term is used more broadly: the \\(\\varepsilon\\)s represent all variation in the outcome that is not explained by the predictors in the model. This variation may have nothing to do with mistakes. For example, if someone is two inches taller than what we would predict based on their parents’ heights, those two inches are not an error; they are simply natural variation. Although the term error is not ideal in this sense, it remains standard terminology, and we will use it to denote the unexplained variability in the model.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Linear Model Framework</span>"
    ]
  },
  {
    "objectID": "linear-models/linear-model-framework.html#sec-lse",
    "href": "linear-models/linear-model-framework.html#sec-lse",
    "title": "\n16  The Linear Model Framework\n",
    "section": "\n16.2 Least Squares Estimates",
    "text": "16.2 Least Squares Estimates\nFor linear models to be useful, we have to estimate the unknown \\(\\beta\\)s. The standard approach is to find the values that minimize the distance of the fitted model to the data. Specifically, we find the \\(\\beta\\)s that minimize the least squares (LS) equation shown below. For Galton’s data, the LS equation looks like this:\n\\[\nRSS = \\sum_{i=1}^n \\left\\{  y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2\n\\]\nThe quantity we try to minimize is called the residual sum of squares (RSS).\nOnce we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them by placing a hat over the parameters. In our example we use \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nWe will demonstrate how we find these values using the previously defined galton dataset:\n\nlibrary(data.table)\nlibrary(HistData)\n\nset.seed(1983)\ngalton &lt;- as.data.table(GaltonFamilies)[gender == \"male\", .SD[sample(.N, 1)], by = family]\ngalton &lt;- galton[, .(father, son = childHeight)]\n\nLet’s start by writing a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\).\n\nrss &lt;- function(beta0, beta1, data){\n  resid &lt;- galton$son - (beta0 + beta1*galton$father)\n  return(sum(resid^2))\n}\n\nSo for any pair of values, we get an RSS. Here is a plot of the RSS as a function of \\(\\beta_1\\), when we keep the \\(\\beta_0\\) fixed at 25.\n\nbeta1 &lt;- seq(0, 1, length = nrow(galton))\nresults &lt;- data.frame(beta1 = beta1, rss = sapply(beta1, rss, beta0 = 25))\nresults |&gt; ggplot(aes(beta1, rss)) + geom_line() + \n  geom_line(aes(beta1, rss))\n\n\n\n\n\n\n\nWe can see a clear minimum for \\(\\beta_1\\) at around 0.65. However, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.\nTrial and error is not going to work in this case. We could search for a minimum within a fine grid of \\(\\beta_0\\) and \\(\\beta_1\\) values, but this is unnecessarily time-consuming since we can use calculus. Specifically, we take the partial derivatives, set them to 0, and solve for \\(\\beta_0\\) and \\(\\beta_1\\). Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will study these next. To learn the mathematics behind this, you can consult one of the books on linear models in the Recommended Reading section.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Linear Model Framework</span>"
    ]
  },
  {
    "objectID": "linear-models/linear-model-framework.html#sec-lm-function",
    "href": "linear-models/linear-model-framework.html#sec-lm-function",
    "title": "\n16  The Linear Model Framework\n",
    "section": "\n16.3 The lm function",
    "text": "16.3 The lm function\nIn R, we can obtain the least squares estimates for a linear model using the lm function. To fit the model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nwith \\(Y_i\\) being the son’s height and \\(x_i\\) being the father’s height, we can use this code:\n\nfit &lt;- lm(son ~ father, data = galton)\n\nThe most common way we use lm is by using the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~). The intercept is added automatically to the model that will be fit.\nThe object fit includes information about the fit. We can use the function summary to extract a summary of this information:\n\nsummary(fit)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = son ~ father, data = galton)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -9.38  -1.59   0.00   1.83   9.39 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  37.4658     5.0052    7.49  3.2e-12 ***\n#&gt; father        0.4592     0.0724    6.34  1.8e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.46 on 177 degrees of freedom\n#&gt; Multiple R-squared:  0.185,  Adjusted R-squared:  0.181 \n#&gt; F-statistic: 40.2 on 1 and 177 DF,  p-value: 1.82e-09\n\nTo understand some of the terms included in this summary, we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables.\nIn Chapter 20, after describing a more complex case study, we gain further insights into the application of regression in R.\nLSE are random variables\nThe LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\). This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size \\(N=50\\), and compute the regression slope coefficient for each one:\n\nN &lt;- 25\nlse &lt;- replicate(10000, {\n  smpl &lt;-  galton[sample(.N, N, replace = TRUE)]\n  coef(lm(son ~ father, data = smpl))\n})\n\nWe can see the variability of the estimates by plotting their distributions:\n\n\n\n\n\n\n\n\nThe reason these look normal is because the central limit theorem applies here as well: for large enough \\(N\\), the least squares estimates will be approximately normal with expected value \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\nThe standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function. The function summary shows us the standard error estimates:\n\nfit &lt;- lm(son ~ father, data = galton[sample(.N, N, replace = TRUE)])\nsummary(fit)$coef\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)   26.574     17.383    1.53   0.1400\n#&gt; father         0.627      0.249    2.52   0.0193\n\nYou can see that the standard errors estimates reported above are close to the standard errors from the simulation:\n\nwith(lse, c(se_0 = sd(beta_0), se_1 = sd(beta_1)))\n#&gt;   se_0   se_1 \n#&gt; 13.012  0.188\n\nThe summary function also reports t-statistics (t value) and p-values (Pr(&gt;|t|)). The t-statistic is not actually based on the central limit theorem, but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\widehat{\\mathrm{SE}}[\\hat{\\beta}_0]\\) and \\(\\hat{\\beta}_1 / \\widehat{\\mathrm{SE}}[\\hat{\\beta}_1]\\), follow a t-distribution with \\(N-p\\) degrees of freedom, with \\(p\\) the number of parameters in our model. In our example \\(p=2\\), and the two p-values are obtained from testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\), respectively.\nRemember that, as we described in Section 10.2.3, for large enough \\(N\\), the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals using the confint function:\n\nconfint(fit, \"father\", level = 0.95)\n#&gt;        2.5 % 97.5 %\n#&gt; father 0.112   1.14\n\nAlthough we do not show examples in this book, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true.\nPredicted values are random variables\nOnce we fit our model, we can obtain a prediction of \\(Y\\) by plugging the estimates into the regression model. For example, if the father’s height is \\(x\\), then our prediction \\(\\hat{y}\\) for the son’s height will be:\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\nWhen we plot \\(\\hat{y}\\) versus \\(x\\), we see the regression line.\nKeep in mind that the prediction \\(\\hat{y}\\) is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the ggplot2 layer geom_smooth(method = \"lm\") surrounds the regession line using these confidence intervals:\n\ngalton |&gt; ggplot(aes(son, father)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nThe R function predict takes an lm object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:\n\nfit &lt;- lm(son ~ father, data = galton) \ny_hat &lt;- predict(fit, se.fit = TRUE)\nnames(y_hat)\n#&gt; [1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Linear Model Framework</span>"
    ]
  },
  {
    "objectID": "linear-models/linear-model-framework.html#sec-diagnostic-plots",
    "href": "linear-models/linear-model-framework.html#sec-diagnostic-plots",
    "title": "\n16  The Linear Model Framework\n",
    "section": "\n16.4 Diagnostic plots",
    "text": "16.4 Diagnostic plots\nWhen the linear model is assumed, rather than derived, all interpretations depend on the usefulness of the model. The lm function will fit the model and return summaries even when the model is wrong and not useful.\nVisually inspecting residuals, defined as the difference between observed values and predicted values:\n\\[\nr = y - \\hat{y} = y- \\left(\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right),\n\\] and summaries of the residuals, is a powerful way to diagnose if the model is useful. Note that the residuals can be thought of estimates of the errors since:\n\\[\n\\varepsilon = Y - \\left(\\beta_0 + \\beta_1 x_i \\right).\n\\] In fact residuals are often denoted as \\(\\hat{\\varepsilon}\\). This motivates several diagnostic plots. Because we observe \\(r\\), but don’t observe \\(\\varepsilon\\), we based the plots on the residuals.\n\nBecause the errors are assumed not to depend on the expected value of \\(Y\\), a plot of \\(r\\) versus the fitted values \\(\\hat{y}\\) should show no relationship.\nIn cases in which we assume the errors follow a normal distribution, a qqplot of standardized \\(r\\) should fall on a line when plotted against theoretical quantiles.\nBecause we assume the standard deviation of the errors is constant, if we plot the absolute value of the residuals, it should appear constant.\n\nWe prefer plots rather than summaries based on, for example, correlation because, as noted in Section 15.2.2, correlation is not always the best summary of association. The function plot applied to an lm object automatically plots these.\n\nplot(fit, which = 1:3)\n\n\n\n\n\n\n\nThis function can produce six different plots, and the argument which let’s you specify which you want to see. You can learn more by reading the plot.lm help file. However, some of the plots are based on more advanced concepts beyond the scope of this book. To learn more, we recommend an advanced book on regression analysis.\nIn Sections Chapter 17 and Chapter 20, we introduce data analysis challenges in which we may decide to not include certain variables in the model. In these cases, an important diagnostic test is to checks if the residuals are related to variables not included in the model.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Linear Model Framework</span>"
    ]
  },
  {
    "objectID": "linear-models/linear-model-framework.html#measurement-error-models",
    "href": "linear-models/linear-model-framework.html#measurement-error-models",
    "title": "\n16  The Linear Model Framework\n",
    "section": "\n16.5 Measurement error models",
    "text": "16.5 Measurement error models\nHistorically, writing linear models in the form \\(Y = \\beta_0 + \\beta_1 x + \\varepsilon\\) originated in contexts where the \\(\\varepsilon\\) term represented measurement error. Early scientists used least squares to estimate physical constants from repeated measurements affected by instrument noise. Later, Galton and Pearson extended this framework to study relationships between variables, such as the height example we examined earlier.\nIn this section, we briefly present a setting where the measurement-error interpretation is appropriate and show that we can fit the corresponding model using the same mathematics and the same R function. This illustrates one of the key advantages of the linear model framework: a single set of tools applies across many seemingly different situations.\nExample: modeling a falling object\nTo understand these models, imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we currently know and adding some measurement error. The dslabs function rfalling_object generates these simulations:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nfalling_object &lt;- rfalling_object()\n\nThe assistants hand the data to Galileo, and this is what he sees:\n\n\n\n\n\n\n\n\nGalileo does not know the exact equation, but by looking at the plot above, he deduces that the position should follow a parabola, which we can write like this:\n\\[\nf(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n\\]\nThe data does not fall exactly on a parabola. Galileo knows this is due to measurement error. His helpers make mistakes when measuring the distance. To account for this, he models the data with:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i, i=1,\\dots,n\n\\]\nwith \\(Y_i\\) representing distance in meters, \\(x_i\\) representing time in seconds, and \\(\\varepsilon_i\\) accounting for measurement error. The measurement error is assumed to be random, independent from each other, and having the same distribution for each \\(i\\). We also assume that there is no bias, which means the expected value of the error terms is 0: \\(\\mathrm{E}[\\varepsilon] = 0\\).\nNote that this is a linear model because it is a linear combination of known quantities (\\(x\\) and \\(x^2\\) are known) and unknown parameters (the \\(\\beta\\)s are unknown parameters to Galileo). Unlike our previous examples, here \\(x\\) is a fixed quantity; we are not conditioning.\n\n\n\n\n\n\nSmall discrepancies between a model’s predictions and observations are often attributed to measurement error, as in our example. In many cases, this is a useful and practical approximation. However, such differences can also reveal limitations in the model itself. Galileo’s experiments on gravity showed slight deviations from his predicted uniform acceleration, largely due to air resistance rather than flawed measurements. Similarly, Newton’s laws of gravity accurately described planetary motion, but small discrepancies in Mercury’s orbit, once considered observational errors, ultimately led to Einstein’s general theory of relativity. While assuming measurement error is often reasonable, it is crucial to recognize when discrepancies signal model limitations. The diagnostic plots discussed in Section 16.4 can help assess such limitations.\n\n\n\nEstimating parameters with least squares\nTo pose a new physical theory and start making predictions about other falling objects, Galileo needs actual numbers, rather than unknown parameters. Using LSE seems like a reasonable approach. How do we find the LSE?\nLSE calculations do not require the errors to be approximately normal. The lm function will find the \\(\\beta\\)s that will minimize the residual sum of squares:\n\nfit &lt;- falling_object |&gt; \n  mutate(time_sq = time^2) |&gt; \n  lm(observed_distance~time+time_sq, data = _)\nsummary(fit)$coefficients\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)   54.753      0.625  87.589 5.36e-17\n#&gt; time           0.575      0.893   0.644 5.33e-01\n#&gt; time_sq       -4.929      0.265 -18.603 1.16e-09\n\nLet’s check if the estimated parabola fits the data. The broom function augment allows us to do this easily:\n\nbroom::augment(fit) |&gt; \n  ggplot() +\n  geom_point(aes(time, observed_distance)) + \n  geom_line(aes(time, .fitted), col = \"blue\")\n\n\n\n\n\n\n\nThanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is:\n\\[\nd(t) = h_0 + v_0 t -  0.5 \\times 9.8 \\, t^2\n\\]\nwith \\(h_0\\) and \\(v_0\\) the starting height and velocity, respectively. The data we simulated above followed this equation, adding measurement error to simulate n observations for dropping the ball \\((v_0=0)\\) from the tower of Pisa \\((h_0=55.86)\\).\nThese are consistent with the parameter estimates:\n\nconfint(fit)\n#&gt;             2.5 % 97.5 %\n#&gt; (Intercept) 53.38  56.13\n#&gt; time        -1.39   2.54\n#&gt; time_sq     -5.51  -4.35\n\nThe Tower of Pisa height is within the confidence interval for \\(\\beta_0\\), the initial velocity 0 is in the confidence interval for \\(\\beta_1\\) (note the p-value is larger than 0.05), and the acceleration constant is in a confidence interval for \\(-2 \\times \\beta_2\\).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Linear Model Framework</span>"
    ]
  },
  {
    "objectID": "linear-models/linear-model-framework.html#exercises",
    "href": "linear-models/linear-model-framework.html#exercises",
    "title": "\n16  The Linear Model Framework\n",
    "section": "\n16.6 Exercises",
    "text": "16.6 Exercises\n1. The co2 dataset is a time series object of 468 CO2 observations, monthly from 1959 to 1997. Plot CO2 levels for the first 12 months and notice it seems to follow a sine wave with a frequency of 1 cycle per year. This means that a measurement error model that might work is\n\\[\ny_i = \\mu + A \\sin(2\\pi \\,t_i / 12 + \\phi) + \\varepsilon_i\n\\] with \\(t_i\\) the month number for observation \\(i\\). Is this a linear model for the parameters \\(mu\\), \\(A\\) and \\(\\phi\\)?\n2. Using trigonometry, we can show that we can rewrite this model as:\n\\[\ny_i = \\beta_0 + \\beta_1 \\sin(2\\pi t_i/12) + \\beta_2 \\cos(2\\pi t_i/12) + \\varepsilon_i\n\\]\nIs this a linear model?\n3. Find least square estimates for the \\(\\beta\\)s using lm. Show a plot of \\(y_i\\) versus \\(t_i\\) with a curve on the same plot showing \\(\\hat{y}_i\\) versus \\(t_i\\).\n4. Now fit a measurement error model to the entire co2 dataset that includes a trend term that is a parabola as well as the sine wave model.\n5. Run diagnostic plots for the fitted model and describe the results.\n9. Fit a regression model to each of the datasets in the Anscombe quartet and examine the diagnostic plots.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Linear Model Framework</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html",
    "href": "linear-models/treatment-effect-models.html",
    "title": "17  Treatment Effect Models",
    "section": "",
    "text": "17.1 Case study: high-fat diet and mouse weight\nUp to now, we have worked with linear models to describe relationships between continuous variables. We motivated these models using the assumption of multivariate normality and showed how the conditional expectation takes a linear form. This covers many common applications of regression.\nHowever, linear models are not limited to describing natural variation among continuous measurements. One of the most important uses of linear models is to quantify the effect of an intervention or treatment. This framework originated in agriculture, where researchers compared crop yields across fields that received different fertilizers or planting strategies. The outcome variable was yield, and the treatment was the fertilizer. The same mathematical ideas now form the basis for analyzing randomized controlled trials in medicine, economics, public health, education, and the social sciences. Modern A/B tests used by internet companies to evaluate website changes or product features also follow this treatment effect framework.\nThe key idea is that a treatment defines groups, and we use linear models to estimate the difference in outcomes across these groups. In randomized experiments, the comparison is straightforward because randomization helps ensure that the groups are comparable. In observational studies, the goal is the same, but analysts must account for differences between groups that are not due to the treatment itself. For example, if we want to estimate the effect of a diet high in fruits and vegetables on blood pressure, we must adjust for factors such as age, sex, or smoking status.\nIn this chapter, we consider an experiment designed to test for the effects of a high-fat diet on mouse physiology. Mice were randomly selected and divided into two groups: one group receiving a high-fat diet, considered the treatment, while the other group served as the control and received the usual chow diet. The data is included in the dslabs package:\nlibrary(dslabs)\ntable(mice_weights$diet)\n#&gt; \n#&gt; chow   hf \n#&gt;  394  386\nA boxplot shows that the high fat diet mice are, on average, heavier.\nwith(mice_weights, boxplot(body_weight ~ diet))\nHowever, given that we divided the mice randomly, is it possible that the observed difference is simply due to chance?\nBefore making the connection to linear models, in the next section we perform statistical inference on the difference of these means, using the approach described in Chapter 13.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Treatment Effect Models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#comparing-group-means",
    "href": "linear-models/treatment-effect-models.html#comparing-group-means",
    "title": "17  Treatment Effect Models",
    "section": "\n17.2 Comparing group means",
    "text": "17.2 Comparing group means\nThe sample averages for the two groups, high-fat and chow diets, are different:\n\nlibrary(tidyverse)\nmice_weights |&gt; group_by(diet) |&gt; summarize(average = mean(body_weight))\n#&gt; # A tibble: 2 × 2\n#&gt;   diet  average\n#&gt;   &lt;fct&gt;   &lt;dbl&gt;\n#&gt; 1 chow     31.5\n#&gt; 2 hf       36.7\n\nHowever, this is a random sample of mice, and the assignment to the diet group is also done randomly. So is this difference due to chance? We will use hypothesis testing, first described in Chapter 13, to answer this question.\nLet \\(\\mu_1\\) and \\(\\sigma_1\\) represent the weight average and standard deviation, respectively, that we would observe if the entire population of mice were on the high-fat diet. Define \\(\\mu_0\\) and \\(\\sigma_0\\) similarly, but for the chow diet. Define \\(N_1\\) and \\(N_0\\) as the sample sizes, and \\(\\bar{X}_1\\) and \\(\\bar{X}_0\\) the sample averages, for the for the high-fat and chow diets, respectively.\nSince the data comes from a random sample, the central limit theorem tells us that, if the sample is large enough, the difference in averages \\(\\bar{X}_1 - \\bar{X}_0\\) follows a normal distribution, with expected value \\(\\mu_1-\\mu_0\\) and standard error \\(\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}}\\).\nIf we define the null hypothesis as the high-fat diet having no effect, or \\(\\mu_1 - \\mu_0 = 0\\), this implies that\n\\[\n\\frac{\\bar{X}_1 - \\bar{X}_0}{\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}}}\n\\]\nhas expected value 0 and standard error 1 and therefore approximately follows a standard normal distribution.\nNote that we can’t compute this quantity in practice because the \\(\\sigma_1\\) and \\(\\sigma_0\\) are unknown. However, if we estimate them with the sample standard deviations, denote them \\(s_1\\) and \\(s_0\\) for the high-fat and chow diets, respectively, the central limit theorem still holds and tells us that\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_0}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_0^2}{N_0}}}\n\\]\nfollows a standard normal distribution when the null hypothesis is true. This implies that we can easily compute the probability of observing a value as large as the one we obtained:\n\nstats &lt;- mice_weights |&gt; \n  group_by(diet) |&gt; \n  summarize(xbar = mean(body_weight), s = sd(body_weight), n = n()) \nt_stat &lt;- with(stats, (xbar[2] - xbar[1])/sqrt(s[2]^2/n[2] + s[1]^2/n[1]))\nt_stat\n#&gt; [1] 9.34\n\nWe can also used the R function to perform the calculation in one line:\n\nwith(mice_weights, t.test(body_weight[diet == \"hf\"], body_weight[diet == \"chow\"]))\n\nHere \\(t\\) is well over 3, so we don’t really need to compute the p-value 1-pnorm(t_stat) as we know it will be very small.\nNote that when \\(N_0\\) and \\(N_1\\) are not large enough, the CLT does not apply. However, if the outcome data, in this case weight, follows a normal distribution, then \\(t\\) follows a t-distribution with \\(N_1+N_2-2\\) degrees of freedom. So the calculation of the p-value is the same except that we use pt instead of pnorm. Specifically, we use 1-pt(t_stat, with(stats, n[2]+n[1]-2)).\nDifferences in means are commonly examined in the scientific studies. As a result this t-statistic is one of the most widely reported summaries. When used to determine if an observed difference is statistically significant, we refer to the procedure as “performing a t test”.\n\n\n\n\n\n\nIn the computation above, we calculated the probability of observing a \\(t\\) value as large as the one obtained. However, when our interest includes deviations in both directions, for example, either an increase or a decrease in weight—we must consider the probability of obtaining a value of \\(t\\) as extreme as the one observed, regardless of sign. In that case, we use the absolute value of \\(t\\) and double the one-sided probability: 2*(1 - pnorm(abs(t-test))) or 2*(1-pt(abs(t_stat), with(stats, n[2]+n[1]-2))).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Treatment Effect Models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#one-factor-design",
    "href": "linear-models/treatment-effect-models.html#one-factor-design",
    "title": "17  Treatment Effect Models",
    "section": "\n17.3 One factor design",
    "text": "17.3 One factor design\nAlthough the t-test is useful for cases in which we compare two treatments, it is common to have other variables affect our outcomes. Linear models permit hypothesis testing in these more general situations. We start the description of the use of linear models for estimating treatment effects by demonstrating how they can be used to perform t-tests.\nIf we assume that the weight distributions for both chow and high-fat diets are normally distributed, we can write the following linear model to represent the data:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nwith \\(x_i = 1\\), if the \\(i\\)-th mice was fed the high-fat diet, and 0 otherwise, and the errors \\(\\varepsilon_i\\) independent and normally distributed with expected value 0 and standard deviation \\(\\sigma\\).\nNote that this mathematical formula looks exactly like the model we wrote out for the father-son heights. However, the fact that \\(x_i\\) is now 0 or 1 rather than a continuous variable, allows us to use it in this different context. In particular, notice that now \\(\\beta_0\\) represents the population average weight of the mice on the chow diet and \\(\\beta_0 + \\beta_1\\) represents the population average for the weight of the mice on the high-fat diet.\nA nice feature of this model is that \\(\\beta_1\\) represents the treatment effect of receiving the high-fat diet. The null hypothesis that the high-fat diet has no effect can be quantified as \\(\\beta_1 = 0\\). To perform hypothesis testing on the effect of the high fat diet we can estimate \\(\\beta_1\\) and compute the probability of an estimate being as large as the observed when the null hypothesis is true.\nSo how do we estimate \\(\\beta_1\\) and compute this probability? A powerful characteristic of linear models is that we can estimate the \\(\\beta\\)s and their standard errors with the same LSE machinery:\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)\n\nBecause diet is a factor with two entries, the lm function knows to fit the linear model above with an \\(x_i\\), an indicator variable. The summary function shows us the resulting estimate, standard error, and p-value:\n\ncoefficients(summary(fit))\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    31.54      0.386   81.74 0.00e+00\n#&gt; diethf          5.14      0.548    9.36 8.02e-20\n\nThe statistic computed here is the estimate divided by its standard error: \\(\\hat{\\beta}_1 / \\widehat{\\mathrm{SE}}[\\hat{\\beta}_1]\\). In the case of the simple one-factor model, we can show that this statistic is almost equivalent to the t-statistics computed in the previous section:\n\nc(coefficients(summary(fit))[2,3], t_stat)\n#&gt; [1] 9.36 9.34\n\nIntuitively, it makes sense, as both \\(\\hat{\\beta_1}\\) and the numerator of the t-test are estimates of the treatment effect.\nThe one minor difference is that the linear model does not assume a different standard deviation for each population. Instead, both populations share \\(\\mathrm{SD}[\\varepsilon]\\) as a standard deviation. Note that, although we don’t demonstrate it with R here, we can redefine the linear model to have different standard errors for each group.\n\n\n\n\n\n\nIn the linear model description provided here, we assumed \\(\\varepsilon\\) follows a normal distribution. This assumption permits us to show that the statistics formed by dividing estimates by their estimated standard errors follow t-distribution, which in turn allows us to estimate p-values or confidence intervals. However, note that we do not need this assumption to compute the expected value and standard error of the least squared estimates. Furthermore, if the number of observations is large enough, then the central limit theorem applies and we can obtain p-values and confidence intervals even without the normal distribution assumption for the errors.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Treatment Effect Models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#two-factor-designs",
    "href": "linear-models/treatment-effect-models.html#two-factor-designs",
    "title": "17  Treatment Effect Models",
    "section": "\n17.4 Two factor designs",
    "text": "17.4 Two factor designs\nWe are now ready to describe a major advantage of the linear model approach over directly comparing averages.\nNote that this experiment included male and female mice, and male mice are known to be heavier. This explains why the residuals depend on the sex variable:\n\nboxplot(fit$residuals ~ mice_weights$sex)\n\n\n\n\n\n\n\nThis misspecification can have real implications; for instance, if more male mice received the high-fat diet, then this could explain the increase. Conversely, if fewer received it, we might underestimate the diet effect. Sex could be a confounder, indicating that our model can certainly be improved.\nFrom examining the data:\n\nmice_weights |&gt; ggplot(aes(diet, log2(body_weight), fill = sex)) + geom_boxplot()\n\n\n\n\n\n\n\nwe see that the diet effect is observed for both sexes and that males are heavier than females. Although not nearly as obvious, it also appears the diet effect is stronger in males.\nA linear model that permits a different expected value for the following four groups, 1) female on chow diet, 2) females on high-fat diet, 3) male on chow diet, and 4) males on high-fat diet, can be written like this:\n\\[\nY_i = \\beta_1 x_{i1} + \\beta_2 x_{i2}  + \\beta_3 x_{i3}  + \\beta_4 x_{i4}  + \\varepsilon_i\n\\]\nwith \\(x_{i1},\\dots,x_{i4}\\) indicator variables for each of the four groups. With this representation we allow the diet effect to be different for males and females.\nHowever, in the original representation, none of the \\(\\beta\\) parameters directly correspond to the treatment effect of interest (the effect of the high-fat diet). A useful feature of linear models is that we can reparameterize the model so that the fitted means remain the same, but the coefficients have interpretations aligned with the scientific question.\nFor example, consider the model\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3, x_{i1} x_{i2} + \\varepsilon_i,\n\\]\nwhere\n\n\n\\(x_{i1} = 1\\) if mouse \\(i\\) received the high-fat diet and \\(0\\) otherwise\n\n\\(x_{i2} = 1\\) if mouse \\(i\\) is male and \\(0\\) if female\n\nThen we interpret the parameters as follows:\n\n\n\\(\\beta_0\\) is the baseline average weight of females on the chow diet\n\n\\(\\beta_1\\) is the diet effect for females\n\n\\(\\beta_2\\) is the average difference between males and females under the chow diet\n\n\\(\\beta_3\\) is the difference in the diet effect between males and females, often called an interaction effect\n\nThis parameterization does not change the predicted values. It simply changes how we interpret the coefficients so that each corresponds to a meaningful comparison.\nStatistical textbooks describe several other ways in which the model can be rewritten to obtain other types of interpretations. For example, we might want \\(\\beta_2\\) to represent the overall diet effect (the average between female and male effect) rather than the diet effect on females. This is achieved by defining what contrasts we are interested in.\nIn R, we can specify the linear model above using the following:\n\nfit &lt;- lm(body_weight ~ diet*sex, data = mice_weights)\n\nHere, the * denotes factor crossing, not multiplication: diet*sex is shorthand for diet+sex+diet:sex, to calculate diet; sex; and diet combined with sex.\n\nsummary(fit)$coef\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|)\n#&gt; (Intercept)    27.83      0.440   63.27 1.48e-308\n#&gt; diethf          3.88      0.624    6.22  8.02e-10\n#&gt; sexM            7.53      0.627   12.02  1.27e-30\n#&gt; diethf:sexM     2.66      0.891    2.99  2.91e-03\n\nNote that the male effect is larger that the diet effect, and the diet effect is statistically significant for both sexes, with diet affecting males more by between 1 and 4.5 grams.\nA common approach applied when more than one factor is thought to affect the measurement is to simply include an additive effect for each factor, like this:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i1}  + \\beta_2 x_{i2}   + \\varepsilon_i\n\\]\nIn this model, the \\(\\beta_1\\) is a general diet effect that applies regardless of sex. In R, we use the following code, employing a + instead of *:\n\nfit &lt;- lm(body_weight ~ diet + sex, data = mice_weights)\n\nNote that this model does not account for the difference in diet effect between males and females. Diagnostic plots would reveal this deficiency by showing that the residuals are biased: they are, on average, negative for females on the diet and positive for males on the diet, rather than being centered around 0.\n\nplot(fit, which = 1)\n\n\n\n\n\n\n\nScientific studies, particularly within epidemiology and social sciences, frequently omit interaction terms from models due to the high number of variables. Adding interactions necessitates numerous parameters, which in extreme cases may prevent the model from fitting. However, this approach assumes that the interaction terms are zero, and if incorrect, it can skew the interpretation of the results. Conversely, when this assumption is valid, models excluding interactions are simpler to interpret, as parameters are typically viewed as the extent to which the outcome increases with the assigned treatment.\n\n\n\n\n\n\nLinear models are highly flexible and applicable in many contexts. For example, we can include many more factors than just 2. We have only just scratched the surface of how linear models can be used to estimate treatment effects. We highly recommend learning more about this by exploring linear model textbooks and R manuals that cover the use of functions such as lm, contrasts, and model.matrix.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Treatment Effect Models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#contrasts",
    "href": "linear-models/treatment-effect-models.html#contrasts",
    "title": "17  Treatment Effect Models",
    "section": "\n17.5 Contrasts",
    "text": "17.5 Contrasts\nIn the examples we have examined, each treatment had only two groups: diet had chow/high-fat, and sex had female/male. However, variables of interest often have more than one level. For example, we might have tested a third diet on the mice. In statistics textbooks, these variables are referred to as a factor, and the groups in each factor are called its levels.\nWhen a factor is included in the formula, the default behavior for lm is to define the intercept term as the expected value for the first level, and the other coefficient are to represent the difference, or contrast, between the other levels and first. We can see when we estimate the sex effect with lm like this:\n\nfit &lt;- lm(body_weight ~ sex, data = mice_weights)\ncoefficients(fit)\n#&gt; (Intercept)        sexM \n#&gt;       29.76        8.82\n\nTo recover the expected mean for males, we can simply add the two coefficients:\n\nsum(fit$coefficients[1:2])\n#&gt; [1] 38.6\n\nThe package emmeans simplifies the calculation and also calculates standard errors:\n\nlibrary(emmeans)\nemmeans(fit, ~sex)\n#&gt;  sex emmean    SE  df lower.CL upper.CL\n#&gt;  F     29.8 0.339 778     29.1     30.4\n#&gt;  M     38.6 0.346 778     37.9     39.3\n#&gt; \n#&gt; Confidence level used: 0.95\n\nNow, what if we really didn’t want to define a reference level? What if we wanted a parameter to represent the difference from each group to the overall mean? Can we write a model like this:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i\n\\] with \\(x_{i1} = 1\\), if observation \\(i\\) is female and 0 otherwise, and \\(x_{i2}=1\\), if observation \\(i\\) is male and 0 otherwise?\nUnfortunately, this representation has a problem. Note that the mean for females and males are represented by \\(\\beta_0 + \\beta_1\\) and \\(\\beta_0 + \\beta_2\\), respectively. This is a problem because the expected value for each group is just one number, say \\(\\mu_f\\) and \\(\\mu_m\\), and there is an infinite number of ways \\(\\beta_0 + \\beta_1 = \\mu_f\\) and \\(\\beta_0 +\\beta_2 = \\mu_m\\) (three unknowns with two equations). This implies that we can’t obtain a unique least squares estimate. When this happens we say the model, or parameters, are unidentifiable. The default behavior in R solves this problem by requiring \\(\\beta_1 = 0\\), forcing \\(\\beta_0 = \\mu_m\\), which permits us to solve the system of equations.\nKeep in mind that this is not the only constraint that permits estimation of the parameters. Any linear constraint will do as it adds a third equation to our system. A widely used constraint is to require \\(\\beta_1 + \\beta_2 = 0\\). To achieve this in R, we can use the argument contrasts in the following way:\n\nfit &lt;- lm(body_weight ~ sex, data = mice_weights, contrasts = list(sex=contr.sum))\ncoefficients(fit)\n#&gt; (Intercept)        sex1 \n#&gt;       34.17       -4.41\n\nWe see that the intercept is now larger, reflecting the overall mean rather than just the mean for females. The other coefficient, \\(\\beta_1\\), represents the contrast between females and the overall mean in our model. The coefficient for males is not shown because it is redundant: \\(\\beta_1= -\\beta_2\\).\nIf we want to see all the estimates, the emmeans package also makes the calculations for us:\n\ncontrast(emmeans(fit, ~sex))\n#&gt;  contrast estimate    SE  df t.ratio p.value\n#&gt;  F effect    -4.41 0.242 778 -18.200  &lt;.0001\n#&gt;  M effect     4.41 0.242 778  18.200  &lt;.0001\n#&gt; \n#&gt; P value adjustment: fdr method for 2 tests\n\nThe use of this alternative constraint is more practical when a factor has more than one level, and choosing a baseline becomes less convenient. Furthermore, we might be more interested in the variance of the coefficients rather than the contrasts between groups and the reference level.\nAs an example, consider that the mice in our dataset are actually from several generations:\n\ntable(mice_weights$gen)\n#&gt; \n#&gt;   4   7   8   9  11 \n#&gt;  97 195 193  97 198\n\nTo estimate the variability due to the different generations, a convenient model is:\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^J \\beta_j x_{ij} + \\varepsilon_i\n\\]\nwith \\(x_{ij}\\) indicator variables: \\(x_{ij}=1\\) if mouse \\(i\\) is in level \\(j\\) and 0 otherwise, \\(J\\) representing the number of levels, in our example 5 generations, and the level effects constrained with\n\\[\n\\frac{1}{J} \\sum_{j=1}^J \\beta_j = 0 \\implies \\sum_{j=1}^J \\beta_j = 0.\n\\]\nThis constraint makes the model identifiable and also allows us to quantify the variability due to generations with:\n\\[\n\\sigma^2_{\\text{gen}} \\equiv \\frac{1}{J}\\sum_{j=1}^J \\beta_j^2\n\\]\nWe can see the estimated coefficients using the following:\n\nfit &lt;- lm(body_weight ~ gen,  data = mice_weights, contrasts = list(gen=contr.sum))\ncontrast(emmeans(fit, ~gen)) \n#&gt;  contrast     estimate    SE  df t.ratio p.value\n#&gt;  gen4 effect    -0.122 0.705 775  -0.174  0.8620\n#&gt;  gen7 effect    -0.812 0.542 775  -1.497  0.3370\n#&gt;  gen8 effect    -0.113 0.544 775  -0.207  0.8620\n#&gt;  gen9 effect     0.149 0.705 775   0.212  0.8620\n#&gt;  gen11 effect    0.897 0.540 775   1.663  0.3370\n#&gt; \n#&gt; P value adjustment: fdr method for 5 tests\n\nIn the next section, we briefly describe a technique useful to study the variability associated with this factor.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Treatment Effect Models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#sec-anova",
    "href": "linear-models/treatment-effect-models.html#sec-anova",
    "title": "17  Treatment Effect Models",
    "section": "\n17.6 Analysis of variance (ANOVA)",
    "text": "17.6 Analysis of variance (ANOVA)\nWhen a factor has more than one level, it is common to want to determine if there is significant variability across the levels rather than specific difference between any given pair of levels. Analysis of variance (ANOVA) provides tools to do this.\nANOVA provides an estimate of \\(\\sigma^2_{\\text{gen}}\\) and a statistical test for the null hypothesis that the factor contributes no variability: \\(\\sigma^2_{\\text{gen}} =0\\).\nOnce a linear model is fit using one or more factors, the aov function can be used to perform ANOVA. Specifically, the estimate of the factor variability is computed along with a statistic that can be used for hypothesis testing:\n\nsummary(aov(fit))\n#&gt;              Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; gen           4    294    73.5    1.13   0.34\n#&gt; Residuals   775  50479    65.1\n\nKeep in mind that if given a model formula, aov will fit the model:\n\nsummary(aov(body_weight ~ gen, data = mice_weights))\n\nWe do not need to specify the constraint because ANOVA needs to constrain the sum to be 0 for the results to be interpretable.\nThis analysis indicates that the effect of generation is not statistically significant.\n\n\n\n\n\n\nWe do not include many details, for example, on how the summary statistics and p-values shown by aov are defined and motivated. There are several books dedicated to the analysis of variance, and textbooks on linear models often include chapters on this topic. Those interested in learning more about these topics can consult one of the textbooks listed in the Recommended Reading section.\n\n\n\nMultiple factors\nANOVA was developed to analyze agricultural data, which typically included several factors such as fertilizers, blocks of lands, and plant breeds.\nNote that we can perform ANOVA with multiple factors:\n\nsummary(aov(body_weight ~ sex + diet + gen,  data = mice_weights))\n#&gt;              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; sex           1  15165   15165  389.80 &lt;2e-16 ***\n#&gt; diet          1   5238    5238  134.64 &lt;2e-16 ***\n#&gt; gen           4    295      74    1.89   0.11    \n#&gt; Residuals   773  30074      39                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis analysis suggests that sex is the biggest source of variability, which is consistent with previously made exploratory plots.\n\n\n\n\n\n\nOne of the key aspects of ANOVA (Analysis of Variance) is its ability to decompose the total variance in the data, represented by \\(\\sum_{i=1}^N y_i^2\\), into individual contributions attributable to each factor in the study. However, for the mathematical underpinnings of ANOVA to be valid, the experimental design must be balanced. This means that for every level of any given factor, there must be an equal representation of the levels of all other factors. In our study involving mice, the design is unbalanced, requiring a cautious approach in the interpretation of the ANOVA results.\n\n\n\nArray representation\nWhen the model includes more than one factor, writing down linear models can become cumbersome. For example, in our two factor model, we would have to include indicator variables for both factors:\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^J \\beta_j x_{i,j} + \\sum_{k=1}^K \\beta_{J+k} x_{i,J+k} + \\varepsilon_i \\mbox{ with }\\sum_{j=1}^J \\beta_j=0 \\mbox{ and } \\sum_{k=1}^K \\beta_{J+k} = 0,\n\\]\nthe \\(x_{i,1},\\dots,x_{i,J}\\) indicator functions for the \\(J\\) levels in the first factor and \\(x_{i,J+1},\\dots,x_{i,J+K}\\) indicator functions for the \\(K\\) levels in the second factor.\nAn alternative approach widely used in ANOVA to avoid indicator variables, is to save the data in an array, using different Greek letters to denote factors and indices to denote levels:\n\\[\nY_{ijk} = \\mu + \\alpha_j + \\beta_k + \\varepsilon_{ijk}, \\mbox{ with } i = 1,\\dots,I, \\, j = 1,\\dots,J, \\mbox{ and } k= 1,\\dots,K\n\\]\nwith \\(\\mu\\) the overall mean, \\(\\alpha_j\\) the effect of level \\(j\\) in the first factor, and \\(\\beta_k\\) the effect of level \\(k\\) in the second factor. The constraint can now be written as:\n\\[\n\\sum_{j=1}^J \\alpha_j = 0 \\text{ and } \\sum_{k=1}^K \\beta_k = 0\n\\]\nThis array notation naturally leads to estimating effects by computing means across dimensions of the array.\nNote that, here, we are implicitly assuming a balanced design, meaning that for every combination of factor levels \\(j\\) and \\(k\\) we observe the same number of replicates indexed by \\(i\\). For example, if the factors are sex and diet, a balanced design would include the same number of mice in each sex–diet combination. In this setting, there is a value \\(Y_{ijk}\\) for each triplet \\((i,j,k)\\), and each cell of the design has the same sample size. This ensures that the group means are well defined and that the model parameters can be estimated directly using simple averages.\nIf the design is unbalanced, meaning that some level combinations have more observations than others, we can still write the model in the same form but the number of replicates becomes \\(I_{jk}\\), varying across level pairs. In that case, some of the convenient interpretations of ANOVA summaries no longer hold directly, because the cell means contribute unequally to the estimates.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Treatment Effect Models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#exercises",
    "href": "linear-models/treatment-effect-models.html#exercises",
    "title": "17  Treatment Effect Models",
    "section": "\n17.7 Exercises",
    "text": "17.7 Exercises\n1. Once you fit a model, the estimate of the standard error \\(\\sigma\\) can be obtained as follows:\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)\nsummary(fit)$sigma\n\nCompute the estimate of \\(\\sigma\\) using both the model that includes only diet and a model that accounts for sex. Are the estimates the same? If not, why not?\n2. One of the assumption of the linear model fit by lm is that the standard deviation of the errors \\(\\varepsilon_i\\) is equal for all \\(i\\). This implies that it does not depend on the expected value. Group the mice by their weight like this:\n\nbreaks &lt;- with(mice_weights, seq(min(body_weight), max(body_weight), 1))\ndat &lt;- mutate(mice_weights, group = cut(body_weight, breaks, include_lowest=TRUE))\n\nCompute the average and standard deviation of body_weight for groups with more than 10 observations and use data exploration to verify if this assumption holds.\n3. The dataset also includes a variable indicating which litter the mice came from. Create a boxplot showing weights by litter. Use faceting to make separate plots for each diet and sex combination.\n4. Use a linear model to test for a litter effect, taking into account sex and diet. Use ANOVA to compare the variability explained by litter with that of other factors.\n5. The mice_weights data includes two other outcomes: bone density and percent fat. Create a boxplot illustrating bone density by sex and diet. Compare what the visualizations reveal about the diet effect by sex.\n6. Fit a linear model and conduct a separate test for the diet effect on bone density for each sex. Note that the diet effect is statistically significant for females but not for males. Then fit the model to the entire dataset that includes diet, sex and their interaction. Notice that the diet effect is significant, yet the interaction effect is not. Explain how this can happen. Hint: To fit a model to the entire dataset with a separate effect for males and females, you can use the formula ~ sex + diet:sex\n7. In Chapter 10, we talked about pollster bias and used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election:\n\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(pollster %in% c(\"Rasmussen Reports/Pulse Opinion Research\",\n                         \"The Times-Picayune/Lucid\") &\n           enddate &gt;= \"2016-10-15\" &\n           state == \"U.S.\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) \n\nWe want to answer the question: is there a pollster bias? Make a plot showing the spreads for each pollster.\n8. The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.\nThe urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(\\mu\\).\nTo answer the question “is there an urn model?” we will model the observed data \\(Y_{ij}\\) in the following way:\n\\[\nY_{ij} = \\mu + b_i + \\varepsilon_{ij}\n\\]\nwith \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\), and \\(\\varepsilon_{ij}\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\), and standard deviation \\(\\sigma_i\\) regardless of \\(j\\).\nWhich of the following best represents our question?\n\nIs \\(\\varepsilon_{ij}\\) = 0?\nHow close are the \\(Y_{ij}\\) to \\(\\mu\\)?\nIs \\(b_1 \\neq b_2\\)?\nAre \\(b_1 = 0\\) and \\(b_2 = 0\\) ?\n\n9. On the right side of this model, only \\(\\varepsilon_{ij}\\) is a random variable; the other two are constants. What is the expected value of \\(Y_{1,j}\\)?\n10. Suppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first poll, \\(Y_{1,1},\\dots,Y_{1,N_1}\\), where \\(N_1\\) is the number of polls conducted by the first pollster:\n\npolls |&gt; \n  filter(pollster==\"Rasmussen Reports/Pulse Opinion Research\") |&gt; \n  summarize(N_1 = n())\n\nWhat is the expected values \\(\\bar{Y}_1\\)?\n11. What is the standard error of \\(\\bar{Y}_1\\) ?\n12. Suppose we define \\(\\bar{Y}_2\\) as the average of poll results from the second poll, \\(Y_{2,1},\\dots,Y_{2,N_2}\\), where \\(N_2\\) is the number of polls conducted by the second pollster. What is the expected value \\(\\bar{Y}_2\\)?\n13. What is the standard error of \\(\\bar{Y}_2\\) ?\n14. Using what we learned by answering the questions above, what is the expected value of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)?\n15. Using what we learned by answering the questions above, what is the standard error of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)?\n16. The answer to the question above depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), which we don’t know. We learned that we can estimate these with the sample standard deviation. Write code that computes these two estimates.\n17. What does the CLT tell us about the distribution of \\(\\bar{Y}_2 - \\bar{Y}_1\\)?\n\nNothing because this is not the average of a sample.\nBecause the \\(Y_{ij}\\) are approximately normal, so are the averages.\nNote that \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages, so if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normally distributed variables is also normally distributed.\nThe data are not 0 or 1, so CLT does not apply.\n\n18. We have constructed a random variable that has an expected value of \\(b_2 - b_1\\), representing the difference in pollster bias. If our model holds, then this random variable has an approximately normal distribution, and we know its standard error. The standard error depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), but we can plug the sample standard deviations we computed above. We began by asking: is \\(b_2 - b_1\\) different from 0? Using all the information we have gathered above, construct a 95% confidence interval for the difference \\(b_2 - b_1\\).\n19. The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. Random variability does not seem to explain it. We can compute a p-value to relay the fact that chance does not explain it. What is the p-value?\n20. The statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error is the t-statistic:\n\\[\n\\frac{\\bar{Y}_2 - \\bar{Y}_1}{\\sqrt{s_2^2/N_2 + s_1^2/N_1}}\n\\]\nNow notice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls.\nFor this exercise, create a new table:\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-15\" &\n           state == \"U.S.\") |&gt;\n  group_by(pollster) |&gt;\n  filter(n() &gt;= 5) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt;\n  ungroup()\n\nCompute the average and standard deviation for each pollster and examine the variability across the averages. Compare this to the variability within the pollsters, summarized by the standard deviation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Treatment Effect Models</span>"
    ]
  },
  {
    "objectID": "linear-models/glm.html",
    "href": "linear-models/glm.html",
    "title": "\n18  Generalized Linear Models\n",
    "section": "",
    "text": "18.1 Association Tests\nThe statistical models we have used so far assume that the outcome of interest is continuous. However, in many real applications the outcomes are binary, categorical, or counts. In these settings, we are often interested in how the outcome changes with one or more explanatory variables.\nFor example, we may want to study how the probability of developing coronary heart disease varies with age, systolic blood pressure, or other clinical measurements; how the chance of surviving the sinking of the Titanic depends on passenger characteristics; or how different insecticides influence the number of insects observed in agricultural field plots.\nAlthough these outcomes are no longer continuous, many of the ideas introduced in the linear model framework continue to apply. With a small adaptation, we can extend the mathematical approach used for linear models so that it can be applied in all of these situations, while also continuing to use the same modeling tools in R.\nThese extended models are known as generalized linear models. To motivate them, we begin this chapter with association tests for two categorical variables. We then show how these tests arise naturally from logistic regression, our first example of a generalized linear model for binary outcomes. We conclude with examples that illustrate how the same framework can be applied across a range of contexts, including outcomes that are binary, or counts.\nWe start with the simplest case: assessing whether two categorical variables are associated. These methods were developed before GLMs, but they can be understood as special cases of logistic regression. Once we establish this connection, the general GLM framework will follow naturally.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear-models/glm.html#sec-association-tests",
    "href": "linear-models/glm.html#sec-association-tests",
    "title": "\n18  Generalized Linear Models\n",
    "section": "",
    "text": "Case study: Funding success rates\nA 2014 PNAS paper1 analyzed success rates from funding agencies in the Netherlands and concluded that their\n\nresults reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials.\n\nThe main evidence supporting this conclusion is based on a comparison of the percentages which we can compute using the data presented in the paper:\n\nlibrary(tidyverse)\nlibrary(dslabs)\ntotals &lt;- research_funding_rates |&gt; select(-discipline) |&gt; \n  summarize_all(sum) |&gt;\n  summarize(yes_men = awards_men, \n            no_men = applications_men - awards_men, \n            yes_women = awards_women, \n            no_women = applications_women - awards_women) \n\ntotals |&gt; summarize(percent_men = yes_men/(yes_men + no_men),\n                    percent_women = yes_women/(yes_women + no_women))\n#&gt;   percent_men percent_women\n#&gt; 1       0.177         0.149\n\nBut could this be due just to random variability? Here we learn how to perform inference for this type of data.\nIn Section 18.1.3, we introduce association tests that permit us to answer this question. Before doing so, we present a historically important example that illustrates the need for such methods.\nThe Lady Tasting Tea\nThe classic motivating example for association tests is the Lady Tasting Tea experiment designed by R. A. Fisher. A colleague at the Rothamsted Experimental Station claimed she could tell whether milk was poured into a cup before or after the tea. Fisher proposed a randomized test and derived the probability of each possible result under the assumption that she was simply guessing. This led to what is now known as Fisher’s exact test, based on the hypergeometric distribution.\nAs an example, suppose she correctly identified 3 out of 4 cups:\n\ntab &lt;- table(guess =  c(\"m\", \"t\", \"m\", \"t\", \"t\", \"m\", \"t\", \"m\"),\n             actual = c(\"m\", \"t\", \"m\", \"t\", \"t\", \"t\", \"m\", \"m\"))\ntab\n#&gt;      actual\n#&gt; guess m t\n#&gt;     m 3 1\n#&gt;     t 1 3\n\nTo evaluate whether this provides evidence of real ability, we consider the null hypothesis that she is guessing. Because she knows there are four of each type, guessing corresponds to randomly drawing 4 cups from an urn containing 4 milk-first and 4 tea-first cups.\nThe probability of correctly guessing \\(k\\) cups is given by the hypergeometric formula:\n\\[\n\\frac{\\binom{4}{k}\\binom{4}{4-k}}{\\binom{8}{4}}.\n\\]\nThe probability of getting 3 or more correct by chance is therefore : \\[\n\\frac{\\binom{4}{3}\\binom{4}{1}}{\\binom{8}{4}} + \\frac{\\binom{4}{4}\\binom{4}{0}}{\\binom{8}{4}} = \\frac{16}{70} \\approx 0.24.\n\\] So correctly identifying 3 cups could easily occur by chance and is not compelling evidence of the claimed ability.\nThis calculation is what Fisher’s exact test performs. In R we can type:\n\nfisher.test(tab, alternative = \"greater\")$p.value\n#&gt; [1] 0.243\n\n\n\n\n\n\n\nHistorical accounts suggest that the lady did perform well in the original demonstration. Fisher’s goal was not to confirm or refute her ability, but to illustrate how to design a fair experiment and evaluate evidence with probability.\n\n\n\nTwo-by-two tables and the Chi-square test\nNotice that our funding example is analogous to the Lady Tasting Tea: under the null hypothesis, funding is assigned at random regardless of gender, just as in Fisher’s example cups are guessed at random regardless of when the milk was poured.\nHowever, in the Lady Tasting Tea example, we were able to use the hypergeometric distribution because the design fixed both the number of cups of each type and the number of selections for each category. In other words, the row totals and the column totals of tab were predetermined by the experiment.\nIn the funding example, and in most real-world data, this is not the case. Neither the number of funded applications nor the number of male and female applicants is fixed in advance; these totals arise from the data we observe. Because the row and column totals are not fixed, the hypergeometric model is no longer appropriate and rarely used in practice.\nIn such settings, we instead use the Chi-squared test, which provides a way to test for association between two categorical variables without requiring fixed margins. Let’s apply it to the funding rates example.\nThe first step in a Chi-squared test is to create the observed two-by-two table:\n\no &lt;- with(totals, data.frame(men = c(no_men, yes_men), \n                             women = c(no_women, yes_women),\n                             row.names = c(\"no\", \"yes\")))\n\nWe then estimate the overall funding rate which we will use to determine what we expect to see if successful funding was assigned independent of gender:\n\nrate &lt;- with(totals, (yes_men + yes_women))/sum(totals)\n\nWe use this to compute what we expect to see by chance:\n\ne &lt;- with(totals, data.frame(men = (no_men + yes_men)*c(1 - rate, rate),\n                             women = (no_women + yes_women)*c(1 - rate, rate),\n                             row.names = c(\"no\", \"yes\")))\n\nWe can see that more men than expected and fewer women than expected received funding:\n\ncbind(o, e)\n#&gt;      men women  men women\n#&gt; no  1345  1011 1365   991\n#&gt; yes  290   177  270   197\n\nHowever, under the null hypothesis these observations are random variables. The Chi-square statistic quantifies how much the observed tables deviates from the expected by:\n\nTaking the difference between each observed and expected cell value.\nSquaring this difference.\nDividing each squared difference by the expected value.\nSumming all these values together to get the final statistic.\n\n\nsum((o - e)^2/e)\n#&gt; [1] 4.01\n\nWe use this summary statistic because its sampling distribution can be approximated by a known parametric distribution: the Chi-square distribution.\nThe R function chisq.test takes a two-by-two table and returns the results from the test:\n\nchisq_test &lt;- chisq.test(o, correct = FALSE)\n\nWe see that the p-value is 0.045:\n\nchisq_test$p.value\n#&gt; [1] 0.0451\n\n\n\n\n\n\n\nBy default, the chisq.test function applies a continuity correction. This correction subtracts 0.5 from the absolute deviation between observed and expected counts:\n\nsum((abs(o - e) - 0.5)^2 / e)\n#&gt; [1] 3.81\n\nand this matches the default behavior of:\n\nchisq.test(o)$statistic\n#&gt; X-squared \n#&gt;      3.81\n\nThe reason for this adjustment is that the chi-square test is based on a continuous probability distribution, while the two-by-two table counts are discrete. When sample sizes are small, the difference between a continuous approximation and the discrete distribution can be noticeable. The continuity correction slightly reduces the test statistic to account for this mismatch, making the approximation more conservative (that is, less likely to produce small p-values by chance).\nWe previously used the argument correct = FALSE to avoid this adjustment because, in moderate-to-large samples, the correction is not needed and can slightly reduce the test’s power.\n\n\n\nOdds ratios\nThe Chi-square test provides a p-value but it does not quantify the size of the effect. As discussed in Chapter 13, we generally prefer confidence intervals over p-values because they communicate effect size and uncertainty. So how do we quantify the effect here?\nFor two categorical groups, our data can be summarized in a two-by-two table:\n\n\n\nWomen\nMen\n\n\n\nAwarded\na\nb\n\n\nNot Awarded\nc\nd\n\n\n\nOne option is to compare the difference in proportions:\n\\[\n\\frac{a}{a+c} - \\frac{b}{b+d}.\n\\]\nHowever, differences in proportions are not directly comparable across baseline levels. For example, a change from 1% to 2% is just as large on the probability scale as a change from 49% to 50%, yet the practical interpretation is very different.\nFor this reason, a more common way to quantify association in our context is the odds ratio:\n\\[\n\\text{odds ratio} = \\frac{a/c}{b/d} = \\frac{ad}{bc}\n\\] This quantity compares the odds of being funded for women to the odds of being funded for men.\nThe log odds ratio is often used because it is symmetric around 0:\n\\[\n\\log\\left(\\frac{ad}{bc}\\right) = 0 \\quad \\text{when there is no difference between groups.}\n\\]\nMoreover, the log odds ratio has a convenient approximate standard error:\n\\[\n\\mathrm{SE}\\left[\\log\\left(\\frac{ad}{bc}\\right)\\right]\n\\approx\\sqrt{\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}}.\n\\]\nand can be shown to be asymptotically normal. This allows us to construct confidence intervals. A 95% confidence interval would be:\n\\[\n\\log\\left(\\frac{ad}{bc}\\right)\n\\pm 1.96 \\sqrt{\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}}\n\\] Once we construct a confidence interval for the log odds ratio, we can obtain a confidence interval for the odds ratio by exponentiating the two endpoints.\nHer we compute the odds ratio for our funding data and construct a 95% confidence interval:\n\nor &lt;- o[1,1]*o[2,2]/(o[2,1]*o[1,2])\nse &lt;- sqrt(sum(1/o))\nexp(log(or) + c(-1,1) * qnorm(0.975) * se)\n#&gt; [1] 0.662 0.996\n\nThis allows us to assess both the magnitude of the association and the uncertainty around it. The estimated odds ratio is less than 1, indicating that women have lower odds of being funded than men in this dataset. The 95% confidence interval does not include 1, which implies the p-value is less than 0.05.\n\n\n\n\n\n\nThe log odds ratio is not defined if any of the cells of the two-by-two table is 0. This is because if \\(a\\), \\(b\\), \\(c\\), or \\(d\\) are 0, the \\(\\log(\\frac{ad}{bc})\\) is either the log of 0 or has a 0 in the denominator. For this situation, it is common practice to avoid 0s by adding 0.5 to each cell. This is referred to as the Haldane-Anscombe correction and has been shown, both in practice and theory, to work well.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear-models/glm.html#sec-glm",
    "href": "linear-models/glm.html#sec-glm",
    "title": "\n18  Generalized Linear Models\n",
    "section": "\n18.2 Logistic regression",
    "text": "18.2 Logistic regression\nWe now connect the association tests described above to the linear model framework introduced in Chapter 17. Once we make this connection, the same ideas extend naturally to more complex settings, including models with continuous predictors and multiple explanatory variables.\nIf we define \\(Y_i\\) to be 1 if applicant \\(i\\) was funded and 0 otherwise, and set \\(x_i = 1\\) for women and \\(x_i = 0\\) for men, we might be tempted to write a linear model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i.\n\\]\nHowever, this model is not appropriate because the left-hand side only takes values 0 or 1, while the right-hand side can take any real value. In particular, the implied expected value\n\\[\n\\mathrm{E}[Y_i] = \\Pr(Y_1=1) = \\beta_0 + \\beta_1 x_i\n\\]\ncould be less than 0 or greater than 1, which is not possible for a probability.\nIf we instead model the log odds as a linear function of \\(x_i\\)\n\\[\n\\log\\left\\{\\frac{\\Pr(Y_i=1)}{1 - \\Pr(Y_i=1)}\\right\\}= \\beta_0 + \\beta_1 x_i.\n\\]\nwe get the following useful interpretation of the parameters:\n\n\n\\(e^{\\beta_0}\\) is the odds of funding for men,\n\n\\(e^{\\beta_0} e^{\\beta_1}\\) is the odds of funding for women.\n\n\\(e^{\\beta_1}\\) is the odds ratio comparing women to men.\n\nThe parameter \\(\\beta_1\\) is thus the log odds ratio.\n\n\n\n\n\n\nThe name logistic regression comes from the fact that\n\\[\ng(p) = \\log\\frac{p}{1-p}\n\\] is called the logistic function or logit transformation.\n\n\n\nNote that we can write the probability \\(p_i = Pr(Y_i = 1)\\) as a function of the paremeters \\(\\beta_0+\\beta_1x_i\\) for any \\(x_i\\) by using the inverse of the logit transformation:\n\\[\n\\Pr(Y_i=1) = g^{-1}(g(p_i)) = g^{-1}\\left(\\beta_0+\\beta_1x_i\\right) = \\frac{e^{\\beta_0+\\beta_1x_i}}{1 + e^{\\beta_0+\\beta_1x_i}} = \\frac{1}{1 + e^{-\\beta_0-\\beta_1x_i}}\n\\]\nEstimation\nUnlike linear regression, least squares is not optimal here because the variance of the outcome depends on its mean and the model for the expected value is nonlinear. Instead, the parameters are estimated using maximum likelihood estimation (MLE). The basic idea of MLE is to choose the parameter values that make the observed data most probable according to the assumed statistical model. For logistic regression, this means finding the values of \\(\\beta_0\\) and \\(\\beta_1\\) that maximize the probability of observing the particular pattern of zeros and ones in the data.\nOur example analyzing a two-by-two table is a special case in which the estimates can be written directly as functions of the observed proportions. But in general, for logistic regression models, the estimates cannot be written in a simple closed-form expression. Instead, numerical optimization algorithms are used to find the values that maximize the likelihood. Under standard conditions, a version of the central limit theorem applies to these estimates. This implies that for large sample sizes the estimates are approximately normally distributed and that their standard errors can be computed from the curvature of the likelihood function near the maximum. This, in turn, permits the construction of confidence intervals and hypothesis tests.\nReaders interested in a deeper and more general treatment of MLE and logistic regression can consult the Recommended Reading section, where we point to textbooks that develop the likelihood framework, the asymptotic results, and the numerical algorithms in more detail.\nFitting the model in R\nWe can fit the logistic regression model in R using the glm function with family = binomial.\nAlthough we do not have individual-level records, we do have totals for each group. This is sufficient because the sum of the outcomes for men is a binomial random variable with \\(N_0\\) trials (the number of male applicants) and success probability \\(p_0\\), which satisfies\n\\[\n\\log\\left(\\frac{p_0}{1-p_0}\\right) = \\beta_0.\n\\]\nSimilarly, the sum of outcomes for women is binomial with \\(N_1\\) trials and success probability \\(p_1\\), where\n\\[\n\\log\\left(\\frac{p_1}{1-p_1}\\right) = \\beta_0 + \\beta_1.\n\\]\nThe glm function can work directly with grouped counts by using a two-column matrix of successes and failures:\n\nsuccess &lt;- with(totals, c(yes_men, yes_women))\nfailure &lt;- with(totals, c(no_men, no_women))\ny &lt;- cbind(success, failure)\n\nand encoding the group indicator:\n\nx &lt;- factor(c(\"men\", \"women\"))\n\nWe then fit the model with glm:\n\nfit &lt;- glm(y ~ x, family = binomial)\ncoefficients(summary(fit))\n#&gt;             Estimate Std. Error z value  Pr(&gt;|z|)\n#&gt; (Intercept)   -1.534     0.0647   -23.7 3.83e-124\n#&gt; xwomen        -0.208     0.1041    -2.0  4.54e-02\n\nTo obtain the estimated odds ratio, we exponentiate the coefficient:\n\nexp(fit$coef[2])\n#&gt; xwomen \n#&gt;  0.812\n\nThis value represents the multiplicative change in the odds of being funded for women compared to men.\nA 95% confidence interval for this odds ratio is obtained by exponentiating the corresponding confidence interval for \\(\\beta_1\\):\n\nexp(confint(fit, 2))\n#&gt;  2.5 % 97.5 % \n#&gt;  0.661  0.995\n\nThis interval provides both an effect size and a measure of uncertainty.\nRelation to earlier methods\nWe can confirm that logistic regression yields the same odds ratio estimate as the simple two-by-two table (saved in o) calculation: and saved in or and se:\n\nor &lt;- o[1,1]*o[2,2]/(o[2,1]*o[1,2])\nse &lt;- sqrt(sum(1/o))\nc(log(or),se)\n#&gt; [1] -0.208  0.104\n\nc(tidy(fit)$estimate[2], tidy(fit)$std.error[2])\n#&gt; [1] -0.208  0.104\n\n\n\n\n\n\n\nThe p-values obtained from chisq.test and from logistic regression\n\nchisq.test(o, correct = FALSE)$p.value\n#&gt; [1] 0.0451\ntidy(fit)$p.value[2]\n#&gt; [1] 0.0454\n\ndiffer slightly because they are based on different approximations.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear-models/glm.html#generalizing-the-model",
    "href": "linear-models/glm.html#generalizing-the-model",
    "title": "\n18  Generalized Linear Models\n",
    "section": "\n18.3 Generalizing the model",
    "text": "18.3 Generalizing the model\nAt first glance, logistic regression may seem like an unnecessarily complicated way to obtain the same results we derived using the odds ratio and the standard error approximation. So why introduce this more general framework?\nThe key advantage becomes clear when the explanatory variables are not just two categories. For example, returning to the question of coronary heart disease, age is a continuous variable. There is no simple way to form a two-by-two table for every possible age value. Similarly, if we wish to adjust for several explanatory variables simultaneously (as we did in Chapter 17), we need a model-based approach rather than a table-based one.\nThe general strategy is to assumes \\(Y_i\\) follow a known distribution, such as binomial or exponential, and model a transformation of the expected value of the outcome using a linear combination of predictors:\n\\[\ng\\left(\\mathrm{E}[Y_i]\\right) = \\beta_0 + \\sum_{j=1}^J \\beta_j x_{ij}\n\\]\nHere \\(x_{ij}\\) is the value of the \\(j\\)-th explanatory variable for individual \\(i\\), and \\(g(\\cdot)\\) is called the link function. For binary outcomes we used the logistic function because it maps probabilities (which must lie between 0 and 1) to all real numbers. For count data, which we often model with a Poisson distribution, the expected value \\(\\mathrm{E}[Y_i]\\) can exceed 1, so the logit link is not suitable. Instead, the log link is typically used: \\(g(\\lambda) = \\log(\\lambda)\\). This link maps positive values (such as rates) to the real line and provides an interpretable multiplicative form: if \\(\\exp(\\beta_1)= 1.10\\), then a one-unit increase in \\(x_{i1}\\) corresponds to a 10% increase in the rate.\nThis framework is known as a generalized linear model (GLM). Logistic regression is one special case; Poisson regression is another.\nLogistic regression example: Coronary heart disease\nThe SAheart dataset contains observations from a retrospective sample of adult males in a high-risk region of South Africa:\n\ndata(\"SAheart\", package = \"bestglm\")\n\nIf we compute the proportion of individuals with heart disease at each age and examine the log-odds, the relationship appears approximately linear:\n\nSAheart |&gt; \n  group_by(age) |&gt;\n  summarize(chd = mean(chd)) |&gt;\n  ggplot(aes(age, log(chd/(1 - chd)))) + \n  geom_point()\n\n\n\n\n\n\n\nThis suggests the logistic regression model:\n\\[\n\\log\\left\\{\\frac{\\Pr(Y_i=1)}{1 - \\Pr(Y_i=1)}\\right\\} = \\beta_0 + \\beta_1 x_i\n\\] with \\(Y_i\\) an indicator of coronary heart disease and \\(x_i\\) representing the age of individual \\(i\\). The estimate for \\(\\beta_1\\) converts into an odds ratio. The quantity \\(\\exp(\\beta_1)\\) represents the multiplicative increase in the odds of heart disease for each one-year increase in age.\nFitting can fit the model using:\n\nfit &lt;- glm(chd ~ age, family = binomial, data = SAheart)\ntidy(fit)[2,]\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 age     0.0641   0.00853      7.51 5.76e-14\n\nWe can then use GLM theory to construct a 95% confidence interval for the odds ratio:\n\nexp(confint(fit))[2, ]\n#&gt; Waiting for profiling to be done...\n#&gt;  2.5 % 97.5 % \n#&gt;   1.05   1.08\n\nWe can also use the fitted model to compute predicted probabilities for specific ages:\n\npredict(fit, data.frame(age = seq(20, 80, 10)), type = \"response\")\n#&gt;      1      2      3      4      5      6      7 \n#&gt; 0.0963 0.1682 0.2774 0.4216 0.5805 0.7243 0.8330\n\nWhen type = respsone the predict function uses the inverse logistic transformation to convert \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) to estimated probabilities.\nNote that the SAheart dataset includes several additional explanatory variables that could plausibly relate to heart disease risk, such as tobacco use, cholesterol level, and family history. With the glm function, we can include these predictors in the model simply by adding them to the right-hand side of the formula. However, we must be cautious when interpreting the resulting coefficients. A logistic regression model will always return parameter estimates, p-values, and confidence intervals, but these numbers are only meaningful if the model is a reasonable approximation to the data-generating mechanism. In many biomedical settings, there is no underlying physiological theory guaranteeing that the relationship between predictors and the log-odds of disease is linear. Therefore, model assumptions should be examined critically. This includes exploratory data analysis, consideration of possible confounding variables, and assessment of model fit through diagnostic tools, rather than accepting the fitted model at face value.\nPoisson regression example: Effectiveness of insect sprays\nThe InsectSpraysdataset records the number of insects found in agricultural field units after treatment with different sprays:\n\nInsectSprays |&gt;  ggplot(aes(spray, count)) + geom_boxplot() + geom_jitter()\n\n\n\n\n\n\n\nSpray C appears most effective. We could compare mean insect counts across sprays:\n\nwith(InsectSprays, tapply(count, spray, mean))\n\nBut how do we assess uncertainty? Can we construct confidence inteverals? Note that counts are small (often under 10), and the sample size per group is only 12, so normal approximations may be unreliable.\nA Poisson regression provides an alternative. First we choose C as the reference group and then we fit the model:\n\nInsectSprays$spray &lt;- relevel(InsectSprays$spray, ref = \"C\") \nfit &lt;- glm(count ~ spray, family = poisson, data = InsectSprays)\ntidy(fit, conf.int = TRUE)\n#&gt; # A tibble: 6 × 7\n#&gt;   term        estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    0.734     0.200      3.67 2.43e- 4   0.315       1.10\n#&gt; 2 sprayA         1.94      0.214      9.07 1.18e-19   1.54        2.38\n#&gt; 3 sprayB         2.00      0.213      9.36 7.65e-21   1.60        2.44\n#&gt; 4 sprayD         0.859     0.239      3.60 3.20e- 4   0.404       1.34\n#&gt; 5 sprayE         0.519     0.253      2.05 4.00e- 2   0.0315      1.03\n#&gt; # ℹ 1 more row\n\nHere the parameters measure the log of the multiplicative change in the expected count relative to Spray C. If a spray has the same effectiveness as Spray C, its corresponding parameter estimate would be 0. The estimated coefficients and their confidence intervals indicate that several sprays differ substantially from Spray C, and these differences are too large to be explained by Poisson sampling variability alone. In other words, Spray C’s superior performance is unlikely to be due to chance.\nThe inference relies on asymptotic approximations similar to the CLT but often performs better than applying the CLT directly to means when counts are small.\n\n\n\n\n\n\nIn this example, a simple comparison of group means would have also worked. The advantage of Poisson regression becomes more evident when the explanatory variable is continuous, e.g., modeling counts as a function of exposure time or concentration levels, where group-by-group averaging is not practical.\n\n\n\nBeyond logistic and Poisson regression\nLogistic and Poisson regression are the two most widely used GLMs because binary and count outcomes are common in scientific applications. However, the GLM framework is more general:\n\n\nNegative binomial regression handles count data for which the variability is larger than what is predicted by the Poisson model.\n\nGamma regression is useful for positive continuous outcomes such as reaction times or insurance claim sizes.\n\nQuasi-likelihood GLMs allow modeling of mean–variance relationships without fully specifying a distribution.\n\nIn practice, GLMs require careful model checking. Diagnostic plots and residual analysis are especially important here because, unlike ordinary least squares, we are explicitly assuming a particular outcome distribution.\nTo go further, consult the recommended readings listed at the end of the chapter. They cover link functions, likelihood theory, diagnostics, and extensions in much greater detail.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear-models/glm.html#large-samples-small-p-values",
    "href": "linear-models/glm.html#large-samples-small-p-values",
    "title": "\n18  Generalized Linear Models\n",
    "section": "\n18.4 Large samples, small p-values",
    "text": "18.4 Large samples, small p-values\nAs mentioned earlier, reporting only p-values is not always a useful way to report the results of data analysis. In scientific journals, some studies seem to overemphasize p-values. Some of these studies have large sample sizes and report impressively small p-values. Yet by looking closely at the results, we realize that the odds ratios are quite modest: barely bigger than 1. In this case, the difference may not be practically significant or scientifically significant.\nNote that the relationship between odds ratio and p-value is not one-to-one; it depends on the sample size. Therefore, a very small p-value does not necessarily mean a very large odds ratio. Observe what happens to the p-value if we multiply our two-by-two table by 10, which does not change the odds ratio:\n\nox10 &lt;- o |&gt; mutate(men = men*10, women = women*10) \nc(chisq.test(o)$p.value, chisq.test(ox10)$p.value)\n#&gt; [1] 5.09e-02 2.63e-10",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear-models/glm.html#exercises",
    "href": "linear-models/glm.html#exercises",
    "title": "\n18  Generalized Linear Models\n",
    "section": "\n18.5 Exercises",
    "text": "18.5 Exercises\n1. A famous athlete boasts an impressive career, winning 70% of her 500 career matches. Nevertheless, this athlete is criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.\n2. Why did we use the Chi-square test instead of Fisher’s exact test in the previous exercise?\n\nIt actually does not matter, since they give the exact same p-value.\nFisher’s exact and the Chi-square are different names for the same test.\nBecause the sum of the rows and columns of the two-by-two table are not fixed so the hypergeometric distribution is not an appropriate assumption for the null hypothesis. For this reason, Fisher’s exact test is rarely applicable with observational data.\nBecause the Chi-square test runs faster.\n\n3. Now compute the odds ratio of “losing under pressure” and use the approximation we learned to construct a 95% confidence interval. What do you say to those critiquing the athlete? Is the criticism fair?\n4. Repeat exercise 3 but using the glm function. Compare the results.\n5. Use the research_funding_rates data to estimate the log odds ratio and standard errors comparing women to men for each discipline. Compute a confidence interval for each discipline. Plot the log odds ratios and use error bars to denote 95% confidence intervals\n6. Report all the disciplines for which one gender appears to be favored over the other.\n7. Divide the log odds ratio estimates by their respective standard errors and generate a qqplot comparing these to a standard normal. Do any of the disciplines clearly deviate from what is expected by chance?\n8. During the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. Todd Vaziri hypothesized that “Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him).” We will test this hypothesis using association tests. The dslabs object sentiment_counts provides a table with the counts for several sentiments from each source (Android or iPhone):\n\nlibrary(tidyverse)\nlibrary(dslabs)\nsentiment_counts\n\nCompute an odds ratio comparing Android to iPhone for each sentiment and add it to the table.\n9. Compute a 95% confidence interval for each odds ratio.\n10. Generate a plot showing the estimated odds ratios along with their confidence intervals.\n11. For each sentiment, test the null hypothesis that there is no difference between tweets from Android and iPhone and report the sentiments with p-values less than 0.05 and more likely to come from Android.\n12. For each sentiment, find the words assigned to that sentiment, keep words that appear at least 25 times, compute the odd ratio for each, and show a barplot for those with odds ratio larger than 2 or smaller than 1/2.\n13. The titanic_train dataset in the titanic package has data for 891 passangers that were on the Titanic:\n\nlibrary(titanic)\ntitanic_train\n\nIn this dataset:\n\n\nSurvived is a binary variable (Yes / No)\n\nSex, Age, and Class are explanatory variables\n\nFit a logistic regression model with survival as the outcome and sex as the predictor.\n\nfit_gender &lt;- glm(Survived ~ Sex, family = binomial, data = titanic_train)\nsummary(fit_gender)\n\n\nInterpret the sign of the coefficient for Sex.\nCompute and interpret the odds ratio.\nConstruct a 95% confidence interval for the odds ratio.\n\n14. Now fit a model with both Sex and Class:\n\ntitanic_train$Pclass &lt;- factor(titanic_train$Pclass)\nfit_class &lt;- glm(Survived ~ Sex + Pclass, family = binomial, data = titanic_train)\nsummary(fit_class)\n\n\nWhich passenger class has the highest odds of survival, based on the estimates?\nHow does the estimated effect of gender change after adjusting for class? Explain why.\n\n15. Fit a model with an interaction term:\n\nfit_interaction &lt;- glm(Survived ~ Sex * Pclass, family = binomial, data = titanic_train)\nsummary(fit_interaction)\n\n\nDoes the effect of gender vary by class?\nUse exp(coef(fit_interaction)) to interpret interaction patterns in terms of odds ratios.\n\n16. The InsectSprays dataset records insect counts for six spray types.\nRecall the Poisson regression model:\n\nfit_glm &lt;- glm(count ~ spray, family = poisson, data = InsectSprays)\nsummary(fit_glm)\n\nFit a linear regression model to the log-transformed counts (we have to add 0.5 because some counts are 0):\n\nInsectSprays$log_count &lt;- log(InsectSprays$count + 0.5)\nfit_lm &lt;- lm(log_count ~ spray, data = InsectSprays)\nsummary(fit_lm)\n\n\nCompare the estimated spray effects from fit_glm and fit_lm.\nDiscuss the advantages of one model over the other.\n\n17. In GLMs there is no separate error term \\(\\varepsilon\\). However, we can still define residuals by comparing the observed outcome \\(y_i\\) to the fitted value \\(\\hat{y}_i\\). However, unlike in standard linear regression, the variance of \\(Y_i\\) is not constant. For instance:\n\nIn logistic regression, $[Y_i] = p_i(1 - p_i) $, which depends on the fitted probability \\(p_i=\\Pr(Y_i=1)\\).\nIn Poisson regression, \\(\\mathrm{Var}[Y_i]= \\mathrm{E}[Y_i]\\), so the standard error can be estimated with \\(\\sqrt{\\hat{y}_i}\\).\n\nBecause the variability changes across observations, raw residuals $ y_i - _i$ are not directly comparable. Pearson residuals address this by dividing by an estimate of their standard deviation:\n\\[\nr_i = \\frac{y_i - \\hat{y}_i}{\\sqrt{\\widehat{\\mathrm{Var}}(Y_i)}}.\n\\]\nFor a Poisson regression model with fitted mean \\(\\hat{\\mu}_i = \\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)\\), the Pearson residual is:\n\\[\nr_i = \\frac{y_i - \\hat{\\mu}_i}{\\sqrt{\\hat{\\mu}_i}}.\n\\]\nThese residuals are scaled so that they are approximately centered at 0 with variance 1, which makes them useful for diagnostic plots.\nYou can plot Pearson residuals in R using:\n\nplot(fit_glm, which = 1)\n\nTo compare this to fitting a linear model to log-transformed counts:\n\nplot(fit_lm, which = 1)\n\n\nWhich model appears to better describe the data?\nIn what situations might the log-transform approach fail while Poisson regression still works?\n\n18. Explain in your own words:\n\nWhy logistic and Poisson regression are useful when modeling binary or count data.\nWhy it is not always appropriate to apply standard linear regression to these outcomes.\nWhy checking model assumptions (for example, plots, fit diagnostics) is critical when using GLMs.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear-models/glm.html#footnotes",
    "href": "linear-models/glm.html#footnotes",
    "title": "\n18  Generalized Linear Models\n",
    "section": "",
    "text": "http://www.pnas.org/content/112/40/12349.abstract↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html",
    "href": "linear-models/association-not-causation.html",
    "title": "19  Association Is Not Causation",
    "section": "",
    "text": "19.1 Spurious correlation\nAssociation is not causation is perhaps the most important life lesson one can learn in a statistics class. Correlation is not causation is another way to say this. We have described tools useful for quantifying associations between variables. However, we must be careful not to over-interpret these associations.\nThere are many reasons why two observed variables might be correlated without a direct causal relationship. Below, we outline four common situations that can lead to misinterpreting an observed association.\nThe following comical example underscores the concept that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.\nDoes this mean that margarine consumption causes divorces? Or do divorces cause people to eat more margarine? Of course not. This is just an example of what we call a spurious correlation.\nYou can see many more absurd examples on the Spurious Correlations website1.\nThe cases presented on the website are all instances of what is generally called data dredging, data fishing, or data snooping. It’s basically a form of what in the US they call cherry picking. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.\nA Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:\nlibrary(data.table)\n#&gt; \n#&gt; Attaching package: 'data.table'\n#&gt; The following objects are masked from 'package:lubridate':\n#&gt; \n#&gt;     hour, isoweek, mday, minute, month, quarter, second, wday,\n#&gt;     week, yday, year\n#&gt; The following objects are masked from 'package:dplyr':\n#&gt; \n#&gt;     between, first, last\n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     transpose\nlibrary(ggplot2)\nN &lt;- 25\ng &lt;- 1000000\nsim_data &lt;- data.table(group = rep(1:g, each = N), x = rnorm(N*g), y = rnorm(N*g))\nNote we created groups and for each we generated a pair of independent vectors, \\(X\\) and \\(Y\\), with 25 observations each. Because we constructed the simulation, we know that \\(X\\) and \\(Y\\) are not correlated.\nNext, we compute the correlation between X and Y for each group and look for the max:\nres &lt;- sim_data[, .(r = cor(x, y)), by = group][order(-r)]\nmax(res$r)\n#&gt; [1] 0.825\nWe see a maximum correlation of 0.8253108. If you just plot the data from the group achieving this correlation, it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated:\nsim_data[group == res[which.max(r), group]] |&gt;\n  ggplot(aes(x, y)) +\n  geom_point() + \n  geom_smooth(formula = 'y ~ x', method = \"lm\")\nRemember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:\nres |&gt; ggplot(aes(x = r)) + geom_histogram(binwidth = 0.1, color = \"black\")\nIt’s simply a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.2040933, the largest one will be close to 1.\nIf we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:\nlibrary(broom)\nsim_data[group == res[which.max(r), group], broom::tidy(lm(y ~ x))][2,]\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic     p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 x        0.895     0.128      7.01 0.000000383\nThis practice, known as p-hacking, is widely discussed because it may undermine the reliability of scientific findings. Since journals often favor statistically significant results over null findings, researchers have an incentive to highlight significance. In fields such as epidemiology and the social sciences, for instance, an analyst might explore associations between an outcome and many exposures, but only report the one with a small p-value. Similarly, they might try several model specifications to adjust for confounding and select the one that gives the strongest result. In experimental settings, a study could be repeated multiple times, with only the “successful” run reported. Such practices are not always intentional misconduct; they often stem from limited statistical understanding or wishful thinking. More advanced statistics courses cover methods for adjusting analyses to account for these multiple comparisons.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Association Is Not Causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#outliers",
    "href": "linear-models/association-not-causation.html#outliers",
    "title": "19  Association Is Not Causation",
    "section": "\n19.2 Outliers",
    "text": "19.2 Outliers\nSuppose we measure two independent outcomes, \\(X\\) and \\(Y\\), and standardize each set of measurements. Now imagine we make a mistake and forget to standardize just one value, say, entry 23. We can illustrate this situation by simulating such data as follows:\n\nset.seed(1985)\nx &lt;- rnorm(100, 100, 1)\ny &lt;- rnorm(100, 84, 1)\nx[-23] &lt;- scale(x[-23])\ny[-23] &lt;- scale(y[-23])\n\nThe data look like this:\n\nplot(x, y)\n\n\n\n\n\n\n\nNot surprisingly, the correlation is very high:\n\ncor(x,y)\n#&gt; [1] 0.988\n\nBut this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:\n\ncor(x[-23], y[-23])\n#&gt; [1] -0.0442\n\nThere is an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:\n\nplot(rank(x), rank(y))\n\n\n\n\n\n\n\nThe outlier is no longer associated with a very large value, and the correlation decreases significantly:\n\ncor(rank(x), rank(y))\n#&gt; [1] 0.00251\n\nSpearman correlation can also be calculated like this:\n\ncor(x, y, method = \"spearman\")\n#&gt; [1] 0.00251\n\n\n\n\n\n\n\nWhile the Spearman correlation is much less affected by outliers, this robustness comes at a cost. Because it relies on the ranks rather than the actual values, it is less sensitive to subtle linear relationships between variables. As a result, when a true correlation exists but is modest, the Spearman estimate may be closer to zero than the Pearson correlation. In practice, Pearson correlation is more efficient for detecting real linear associations in data with no outliers, while Spearman correlation is preferred when outliers or non-linear monotonic relationships are a concern.\n\n\n\nIn the recommended reading section, we include references on estimation techniques that are robust to outliers and applicable to a wide range of situations.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Association Is Not Causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#reversing-cause-and-effect",
    "href": "linear-models/association-not-causation.html#reversing-cause-and-effect",
    "title": "19  Association Is Not Causation",
    "section": "\n19.3 Reversing cause and effect",
    "text": "19.3 Reversing cause and effect\nAnother way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.\nA form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated2. Consider this quote from the article:\n\n\nWhen we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\n\n\nA very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.\nWe can illustrate cause-effect reversal by fitting the regression\n\\[\nX = \\beta_0 + \\beta_1 y + \\varepsilon\n\\]\nto the father–son height data, where \\(X\\) and \\(y\\) represents father and son heights, respectively. Interpreting \\(\\beta_1\\) causally here would be backwards, as it would imply that sons’ heights determine fathers’ heights. Using the previously defined galton_heights dataset, we do in fact obtain a statistically significant slope, demonstrating that statistical significance does not imply causation or the correct direction of effect.\n\nlm(father ~ son, data = galton_heights) |&gt; tidy() |&gt; filter(term == \"son\")\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic       p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 son      0.407    0.0636      6.40 0.00000000136\n\nThe model fits the data very well. However, if we look only at its mathematical formulation, it could easily be misinterpreted as implying that a son’s height causes his father’s height. From our knowledge of genetics and biology, we know the direction of influence is the opposite. The statistical model itself is not at fault, and the estimates and p-values are calculated correctly. What is misleading here is the interpretation, not the model or computation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Association Is Not Causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#confounders",
    "href": "linear-models/association-not-causation.html#confounders",
    "title": "19  Association Is Not Causation",
    "section": "\n19.4 Confounders",
    "text": "19.4 Confounders\nConfounders are perhaps the most common reason that leads to associations begin misinterpreted.\nIf \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if it, or some factor associated with it, is a cause of both \\(X\\) and \\(Y\\), thereby creating a spurious association between them unless we properly account for \\(Z\\).\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.\nExample: UC Berkeley admissions\nAdmission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and compute a statistical test, which clearly rejects the hypothesis that gender and admission are independent:\n\ntwo_by_two &lt;- admissions |&gt; group_by(gender) |&gt; \n  summarize(total_admitted = round(sum(admitted / 100 * applicants)), \n            not_admitted = sum(applicants) - sum(total_admitted)) |&gt;\n  select(-gender) \nchisq.test(two_by_two)$p.value\n#&gt; [1] 1.06e-21\n\nBut closer inspection shows a paradoxical result. Here are the percent admissions by major:\n\nadmissions |&gt; select(major, gender, admitted) |&gt;\n  pivot_wider(names_from = \"gender\", values_from = \"admitted\") |&gt;\n  mutate(women_minus_men = women - men)\n#&gt; # A tibble: 6 × 4\n#&gt;   major   men women women_minus_men\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1 A        62    82              20\n#&gt; 2 B        63    68               5\n#&gt; 3 C        37    34              -3\n#&gt; 4 D        33    35               2\n#&gt; 5 E        28    24              -4\n#&gt; # ℹ 1 more row\n\nFor five of the six majors, the differences are small, with major A favoring women. More importantly, these differences are all much smaller than the 14.2-point difference in favor of men observed when looking at the overall totals.\nThe paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.\nDiscovering confounders, and understanding how they can lead to misleading conclusions, often requires exploratory data analysis and critical thinking. Looking at the table above, we notice substantial variability in admission rates across majors. Could this be influencing the overall results? To investigate, we add a selectivity column, the overall admission rate within each major, to the admissions table.\n\nselectivity &lt;- admissions |&gt;\n  group_by(major) |&gt;\n  summarize(selectivity = sum(admitted*applicants/100)/sum(applicants))\n\nNext, we examine how the number of applicants relates to major selectivity. If selectivity is a confounder, patterns may differ by gender across majors with varying selectivity.\n\nleft_join(admissions, selectivity, by = \"major\") |&gt;\n  ggplot(aes(selectivity, applicants, label = major)) +\n  geom_text() +\n  facet_wrap(~gender) \n\n\n\n\n\n\n\nWe can see the key issue right away: women are submitting far fewer applications to the less selective majors (A and B). So department selectivity is a confounder that influenced both gender (different application patterns) and admission outcome.\nStratifying\nThe first hint that something was off in the interpretation of the overall admission rates came from stratifying by major. Stratifying by a known or potential confounder is a powerful technique for examining the relationship between two other variables. In exploratory data analysis, stratification can help us detect how a confounder might be distorting the analysis and can offer ideas for how to adjust for it.\nHere is an example in which we plot admissions stratified by major and show that women tend to apply to the more selective majors:\n\nadmissions |&gt; ggplot(aes(major, admitted, col = gender, size = applicants)) + \n  geom_point()\n\n\n\n\n\n\n\nWe see that, major by major, there is not much difference in admission rates between genders. However, the large number of men applying to Major B, which admits over 60% of applicants, causes the confusion in the overall comparison. This plot suggests that a more sophisticated analysis adjusting for major is needed. We will learn how to perform such an analysis in the next chapter, in Chapter 20.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Association Is Not Causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#simpsons-paradox",
    "href": "linear-models/association-not-causation.html#simpsons-paradox",
    "title": "19  Association Is Not Causation",
    "section": "\n19.5 Simpson’s paradox",
    "text": "19.5 Simpson’s paradox\nThe case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication to specific strata. As an illustrative example, suppose you have three random variables \\(X\\), \\(Y\\), and \\(Z\\), and we observe realizations of these. Here is a plot of simulated observations for \\(X\\) and \\(Y\\) along with the sample correlation:\n\n\n\n\n\n\n\n\nYou can see that \\(X\\) and \\(Y\\) are negatively correlated. However, once we stratify by \\(Z\\) (shown in different colors below), another pattern emerges:\n\n\n\n\n\n\n\n\nIt is really \\(Z\\) that is negatively correlated with \\(X\\). If we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated, as seen in the plot above.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Association Is Not Causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#other-sources-of-bias",
    "href": "linear-models/association-not-causation.html#other-sources-of-bias",
    "title": "19  Association Is Not Causation",
    "section": "\n19.6 Other Sources of Bias",
    "text": "19.6 Other Sources of Bias\nWe discussed confounding and reverse causation, but there are many other mechanisms that can create associations that are not causal. For example, Collider bias occurs when we condition on a variable influenced by two or more other variables, inadvertently inducing a false relationship between them. A classic example is the negative association sometimes observed between academic ability and athletic skill among students at elite colleges. In the general population, these traits may be independent, but both increase the likelihood of admission. Conditioning on admission, our collider, can create a spurious negative correlation: among admitted students, those who are less athletic may have higher academic ability, and vice versa. A similar effect can occur in hospitals, where two unrelated health conditions, such as diabetes and heart disease, may appear negatively associated among patients simply because both increase the likelihood of hospitalization.\nSelection bias is another common source of spurious association. It arises when the process determining who or what enters a dataset depends on factors related to both the exposure/treatement and the outcome. For example, comparing mortality rates between two hospitals might suggest that Hospital A provides worse care than Hospital B. However, if Hospital A is a major trauma center treating more severely ill patients, while Hospital B handles mostly routine cases, the comparison is misleading. The difference reflects who is admitted, not differences in quality of care.\nMeasurement bias (or misclassification) occurs when variables are measured with error, especially when those errors differ across groups. Consider a study on smoking and lung cancer: if some participants underreport smoking due to stigma, smokers may be misclassified as nonsmokers. If this underreporting occurs equally across groups, the bias is nondifferential and tends to weaken the observed association. If it differs between groups, say, lung cancer patients are more likely to admit to smoking, the bias is differential and may exaggerate or even reverse the association.\nReaders interested in exploring these sources of bias in greater depth—including how they are represented in causal diagrams, will find recommended references in the Recommended Reading section. These works provide clear explanations and practical guidance for recognizing and avoiding such biases in real data.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Association Is Not Causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#exercises",
    "href": "linear-models/association-not-causation.html#exercises",
    "title": "19  Association Is Not Causation",
    "section": "\n19.7 Exercises",
    "text": "19.7 Exercises\nFor the next set of exercises, we examine the data from a 2014 PNAS paper3 that analyzed success rates from funding agencies in the Netherlands and concluded:\n\nOur results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language used in instructional and evaluation materials.\n\nA response4 was published a few months later titled No evidence that gender contributes to personal research funding success in The Netherlands: A reaction to Van der Lee and Ellemers which concluded:\n\nHowever, the overall gender effect borders on statistical significance, despite the large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines (i.e., with low application success rates for both men and women), then an analysis across all disciplines could incorrectly show “evidence” of gender inequality.\n\nWho is correct here, the original paper or the response? Below, you will examine the data and come to your own conclusion.\n1. The primary evidence for the conclusion of the original paper relies on a comparison of the percentages. Table S1 in the paper includes the information we need:\n\nlibrary(dslabs)\nresearch_funding_rates\n\nConstruct the two-by-two table used for the conclusion about differences in awards by gender.\n2. Compute the difference in percentage from the two-by-two table.\n3. In the previous exercise, we noticed that the success rate is lower for women. But is it significant? Compute a p-value using a Chi-square test.\n4. We see that the p-value is about 0.05. So there appears to be some evidence of an association. But can we infer causation here? Is gender bias causing this observed difference? The response to the original paper claims that what we see here is similar to the UC Berkeley admissions example. Specifically, they state that this “could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines, then an analysis across all disciplines could incorrectly show ‘evidence’ of gender inequality.” To settle this dispute, create a dataset with number of applications, awards, and success rate for each gender. Re-order the disciplines by their overall success rate. Hint: use the reorder function to re-order the disciplines in a first step, then use pivot_longer, separate, and pivot_wider to create the desired table.\n5. To check if this is a case of Simpson’s paradox, plot the success rates versus disciplines, which have been ordered by overall success, with colors to denote the genders and size to denote the number of applications.\n6. We definitely do not see the same level of confounding as in the UC Berkeley example. It is hard to say that there is a clear confounder here. However, we do see that, based on the observed rates, some fields favor men and others favor women. We also see that the two fields with the largest difference favoring men are also the fields with the most applications. But, unlike the UC Berkeley example, women are not more likely to apply for the harder subjects. Is it possible some of the selection committees are biased and others are not?\nTo answer this question we start by checking if any of the differences seen above are statistically significant. Remember that even when there is no bias, we will see differences due to random variability in the review process as well as random variability across candidates. Perform a Chi-square test for each discipline. Hint: define a function that receives the total of a two-by-two table and returns a data frame with the p-value. Use the 0.5 correction. Then use the summarize function.\n7. In the medical sciences, there appears to be a statistically significant difference, but could this be a spurious correlation? We performed 9 tests. Reporting only the one case with a p-value less than 0.05 might be considered an example of cherry picking. Repeat the exercise above, but instead of a p-value, compute a log odds ratio divided by their standard error. Then use qq-plot to see how much these log odds ratios deviate from the normal distribution we would expect: a standard normal distribution.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Association Is Not Causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#footnotes",
    "href": "linear-models/association-not-causation.html#footnotes",
    "title": "19  Association Is Not Causation",
    "section": "",
    "text": "http://tylervigen.com/spurious-correlations↩︎\nhttps://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated↩︎\nhttp://www.pnas.org/content/112/40/12349.abstract↩︎\nhttp://www.pnas.org/content/112/51/E7036.extract↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Association Is Not Causation</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html",
    "href": "linear-models/multivariable-regression.html",
    "title": "20  Multivariable Regression",
    "section": "",
    "text": "20.1 Case study: Moneyball\nSince Galton’s original development, regression has become one of the most widely used tools in data analysis. One reason is that an adaptation of the original regression approach, based on linear models, permits us to find relationships between two variables taking into account the effects of other variables that affect both. This has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology.\nWhen we are unable to randomly assign each individual to a treatment or control groups, confounding becomes particularly prevalent. For instance, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in New York City. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Consequently, a naive regression model may lead to an overestimate of the negative health effects of fast food. So, how do we account for confounding in practice? In this chapter, we learn how multivariable regression can help with such situations and can be used to describe how one or more variables affect an outcome variable. We illustrate with a real-world example in which data was used to help pick under appreciated players to improve a resource-limited sports team.\nMoneyball: The Art of Winning an Unfair Game by Michael Lewis focuses on the Oakland Athletics (A’s) baseball team and its general manager, Billy Beane, the person tasked with building the team.\nTraditionally, baseball teams had used scouts to help them decide what players to hire. These scouts evaluate players by observing them perform, tending to favor athletic players with observable physical abilities. For this reason, scouts generally agree on who the best players are and, as a result, these players are often in high demand. This in turn drives up their salaries.\nFrom 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to hire the best players and, during that time, were one of the best teams. However, in 1995, the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball: in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s payroll of $39,679,746. The A’s could no longer afford the most sought-after players. As a result, Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to relying exclusively on scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a significant role in this approach. In this section, we will illustrate how data can be used to support this approach.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#case-study-moneyball",
    "href": "linear-models/multivariable-regression.html#case-study-moneyball",
    "title": "20  Multivariable Regression",
    "section": "",
    "text": "Baseball data\nStatistics have been recorded in baseball since its beginnings. The dataset we will be using, included in the Lahman library, goes back to the 19th century. For example, a summary statistic we will describe soon, the batting average (AVG), has been used for decades to summarize a player’s success. Other statistics1, such as home runs (HR) and runs batted in (RBI) are reported for each player in the game summaries included in the sports section of news outlets, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily chosen without much thought as to whether they actually predicted anything or were related to helping a team win.\nThis changed with Bill James2. In the late 1970s, he started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to determine which outcomes best predicted if a team would win sabermetrics3. Yet until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Today, sabermetrics is no longer confined to baseball; its principles have spread to other sports, where it is now known as sports analytics.\nIn this chapter, we will conduct a data analysis to evaluate whether one of Billy Beane’s key strategies—hiring players who “get on base”, holds up statistically. We will break down what this concept means, and through the lens of multivariable regression, demonstrate that it was not only statistically valid but also an economically effective approach. To understand the analysis, we will need to learn a bit about how baseball works.\nBaseball basics\nTo understand how regression helps us find undervalued players, we don’t need to delve into all the details of the game of baseball, which has over 100 rules. Here, we distill the basic knowledge necessary for effectively addressing the data science challenge.\nThe goal of baseball is simple: score more runs (points) than the other team. A player scores a run by starting at a spot called home plate, passing three bases in order (first, second, and third), and returning to home plate. The game begins with the opposing team’s pitcher throwing the ball toward the batter, who stands at home plate and tries to hit the ball. If the batter hits the ball far enough to run around all three bases and back to home plate in one play, this is called a home run4. If the batter doesn’t hit a home run, theystop at one of the bases. From there, they wait for their teammates to hit the ball so they can move to the next base. If the pitcher throws poorly, the batter gets to walk to first base as a penalty for the pitcher, referred to as base on balls (BB). A player on a base can try to run to the next base without waiting for a teammate’s hit. This is called stealing a base5 (SB).\nBatters can also fail to reach base, resulting in an out. Another way to make an out is a failed attempt at stealing a base. Each team continues batting until they accumulate three outs. Once this happens, the other team takes their turn to bat. Each team gets nine turns, called innings, to score runs.\nEach time a batter attempts to reach base, with the goal of eventually scoring a run, it is referred to as a plate appearance (PA). There are, with rare exceptions, five ways a plate appearance can be successful:\n\n\nSingle – The batter reaches first base.\n\nDouble – The batter reaches second base.\n\nTriple – The batter reaches third base.\n\nHome Run (HR) – The batter circles all bases and returns to home plate.\n\nBase on Balls (BB) – The pitcher throws poorly, and the batter is allowed to go to first base as a penalty for the pitcher.\n\nThe first four outcomes (Single, Double, Triple, and HR) are all considered hits, while a BB is not. This distinction is important for understanding the data and analysis that follow.\nNo awards for base on balls\nHistorically, the batting average has been considered the most important offensive statistic. To define this average, we divide the total number of hits (H) by the total number of at bats (AB) defined as the number of times you either get a hit or make an out; BB are excluded. Today, this success rate ranges from 20% to 38%.\n\n\n\n\n\n\n\n\nOne of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. Instead of batting average, James proposed the use of the on-base percentage (OBP), which he defined as (H+BB)/PA, or simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that accumulates many more BB than the average player might go unrecognized if the batter does not excel in batting average.\nBut is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. We can use data to try and make a case that BB do indeed help produce runs and should be valued.\nUsign baseball data\nTo illustrate how data can be used to answer questiosn in Baseball we start with a simple example. We will compare BB to stolen bases (SB). In contrast to BB, total SB were considered important and an award6 given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data to determine if it’s better to pay for players with high BB or high SB?\nOne of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. Although we keep track of the number of runs scored by a player, remember that if player X bats right before someone who hits many HR, batter X will score many runs. Note these runs don’t necessarily happen if we hire player X, but not his HR hitting teammate.\nHowever, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? Let’s examine some data! We start by creating a data frame with statistics from 1962 (the first year all teams played 162 games, like today, instead of 154) to 2001 (the year before the team featured in Money Ball was constructed). We convert the data to a per game rate, because a small proportion of seasons had less games than usual due to strikes, and some teams played extra games due to tie breakers. We also define a singles column for later use.\n\nlibrary(tidyverse)\nlibrary(Lahman)\ndat &lt;- Teams |&gt; filter(yearID %in% 1962:2001) |&gt;\n  mutate(singles = H - X2B - X3B - HR) |&gt;\n  select(teamID, yearID, G, R, SB, singles, BB, HR) |&gt;\n  mutate(across(-c(teamID, yearID), ~ ./G)) |&gt; select(-G)\n\nNow let’s start with a obvious question: do teams that hit more HR score more runs?\n\ndat |&gt; ggplot(aes(HR, R)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nThe plot shows a strong association: teams with more HR tend to score more runs. Now let’s examine the relationship between stolen bases and runs:\n\ndat |&gt; ggplot(aes(SB, R)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nHere the relationship is not as clear.\nFinally, let’s examine the relationship between BB and runs:\n\ndat |&gt; ggplot(aes(BB, R)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nHere again we see a clear association. But does this mean that increasing a team’s BB causes an increase in runs? As we learned in Chapter 19, association is not causation. In fact, it looks like BB and HR are also associated:\n\ndat |&gt; ggplot(aes(HR, BB)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nWe know that HR cause runs because when a player hits a HR, they are guaranteed at least one run. Could it be that HR also cause BB and this makes it appear as if BB cause runs? Are BB cofounded with HR? Linear regression can help us parse out the information and quantify the associations. This, in turn, will aid us in determining what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BB, but keep the HR fixed?\nRegression applied to baseball statistics\nCan we use regression with these data? First, notice that the HR and runs data, shown above, appear to be bivariate normal. Specifically, qqplots confirm that the normal approximation for runs for each HR strata is useful here:\n\ndat |&gt; mutate(hr_strata = round(scale(HR))) |&gt;\n  filter(hr_strata %in% -2:3) |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = R)) +\n  facet_wrap(~hr_strata) \n\n\n\n\n\n\n\nSo we are ready to use linear regression to predict the number of runs a team will score, if we know how many HR the team hits using regression:\n\nhr_fit  &lt;- lm(R ~ HR, data = dat)\nsummary(hr_fit)$coef[2,]\n#&gt;   Estimate Std. Error    t value   Pr(&gt;|t|) \n#&gt;   1.86e+00   4.97e-02   3.74e+01  8.90e-193\n\nThe regression line can be plotted using the geom_smooth function:\n\ndat |&gt; ggplot(aes(HR, R)) + geom_point(alpha = 0.5) + geom_smooth(method = \"lm\")\n\nThe slope is 1.86. This tells us that teams that hit 1 more HR per game than the average team, score 1.86 more runs per game than the average team. Given that the most common final score is a difference of one run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because the As were working on a budget, they needed to find some other way to increase wins. In the next section we examine this more carefully.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#confounding",
    "href": "linear-models/multivariable-regression.html#confounding",
    "title": "20  Multivariable Regression",
    "section": "\n20.2 Confounding",
    "text": "20.2 Confounding\nPreviously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from BB, we a get slope of:\n\nbb_slope &lt;- lm(R ~ BB, data = dat)$coef[2]\nbb_slope \n#&gt;    BB \n#&gt; 0.743\n\nDoes this mean that if we go and hire low salary players with many BB, and who increase the number of walks per game by 2, our team will score 1.5 more runs per game? Association is not causation: although the data shows strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game, this does not mean that BB are the cause.\nNote that, if we compute the regression line slope for singles, we get:\n\nlm(R ~ singles, data = dat)$coef[2]\n#&gt; singles \n#&gt;   0.452\n\nwhich is a lower value than what we obtain for BB. Remember that a single gets you to first base just like a BB. Baseball fans will point out that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason is because of confounding.\nHere we show the correlation between HR, BB, and singles:\n\ndat |&gt; select(singles, BB, HR) |&gt; cor()\n#&gt;         singles      BB     HR\n#&gt; singles  1.0000 -0.0495 -0.171\n#&gt; BB      -0.0495  1.0000  0.406\n#&gt; HR      -0.1714  0.4064  1.000\n\nHR and BB are highly correlated! Experts will point out that pitchers are intimidated by players that excel at hitting HR and this leads to the bad performance that awards batters BB. As a result, HR hitters tend to have more BB, and a team with many HR will also have more BB. Although it may appear that BB cause runs, it is actually the HR that cause most of these runs. BB are confounded with HR. Nonetheless, could it be that BB still help? To find out, we somehow have to adjust for the HR effect. Multivariable regression can help with this.\nA first approach is to keep HR fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest tenth. We filter out the strata with few points, to avoid highly variable estimates, and then make a scatterplot for each strata:\n\ndat |&gt; mutate(hr_strata = round(HR, 1)) |&gt; \n  filter(hr_strata &gt;= 0.4 & hr_strata &lt;= 1.2) |&gt;\n  ggplot(aes(BB, R)) +  \n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = \"y~x\", method = \"lm\") +\n  facet_wrap(~hr_strata) \n\n\n\n\n\n\n\nOnce we stratify by HR, these slopes are substantially reduced:\n\ndat |&gt; mutate(hr_strata = round(HR, 1)) |&gt; \n  filter(hr_strata &gt;= 0.5 & hr_strata &lt;= 1.2) |&gt;  \n  group_by(hr_strata) |&gt;\n  summarize(coef = lm(R ~ BB)$coef[2])\n#&gt; # A tibble: 8 × 2\n#&gt;   hr_strata  coef\n#&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1       0.5 0.566\n#&gt; 2       0.6 0.405\n#&gt; 3       0.7 0.284\n#&gt; 4       0.8 0.370\n#&gt; 5       0.9 0.266\n#&gt; # ℹ 3 more rows\n\nRemember that the regression slope for predicting runs with BB was 0.7.\nThe slopes are reduced, but they are not 0, which indicates that BB are helpful for producing runs, just not as much as suggested by the single variable analysis. In fact, the values above are closer to the slope we obtained from singles, 0.5, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.\nAlthough our understanding of the application tells us that HR cause BB, but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BB. In this case, the slopes do not change much from the original:\n\ndat |&gt; mutate(bb_strata = round(BB, 1)) |&gt; \n  filter(bb_strata &gt;= 2.5 & bb_strata &lt;= 3.2) |&gt;  \n  group_by(bb_strata) |&gt;\n  summarize(coef = lm(R ~ HR)$coef[2])\n#&gt; # A tibble: 8 × 2\n#&gt;   bb_strata  coef\n#&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1       2.5  1.98\n#&gt; 2       2.6  1.07\n#&gt; 3       2.7  1.61\n#&gt; 4       2.8  1.50\n#&gt; 5       2.9  1.57\n#&gt; # ℹ 3 more rows\n\nThey are reduced slightly from 1.86, which is consistent with the fact that BB do in fact cause some runs.\nRegardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#sec-regression-in-r",
    "href": "linear-models/multivariable-regression.html#sec-regression-in-r",
    "title": "20  Multivariable Regression",
    "section": "\n20.3 Multivariable regression",
    "text": "20.3 Multivariable regression\nIt is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:\n\\[\n\\mathrm{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2\n\\]\nwith the slopes for \\(x_1\\) changing for different values of \\(x_2\\) and the other way around. But is there an easier approach?\nIf we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that \\(\\beta_1(x_2)\\) and \\(\\beta_2(x_1)\\) are constants. This, in turn, implies that the expectation of runs conditioned on HR and BB can be written as follows:\n\\[\n\\mathrm{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nThis model suggests that, if the number of HR is fixed at \\(x_2\\), we observe a linear relationship between runs and BB with an intercept of \\(\\beta_0 + \\beta_2 x_2\\). Our exploratory data analysis suggested that this is the case. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by \\(\\beta_1\\). In this analysis, referred to as multivariable regression, you will often hear people say that the BB slope \\(\\beta_1\\) is adjusted for the HR effect.\nBecause the data is approximately normal and conditional distributions were also normal, we are justified in using a linear model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n\\]\nwith \\(Y_i\\) runs per game for team \\(i\\), \\(x_{i,1}\\) BB per game, \\(x_{i,2}\\) the HR per game, and \\(\\varepsilon_i\\) assumed to be independent and identically distributed.\nTo use lm here, we need to let the function know we have two predictor variables. We use the + symbol as follows:\n\nfit &lt;- lm(R ~ BB + HR, data = dat)\nsummary(fit)$coef[2:3,]\n#&gt;    Estimate Std. Error t value  Pr(&gt;|t|)\n#&gt; BB    0.391     0.0273    14.3  1.55e-42\n#&gt; HR    1.570     0.0495    31.7 2.39e-153\n\nWhen we fit the model with only one variable, the estimated slopes were 0.39 and 0.39 for BB and HR, respectively. Note that when fitting the multivariable model both go down, with the BB effect decreasing much more.\n\n\n\n\n\n\nYou are ready to do exercises 1-12, if you want to practice before continuing.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#baseball-analytics",
    "href": "linear-models/multivariable-regression.html#baseball-analytics",
    "title": "20  Multivariable Regression",
    "section": "\n20.4 Baseball analytics",
    "text": "20.4 Baseball analytics\nIn the previous section, we used multivariable regression to confirm that walks (BB) are a significant predictor of runs scored. In this section, we will take a data-driven approach to develop a metric for ranking the offensive production of baseball players. Specifically, we will construct a regression model to predict the number of runs a player contributes based on their offensive statistics. By incorporating salary information into this analysis, we can identify players in 2002 who were projected to generate runs but were under-compensated. Importantly, this analysis uses data that excludes the 2002 season to mimic the challenge of building a team for the upcoming season.\nAt the individual player level, distinguishing between runs scored and runs produced is crucial for accurately assessing offensive contributions. While runs scored are directly recorded, they don’t fully capture the extent of a player’s impact. For instance, if player X hits a single and later scores on a teammate’s home run, the run is attributed to player X, but the teammate’s effort was essential in making it happen. This overlap highlights the shared nature of offensive production, complicating individual performance analysis.\nTo address this, we will first fit the model at the team level, where such individual-level nuances average out and do not affect overall results. Once the model is validated for teams, we will apply it to estimate contributions at the player level.\n\n\n\n\n\n\nTeams are divided into two leagues, American and National. Since they had slightly different rules during the period in question, we will fit the model only to the American League (AL), where the Oakland A’s played.\n\n\n\nSince teams accumulate significantly more plate appearances than individual players, we will model per plate appearance (PA) rates. This allows us to generalize the model fit to teams and apply it to players. Additionally, since triples are relatively rare, we will combine them with doubles into a single category, extra bases (XB), to simplify the model. Stolen bases will be excluded because prior analyses suggest they do not reliably correlate with increased scoring. As a result, the predictors in the model will include: base on balls (BB), singles, extra bases (XB) and home runs (HR). The data preparation step for this model is shown below:\n\ndat &lt;- Teams |&gt; \n  filter(yearID %in% 1962:2002 & lgID == \"AL\") |&gt;\n  mutate(XB = X2B + X3B, singles = H - XB - HR, PA = AB + BB) |&gt;\n  select(yearID, teamID, R, BB, singles, XB, HR, PA) |&gt;\n  mutate(across(-c(yearID, teamID, PA), ~ ./PA)) \n\nTo build the model, we make a reasonable assumption that our outcome variable (runs per plate appearance) and the four predictor variables (BB, singles, extra bases, and home runs) are jointly normal. This implies that for any one predictor, the relationship with the outcome is linear when the other predictors are held constant. Under this assumption, the linear regression model can be expressed as:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\varepsilon_i\n\\]\nwhere \\(Y_i\\) represents runs produced per plate appearance, and \\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}\\) represent BB, singles, extra bases, and HR per plate appearance, respectively.\nWe can fit the model to data before the 2002 season, since we are building a team for 2002 with the information we had before the season started:\n\nfit &lt;- dat |&gt; filter(yearID &lt; 2002) |&gt; lm(R ~ BB + singles + XB + HR, data = _)\n\nWe can use the fitted model to predict run production for each team:\n\nR_hat &lt;- dat |&gt; filter(yearID == 2002) |&gt; predict(fit, newdata = _)\n\nNote that this model, fit to 1962 to 2001 data, predicts runs for 2002 very well:\n\n\n\n\n\n\n\n\nWe also observe that Oakland, despite being one of the lowest-spending teams, managed to perform as an above-average offensive team. Additionally, note that the fitted model assigns similar weight to BB and singles:\n\nfit$coefficients[-1]\n#&gt;      BB singles      XB      HR \n#&gt;   0.454   0.586   0.914   1.452\n\nNow let’s apply the model to players. The Batting data frame includes player specific statistics. We prepare the data so that we save the same per-PA rates the model was fit to. Because players abilities change through time we only use data close to 2002 but to improve the precision of our estimates we use data from the three years before 2002, rather than just 2001. We also remove players with less than 100 PA to avoid imprecise summaries:\n\nplayers &lt;- Batting |&gt; \n  filter(yearID %in% 1999:2001) |&gt; \n  group_by(playerID) |&gt;\n  summarize(XB = sum(X2B + X3B), PA = sum(AB + BB), HR = sum(HR), H = sum(H), \n            singles = H - XB - HR, BB = sum(BB), AVG = sum(H)/sum(AB)) |&gt;\n  filter(PA &gt;= 100) |&gt;\n  mutate(across(-c(playerID, PA, AVG), ~ ./PA)) \n\nNow that statistics are in per-PA rates, we can use the model fitted to team-level data to predict how many runs per plate appearce each player will produce:\n\nplayers$R_hat &lt;- predict(fit, players)\n\nNext we use the Salary, People,Apperances data frames to add information we need for the rest of the analysis.\nWe start by adding the salary each player garnered in 2002 and remove players that did not play that year:\n\nplayers &lt;-  Salaries |&gt; \n  filter(yearID == 2002) |&gt; \n  select(playerID, salary) |&gt;\n  right_join(players, by = \"playerID\") |&gt;\n  filter(!is.na(salary))\n\nWe then add their first and last names for context. Additionally, because Major League Baseball players cannot negotiate their contracts or choose the team they play for until they have accumulated six years of playing time and become free agents, we include their debut year as we need to take this into consideration in our analysis.\n\nplayers &lt;- People |&gt; \n  select(playerID, nameFirst, nameLast, debut) |&gt;\n  mutate(debut = year(as.Date(debut))) |&gt;\n  right_join(players, by = \"playerID\")\n\nFinally, we remove pitchers since we are only interested in batters.\n\nplayers &lt;- Appearances |&gt; filter(yearID == 2002) |&gt; \n  group_by(playerID) |&gt;\n  summarize(G_p = sum(G_p)) |&gt;\n  right_join(players, by = \"playerID\") |&gt;\n  filter(G_p == 0) |&gt; select(-G_p)\n\nIf you followed baseball during the era in question you will recognize the top run producers and not be surprised that garnered high salaries:\n\nplayers |&gt; select(nameFirst, nameLast, R_hat, salary) |&gt; \n  arrange(desc(R_hat)) |&gt; head()\n#&gt; # A tibble: 6 × 4\n#&gt;   nameFirst nameLast R_hat   salary\n#&gt;   &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1 Barry     Bonds    0.239 15000000\n#&gt; 2 Todd      Helton   0.218  5000000\n#&gt; 3 Manny     Ramirez  0.216 15462727\n#&gt; 4 Jason     Giambi   0.215 10428571\n#&gt; 5 Larry     Walker   0.213 12666667\n#&gt; # ℹ 1 more row\n\nNote that because each team has, on average, 37.5 plate appearances a game, a rate of 0.24 runs per plate appearance translates to 9 runs a game, almost double the league average!\nIf we plot predicted runs produced versus salary we can see that there is substantial variation across players in predicted runs produced and that, not surprisingly, players that produce runs garner higher salaries.\n\n\n\n\n\n\n\n\nThe plot also highlights four players. Before the 2002 season Oakland lost their best run producer, Jason Giambi, because they could not compete with the New York Yankees offer of over 10 million salary. Johnny Damon, another above average run producers (the dashed line shows the average) also left as Oakland was not willing to pay the 7.25 million salary the Boston Red Sox offered him. Oakland had to make up for this run production, but with a very limited budget. To do this, their first addition was Scott Hatterberg, whose salary was only $900,000, one of the lowest in the league, yet was predicted to produce more than average runs. Note that among the players in our data frame, Hatterberg while having a below league average AVG, ranked near the top 10% in BB per plate appearance. Similarly, David Justice had a near league average AVG, but ranked near the top 5% in terms of BB per plate appearance. These savings permitted the As to upgrade one of their current players, Frank Menechino, for Ray Durham.\n\nfilter(players, playerID %in% c(\"damonjo01\", \"giambja01\", \"menecfr01\")) |&gt; \n  bind_rows(filter(players, playerID %in% c(\"justida01\", \"durhara01\", \"hattesc01\"))) |&gt; \n  select(nameFirst, nameLast, AVG, BB,  R_hat) \n#&gt; # A tibble: 6 × 5\n#&gt;   nameFirst nameLast    AVG     BB R_hat\n#&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Johnny    Damon     0.296 0.0930 0.141\n#&gt; 2 Jason     Giambi    0.330 0.188  0.215\n#&gt; 3 Frank     Menechino 0.245 0.137  0.126\n#&gt; 4 Ray       Durham    0.281 0.103  0.141\n#&gt; 5 Scott     Hatteberg 0.257 0.131  0.128\n#&gt; # ℹ 1 more row\n\nDespite losing two star players, the A’s predicted runs per game (R_hat) dropped only slightly, from 0.482 to 0.431, and the savings allowed them to acquire pitcher Billy Koch, who helped reduce opponents’ scoring.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#exercises",
    "href": "linear-models/multivariable-regression.html#exercises",
    "title": "20  Multivariable Regression",
    "section": "\n20.5 Exercises",
    "text": "20.5 Exercises\nWe have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing their stability across the years. Since we have to pick players based on their previous performances, we prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BB.\n1. Before we begin, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2002 table, keeping only players with more than 100 plate appearances:\n\nlibrary(Lahman)\ndat &lt;- Batting |&gt; filter(yearID == 2002) |&gt;\n  mutate(pa = AB + BB, \n         singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) |&gt;\n  filter(pa &gt;= 100) |&gt;\n  select(playerID, singles, bb)\n\nNow, compute a similar table called avg, but with rates computed over 1999-2001.\n2. You can use the inner_join function to combine the 2002 data and averages in the same table:\n\ndat &lt;- inner_join(dat, avg, by = \"playerID\")\n\nCompute the correlation between 2002 and the previous seasons for singles and BB.\n3. Note that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate.\n4. Now fit a linear model for each metric and use the confint function to compare the estimates.\n5. In a previous section, we computed the correlation between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons. We noticed that the highest correlation is between fathers and sons and the lowest is between mothers and sons. We can compute these correlations using:\n\nlibrary(HistData)\nset.seed(1)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  group_by(family, gender) |&gt;\n  sample_n(1) |&gt;\n  ungroup()\n\ncors &lt;- galton_heights |&gt; \n  pivot_longer(father:mother, names_to = \"parent\", values_to = \"parentHeight\") |&gt;\n  mutate(child = ifelse(gender == \"female\", \"daughter\", \"son\")) |&gt;\n  unite(pair, c(\"parent\", \"child\")) |&gt; \n  group_by(pair) |&gt;\n  summarize(cor = cor(parentHeight, childHeight))\n\nAre these differences statistically significant? To answer this, we will compute the slopes of the regression lines along with their standard errors. Start by using lm and the broom package to compute the slopes LSE and the standard errors.\n6. Repeat the exercise above, but compute a confidence interval as well.\n7. Plot the confidence intervals and notice that they overlap, which implies that the data is consistent with the inheritance of height being independent of sex.\n8. Because we are selecting children at random, we can actually do something like a permutation test here. Repeat the computation of correlations 100 times taking a different sample each time. Hint: Use similar code to what we used with simulations.\n9. Fit a linear regression model to obtain the effects of BB and HR on Runs (at the team level) in 1971. Use the tidy function in the broom package to obtain the results in a data frame.\n10. Now let’s repeat the above for each year since 1962 and make a plot. Use summarize and the broom package to fit this model for every year since 1962.\n11. Use the results of the previous exercise to plot the estimated effects of BB on runs.\n12. Advanced. Write a function that takes R, HR, and BB as arguments and fits two linear models: R ~ BB and R~BB+HR. Then use the summary function to obtain the BB for both models for each year since 1962. Then plot these against each other as a function of time.\n13. Since the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:\n\\[\n\\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{\\mbox{Singles} + 2 \\mbox{Doubles} + 3 \\mbox{Triples} + 4\\mbox{HR}}{\\mbox{AB}}\n\\]\nThey called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we demonstrate how this metric closely aligns with regression results.\nCompute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.\n14. For every year since 1962, compute the correlation between runs per game and OPS. Then plot these correlations as a function of year.\n15. Keep in mind that we can rewrite OPS as a weighted average of BB, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: The weight for BB relative to singles will be a function of AB and PA.\n16. Consider that the weight for BB, \\(\\frac{\\mbox{AB}}{\\mbox{PA}}\\), will change from team to team. To assess its variability, compute and plot this quantity for each team for each year since 1962. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.\n17. So now we know that the formula for OPS is proportional to \\(0.91 \\times \\mbox{BB} + \\mbox{singles} + 2 \\times \\mbox{doubles} + 3 \\times \\mbox{triples} + 4 \\times \\mbox{HR}\\). Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1962, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.\n18. We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1962, compute the OPS, the predicted runs with the regression model, and compute the correlation between the two, as well as the correlation with runs per game.\n19. We see that using the regression approach predicts runs slightly better than OPS, but not that much. However, note that we have been computing OPS and predicting runs for teams when these measures are used to evaluate players. Let’s show that OPS is quite similar to what one obtains with regression at the player level. For the 1962 season and onward, compute the OPS and the predicted runs from our model for each player, and plot them. Use the PA per game correction we used in the previous chapter:\n20. Which players have shown the largest difference between their rank by predicted runs and OPS?",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#footnotes",
    "href": "linear-models/multivariable-regression.html#footnotes",
    "title": "20  Multivariable Regression",
    "section": "",
    "text": "https://www.mlb.com/stats/↩︎\nhttps://en.wikipedia.org/wiki/Bill_James↩︎\nhttps://en.wikipedia.org/wiki/Sabermetrics↩︎\nhttps://www.youtube.com/watch?v=JSE5kfxkzfk↩︎\nhttps://www.youtube.com/watch?v=JSE5kfxkzfk↩︎\nhttp://www.baseball-almanac.com/awards/lou_brock_award.shtml↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/reading-lm.html",
    "href": "linear-models/reading-lm.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "Gelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\nA comprehensive and applied introduction to regression modeling, emphasizing causal inference and hierarchical data structures. Excellent bridge between classical and modern approaches.\nFox, J. (2015). Applied Regression Analysis and Generalized Linear Models (3rd ed.). Sage.\nClear exposition of linear and multiple regression, diagnostics, and model interpretation. Useful for both theory and R-based practice.\nKutner, M. H., Nachtsheim, C. J., & Neter, J. (2004). Applied Linear Regression Models (4th ed.). McGraw-Hill.\nA widely used undergraduate/graduate text emphasizing linear model formulation, assumptions, and practical applications.\nMontgomery, D. C. (2017). Design and Analysis of Experiments (9th ed.). Wiley.\nThe standard reference for experimental design, randomization, and treatment effect estimation ,from randomized blocks to factorial designs.\nAngrist, J. D., & Pischke, J.-S. (2009). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\nFocuses on causal inference and treatment effects through the lens of linear models, including instrumental variables and difference-in-differences.\nFreedman, D. A. (2009). Statistical Models: Theory and Practice. Cambridge University Press.\nConceptually sharp and rigorous introduction to what regression models mean and how assumptions affect inference.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R (2nd ed.). Springer.\nChapters 3–4 provide a gentle, modern introduction to simple and multiple regression with R code examples.\nAgresti, A. (2013). Categorical Data Analysis (3rd ed.).\nAuthoritative resource on association tests and categorical data.\nDobson, A. J., & Barnett, A. G. (2018). An Introduction to Generalized Linear Models (4th ed.).\nA clear, accessible introduction to GLMs.\nMcCullagh, P., & Nelder, J. A. (1989). Generalized Linear Models (2nd ed.).\nThe classic and definitive text on GLMs; more theoretical but essential for deep understanding.\nTyler Vigen. Spurious Correlations.\nA lighthearted but instructive look at how unrelated variables can appear strongly correlated. Helps develop critical thinking about association versus causation.\nWebsite\nGreenland, S., Pearl, J., & Robins, J. M. (1999). Causal Diagrams for Epidemiologic Research. Epidemiology, 10(1), 37–48.\nA foundational paper explaining confounding and causal thinking using directed acyclic graphs (DAGs).\nRothman, K. J., Greenland, S., & Lash, T. L. (2021). Modern Epidemiology (4th ed.).\nA thorough reference on confounding, bias, and study design, often used in epidemiology and public health.",
    "crumbs": [
      "Linear Models",
      "Recommended Reading"
    ]
  },
  {
    "objectID": "highdim/intro-highdim.html",
    "href": "highdim/intro-highdim.html",
    "title": "High Dimensional Data",
    "section": "",
    "text": "High-dimensional datasets are increasingly common in modern data analysis, especially in fields like genomics, image processing, natural language processing, and recommender systems. There is a variety of computational techniques and statistical concepts that are useful for analyzing datasets in which each observation is associated with a large number of numerical variables. In this part of the book, we introduce ideas that are useful in the analysis of these high-dimensional datasets. Specifically, we provide brief introductions to linear algebra, dimension reduction, matrix factorization, and regularization. As motivating examples, we use handwritten digit recognition and movie recommendation systems, both of which involve high-dimensional datasets with hundreds or thousands of variables per observation. We start this part of the book by demonstrating how to work with matrices in R.\nOne specific task we use to motivate linear algebra is measuring the similarity between two handwritten digits. Because each digit is represented by \\(28 \\times 28 = 784\\) pixel values, we cannot simply subtract two vectors as we would in a one-dimensional setting. Instead, we treat each observation as a point in a high-dimensional space and use a mathematical definition of distance to quantify similarity. Many machine learning techniques introduced later in the book rely on this geometric interpretation.\nWe also use this high-dimensional concept of distance to motivate dimension reduction, a set of techniques that summarize high-dimensional data in lower-dimensional representations that are easier to visualize and analyze, while preserving the essential information. Distance between observations provides a concrete example: we aim to reduce the number of variables while preserving the pairwise distances between observations as much as possible. This leads naturally to matrix factorization methods, which arise from the mathematical structure underlying these techniques.\nFinally, we introduce the concept of regularization, which is useful when analyzing high-dimensional data. In many applications, the large number of variables increases the risk of overfitting or cherry-picking results that appear significant by chance. Regularization provides a mathematically principled way to constrain models, improve generalization, and avoid misleading conclusions.\nTogether, these topics lay the groundwork for understanding and implementing many of the machine learning techniques we cover in the next part of the book.",
    "crumbs": [
      "High Dimensional Data"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html",
    "href": "highdim/matrices-in-R.html",
    "title": "\n21  Matrices in R\n",
    "section": "",
    "text": "21.1 Notation\nWhen the number of variables associated with each observation is large and they can all be represented as numbers, it is often more convenient to store the data in a matrix and perform the analysis using linear algebra operations, rather than storing the data in a data frame and using tidyverse or data.table functions. Matrix operations form the computational foundation for many applications of linear algebra in data analysis.\nIn fact, many of the most widely used machine learning algorithms—including linear regression, principal component analysis, neural networks, and deep learning—are built around linear algebra concepts, and their implementations rely heavily on efficient matrix operations. Becoming fluent in creating, manipulating, and interpreting matrices in R will make it easier to understand and implement these methods in practice.\nAlthough we introduce the mathematical framework of linear algebra in the next chapter, the examples we use there rely on being able to work with matrices in R. Therefore, before diving into the mathematical concepts, we begin here with the tools needed to create and operate on matrices in R.\nIn Section 21.11, at the end of the chapter, we present motivating examples and specific practical tasks that commonly arise in machine learning workflows, such as centering and scaling variables, computing distances, and applying linear transformations. These tasks can be completed efficiently using matrix operations, and the skills you develop in the earlier sections of the chapter will help you solve them. If you would like to understand the motivation for learning the matrix operations introduced in the previous sections, you may find it helpful to read that section first.\nA matrix is a two-dimensional object defined by its number of rows and columns. In data analysis, it is common to organize data so that each row represents an observation and each column represents a variable measured on those observations. This structure allows us to perform computations across observations or variables using matrix operations, which are both efficient and conceptually aligned with the linear algebra techniques introduced in the next chapters.\nIn mathematical notation, matrices are usually represented with bold uppercase letters:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\nx_{1,1}&x_{1,2}&\\dots & x_{1,p}\\\\\nx_{2,1}&x_{2,2}&\\dots & x_{2,p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n,1}&x_{n,2}&\\dots&x_{n,p}\\\\\n\\end{bmatrix}\n\\]\nwith \\(x_{i,j}\\) representing the \\(j\\)-th variable for the \\(i\\)-th observation. The matrix is said to have dimensions \\(n \\times p\\), meaning it has \\(n\\) rows and \\(p\\) columns.\nWe denote vectors with lower case bold letters and represent them as one column matrices, often referred to as column vectors. R follows this convention when converting a vector to a matrix.\nHowever, column vectors should not be confused with the columns of the matrix. They have this name simply because they have one column.\nMathematical descriptions of machine learning often make reference to vectors representing the \\(p\\) variables:\n\\[\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1\\\\\\\nx_2\\\\\\\n\\vdots\\\\\\\nx_p\n\\end{bmatrix}\n\\]\nTo distinguish between variables associated with the observations \\(i=1,\\dots,n\\), we add an index:\n\\[\n\\mathbf{x}_i = \\begin{bmatrix}\nx_{i,1}\\\\\nx_{i,2}\\\\\n\\vdots\\\\\nx_{i,p}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-matrix-notation",
    "href": "highdim/matrices-in-R.html#sec-matrix-notation",
    "title": "\n21  Matrices in R\n",
    "section": "",
    "text": "In machine learning, variables are often called features, while in statistics, they are often called covariates. Regardless of the terminology, when working with matrices, these are typically represented by the columns of the matrix, each column corresponds to one variable measured across all observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBold lower case letters are also commonly used to represent matrix columns rather than rows. This can be confusing because \\(\\mathbf{x}_1\\) can represent either the first row or the first column of \\(\\mathbf{X}\\). One way to distinguish is to use notation similar to computer code: using the colon \\(:\\) to represent all. So \\(\\mathbf{X}_{1,:}\\) represents the first row and \\(\\mathbf{X}_{:,1}\\) is the first column. Another approach is to distinguish by the letter used to index, with \\(i\\) used for rows and \\(j\\) used for columns. So \\(\\mathbf{x}_i\\) is the \\(i\\)th row and \\(\\mathbf{x}_j\\) is the \\(j\\)th column. With this approach, it is important to clarify which dimension, row or column is being represented. Further confusion can arise because, as aforementioned, it is common to represent all vectors, including the rows of a matrix, as one-column matrices.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-mnist",
    "href": "highdim/matrices-in-R.html#sec-mnist",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.2 Case study: MNIST",
    "text": "21.2 Case study: MNIST\nThe first step in handling mail received in the post office is to sort letters by zip code:\n\nIn the Machine Learning part of this book, we will describe how we can build computer algorithms to read handwritten digits, which robots then use to sort the letters. To do this, we first need to collect data, which in this case is a high-dimensional dataset and best stored in a matrix.\nThe MNIST dataset was generated by digitizing thousands of handwritten digits, already read and annotated by humans1. Below are three images of written digits.\n\n\n\n\n\n\n\n\nThe images are converted into \\(28 \\times 28 = 784\\) pixels and, for each pixel, we obtain a grey scale intensity between 0 (white) and 255 (black). The following plot shows the individual variables for each image:\n\n\n\n\n\n\n\n\nFor each digitized image, indexed by \\(i\\), we are provided with 784 variables and a categorical outcome, or label, representing the digit among \\(0, 1, 2, 3, 4, 5, 6, 7 , 8,\\) and \\(9\\) that the image is representing. Let’s load the data using the dslabs package:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nmnist &lt;- read_mnist()\n\nIn these cases, the pixel intensities are saved in a matrix:\n\nclass(mnist$train$images)\n#&gt; [1] \"matrix\" \"array\"\n\nThe labels associated with each image are included in a vector:\n\ntable(mnist$train$labels)\n#&gt; \n#&gt;    0    1    2    3    4    5    6    7    8    9 \n#&gt; 5923 6742 5958 6131 5842 5421 5918 6265 5851 5949\n\nTo simplify the code below, we will rename these x and y respectively:\n\nx &lt;- mnist$train$images\ny &lt;- mnist$train$labels\n\nBefore we define and work through the tasks designed to teach matrix operations, we begin by reviewing some basic matrix functionality in R.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-creating-a-matrix",
    "href": "highdim/matrices-in-R.html#sec-creating-a-matrix",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.3 Creating a matrix",
    "text": "21.3 Creating a matrix\nScalars, vectors, and matrices are the basic building blocks of linear algebra. We have already encountered vectors in earlier chapters, and in R, scalars are represented as vectors of length 1. We now extend this understanding to matrices, starting with how to create them in R.\nWe can create a matrix using the matrix function. The first argument is a vector containing the elements that will fill up the matrix. The second and third arguments determine the number of row and columns, respectively. So a typical way to create a matrix is to first obtain a vector of numbers containing the elements of the matrix and feeding it to the matrix function. For example, to create a \\(100 \\times 2\\) matrix of normally distributed random variables, we write:\n\nmat &lt;- matrix(rnorm(100*2), 100, 2)\n\nNote that by default the matrix is filled in column by column:\n\nmatrix(1:15, 3, 5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    4    7   10   13\n#&gt; [2,]    2    5    8   11   14\n#&gt; [3,]    3    6    9   12   15\n\nTo fill the matrix row by row, we can use the byrow argument:\n\nmatrix(1:15, 3, 5, byrow = TRUE)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    2    3    4    5\n#&gt; [2,]    6    7    8    9   10\n#&gt; [3,]   11   12   13   14   15\n\nThe function as.vector converts a matrix back into a vector:\n\nas.vector(matrix(1:15, 3, 5))\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\n\n\n\n\n\nIf the product of columns and rows does not match the length of the vector provided in the first argument, matrix recycles values. If the length of the vector is a sub-multiple or multiple of the number of rows, this happens without warning:\n\nmatrix(1:3, 3, 5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    1    1    1    1\n#&gt; [2,]    2    2    2    2    2\n#&gt; [3,]    3    3    3    3    3\n\n\n\n\nThe function as.matrix() attempts to coerce its input into a matrix:\n\ndf &lt;- data.frame(a = 1:2, b = 3:4, c = 5:6)\nas.matrix(df)\n#&gt;      a b c\n#&gt; [1,] 1 3 5\n#&gt; [2,] 2 4 6",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#dimensions-of-a-matrix",
    "href": "highdim/matrices-in-R.html#dimensions-of-a-matrix",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.4 Dimensions of a matrix",
    "text": "21.4 Dimensions of a matrix\nThe dimension of a matrix is an important characteristic needed to assure that certain linear algebra operations can be performed. The dimension is a two-number summary defined as the number of rows \\(\\times\\) the number of columns.\nThe nrow function tells us how many rows that matrix has. Here is the number of rows x previously defined to store the MNIST training data:\n\nnrow(x)\n#&gt; [1] 60000\n\nThe function ncol tells us how many columns:\n\nncol(x)\n#&gt; [1] 784\n\nWe learn that our dataset contains 60,000 observations (images) and 784 variables (pixels).\nThe dim function returns the rows and columns:\n\ndim(x)\n#&gt; [1] 60000   784\n\nNow we can confirm that R follows the convention of defining vectors of length \\(n\\) as \\(n\\times 1\\) matrices or column vectors:\n\nvec &lt;- 1:10\ndim(matrix(vec))\n#&gt; [1] 10  1",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-matrix-subsetting",
    "href": "highdim/matrices-in-R.html#sec-matrix-subsetting",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.5 Subsetting",
    "text": "21.5 Subsetting\nTo extract a specific entry from a matrix, for example the 300th row of the 100th column, we write:\n\nx[300, 100]\n\nWe can extract subsets of the matrices by using vectors of indexes. For example, we can extract the first 100 pixels from the first 300 observations like this:\n\nx[1:300, 1:100]\n\nTo extract an entire row or subset of rows, we leave the column dimension blank. So the following code returns all the pixels for the first 300 observations:\n\nx[1:300,]\n\nSimilarly, we can subset any number of columns by keeping the first dimension blank. Here is the code to extract the first 100 pixels:\n\nx[,1:100]\n\n\n\n\n\n\n\nIf we subset just one row or just one column, the resulting object is no longer a matrix. For example notice what happens here:\n\ndim(x[300,])\n#&gt; NULL\n\nTo avoid this, we can use the drop argument:\n\ndim(x[100,,drop = FALSE])\n#&gt; [1]   1 784",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-transpose",
    "href": "highdim/matrices-in-R.html#sec-transpose",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.6 The transpose",
    "text": "21.6 The transpose\nA common operation when working with matrices is the transpose. We use the transpose to understand several concepts described in the next several sections. This operation simply converts the rows of a matrix into columns. We use the symbols \\(\\top\\) or \\('\\) next to the bold upper case letter to denote the transpose:\n\\[\n\\text{if } \\,\n\\mathbf{X} =\n\\begin{bmatrix}\n  x_{1,1}&\\dots & x_{1,p} \\\\\n  x_{2,1}&\\dots & x_{2,p} \\\\\n  \\vdots & \\ddots & \\vdots & \\\\\n  x_{n,1}&\\dots & x_{n,p}\n  \\end{bmatrix} \\text{ then }\\,\n\\mathbf{X}^\\top =\n\\begin{bmatrix}\n  x_{1,1}&x_{2,1}&\\dots & x_{n,1} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  x_{1,p}&x_{2,p}&\\dots & x_{n,p}\n  \\end{bmatrix}\n\\]\nIn R we compute the transpose using the function t\n\ndim(x)\n#&gt; [1] 60000   784\ndim(t(x))\n#&gt; [1]   784 60000\n\nOne use of the transpose is that we can write the matrix \\(\\mathbf{X}\\) as rows of the column vectors representing the variables for each individual observation in the following way:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n\\mathbf{x}_1^\\top\\\\\n\\mathbf{x}_2^\\top\\\\\n\\vdots\\\\\n\\mathbf{x}_n^\\top\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-row-col-summaries",
    "href": "highdim/matrices-in-R.html#sec-row-col-summaries",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.7 Row and column summaries",
    "text": "21.7 Row and column summaries\nA common operation with matrices is to apply the same function to each row or to each column. For example, we may want to compute row averages and standard deviations. The apply function lets you do this. The first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function to be applied.\nSo, for example, to compute the averages and standard deviations of each row, we write:\n\navgs &lt;- apply(x, 1, mean)\nsds &lt;- apply(x, 1, sd)\n\nTo compute these for the columns, we simply change the 1 to a 2:\n\navgs &lt;- apply(x, 2, mean)\nsds &lt;- apply(x, 2, sd)\n\nBecause these operations are so common, special functions are available to perform them. So, for example, the functions rowMeans computes the average of each row:\n\navg &lt;- rowMeans(x)\n\nand the function rowSds from the matrixStats packages computes the standard deviations for each row:\n\nlibrary(matrixStats)\nsds &lt;- rowSds(x)\n\nThe functions colMeans and colSds provide the version for columns.\nFor other fast implementations of common operation take a look at what is availabe in the matrixStats package.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-conditional-filtering",
    "href": "highdim/matrices-in-R.html#sec-conditional-filtering",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.8 Conditional filtering",
    "text": "21.8 Conditional filtering\nOne of the advantages of matrices operations over tidyverse operations is that we can easily select columns based on summaries of the columns.\nNote that logical filters can be used to subset matrices in a similar way in which they can be used to subset vectors. Here is a simple example subsetting columns with logicals:\n\nmatrix(1:15, 3, 5)[,c(FALSE, TRUE, TRUE, FALSE, TRUE)]\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    4    7   13\n#&gt; [2,]    5    8   14\n#&gt; [3,]    6    9   15\n\nThis implies that we can select rows with conditional expression. In the following example we remove all observations containing at least one NA:\n\nx[apply(!is.na(x), 1, all),]\n\nThis being a common operation, we have a matrixStats function to do it faster:\n\nx[!rowAnyNAs(x),]",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-indexing-matrices",
    "href": "highdim/matrices-in-R.html#sec-indexing-matrices",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.9 Indexing with matrices",
    "text": "21.9 Indexing with matrices\nAn operation that facilitates efficient coding is that we can change entries of a matrix based on conditionals applied to that same matrix. Here is a simple example:\n\nmat &lt;- matrix(1:15, 3, 5)\nmat[mat &gt; 6 & mat &lt; 12] &lt;- 0\nmat\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    4    0    0   13\n#&gt; [2,]    2    5    0    0   14\n#&gt; [3,]    3    6    0   12   15\n\nA useful application of this approach is that we can change all the NA entries of a matrix to something else:\n\nx[is.na(x)] &lt;- 0",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-vectorization-for-matrices",
    "href": "highdim/matrices-in-R.html#sec-vectorization-for-matrices",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.10 Vectorization for matrices",
    "text": "21.10 Vectorization for matrices\nIn R, if we subtract a vector from a matrix, the first element of the vector is subtracted from the first row, the second element from the second row, and so on. Using mathematical notation, we would write it as follows:\n\\[\n\\begin{bmatrix}\n  X_{1,1}&\\dots & X_{1,p} \\\\\n  X_{2,1}&\\dots & X_{2,p} \\\\\n   & \\vdots & \\\\\n  X_{n,1}&\\dots & X_{n,p}\n  \\end{bmatrix}\n-\n\\begin{bmatrix}\na_1\\\\\\\na_2\\\\\\\n\\vdots\\\\\\\na_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  X_{1,1}-a_1&\\dots & X_{1,p} -a_1\\\\\n  X_{2,1}-a_2&\\dots & X_{2,p} -a_2\\\\\n   & \\vdots & \\\\\n  X_{n,1}-a_n&\\dots & X_{n,p} -a_n\n  \\end{bmatrix}\n\\]\nThe same holds true for other arithmetic operations.\nThe function sweep facilitates this type of operation. It works similarly to apply. It takes each entry of a vector and applies an arithmetic operation to the corresponding row. Subtraction is the default arithmetic operation. So, for example, to center each row around the average, we can use:\n\nsweep(x, 1, rowMeans(x))\n\nIn R, if you add, subtract, multiple or divide two matrices, the operation is done elementwise. For example, if two matrices are stored in x and y, then:\n\nx*y\n\ndoes not result in matrix multiplication. Instead, the entry in row \\(i\\) and column \\(j\\) of this product is the product of the entry in row \\(i\\) and column \\(j\\) of x and y, respectively.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-motivating-tasks",
    "href": "highdim/matrices-in-R.html#sec-motivating-tasks",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.11 Motivating tasks",
    "text": "21.11 Motivating tasks\nTo motivate the use of matrices in R, we will pose six tasks that illustrate how matrix operations can help with data exploration of the handwritten digits dataset. Each task highlights a basic matrix operation that is commonly used in R. By seeing how these operations can be implemented with fast and simple code, you will gain hands-on experience with the kinds of computations that arise frequently in high-dimensional data analysis. The primary goal of these tasks is to help you learn how matrix operations work in practice.\nVisualize the original image\nThe pixel intensities are provided as rows in a matrix. Using what we learned in Section 21.3, we can convert each row into a \\(28 \\times 28\\) matrix that we can visualize as an image. As an example, we will use the third observation. From the label, we know this is a:\n\ny[3]\n#&gt; [1] 4\n\nThe third row of the matrix x[3,] contains the 784 pixel intensities. If we assume these were entered in order, we can convert them back to a \\(28 \\times 28\\) matrix using:\n\ngrid &lt;- matrix(x[3,], 28, 28)\n\nTo visualize the data, we can use image in the following way:\n\nimage(1:28, 1:28, grid)\n\nHowever, because the y-axis in image goes bottom to top and x stores pixels top to bottom the code above shows shows a flipped image. To flip it back we can use:\n\nimage(1:28, 1:28, grid[, 28:1])\n\n\n\n\n\n\n\n\n\nDo some digits require more ink to write than others?\nLet’s study the distribution of the total pixel darkness and how it varies by digits.\nWe can use what we learned in Section Section 21.7}] to computed this average for each pixel and the display the values for each digit using a boxplot:\n\navg &lt;- rowMeans(x)\nboxplot(avg ~ y)\n\n\n\n\n\n\n\nFrom this plot we see that, not surprisingly, 1s use less ink than other digits.\nAre some pixels uninformative?\nLet’s study the variation of each pixel across digits and remove columns associated with pixels that don’t change much and thus can’t provide much information for classification.\nWe can what we learned in Section 21.8 to efficiently remove columns associated with pixels that don’t change much and thus do not inform digit classification.\nWe will also use what we learned in Section 21.7 to quantify the variation of each pixel with its standard deviation across all entries. Since each column represents a pixel, we use the colSds function from the matrixStats package:\n\nsds &lt;- colSds(x)\n\nA quick look at the distribution of these values shows that some pixels have very low entry to entry variability:\n\n\n\n\n\n\n\n\n\nhist(sds, breaks = 30, main = \"SDs\")\n\nThis makes sense since we don’t write in some parts of the box. Here is the variance plotted by location after using what we learned in Section 21.3 to creat a \\(28 \\times 28\\) matrix:\n\nimage(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])\n\n\n\n\n\n\n\n\n\nWe see that there is little variation in the corners.\nWe could remove features that have no variation since these can’t help us predict.\nSo if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:\n\nnew_x &lt;- x[,colSds(x) &gt; 60]\ndim(new_x)\n#&gt; [1] 60000   322\n\nOnly the columns for which the standard deviation is above 60 are kept, which removes over half the predictors.\nCan we remove smudges?\nWe will first look at the distribution of all pixel values.\n\n\n\n\n\n\n\n\n\nhist(as.vector(x), breaks = 30, main = \"Pixel intensities\")\n\nThis shows a clear dichotomy which is explained as parts of the image with ink and parts without. If we think that values below, say, 50 are smudges, we can quickly make them zero using what we learned in Section 21.9\n\nnew_x &lt;- x\nnew_x[new_x &lt; 50] &lt;- 0\n\nBinarize the data\nThe histogram above seems to suggest that this data is mostly binary. A pixel either has ink or does not. Applying what we learned in Section 21.9, we can binarize the data using just matrix operations:\n\nbin_x &lt;- x\nbin_x[bin_x &lt; 255/2] &lt;- 0 \nbin_x[bin_x &gt; 255/2] &lt;- 1\n\nWe can also convert to a matrix of logicals and then coerce to numbers using what we learned in Section 21.10:\n\nbin_X &lt;- (x &gt; 255/2)*1\n\nStandardize the digits\nFinally, we will scale each column to have the same average and standard deviation.\nUsing what we learned in Section 21.10 implies that we can scale each row of a matrix as follows:\n\n(x - rowMeans(x))/rowSds(x)\n\nYet this approach does not work for columns. For columns, we can use the sweep function we learned in [sec-vectorization-for-matrices]:\n\nx_mean_0 &lt;- sweep(x, 2, colMeans(x))\n\nTo divide by the standard deviation, we change the default arithmetic operation to division as follows:\n\nx_standardized &lt;- sweep(x_mean_0, 2, colSds(x), FUN = \"/\")",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#exercises",
    "href": "highdim/matrices-in-R.html#exercises",
    "title": "\n21  Matrices in R\n",
    "section": "\n21.12 Exercises",
    "text": "21.12 Exercises\n1. Create a 100 by 10 matrix of randomly generated normal numbers. Put the result in x.\n2. Apply the three R functions that give you the dimension of x, the number of rows of x, and the number of columns of x, respectively.\n3. Add the scalar 1 to row 1, the scalar 2 to row 2, and so on, to the matrix x.\n4. Add the scalar 1 to column 1, the scalar 2 to column 2, and so on, to the matrix x. Hint: Use sweep with FUN = \"+\".\n5. Compute the average of each row of x.\n6. Compute the average of each column of x.\n7. For each digit in the MNIST training data, compute the proportion of pixels that are in a grey area, defined as values between 50 and 205. Make a boxplot by digit class. Hint: Use logical operators and rowMeans.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#footnotes",
    "href": "highdim/matrices-in-R.html#footnotes",
    "title": "\n21  Matrices in R\n",
    "section": "",
    "text": "http://yann.lecun.com/exdb/mnist/↩︎",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html",
    "href": "highdim/linear-algebra.html",
    "title": "22  Applied Linear Algebra",
    "section": "",
    "text": "22.1 The identity matrix\nLinear algebra is the main mathematical technique used to describe and motivate statistical methods and machine learning approaches. In this chapter, we introduce some of the mathematical concepts needed to understand these techniques. We use these concepts and techniques throughout the remainder of the book. ## Matrix multiplication\nA commonly used operation in data analysis is matrix multiplication. Here, we define and motivate the operation.\nLinear algebra originated from mathematicians developing systematic ways to solve systems of linear equations. For example:\n\\[\n\\begin{aligned}\nx +  3 y  - 2 z  &= 5\\\\\n3x + 5y + 6z &= 7\\\\\n2x + 4y + 3z &= 8\n\\end{aligned}\n\\]\nMathematicians figured out that by representing these linear systems of equations using matrices and vectors, predefined algorithms could be designed to solve any system of linear equations. A basic linear algebra class will teach some of these algorithms, such as Gaussian elimination, the Gauss-Jordan elimination, and the LU and QR decompositions. These methods are usually covered in detail in university level linear algebra courses.\nTo explain matrix multiplication, define two matrices: \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}&a_{m2}&\\dots&a_{mn}\n\\end{pmatrix}, \\,\n\\mathbf{B} = \\begin{pmatrix}\nb_{11}&b_{12}&\\dots&b_{1p}\\\\\nb_{21}&b_{22}&\\dots&b_{2p}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nb_{n1}&b_{n2}&\\dots&b_{np}\n\\end{pmatrix}\n\\]\nand define the product of matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) as the matrix \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) that has entries \\(c_{ij}\\) equal to the sum of the component-wise product of the \\(i\\)th row of \\(\\mathbf{A}\\) with the \\(j\\)th column of \\(\\mathbf{B}\\). Using R code, we can define \\(\\mathbf{C}= \\mathbf{A}\\mathbf{B}\\) as follows:\nBecause this operation is so common, R includes a mathematical operator %*% for matrix multiplication:\nUsing mathematical notation \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) looks like this:\n\\[\n\\begin{pmatrix}\na_{11}b_{11} + \\dots + a_{1n}b_{n1}&\na_{11}b_{12} + \\dots + a_{1n}b_{n2}&\n\\dots&\na_{11}b_{1p} + \\dots + a_{1n}b_{np}\\\\\na_{21}b_{11} + \\dots + a_{2n}b_{n1}&\na_{21}b_{12} + \\dots + a_{2n}b_{n2}&\n\\dots&\na_{21}b_{1p} + \\dots + a_{2n}b_{np}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}b_{11} + \\dots +a_{mn}b_{n1}&\na_{m1}b_{12} + \\dots + a_{mn}b_{n2}&\n\\dots&\na_{m1}b_{1p} + \\dots + a_{mn}b_{np}\\\\\n\\end{pmatrix}\n\\]\nNote this definition implies that the multiplication \\(\\mathbf{A}\\mathbf{B}\\) is only possible when the number of rows of \\(\\mathbf{A}\\) matches the number of columns of \\(\\mathbf{B}\\).\nSo how does this definition of matrix multiplication help solve systems of equations? First, any system of equations with unknowns \\(x_1, \\dots x_n\\)\n\\[\n\\begin{aligned}\na_{11} x_1 + a_{12} x_2 \\dots + a_{1n}x_n &= b_1\\\\\na_{21} x_1 + a_{22} x_2 \\dots + a_{2n}x_n &= b_2\\\\\n\\vdots\\\\\na_{n1} x_1 + a_{n2} x_2 \\dots + a_{nn}x_n &= b_n\\\\\n\\end{aligned}\n\\]\ncan now be represented as matrix multiplication by defining the following matrices:\n\\[\n\\mathbf{A} =\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{n1}&a_{n2}&\\dots&a_{nn}\n\\end{pmatrix}\n,\\,\n\\mathbf{b} =\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\n,\\, \\mbox{ and }\n\\mathbf{x} =\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{pmatrix}\n\\]\nand rewriting the equation simply as:\n\\[\n\\mathbf{A}\\mathbf{x} =  \\mathbf{b}\n\\]\nThe linear algebra algorithms listed above, such as Gaussian elimination, provide a way to compute the inverse matrix \\(A^{-1}\\) that solves the equation for \\(\\mathbf{x}\\):\n\\[\n\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} =   \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\n\\]\nTo solve the first equation we wrote out in R, we can use the function solve:\nThe identity matrix, represented with a bold \\(\\mathbf{I}\\), is like the number 1, but for matrices: if you multiply a matrix by the identity matrix, you get back the matrix.\n\\[\n\\mathbf{I}\\mathbf{X} = \\mathbf{X}\n\\]\nIf you define \\(\\mathbf{I}\\) as matrix with the same number of rows and columns (referred to as square matrix) with 0s everywhere except the diagonal,\n\\[\n\\mathbf{I}=\\begin{pmatrix}\n1&0&\\dots&0\\\\\n0&1&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&1\n\\end{pmatrix},\n\\]\nyou will obtain the desired property.\nNote that the definition of an inverse matrix implies that:\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{1}\n\\]\nBecause the default for the second argument in solve is an identity matrix, if we simply type solve(A), we obtain the inverse \\(\\mathbf{A}^{-1}\\). This means we can also obtain a solution to our system of equations with:\nsolve(A) %*% b",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html#distance",
    "href": "highdim/linear-algebra.html#distance",
    "title": "22  Applied Linear Algebra",
    "section": "\n22.2 Distance",
    "text": "22.2 Distance\nMany of the analyses we perform with high-dimensional data relate directly or indirectly to distance. For example, most machine learning techniques rely on being able to define distances between observations, using features or predictors. Clustering algorithms, for example, search for observations that are similar. But what does this mean mathematically?\nTo define distance, we introduce another linear algebra concept: the norm. Recall that a point in two dimensions can be represented in polar coordinates as:\n\n\n\n\n\n\n\n\nwith \\(\\theta = \\arctan{\\frac{x2}{x1}}\\) and \\(r = \\sqrt{x_1^2 + x_2^2}\\). If we think of the point as two dimensional column vector \\(\\mathbf{x} = (x_1, x_2)^\\top\\), \\(r\\) defines the norm of \\(\\mathbf{x}\\). The norm can be thought of as the size of the two-dimensional vector disregarding the direction: if we change the angle, the vector changes but the size does not. The point of defining the norm is that we can extrapolate the concept of size to higher dimensions. Specifically, we write the norm for any vector \\(\\mathbf{x}\\) as:\n\\[\n\\|\\mathbf{x}\\| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_p^2}\n\\]\nNote that we can use the linear algebra concepts we have learned to define the norm like this:\n\\[\n\\|\\mathbf{x}\\|^2 = \\mathbf{x}^\\top\\mathbf{x}\n\\]\nTo define distance, suppose we have two two-dimensional points: \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). We can define how similar they are by simply using euclidean distance:\n\n\n\n\n\n\n\n\nWe know that the distance is equal to the length of the hypotenuse:\n\\[\n\\sqrt{(x_{11} - x_{12})^2 + (x_{21} - x_{22})^2}\n\\]\nThe reason we introduced the norm is because this distance is the size of the vector between the two points and this can be extrapolated to any dimension. The distance between two points, regardless of the dimensions, is defined as the norm of the difference\n\\[\n\\| \\mathbf{x}_1 - \\mathbf{x}_2\\|.\n\\]\nIf we use the digit data, the distance between the first and second observation will compute distance using all 784 features:\n\\[\n\\| \\mathbf{x}_1 - \\mathbf{x}_2 \\| = \\sqrt{ \\sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2 }\n\\]\nTo demonstrate, let’s pick the features for three digits:\n\nx_1 &lt;- x[6,]\nx_2 &lt;- x[17,]\nx_3 &lt;- x[16,]\n\nWe can compute the distances between each pair using the definitions we just learned:\n\nc(sum((x_1 - x_2)^2), sum((x_1 - x_3)^2), sum((x_2 - x_3)^2)) |&gt; sqrt()\n#&gt; [1] 2320 2331 2519\n\nIn R, the function crossprod(x) is convenient for computing norms. It multiplies t(x) by x:\n\nc(crossprod(x_1 - x_2), crossprod(x_1 - x_3), crossprod(x_2 - x_3)) |&gt; sqrt()\n#&gt; [1] 2320 2331 2519\n\nNote crossprod takes a matrix as the first argument. As a result, the vectors used here are being coerced into single column matrices. Also, note that crossprod(x,y) multiplies t(x) by y.\nWe can see that the distance is smaller between the first two. This agrees with the fact that the first two are 2s and the third is a 7.\n\ny[c(6, 17, 16)]\n#&gt; [1] 2 2 7\n\nWe can also compute all the distances at once relatively quickly using the function dist, which takes a matrix as input and computes the distance between each row and produces an object of class dist:\n\nd &lt;- dist(x[c(6,17,16),])\nclass(d)\n#&gt; [1] \"dist\"\n\nThis is convenient becasue there are several machine learning related functions in R that take objects of class dist as input.\nWe can see the distance we calculated above like this:\n\nd\n#&gt;      1    2\n#&gt; 2 2320     \n#&gt; 3 2331 2519\n\nNote that the diagonal is omitted because all self-distances are zero, and the upper triangle is also excluded due to the symmetry of the distance matrix, which follows from the fact that distance is commutative.\nTo access the entries using row and column indices, we need to coerce it into a matrix.\nThe image function allows us to quickly see an image of distances between observations. As an example, we compute the distance between each of the first 300 observations and then make an image:\n\nd &lt;- dist(x[1:300,])\nimage(as.matrix(d))\n\nIf we order this distance by the labels, we can see yellowish squares near the diagonal. This is because observations from the same digits tend to be closer than to different digits:\n\nimage(as.matrix(d)[order(y[1:300]), order(y[1:300])])",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html#sec-predictor-space",
    "href": "highdim/linear-algebra.html#sec-predictor-space",
    "title": "22  Applied Linear Algebra",
    "section": "\n22.3 Spaces",
    "text": "22.3 Spaces\nPredictor space is a concept that is often used to describe machine learning algorithms. The term space refers to an advanced mathematical definition for which we provide a simplified explanation to help understand the term predictor space when used in the context of machine learning algorithms.\nWe can think of all predictors \\((x_{i,1}, \\dots, x_{i,p})^\\top\\) for all observations \\(i=1,\\dots,n\\) as \\(n\\) \\(p\\)-dimensional points. A space can be thought of as the collection of all possible points that should be considered for the data analysis in question. This includes points we could see, but have not been observed yet. In the case of the handwritten digits, we can think of the predictor space as any point \\((x_{1}, \\dots, x_{p})^\\top\\) as long as each entry \\(x_i, \\, i = 1, \\dots, p\\) is between 0 and 255.\nSome Machine Learning algorithms also define subspaces. A commonly defined subspace in machine learning are neighborhoods composed of points that are close to a predetermined center. We do this by selecting a center \\(\\mathbf{x}_0\\), a minimum distance \\(r\\), and defining the subspace as the collection of points \\(\\mathbf{x}\\) that satisfy:\n\\[\n\\| \\mathbf{x} - \\mathbf{x}_0 \\| \\leq r.\n\\]\nWe can think of this subspace as a multidimensional sphere since every point is the same distance away from the center.\nOther machine learning algorithms partition the predictor space into non-overlapping regions and then make different predictions for each region using the data in the region. We will learn about these in Section 31.4.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html#exercises",
    "href": "highdim/linear-algebra.html#exercises",
    "title": "22  Applied Linear Algebra",
    "section": "\n22.4 Exercises",
    "text": "22.4 Exercises\n1. Generate two matrices, A and B, containing randomly generated and normally distributed numbers. The dimensions of these two matrices should be \\(4 \\times 3\\) and \\(3 \\times 6\\), respectively. Confirm that C &lt;- A %*% B produces the same results as:\n\nm &lt;- nrow(A)\np &lt;- ncol(B)\nC &lt;- matrix(0, m, p)\nfor(i in 1:m){\n  for(j in 1:p){\n    C[i,j] &lt;- sum(A[i,] * B[,j])\n  }\n}\n\n2. Solve the following system of equations using R:\n\\[\n\\begin{aligned}\nx + y + z + w &= 10\\\\\n2x + 3y - z - w &= 5\\\\\n3x - y + 4z - 2w &= 15\\\\\n2x + 2y - 2z - 2w &= 20\\\\\n\\end{aligned}\n\\]\n3. Define x and y:\n\nmnist &lt;- read_mnist()\nx &lt;- mnist$train$images[1:300,] \ny &lt;- mnist$train$labels[1:300]\n\nand compute the distance matrix:\n\nd &lt;- dist(x)\nclass(d)\n\nGenerate a boxplot showing the distances for the second row of d stratified by digits. Do not include the distance to itself, which we know is 0. Can you predict what digit is represented by the second row of x?\n4. Use the apply function and matrix algebra to compute the distance between the fourth digit mnist$train$images[4,] and all other digits represented in mnist$train$images. Then generate a boxplot as in exercise 2 and predict what digit is the fourth row.\n5. Compute the distance between each feature and the feature representing the middle pixel (row 14 column 14). Create an image plot of where the distance is shown with color in the pixel position.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html",
    "href": "highdim/dimension-reduction.html",
    "title": "\n23  Dimension Reduction\n",
    "section": "",
    "text": "23.1 Motivation: preserving distance\nHigh-dimensional data can make data analysis challenging, especially when it comes to visualization and pattern discovery. For example, in the MNIST dataset, each image is represented by 784 pixels. To explore the relationships between all pairs of pixels, we would need to examine over 300,000 scatterplots. This quickly becomes impractical.\nIn this chapter, we introduce a powerful set of techniques known collectively as dimension reduction. The core idea is to reduce the number of variables in a dataset while preserving important characteristics, such as the distances between observations. With fewer dimensions, visualization becomes more feasible, and patterns in the data become easier to detect.\nThe main technique we focus on is Principal Component Analysis (PCA), a widely used method for unsupervised learning and exploratory data analysis. PCA is based on a mathematical tool called the singular value decomposition (SVD), which has applications beyond dimension reduction as well.\nWe begin with a simple illustrative example to build intuition, then introduce the mathematical concepts needed to understand PCA. We conclude the chapter by applying PCA to two more complex, real-world datasets to demonstrate its practical value.\nFor illustrative purposes, we consider a simple example with twin heights. Some pairs are adults, the others are children. Here we simulate 100 two-dimensional points where each point a pair of twins. We use the mvrnorm function from the MASS package to simulate bivariate normal data.\nset.seed(1983)\nlibrary(MASS)\nn &lt;- 100\nrho &lt;- 0.9\nsigma &lt;- 3\ns &lt;- sigma^2*matrix(c(1, rho, rho, 1), 2, 2)\nx &lt;- rbind(mvrnorm(n/2, c(69, 69), s),\n           mvrnorm(n/2, c(60, 60), s))\nA scatterplot quickly reveals that the correlation is high and that there are two groups of twins, the adults (upper right points) and the children (lower left points):\nOur features are \\(n\\) two-dimensional points, the two heights. For illustrative purposes, we will pretend that visualizing two dimensions is too challenging and we want to explore the data through a histogram of a one-dimensional variable. We therefore want to reduce the dimensions from two to one, but still be able to understand important characteristics of the data, in particular that the observations cluster into two groups: adults and children.\nTo show that the ideas presented here are generally useful, we will standardize the data so that observations are in standard units rather than inches:\nlibrary(matrixStats)\nx &lt;- sweep(x, 2, colMeans(x))\nx &lt;- sweep(x, 2, colSds(x), \"/\")\nIn the scatterplot above, we also show the distance between observation 1 and 2 (blue), and observation 1 and 51 (red). Note that the blue line is shorter, which implies that 1 and 2 are closer.\nWe can compute these distances using dist:\nd &lt;- as.matrix(dist(x))\nc(d[1, 2], d[2, 51])\n#&gt; [1] 0.595 1.388\nThis distance is based on two dimensions and we need a distance approximation based on just one.\nIf we look back at the hieght scatterplot above and imagine drawing a line between any pair of points, the length of that line represents the distance between the two points. Many of these lines tend to align along the diagonal direction, suggesting that most of the variation in the data is not purely horizontal or vertical but spread diagonally across both axes.\nNow imagine we rotate the entire point cloud so that the variation that was previously along the diagonal is now aligned with the horizontal axis. In this rotated view, the largest differences between points would show up in the first (horizontal) dimension. This would make it easier to describe the data using just one variables, the one that captures the most meaningful variation.\nIn the next section, we introduce a mathematical approach that makes this intuition precise. It allows us to find a rotation of the data that preserves the distances between points while reorienting the axes to highlight the most important directions of variation. This is the foundation of an approach called Principal Component Analysis (PCA), the most widely used technique for dimension reduction.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#rotations",
    "href": "highdim/dimension-reduction.html#rotations",
    "title": "\n23  Dimension Reduction\n",
    "section": "\n23.2 Rotations",
    "text": "23.2 Rotations\nAny two-dimensional point \\((x_1, x_2)^\\top\\) can be written as the base and height of a triangle with a hypotenuse going from \\((0,0)^\\top\\) to \\((x_1, x_2)^\\top\\):\n\\[\nx_1 = r \\cos\\phi, \\,\\, x_2 = r \\sin\\phi\n\\]\nwith \\(r\\) the length of the hypotenuse and \\(\\phi\\) the angle between the hypotenuse and the x-axis.\nTo rotate the point \\((x_1, x_2)^\\top\\) around a circle with center \\((0,0)^\\top\\) and radius \\(r\\) by an angle \\(\\theta\\) we simply change the angle in the previous equation to \\(\\phi + \\theta\\):\n\\[\nz_1 = r \\cos(\\phi+ \\theta), \\,\\,\nz_2 = r \\sin(\\phi + \\theta)\n\\]\n\n\n\n\n\n\n\n\nWe can use trigonometric identities to rewrite \\((z_1, z_2)\\) as follows:\n\\[\n\\begin{aligned}\nz_1 = r \\cos(\\phi + \\theta) = r \\cos \\phi \\cos\\theta -  r \\sin\\phi \\sin\\theta =  x_1 \\cos(\\theta) -  x_2 \\sin(\\theta)\\\\\nz_2 = r \\sin(\\phi + \\theta) =  r \\cos\\phi \\sin\\theta + r \\sin\\phi \\cos\\theta =  x_1 \\sin(\\theta) + x_2 \\cos(\\theta)\n\\end{aligned}\n\\]\nNow we can rotate each point in the dataset by simply applying the formula above to each pair \\((x_{i,1}, x_{i,2})^\\top\\).\nHere is what the twin standardized heights look like after rotating each point by \\(-45\\) degrees:\n\n\n\n\n\n\n\n\nNote that while the variability of \\(x_1\\) and \\(x_2\\) are similar, the variability of \\(z_1\\) is much larger than the variability of \\(z_2\\). Also, notice that the distances between points appear to be preserved. In the next sections, we show mathematically that this in fact the case.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#linear-transformations",
    "href": "highdim/dimension-reduction.html#linear-transformations",
    "title": "\n23  Dimension Reduction\n",
    "section": "\n23.3 Linear transformations",
    "text": "23.3 Linear transformations\nAny time a matrix \\(\\mathbf{X}\\) is multiplied by another matrix \\(\\mathbf{A}\\), we refer to the product \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{A}\\) as a linear transformation of \\(\\mathbf{X}\\). Below, we show that the rotations described above are a linear transformation. To see this, note that for any row \\(i\\), the first entry was:\n\\[\nz_{i,1} = a_{1,1} x_{i,1} + a_{2,1} x_{i,2}\n\\]\nwith \\(a_{1,1} = \\cos\\theta\\) and \\(a_{2,1} = -\\sin\\theta\\).\nThe second entry was also a linear transformation:\n\\[z_{i,2} = a_{1,2} x_{i,1} + a_{2,2} x_{i,2}\\]\nwith \\(a_{1,2} = \\sin\\theta\\) and \\(a_{2,2} = \\cos\\theta\\).\nWe can write these equations using matrix notation:\n\\[\n\\begin{pmatrix}\nz_1\\\\z_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na_{1,1}&a_{1,2}\\\\\na_{2,1}&a_{2,2}\n\\end{pmatrix}^\\top\n\\begin{pmatrix}\nx_1\\\\x_2\n\\end{pmatrix}\n\\]\nAn advantage of using linear algebra is that we can write the transformation for the entire dataset by saving all observations in a \\(N \\times 2\\) matrix:\n\\[\n\\mathbf{X} \\equiv\n\\begin{bmatrix}\n\\mathbf{x_1}^\\top\\\\\n\\vdots\\\\\n\\mathbf{x_n}^\\top\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{1,1}&x_{1,2}\\\\\n\\vdots&\\vdots\\\\\nx_{n,1}&x_{n,2}\n\\end{bmatrix}\n\\]\nWe can then obtain the rotated values \\(\\mathbf{z}_i\\) for each row \\(i\\) by applying a linear transformation of \\(X\\):\n\\[\n\\mathbf{Z} = \\mathbf{X} \\mathbf{A}\n\\mbox{ with }\n\\mathbf{A} = \\,\n\\begin{pmatrix}\na_{1,1}&a_{1,2}\\\\\na_{2,1}&a_{2,2}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\cos \\theta&\\sin \\theta\\\\\n-\\sin \\theta&\\cos \\theta\n\\end{pmatrix}\n.\n\\]\nIf we define:\n\ntheta &lt;- 2*pi*-45/360 #convert to radians\nA &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n\nWe can write code implementing a rotation by any angle \\(\\theta\\) using linear algebra:\n\nrotate &lt;- function(x, theta){\n  theta &lt;- 2*pi*theta/360\n  A &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n  x %*% A\n}\n\nThe columns of \\(\\mathbf{A}\\) are referred to as directions because if we draw a vector from \\((0,0)\\) to \\((a_{1,j}, a_{2,j})\\), it points in the direction of the line that will become the \\(j\\)-th dimension.\nAnother advantage of linear algebra is that if we can find the inverse matrix of \\(\\mathbf{A}^\\top\\), we can convert \\(\\mathbf{Z}\\) back to \\(\\mathbf{X}\\), again using a linear transformation.\nIn this particular case, we can use trigonometry to show that:\n\\[\nx_{i,1} = b_{1,1} z_{i,1} + b_{2,1} z_{i,2}\\\\\nx_{i,2} = b_{1,2} z_{i,1} + b_{2,2} z_{i,2}\n\\]\nwith \\(b_{2,1} = \\cos\\theta\\), \\(b_{2,1} = \\sin\\theta\\), \\(b_{1,2} = -\\sin\\theta\\), and \\(b_{2,2} = \\cos\\theta\\).\nThis implies that:\n\\[\n\\mathbf{X} = \\mathbf{Z}\n\\begin{pmatrix}\n\\cos \\theta&-\\sin \\theta\\\\\n\\sin \\theta&\\cos \\theta\n\\end{pmatrix}.\n\\] Note that the transformation used above is actually \\(\\mathbf{A}^\\top\\) which implies that\n\\[\n\\mathbf{Z} \\mathbf{A}^\\top = \\mathbf{X} \\mathbf{A}\\mathbf{A}^\\top\\ = \\mathbf{X}\n\\]\nand therefore that \\(\\mathbf{A}^\\top\\) is the inverse of \\(\\mathbf{A}\\). This also implies that all the information in \\(\\mathbf{X}\\) is included in the rotation \\(\\mathbf{Z}\\), and it can be retrieved via a linear transformation. A consequence is that for any rotation the distances are preserved. Here is an example for a 30 degree rotation, although it works for any angle:\n\nall.equal(as.matrix(dist(rotate(x, 30))), as.matrix(dist(x)))\n#&gt; [1] TRUE\n\nThe next section explains why this happens.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#orthogonal-transformations",
    "href": "highdim/dimension-reduction.html#orthogonal-transformations",
    "title": "\n23  Dimension Reduction\n",
    "section": "\n23.4 Orthogonal transformations",
    "text": "23.4 Orthogonal transformations\nRecall that the distance between two points, say rows \\(h\\) and \\(i\\) of the transformation \\(\\mathbf{Z}\\), can be written like this:\n\\[\n\\|\\mathbf{z}_h - \\mathbf{z}_i\\| = (\\mathbf{z}_h - \\mathbf{z}_i)^\\top(\\mathbf{z}_h - \\mathbf{z}_i)\n\\]\nwith \\(\\mathbf{z}_h\\) and \\(\\mathbf{z}_i\\) the \\(p \\times 1\\) column vectors stored in the \\(h\\)-th and \\(i\\)-th rows of \\(\\mathbf{X}\\), respectively.\n\n\n\n\n\n\nRemember that we represent the rows of a matrix as column vectors. This explains why we use \\(\\mathbf{A}\\) when showing the multiplication for the matrix \\(\\mathbf{Z}=\\mathbf{X}\\mathbf{A}\\), but transpose the operation when showing the transformation for just one observation: \\(\\mathbf{z}_i = \\mathbf{A}^\\top\\mathbf{x}_i\\)\n\n\n\nUsing linear algebra, we can rewrite the quantity above as:\n\\[\n\\|\\mathbf{z}_h - \\mathbf{z}_i\\| =\n\\|\\mathbf{A}^\\top \\mathbf{x}_h - \\mathbf{A}^\\top\\mathbf{x}_i\\|^2 =\n(\\mathbf{x}_h - \\mathbf{x}_i)^\\top \\mathbf{A} \\mathbf{A}^\\top (\\mathbf{x}_h - \\mathbf{x}_i)\n\\]\nNote that if \\(\\mathbf{A} \\mathbf{A} ^\\top= \\mathbf{I}\\), then the distance between the \\(h\\)th and \\(i\\)th rows is the same for the original and transformed data.\nWe refer to transformations with the property \\(\\mathbf{A} \\mathbf{A}^\\top = \\mathbf{I}\\) as orthogonal transformations. These are guaranteed to preserve the distance between any two points.\nWe previously demonstrated our rotation has this property. We can confirm using R:\n\nA %*% t(A)\n#&gt;          [,1]     [,2]\n#&gt; [1,] 1.00e+00 1.01e-17\n#&gt; [2,] 1.01e-17 1.00e+00\n\nNotice that \\(\\mathbf{A}\\) being orthogonal also guarantees that the total sum of squares (TSS) of \\(\\mathbf{X}\\), defined as \\(\\sum_{i=1}^n \\sum_{j=1}^p x_{i,j}^2\\) is equal to the total sum of squares of the rotation \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{A}^\\top\\). To illustrate, observe that if we denote the rows of \\(\\mathbf{Z}\\) as \\(\\mathbf{z}_1, \\dots, \\mathbf{z}_n\\), then the sum of squares can be written as:\n\\[\n\\sum_{1=1}^n \\|\\mathbf{z}_i\\|^2 = \\sum_{i=1}^n \\|\\mathbf{A}^\\top\\mathbf{x}_i\\|^2 = \\sum_{i=1}^n \\mathbf{x}_i^\\top \\mathbf{A}\\mathbf{A}^\\top  \\mathbf{x}_i = \\sum_{i=1}^n \\mathbf{x}_i^\\top\\mathbf{x}_i = \\sum_{i=1}^n\\|\\mathbf{x}_i\\|^2\n\\]\nWe can confirm using R:\n\ntheta &lt;- -45\nz &lt;- rotate(x, theta) # works for any theta\nsum(x^2)\n#&gt; [1] 198\nsum(z^2)\n#&gt; [1] 198\n\nThis can be interpreted as a consequence of the fact that an orthogonal transformation guarantees that all the information is preserved.\nHowever, although the total is preserved, the sum of squares for the individual columns changes. Here we compute the proportion of TSS attributed to each column, referred to as the variance explained or variance captured by each column, for \\(\\mathbf{X}\\):\n\ncolSums(x^2)/sum(x^2)\n#&gt; [1] 0.5 0.5\n\nand \\(\\mathbf{Z}\\):\n\ncolSums(z^2)/sum(z^2)\n#&gt; [1] 0.9848 0.0152\n\nIn the next section, we describe how this last mathematical result can be useful.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#sec-pca",
    "href": "highdim/dimension-reduction.html#sec-pca",
    "title": "\n23  Dimension Reduction\n",
    "section": "\n23.5 Principal Component Analysis (PCA)",
    "text": "23.5 Principal Component Analysis (PCA)\nWe have established that orthogonal transformations preserve both the distances between observations and the total sum of squares (TSS). However, while the TSS remains unchanged, the way it is distributed across the columns can vary depending on the transformation.\nThe main idea behind Principal Component Analysis (PCA) is to find an orthogonal transformation that concentrates as much of the variance as possible into the first few columns. This allows us to reduce the dimensionality of the problem by focusing only on those columns that capture most of the variation in the data.\nIn our example, we aim to find a rotation that maximizes the variance explained in the first column. The following code performs a grid search across rotations from –90 to 0 degrees to identify such a transformation:\n\n\n\n\n\n\n\n\n\nangles &lt;- seq(0, -90)\nv &lt;- sapply(angles, function(angle) colSums(rotate(x, angle)^2))\nvariance_explained &lt;- v[1,]/sum(x^2)\nplot(angles, variance_explained, type = \"l\")\n\nWe find that a -45 degree rotation appears to achieve the maximum, with over 98% of the total variability explained by the first dimension. We denote this rotation matrix with \\(\\mathbf{V}\\):\n\ntheta &lt;- 2*pi*-45/360 #convert to radians\nV &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n\nWe can rotate the entire dataset using:\n\\[\n\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\n\\]\n\nz &lt;- x %*% V\n\nThe following animation further illustrates how different rotations affect the variability explained by the dimensions of the rotated data:\n\n\n\n\n\n\n\n\nThe first dimension of z is referred to as the first principal component (PC). Because almost all the variation is explained by this first PC, the distance between rows in x can be very well approximated by the distance calculated with just z[,1].\n\n\n\n\n\n\n\n\nWe also notice that the two groups, adults and children, can be clearly observed with the one number summary, better than with any of the two original dimensions.\n\nhist(x[,1], breaks = seq(-4,4,0.5))\nhist(x[,2], breaks = seq(-4,4,0.5))\nhist(z[,1], breaks = seq(-4,4,0.5))\n\n\n\n\n\n\n\n\n\nWe can visualize these to see how the first component summarizes the data. In the plot below, red represents high values and blue negative values:\n\n\n\n\n\n\n\n\nThis idea extends naturally to more than two dimensions. As in the two-dimensional case, we begin by finding a \\(p \\times 1\\) vector \\(\\mathbf{v}_1\\) with \\(\\|\\mathbf{v}_1\\| = 1\\) that maximizes \\(\\|\\mathbf{X} \\mathbf{v}_1\\|\\). The projection \\(\\mathbf{X} \\mathbf{v}_1\\) defines the first principal component (PC), with \\(\\mathbf{v}_1\\|\\) pointing in the direction of greatest variation in the data.\nTo find the second principal component, we remove the variation explained by the first principal component from \\(\\mathbf{X}\\). This gives the residual matrix:\n\\[\n\\mathbf{r} = \\mathbf{X} - \\mathbf{X} \\mathbf{v}_1 \\mathbf{v}_1^\\top\n\\]\nWe then find a vector \\(\\mathbf{v}_2\\) with \\(\\|\\mathbf{v}_2\\| = 1\\) that maximizes \\(\\|\\mathbf{r} \\mathbf{v}_2\\|\\). The projection \\(\\mathbf{X} \\mathbf{v}_2\\) is the second principal component.\nThis process continues: at each step, we subtract the variation explained by the previous components and find the next direction that captures the greatest remaining variance. Repeating this until all \\(p\\) directions have been found, we construct the full rotation matrix \\(\\mathbf{V}\\) and the corresponding principal component matrix \\(\\mathbf{Z}\\):\n\\[\n\\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{v}_1 & \\dots & \\mathbf{v}_p\n\\end{bmatrix}, \\quad\n\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\n\\]\nEach column of \\(\\mathbf{Z}\\) is a principal component, and the columns of \\(\\mathbf{V}\\) define the new rotated coordinate system.\nThe idea of distance preservation extends naturally to higher dimensions. For a matrix \\(\\mathbf{X}\\) with \\(p\\) columns, the transformation \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\\) preserves the pairwise distances between rows, while reorienting the data so that the variance explained by each column of \\(\\mathbf{Z}\\) is ordered from largest to smallest. If the variance of the columns \\(\\mathbf{Z}_j\\) for \\(j &gt; k\\) is very small, those dimensions contribute little to the overall variation in the data and, by extension, to the distances between observations.\nIn such cases, we can approximate the original distances between rows using only the first \\(k\\) columns of \\(\\mathbf{Z}\\). If \\(k \\ll p\\), this yields a much lower-dimensional representation of the data that still retains the essential structure. This is the key advantage of PCA: it allows us to summarize high-dimensional data efficiently, making visualization, computation, and modeling more tractable without losing the main patterns in the data.\n\n\n\n\n\n\nThe solution to the PCA maximization problem is not unique. For example, \\(\\|\\mathbf{X} \\mathbf{v}\\| = \\|-\\mathbf{X} \\mathbf{v}\\|\\), so both \\(\\mathbf{v}\\) and \\(-\\mathbf{v}\\) produce the same principal component direction. Similarly, if we flip the sign of any column in \\(\\mathbf{Z}\\) (the principal components), we can preserve the equality \\(\\mathbf{X} = \\mathbf{Z} \\mathbf{V}^\\top\\) as long as we also flip the sign of the corresponding column in \\(\\mathbf{V}\\) (the rotation matrix). This means we are free to change the sign of any column in \\(\\mathbf{Z}\\) and \\(\\mathbf{V}\\) without affecting the PCA result.\n\n\n\nIn R, we can find the principal components of any matrix with the function prcomp:\n\npca &lt;- prcomp(x, center = FALSE)\n\nKeep in mind that default behavior is to center the columns of x before computing the PCs, an operation we don’t need in our example because our matrix is scaled.\nThe object pca includes the rotated data \\(Z\\) in pca$x and the rotation \\(\\mathbf{V}\\) in pca$rotation.\nWe can see that columns of the pca$rotation are indeed the rotation obtained with -45 (remember the sign is arbitrary):\n\npca$rotation\n#&gt;         PC1    PC2\n#&gt; [1,] -0.707  0.707\n#&gt; [2,] -0.707 -0.707\n\nThe square root of the variation of each column is included in the pca$sdev component. This implies we can compute the variance explained by each PC using:\n\npca$sdev^2/sum(pca$sdev^2)\n#&gt; [1] 0.9848 0.0152\n\nThe function summary performs this calculation for us:\n\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                          PC1    PC2\n#&gt; Standard deviation     1.403 0.1745\n#&gt; Proportion of Variance 0.985 0.0152\n#&gt; Cumulative Proportion  0.985 1.0000\n\nWe also see that we can rotate x (\\(\\mathbf{X}\\)) and pca$x (\\(\\mathbf{Z}\\)) as explained with the mathematical formulas above:\n\nall.equal(pca$x, x %*% pca$rotation)\n#&gt; [1] TRUE\nall.equal(x, pca$x %*% t(pca$rotation))\n#&gt; [1] TRUE",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#examples",
    "href": "highdim/dimension-reduction.html#examples",
    "title": "\n23  Dimension Reduction\n",
    "section": "\n23.6 Examples",
    "text": "23.6 Examples\nIris example\nThe iris dataset is a widely used example in data analysis courses. It contains four botanical measurements—sepal length, sepal width, petal length, and petal width—for flowers from three different species:\n\nnames(iris)\n#&gt; [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n#&gt; [5] \"Species\"\n\nIf you inspect the species column using iris$Species, you’ll see that the observations are grouped by species.\nWhen we visualize the pairwise distances between observations, a clear pattern emerges: the data appears to cluster into three distinct groups, with one species standing out as more clearly separated from the other two. This structure is consistent with the known species labels:\n\nx &lt;- iris[,1:4] |&gt; as.matrix()\nd &lt;- dist(x)\nimage(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, \"RdBu\")))\n\n\n\n\n\n\n\n\n\nOur features matrix has four dimensions, but three are very correlated:\n\ncor(x)\n#&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; Sepal.Length        1.000      -0.118        0.872       0.818\n#&gt; Sepal.Width        -0.118       1.000       -0.428      -0.366\n#&gt; Petal.Length        0.872      -0.428        1.000       0.963\n#&gt; Petal.Width         0.818      -0.366        0.963       1.000\n\nIf we apply PCA to the iris dataset, we expect that the first few principal components will capture most of the variation in the data. Because the original variables are highly correlated, PCA should allow us to approximate the distances between observations using just two dimensions, effectively compressing the data while preserving its structure.\nWe can use the summary function to examine how much variance is explained by each principal component:\n\npca &lt;- prcomp(x)\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                          PC1    PC2    PC3     PC4\n#&gt; Standard deviation     2.056 0.4926 0.2797 0.15439\n#&gt; Proportion of Variance 0.925 0.0531 0.0171 0.00521\n#&gt; Cumulative Proportion  0.925 0.9777 0.9948 1.00000\n\nThe first two dimensions account for almost 98% of the variability. Thus, we should be able to approximate the distance very well with two dimensions. We confirm this by computing the distance with just the first two dimensions and comparing to the original:\n\nd_approx &lt;- dist(pca$x[, 1:2])\nplot(d, d_approx); abline(0, 1, col = \"red\")\n\n\n\n\n\n\n\n\n\nA useful application of this result is that we can now visualize the distances between observations in a two-dimensional plot. In this plot, the distance between any pair of points approximates the actual distance between the corresponding observations in the original high-dimensional space.\n\ndata.frame(pca$x[,1:2], Species = iris$Species) |&gt;\n  ggplot(aes(PC1, PC2, fill = Species)) +\n  geom_point(cex = 3, pch = 21) +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\n\nWe color the observations by their labels and notice that, with these two dimensions, we achieve almost perfect separation.\nLooking more closely at the resulting PCs and rotations:\n\n\n\n\n\n\n\n\nwe learn that the first PC is obtained by taking a weighted average of sepal length, petal length, and petal width (red in first column), and subtracting a quantity proportional to sepal width (blue in first column). The second PC is a weighted average of petal length and petal width, minus a weighted average of sepal length and sepal width.\nMNIST example\nThe written digits example has 784 features. Is there any room for data reduction? We will use PCA to answer this.\nIf not already loaded, let’s begin by loading the data:\n\nlibrary(dslabs)\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\nBecause the pixels are so small, we expect pixels close to each other on the grid to be correlated, meaning that dimension reduction should be possible.\nLet’s compute the PCs. This will take a few seconds as it is a rather large matrix:\n\npca &lt;- prcomp(mnist$train$images)\n\nand examine the variance explained by each PC:\n\n\n\n\n\n\n\n\n\nplot(pca$sdev^2/sum(pca$sdev^2), xlab = \"PC\", ylab = \"Variance explained\")\n\nWe can see that the first few PCs already explain a large percent of the variability. Furthermore, simply by looking at the first two PCs we already see information about the labels. Here is a random sample of 500 digits:\n\n\n\n\n\n\n\n\nWe can also see the rotation values on the 28 \\(\\times\\) 28 grid to get an idea of how pixels are being weighted in the transformations that result in the PCs:\n\n\n\n\n\n\n\n\nWe can clearly see that first PC appears to be separating the 1s (red) from the 0s (blue). We can vaguely discern digits, or parts of digits, in the other three PCs as well. By looking at the PCs stratified by digits, we get further insights. For example, we see that the second PC separates 4s, 7s, and 9s from the rest:\n\n\n\n\n\n\n\n\nWe can also confirm that the lower variance PCs appear related to unimportant variability, mainly smudges in the corners:",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#exercises",
    "href": "highdim/dimension-reduction.html#exercises",
    "title": "\n23  Dimension Reduction\n",
    "section": "\n23.7 Exercises",
    "text": "23.7 Exercises\n1. We want to explore the tissue_gene_expression predictors by plotting them.\n\ndim(tissue_gene_expression$x)\n\nWe hope to get an idea of which observations are close to each other, but the predictors are 500-dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type.\n2. The predictors for each observation are measured on the same measurement device (a gene expression microarray) after an experimental procedure. A different device and procedure is used for each observation. This may introduce biases that affect all predictors for each observation in the same way. To explore the effect of this potential bias, for each observation, compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation.\n3. We see an association with the first PC and the observation averages. Redo the PCA, but only after removing the center.\n4. For the first 10 PCs, make a boxplot showing the values for each tissue.\n5. Plot the percent variance explained by PC number. Hint: Use the summary function.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html",
    "href": "highdim/regularization.html",
    "title": "24  Regularization",
    "section": "",
    "text": "24.1 Case study: recommendation systems\nRecommendation systems, such as the one used by Amazon, operate by analyzing the ratings that customers give to various products. These ratings form a large dataset. The system uses this data to predict how likely a specific user is to favorably rate a particular product. For example, if the system predicts that a user is likely to give a high rating to a certain book or gadget, it will recommend that item to them. In essence, the system tries to guess which products a user will like based on the ratings provided by them and other customers for various items. This approach helps in personalizing recommendations to suit individual preferences.\nDuring its initial years of operation, Netflix used a 5-star recommendation system. One star suggested it was not a good movie, whereas five stars suggested it was an excellent movie. Here, we provide the basics of how these recommendations are made, motivated by some of the approaches taken by the winners of the Netflix challenges.\nIn October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009, the winners were announced1. You can read a summary of how the winning algorithm was put together here: http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/ and a more detailed explanation here: https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf. We will now show you some of the data analysis strategies used by the winning team.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#sec-recommendation-systems",
    "href": "highdim/regularization.html#sec-recommendation-systems",
    "title": "24  Regularization",
    "section": "",
    "text": "The Netflix data is not publicly available, but the GroupLens research lab2 generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users. We make a small subset of this data available via the dslabs package:\n\nlibrary(data.table)\nlibrary(dslabs)\nmovielens |&gt; tibble:::tibble() |&gt; head(5)\n#&gt; # A tibble: 5 × 7\n#&gt;   movieId title                      year genres userId rating timestamp\n#&gt;     &lt;int&gt; &lt;chr&gt;                     &lt;int&gt; &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;\n#&gt; 1      31 Dangerous Minds            1995 Drama       1    2.5    1.26e9\n#&gt; 2    1029 Dumbo                      1941 Anima…      1    3      1.26e9\n#&gt; 3    1061 Sleepers                   1996 Thril…      1    3      1.26e9\n#&gt; 4    1129 Escape from New York       1981 Actio…      1    2      1.26e9\n#&gt; 5    1172 Cinema Paradiso (Nuovo c…  1989 Drama       1    4      1.26e9\n\nEach row represents a rating given by one user to one movie.\nBecause we are working with a relatively large dataset, we will convert the data frame to a data.table object to take advantage of more efficient data wrangling.\n\ndt &lt;- as.data.table(movielens)\n\nWe can see the number of unique users that provided ratings and how many unique movies were rated:\n\ndt[, .(n_users = uniqueN(userId), n_movies = uniqueN(movieId))]\n#&gt;    n_users n_movies\n#&gt;      &lt;int&gt;    &lt;int&gt;\n#&gt; 1:     671     9066\n\nIf we multiply those two numbers, we get a figure exceeding 5 million, yet our dataset contains only about 100,000 rows. This tells us that not every user rated every movie. In fact, the median number of movies rated per user is 71. We can think of the data as a very large matrix, with users as rows and movies as columns, filled with many missing entries. The goal of a movie recommendation system is to accurately predict those missing values.\nLet’s look at some of the general properties of the data to better understand the challenges.\nThe first thing we notice is that some movies receive far more ratings than others. The distribution below highlights this disparity, which isn’t surprising, blockbuster films tend to be seen and rated by millions, while niche or independent films attract far fewer viewers. A second observation is that users vary significantly in how active they are at rating movies:\n\n\n\n\n\n\n\n\nWe need to build an algorithm with the collected data that will then be applied outside our control when users look for movie recommendations. To test our idea, we will split the data into a training set, which we will use to develop our approach, and a test set in which we will compute the accuracy of our predictions.\nWe will do this only for users that have provided at least 100 ratings.\n\ndt &lt;- dt[, if (.N &gt;= 100) .SD, by = userId]\n\nFor each one of these users, we will split their ratings into 80% for training and 20% for testing.\n\nset.seed(2006)\nindexes &lt;- split(1:nrow(dt), dt$userId)\n\ntest_ind &lt;- sapply(indexes, function(i) sample(i, ceiling(length(i)*.2))) \ntest_ind &lt;- sort(unlist(test_ind))\n\nWe keep only the columns we use in this analysis and convert userId ad movieId to characters so we can use as indeces:\n\ndt[, `:=`(userId = as.character(userId), movieId = as.character(movieId))]\ncols &lt;- c(\"userId\", \"movieId\", \"title\", \"rating\")\ntest_set &lt;- dt[test_ind, ..cols] \ntrain_set &lt;- dt[-test_ind, ..cols]\n## Remove  movies in the test set that have no ratings in the training set\ntest_set &lt;- test_set[movieId %in% train_set$movieId]\n\nWe will use the array representation described in Section 17.6, for the training data. Specifically, we denote ranking for movie \\(j\\) by user \\(i\\) as \\(y_{ij}\\).\nNote that two different movies can have the same title. For example, our dataset has three movies titled “King Kong”. Titles are therefore not unique and we can’t use them as IDs.\n\n\n\n\n\n\nAlthough it is convenient to use the notation \\(y_{ij}\\), it is important to remember that we do not observe a value for every pair \\((i,j)\\). While the total number of users \\(i = 1, \\dots, I\\) and movies \\(j = 1, \\dots, J\\) defines a theoretical space of \\(I \\times J\\) possible ratings, the actual number of observed ratings,denoted by \\(N\\), is typically much smaller. In our example, \\(N\\) is far less than \\(I \\times J\\), reflecting the fact that most users rate only a small fraction of all available movies. We will use the mathematical notation \\(\\sum_{i,j}\\) to represent a summation over all \\(N\\) observed pairs.\n\n\n\nLoss function\nThe Netflix challenge decided on a winner based on the root mean squared error (RMSE) computed on the test set. Specifically, if \\(y_{ij}\\) is the rating for movie \\(j\\) by user \\(i\\) in the test set and \\(\\hat{y}_{ij}\\) is our prediction based on the training set, RMSE was defined as:\n\\[\n\\mbox{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{ij} (y_{ij} - \\hat{y}_{ij})^2}\n\\]\nwith \\(N\\) being the number of user/movie combinations for which we made predictions and the sum occurring over all these combinations.\nWe can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good. We define a function to compute this quantity for any set of residuals:\n\nrmse &lt;- function(r) sqrt(mean(r^2))\n\nIn this chapter and the next, we introduce two concepts, regularization and latent factor analysis, that were used by the winners of the Netflix challenge to obtain the winning RMSE.\n\n\n\n\n\n\nIn Chapter 30, we provide a formal discussion of the mean squared error.\n\n\n\nA first model\nLet’s start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. What number should this prediction be? We can use a model based approach to answer this. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look as follows:\n\\[\nY_{ij} = \\mu + \\varepsilon_{ij}\n\\]\nwith \\(\\varepsilon_{ij}\\) independent errors sampled from the same distribution centered at 0 and \\(\\mu\\) the true rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of \\(\\mu\\) and, in this case, is the average of all ratings:\n\nmu &lt;- mean(train_set$rating)\n\nIf we predict all unknown ratings with \\(\\hat{\\mu}\\) we obtain an RMSE of\n\nrmse(test_set$rating - mu)\n#&gt; [1] 1.04\n\nMathematical theory tells us that if you plug in any other number, you get a higher RMSE. Here is an example:\n\nrmse(test_set$rating - 3)\n#&gt; [1] 1.16\n\nTo win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do better!\nUser effects\nIf we visualize the average rating for each user\n\nhist(with(dt, tapply(rating, userId, mean)), nclass = 30)\n\n\n\n\n\n\n\nwe notice that there is substantial variability across users: some users are very cranky and others love most movies. To account for this, we can use a linear model with a treatment effect \\(\\alpha_i\\) for each user. The sum \\(\\mu + \\alpha_i\\) can be interpreted as the typical rating user \\(i\\) gives to movies. We can write the model as:\n\\[\nY_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij}\n\\]\nStatistics textbooks refer to the \\(\\alpha\\)s as treatment effects. In the Netflix challenge papers, they refer to them as bias.\nNote that this model has 263 user effect parameters.\nWe can again use least squares to estimate the \\(\\alpha_i\\) in the following way:\n\nfit &lt;- lm(rating ~ userId, data = train_set)\n\nNote that because there are hundreds of \\(\\alpha_i\\), as each movie gets one, the lm() function will be slow here. In this case, we can show that the least squares estimate \\(\\hat{\\alpha}_i\\) is just the average of \\(y_{ij} - \\hat{\\mu}\\) for each user \\(i\\). So we can compute them this way:\n\nmu &lt;- mean(train_set$rating)\na &lt;- with(train_set[, .(a = mean(rating - mu)), by = userId], \n          setNames(a, userId))\n\nNote that going forward, in the code, we use a to represent \\(\\alpha\\), b to represent \\(\\beta\\), and we drop the hat notation to represent estimates.\nLet’s see how much our prediction improves once we use \\(\\hat{y}_{ij} = \\hat{\\mu} + \\hat{\\alpha}_i\\). Because we know ratings can’t be below 0.5 or above 5, we define the function clamp:\n\nclamp &lt;- function(x, min = 0.5, max = 5) pmax(pmin(x, max), min)\n\nto keep predictions in that range and then compute the RMSE:\n\nresid &lt;- with(test_set, rating - clamp(mu + a[userId]))\nrmse(resid)\n#&gt; [1] 0.958\n\nThe RMSE is reduced so we already see an improvement. But can we make it better?\nMovie effects\nWe know from experience that some movies are just generally rated higher than others. We can use a linear model with a treatment effect \\(\\beta_j\\) for each movie, which can be interpreted as movie effect or the difference between the average ranking for movie \\(j\\) and the overall average \\(\\mu\\):\n\\[\nY_{ij} = \\mu + \\alpha_i + \\beta_j +\\varepsilon_{ij}\n\\]\nWe can again use least squares to estimate the \\(b_i\\) in the following way:\n\nfit &lt;- lm(rating ~ userId + movieId, data = train_set)\n\nHowever, this code generates a very large matrix with all the indicator variables needed to represent all 263 users and 8133 movies and the code will take time to run. So instead, for illustrative purposes, we use an approximation used by the wining team: first computing the least square estimate \\(\\hat{\\mu}\\) and \\(\\hat{\\alpha}_i\\), and then estimating \\(\\hat{\\beta}_j\\) as the average of the residuals \\(y_{ij} - \\hat{\\mu} - \\hat{\\alpha}_i\\):\n\nmu &lt;- mean(train_set$rating)\na &lt;- with(train_set[, .(a = mean(rating - mu)), by = userId], \n          setNames(a, userId))\nb &lt;- with(train_set[, .(b = mean(rating - mu - a[userId])), by = movieId], \n          setNames(b, movieId))\n\nWe can now construct predictors with this code and see that our RMSE improves.\n\nresid &lt;- with(test_set, rating - clamp(mu + a[userId] + b[movieId]))\nrmse(resid)\n#&gt; [1] 0.911\n\nPenalized least squares\nIf we look at the top movies based on our estimates of the movie effect \\(\\hat{\\beta}_j\\), we find that they are all obscure movies, with just one rating:\n\ntop_movies &lt;- names(b[b == max(b)])\ntrain_set[, .(n = .N), by = .(movieId, title)][movieId %in% top_movies]\n#&gt;    movieId                                         title     n\n#&gt;     &lt;char&gt;                                        &lt;char&gt; &lt;int&gt;\n#&gt; 1:    1450 Prisoner of the Mountains (Kavkazsky plennik)     1\n#&gt; 2:    1563                         Dream With the Fishes     1\n#&gt; 3:    1819                          Storefront Hitchcock     1\n#&gt; 4:    3892                            Anatomy (Anatomie)     1\n#&gt; 5:    4076                                     Two Ninas     1\n#&gt; 6:    4591                               Erik the Viking     1\n#&gt; 7:    4796                         Grass Is Greener, The     1\n#&gt; 8:    5427                                       Caveman     1\n\nDo we really think these are the top movies in our database?\nNote that only one of these top rated movies appears in our test set and it received the worst rating:\n\ntest_set[movieId %in% top_movies, .(movieId, title, rating)]\n#&gt;    movieId              title rating\n#&gt;     &lt;char&gt;             &lt;char&gt;  &lt;num&gt;\n#&gt; 1:    3892 Anatomy (Anatomie)      1\n\nLarge positive or negative estimates should be treated with caution when they are based on just a few ratings. These estimates are more likely to be noisy. And since large prediction errors can increase our Root Mean Squared Error (RMSE), it’s better to be conservative when we are uncertain.\nIn earlier chapters, we handled uncertainty by using standard errors and confidence intervals. But when making predictions, we don’t get to give a range, we must pick a single number. That’s where regularization comes in.\nRegularization is a technique that helps us make more stable predictions when dealing with small sample sizes. It does this by shrinking large estimates toward zero, especially when they are based on only a few data points. In this way, regularization has similarities to the Bayesian shrinkage ideas we discussed in Chapter 11.\nAs an example, suppose the overall average movie rating is \\(\\mu = 3\\). Now imagine:\n\nMovie 1 has 100 user ratings.\nMovies 2, 3, 4, and 5 each have only 1 user rating.\n\nIf we estimate movie effects using least squares, we subtract \\(\\mu\\) from each movie’s average rating. For Movie 1, this average is based on 100 users, so it’s a pretty reliable estimate. But for Movies 2-5, we are relying on a single rating, which is much less reliable. These one-off ratings might look accurate now, but there is a good chance they won’t generalize well to new users.\nIn fact, for Movies 2–5, it might be safer to ignore the single rating and simply assume they are average movies. This cautious approach helps prevent extreme predictions based on limited data. By shrinking these uncertain estimates toward the overall average, we make the results more regularm hence the term regularization. This strategy often leads to smaller errors when predicting new data, ultimately helping reduce the RMSE.\nA penalized approach\nThe most common way to regularize predictions is through penalized regression, which controls how much the movie effects \\(\\beta_j\\) are allowed to vary. Instead of minimizing only the sum of squared errors, we modify the objective function by adding a penalty term that discourages large values of \\(\\beta_j\\), especially when these estimates are based on limited data. Specifically, we redefine the quantity we minimize by adding a penalty to the residual sum of squares:\n\\[\n\\sum_{i,j} \\left(y_{ij} - (\\mu + \\alpha_i + \\beta_j) \\right)^2 + \\lambda \\sum_{j} \\beta_j^2\n\\] The first term is just the sum of squares and the second is a penalty that gets larger when many \\(\\beta_j\\)s are large.\nIn this particular model, using calculus, we can actually show that, if we know \\(\\mu\\) and the \\(\\alpha_i\\)s, the values of \\(\\beta_j\\) that minimize this equation are:\n\\[\n\\hat{\\beta}_j(\\lambda) = \\frac{1}{n_j + \\lambda} \\sum_{i=1}^{n_i} \\left(y_{ij} - \\mu - \\alpha_i\\right)\n\\]\nwhere \\(n_j\\) is the number of ratings made for movie \\(j\\).\nLet’s examine what happens when we set the penality term \\(\\lambda\\) to 4 and we plug our previously calcualted estimates \\(\\hat{\\mu}\\) and \\(\\hat{\\alpha}_i\\) to estimate obtain a regularized estimate \\(\\hat{\\beta}_j(\\lambda)\\).\n\nlambda &lt;- 4\nmu &lt;- mean(train_set$rating)\na &lt;- with(train_set[, .(a = mean(rating - mu)), by = userId], \n          setNames(a, userId))\ntmp &lt;- train_set[, .(b = sum(rating - mu - a[userId])/(.N + lambda)), by = movieId]\nb_reg &lt;- with(tmp, setNames(b, movieId))\n\nTo see how the estimates shrink, let’s make a plot of the regularized estimates versus the least squares estimates.\n\n\n\n\n\n\n\n\nWe see that some of the largest movie effects estimates come from movies with few ratings and that regularization shrinks these towards 0.\nNow, let’s look at the top 5 best movies based on the penalized estimates \\(\\hat{b}_i(\\lambda)\\):\n\ntop_movies &lt;- names(sort(-b_reg)[1:5])\ntrain_set[, .(n = .N), by = .(movieId, title)][movieId %in% top_movies]\n#&gt;    movieId                     title     n\n#&gt;     &lt;char&gt;                    &lt;char&gt; &lt;int&gt;\n#&gt; 1:     858            Godfather, The   107\n#&gt; 2:     318 Shawshank Redemption, The   138\n#&gt; 3:    1276            Cool Hand Luke    32\n#&gt; 4:    2318                 Happiness    18\n#&gt; 5:      50       Usual Suspects, The   109\n\nThese make more sense with some movies that are watched more and have more ratings in the training set.\nThis approach will have our desired effect: when our sample size \\(n_j\\) is very large, we obtain a stable estimate and the penalty \\(\\lambda\\) is effectively ignored since \\(n_j+\\lambda \\approx n_j\\). Yet when the \\(n_j\\) is small, then the estimate \\(\\hat{\\beta}_i(\\lambda)\\) is shrunken towards 0. The larger the \\(\\lambda\\), the more we shrink.\nThe RMSE also improves:\n\nresid &lt;- with(test_set, rating - clamp(mu + a[userId] + b_reg[movieId]))\nrmse(resid)\n#&gt; [1] 0.89",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#penalized-least-squares-1",
    "href": "highdim/regularization.html#penalized-least-squares-1",
    "title": "24  Regularization",
    "section": "\n24.2 Penalized least squares",
    "text": "24.2 Penalized least squares\nIn the previous section, we motivated penalizing the size of the movie effect estimates to reduce overfitting. This idea generalizes to a very popular approach for fitting linear models called penalized least squares. If you have a multivariate linear model\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j + \\varepsilon_i, \\quad i = 1, \\dots, N\n\\] with a large number of parameters \\(p\\), fitting a least squares model can lead to overfitting. As discussed in the previous section, one way to address this issue is to penalize the size of the parameter estimates by using a penalized least squares approach:\n\\[\n\\frac{1}{N}\\sum_{i=1}^N \\left( y_i - \\left(\\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j\\right)\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nThe first term is the normalized residual sum of squares, minimized by the lm function in R. We normalize, divide by \\(N\\), so that the interpretation of \\(\\lambda\\) does not depend on data size. The second term is a penalty that increases with the size of the coefficients. The mean level \\(\\beta_0\\) is not penalized.\n\n\n\n\n\n\nThe concept of penalization extends beyond least squares and can be applied to more general models, such as penalized likelihood.\n\n\n\nWe can use calculus and linear algebra to find the \\(\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_p)^\\top\\) that minimizes this expression. The MASS package includes the lm.ridge function to perform this computation. It works similarly to lm, but requires you to specify a \\(\\lambda\\):\n\nlibrary(MASS)\nfit &lt;- lm.ridge(y ~ x, lambda = 0.0001)\n\nThis approach is called ridge regression, which is just another name for least squares with an the penalty term \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\).\nOur movie rating model is also linear, with one parameter for each user and one for each movie. Regularization is especially important in this setting given the large number of parameters. We can compute regularized estimates using:\n\nlibrary(MASS)\nl &lt;- 4/nrow(train_set)\nfit &lt;- lm.ridge(rating ~ userId + movieId, lambda = l, data = train_set)\n\nSimilar to the reason we didn’t run lm, we don’t run this because it’s computationally expensive, userId and movieId are factors that result in over 8,000 indicator variables in the design matrix (see [Section 17.6]).\nIn the next section, we present an approximation that allows fast estimation of the model parameters.\n\n\n\n\n\n\nThe linear model variables \\(x_{i1},\\dots,x_{ip}\\) needed to build a movie recommendation system would include hundreds of indicator variables, one for each movie and one for each user. Although we don’t cover the mathematical or computational details in this book, it is important to note that much more efficient algorithms exist than those used by general-purpose functions like lm and lm.ridge. These functions are not optimized for scenarios where most of the variables in each row are zeros, as is the case here. Therefore, we do not recommend using them for problems of this scale and sparsity.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#selecting-penalty-terms",
    "href": "highdim/regularization.html#selecting-penalty-terms",
    "title": "24  Regularization",
    "section": "\n24.3 Selecting Penalty Terms",
    "text": "24.3 Selecting Penalty Terms\nHow do we select $$? In [Chapter 30], we describe formal methods, but here we simply compute RMSE for a range of $$ values. We implement a Alternative Least Square (ALS) type algorithm to estimate \\(\\alpha\\) and \\(\\beta\\). Namely, if because if the \\(\\alpha\\)s are known we have a closed mathematical solution for the penalized lease square etimate of \\(\\beta\\), and similar for the reverse case, alternating iteratively converages to the actual penalized squares estimates.\n\nn_iter &lt;- 10\nlambdas &lt;- seq(0, 0.0002, len = 100)\nN &lt;- nrow(train_set)\nmu &lt;- mean(train_set$rating)\nrmses &lt;- sapply(lambdas, function(lambda){\n  b &lt;- with(train_set[, .(b = 0), by = movieId], setNames(b, movieId))\n  for (iter in 1:n_iter) {\n    a &lt;- with(train_set[, .(a = mean(rating - mu - b[movieId])), by = userId], \n          setNames(a, userId))\n    tmp &lt;- train_set[, .(b = sum(rating - mu - a[userId])/(.N + N*lambda)), by = movieId]\n    b &lt;- with(tmp, setNames(b, movieId))\n  }\n  resid &lt;- with(test_set, rating - clamp(mu + a[userId] + b[movieId]))\n  rmse(resid)\n})\n\nWe then plot RMSE as a function of \\(\\lambda\\):\n\nplot(lambdas, rmses, type = \"l\")\n\n\n\n\n\n\n\nThe minimum RMSE is obtained for:\n\nlambdas[which.min(rmses)]\n#&gt; [1] 3.43e-05\n\nWe use this \\(\\lambda\\) to compute the final regularized estimates:\n\nn_iter &lt;- 10\nlambda &lt;- lambdas[which.min(rmses)]\nN &lt;- nrow(train_set)\nmu &lt;- mean(train_set$rating)\nb &lt;- with(train_set[, .(b = 0), by = movieId], setNames(b, movieId))\nfor (iter in 1:n_iter) {\n  a &lt;- with(train_set[, .(a = mean(rating - mu - b[movieId])), by = userId], \n            setNames(a, userId))\n  tmp &lt;- train_set[, .(b = sum(rating - mu - a[userId])/(.N + N*lambda)), by = movieId]\n  b &lt;- with(tmp, setNames(b, movieId))\n}\n\nAnd make final predictions:\n\nresid &lt;- with(test_set, rating - clamp(mu + a[userId] + b[movieId]))\nrmse(resid)\n#&gt; [1] 0.874\n\nWe can see that regularization improves our RMSE by comparing to the other three approaches, but now using the least square estimates rather than the approximations.\n\n\n\n\nmodel\nRMSE\n\n\n\nJust the mean\n1.043\n\n\nUser effect\n0.958\n\n\nUser + movie effect\n0.893\n\n\nRegularized\n0.874",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#exercises",
    "href": "highdim/regularization.html#exercises",
    "title": "24  Regularization",
    "section": "\n24.4 Exercises",
    "text": "24.4 Exercises\n1. For the movielens data, compute the number of ratings for each movie and then plot it against the year the movie was released. Use the square root transformation on the counts.\n2. We see that, on average, movies that were released after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.\nAmong movies that came out in 1993 or later, what are the 25 movies with the most ratings per year? Also, report their average rating.\n3. From the table constructed in the previous example, we see that the most rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post 1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.\n4. In the previous exercise, we see that the more a movie is rated, the higher the rating. Suppose you are doing a predictive analysis in which you need to fill in the missing ratings with some value. Which of the following strategies would you use?\n\nFill in the missing values with average rating of all movies.\nFill in the missing values with 0.\nFill in the value with a lower value than the average since lack of rating is associated with lower ratings. Try out different values and evaluate prediction in a test set.\nNone of the above.\n\n5. The movielens dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column date with the date. Hint: Use the as_datetime function in the lubridate package.\n6. Compute the average rating for each week and plot this average against day. Hint: Use the round_date function before you group_by.\n7. The plot shows some evidence of a time effect. If we define \\(d_{u,i}\\) as the day for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate:\n\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i} + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i}\\beta + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i}\\beta_i + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\).\n\n8. The movielens data also has a genres column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots.\n9. The plot shows strong evidence of a genre effect. If we define \\(g_{u,i}\\) as the genre for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate:\n\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i} + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i}\\beta + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + \\sum_{k=1}^K x_{u,i} \\beta_k + \\varepsilon_{u,i}\\), with \\(x^k_{u,i} = 1\\) if \\(g_{u,i}\\) is genre \\(k\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\).\n\nAn education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let’s simulate a dataset for 100 schools. First, let’s simulate the number of students in each school.\n\nset.seed(1986)\nn &lt;- round(2^rnorm(1000, 8, 1))\n\nNow let’s assign a true quality for each school completely independent from size. This is the parameter we want to estimate.\n\nmu &lt;- round(80 + 2 * rt(1000, 5))\nrange(mu)\nschools &lt;- data.frame(id = paste(\"PS\",1:100), \n                      size = n, \n                      quality = mu,\n                      rank = rank(-mu))\n\nWe can see that the top 10 schools are:\n\nschools |&gt; top_n(10, quality) |&gt; arrange(desc(quality))\n\nNow let’s have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points:\n\nscores &lt;- sapply(1:nrow(schools), function(i){\n  scores &lt;- rnorm(schools$size[i], schools$quality[i], 30)\n  scores\n})\nschools &lt;- schools |&gt; mutate(score = sapply(scores, mean))\n\n10. What are the top schools based on the average score? Show just the ID, size, and the average score.\n11. Compare the median school size to the median school size of the top 10 schools based on the score.\n12. According to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercise for the worst 10 schools.\n13. The same is true for the worst schools! They are small as well. Plot the average score versus school size to see what’s going on. Highlight the top 10 schools based on the true quality. Use the log scale transform for the size.\n14. We can see that the standard error of the score has larger variability when the school is smaller. This is a basic statistical reality we learned in the probability and inference sections. In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score.\nLet’s use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools:\n\noverall &lt;- mean(sapply(scores, mean))\n\nand then define, for each school, how it deviates from that average. Write code that estimates the score above average for each school, but dividing by \\(n + \\lambda\\) instead of \\(n\\), with \\(n\\) the school size and \\(\\lambda\\) a regularization parameter. Try \\(\\lambda = 3\\).\n15. Notice that this improves things a bit. The number of small schools that are not highly ranked is now 4. Is there a better \\(\\lambda\\)? Find the \\(\\lambda\\) that minimizes the RMSE = \\(1/100 \\sum_{i=1}^{100} (\\mbox{quality} - \\mbox{estimate})^2\\).\n16. Rank the schools based on the average obtained with the best \\(\\alpha\\). Note that no small school is incorrectly included.\n17. A common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don’t subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from exercise 14, but without removing the overall mean.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#footnotes",
    "href": "highdim/regularization.html#footnotes",
    "title": "24  Regularization",
    "section": "",
    "text": "http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/↩︎\nhttps://grouplens.org/↩︎",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html",
    "href": "highdim/latent-factor-models.html",
    "title": "25  Latent Factor Models",
    "section": "",
    "text": "25.1 Factor analysis\nIn the previous chapter, we described how the model:\n\\[\nY_{ij} = \\mu + \\alpha_i + \\beta_j + \\varepsilon_{ij}\n\\]\ncan be used to model movie ratings and help make useful predictions. This model accounts for user differences through \\(\\alpha_i\\) and movie effects through \\(\\beta_j\\). However, the model ignores an important source of information related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well.\nTo see an example of this, we compute residuals:\n\\[\nr_{ij} = y_{ij} - (\\hat{\\mu} + \\hat{\\alpha}_i + \\hat{\\beta}_j)\n\\]\nusing the mu, a and b computed in the previous chapter,\nand see how the residuals for three different movies correlate with The Godfather:\nWe see that the correlation varies from very strong positive to negative across the other three movies.\nIn this chapter, we introduce Latent Factor Models, an approach that captures these correlation patterns and improves prediction accuracy. We present the ideas in the context of movie recommendation systems, where latent factors represent hidden groups of users with similar preferences and movie characteristics. We highlight the connection between factor models and Principal Component Analysis (PCA), showing how both reduce dimensionality by uncovering structure in the data. Finally, we close the chapter by introducing a related technique, the Singular Value Decomposition (SVD), which provides a general mathematical framework underlying both PCA and latent factor methods.\nWe start with a simple simulated example. Specifically, we simulate ratings \\(Y_{ij}\\) for six movies and 120 users. For simplicity we assume that these ratings have been adjusted for the movie and user effects described in Chapter 24. We stored the results for 120 user and 6 movie effects in the object y:\ndim(y)\n#&gt; [1] 120   6\nIf we examine the correlation between movies based on user ratings, we notice a pattern:\ncor(y)\n#&gt;                      Godfather Godfather 2 Goodfellas Scent of a Woman\n#&gt; Godfather                1.000       0.671      0.558           -0.527\n#&gt; Godfather 2              0.671       1.000      0.471           -0.450\n#&gt; Goodfellas               0.558       0.471      1.000           -0.888\n#&gt; Scent of a Woman        -0.527      -0.450     -0.888            1.000\n#&gt; You've Got Mail         -0.734      -0.649     -0.487            0.451\n#&gt; Sleepless in Seattle    -0.721      -0.739     -0.505            0.475\n#&gt;                      You've Got Mail Sleepless in Seattle\n#&gt; Godfather                     -0.734               -0.721\n#&gt; Godfather 2                   -0.649               -0.739\n#&gt; Goodfellas                    -0.487               -0.505\n#&gt; Scent of a Woman               0.451                0.475\n#&gt; You've Got Mail                1.000                0.756\n#&gt; Sleepless in Seattle           0.756                1.000\nIt appears that ratings are positively correlated within genres, for example, among mob movies or among romance movies, and negatively correlated across the two genres. In statistics, such patterns are often explained by factors: unobserved, or latent, variables that account for the correlations or associations among the observed variables. Under certain assumptions, which we will describe below, these latent factors can be estimated from the data and used to capture the underlying structure driving the correlations.\nOne approach is to use our knowledge of movies genres to define a factor that distinguishes between mob and romance movies:\nq &lt;- c(1, 1, 1, -1, -1, -1)\nWe code mob movies with -1 and romance movies with 1.\nTo quantify each user’s genre preference, we fit a linear model for every user \\(i\\):\np &lt;- apply(y, 1, function(y_i) lm(y_i ~ q - 1)$coef)\nHere, we use -1 in the formula because the residuals have mean 0 and no intercept is needed.\nThe resulting vector p represents, for each user, the difference in average ratings between mob and romance movies. A positive value indicates a preference for mob movies, a negative value indicates a preference for romance movies, and values near 0 suggest no strong preference. The histogram below shows users cluster into these three type:\nhist(p, breaks = seq(-2, 2, 0.1))\nTo see that we can approximate \\(Y_{ij}\\) with \\(p_iq_j\\) we convert the vectors to matrices and use linear algebra:\np &lt;- matrix(p); q &lt;- matrix(q)\nplot(p %*% t(q), y)\nHowever, after removing this mob/romance effect, we still see structure in the correlation:\ncor(y - p %*% t(q))\n#&gt;                      Godfather Godfather 2 Goodfellas Scent of a Woman\n#&gt; Godfather                1.000       0.185     -0.545            0.557\n#&gt; Godfather 2              0.185       1.000     -0.618            0.594\n#&gt; Goodfellas              -0.545      -0.618      1.000           -0.671\n#&gt; Scent of a Woman         0.557       0.594     -0.671            1.000\n#&gt; You've Got Mail         -0.280      -0.186      0.619           -0.641\n#&gt; Sleepless in Seattle    -0.198      -0.364      0.650           -0.656\n#&gt;                      You've Got Mail Sleepless in Seattle\n#&gt; Godfather                     -0.280               -0.198\n#&gt; Godfather 2                   -0.186               -0.364\n#&gt; Goodfellas                     0.619                0.650\n#&gt; Scent of a Woman              -0.641               -0.656\n#&gt; You've Got Mail                1.000                0.353\n#&gt; Sleepless in Seattle           0.353                1.000\nThis structure seems to be driven by Al Pacino being in the movie or not. This implies we could add another factor in a second column:\nq &lt;- cbind(c(1, 1, 1, -1, -1, -1),\n           c(1, 1, -1, 1, -1, -1))\nWe can then, for each factor, obtain an estimates preference for each user:\np &lt;- t(apply(y, 1, function(y_i) lm(y_i ~ q-1)$coefficient))\nWe use the transpose t because apply binds results into columns and we want a row for each user.\nOur approximation based on two factors does an even better job of predicting how our residuals deviate from 0:\nplot(p %*% t(q), y)\nThis approximation with two factors can be written as:\n\\[\nY_{ij} \\approx p_{i1}q_{j1} + p_{i2}q_{j2}, i = 1 \\dots, I \\mbox{ and } j = 1, \\dots, J\n\\] with \\(I\\) the number of users and \\(J\\) the number of movies.\nUsing matrix representation we can rewrite the above question like this:\n\\[\n\\mathbf{Y} \\approx \\mathbf{P}\\mathbf{Q}^\\top\n\\] with \\(\\mathbf{Y}\\) an \\(I\\times J\\) matrix with entries \\(Y_{ij}\\), \\(\\mathbf{P}\\) a \\(I\\times K\\) matrix with entries \\(p_{ik}\\), and \\(\\mathbf{Q}\\) a \\(J\\times K\\) matrix with entries \\(q_{jk}\\).\nThis analysis provides insights into the process generating our data since \\(\\mathbf{P}\\) contains user-specific parameters and \\(\\mathbf{Q}\\) contains movie-specific parameters. The approach is often referred to as matrix factorization because the rating matrix has been factorized into two lower-dimensional, interpretable matrices.\nNote that the apporach also provides compression since the \\(120 \\times 6 = 720\\) observations can be well approximated by a matrix multiplication of a \\(120 \\times 2\\) matrix \\(\\mathbf{P}\\) and a \\(6 \\times 2\\) matrix \\(\\mathbf{Q}\\), a total of 252 parameters.\nIn our example with simulated data, we deduced the factors \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\) from the sample correlation and our knowledge of movies. These ended up working well. However, in general deducing factors is not this easy. Furthermore, factors that provide good approximation might be more complicated than containing just two values. For example, The Godfather III has a romantic subplot so we might not know what value to assign it in q.\nSo, can we estimate the factors? A challenge is that if \\(\\mathbf{P}\\) is unknown, our model is no longer linear: we can’t use lm to estimate both \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\). In the next sections, we describe how PCA can be used to estimate \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\).",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Latent Factor Models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#sec-factor-analysis",
    "href": "highdim/latent-factor-models.html#sec-factor-analysis",
    "title": "25  Latent Factor Models",
    "section": "",
    "text": "There is a faster way to make this computation using linear algebra. This is because the lm function is computing the least squares estimates by taking the derivative of the sum of squares, equaling it to 0, and noting the solution \\(\\hat{\\boldsymbol{\\beta}}_i\\) satisfies:\n\\[\n(\\mathbf{q}^\\top\\mathbf{q}) \\, \\hat{\\boldsymbol{\\beta}}_i  = \\mathbf{q}^\\top \\mathbf{y}_i\n\\]\nwith \\(\\mathbf{y}_i\\) column vector represeting the row of y passed to y_i in the apply function. Because \\(\\mathbf{q}\\) does not change for each user, rather than have lm recompute the equation for each user, we can perform the calculation on each row of y to get the \\(\\beta_i\\) for all users \\(i\\) like this:\n\np &lt;- t(qr.solve(crossprod(q)) %*% t(q) %*% t(y))",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Latent Factor Models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#connection-to-pca",
    "href": "highdim/latent-factor-models.html#connection-to-pca",
    "title": "25  Latent Factor Models",
    "section": "\n25.2 Connection to PCA",
    "text": "25.2 Connection to PCA\nNotice that in Section 23.5 we learned that if we perform PCA on the matrix \\(\\boldsymbol{\\varepsilon}\\), we obtain a transformation \\(\\mathbf{V}\\) that permits us to rewrite:\n\\[\n\\mathbf{Y} = \\mathbf{Z} \\mathbf{V}^\\top\n\\]\nwith \\(\\mathbf{Z}\\) the matrix of principal components.\nLet’s perform PCA and examine the results:\n\npca &lt;- prcomp(y, center = FALSE)\n\nFirst, notice that the first two PCs explain over 85% of the variability:\n\npca$sdev^2/sum(pca$sdev^2)\n#&gt; [1] 0.6939 0.1790 0.0402 0.0313 0.0303 0.0253\n\nNext, notice that the first column of \\(\\mathbf{V}\\):\n\npca$rotation[,1]\n#&gt;            Godfather          Godfather 2           Goodfellas \n#&gt;                0.306                0.261                0.581 \n#&gt;     Scent of a Woman      You've Got Mail Sleepless in Seattle \n#&gt;               -0.570               -0.294               -0.300\n\nis assigning positive values to the mob movies and negative values to the romance movies.\nThe second column:\n\npca$rotation[,2]\n#&gt;            Godfather          Godfather 2           Goodfellas \n#&gt;                0.354                0.377               -0.382 \n#&gt;     Scent of a Woman      You've Got Mail Sleepless in Seattle \n#&gt;                0.437               -0.448               -0.442\n\nis coding for Al Pacino movies.\nPCA is automatically discovering the structure we inferred using our knowledge of the movies. This is not a coincidence, there is a mathematical connection that explains why PCA aligns with these latent patterns. To see this assume that data \\(\\mathbf{Y}\\) follows the model:\n\\[\nY_{ij} = \\sum_{k=1}^K p_{ik}q_{jk} + \\varepsilon_{ij}, i=1,\\dots,I, \\, j = 1,\\dots J \\mbox{ or }\n\\mathbf{Y} =  \\mathbf{P}\\mathbf{Q} ^\\top + \\boldsymbol{\\varepsilon}\n\\] with the constraint\n\\[\n\\mathbf{Q}^\\top\\mathbf{Q} = \\mathbf{I}\n\\]\nTo understand why we need this constraint, notice that without it the model is not uniquely defined, it is not identifiable. For example, we can multiply any column of \\(\\mathbf{P}\\) by a constant \\(c &gt; 0\\) and divide the corresponding column of \\(\\mathbf{Q}\\) by the same constant, and the product \\(\\mathbf{P}\\mathbf{Q}^\\top\\) remains unchanged. This constraint removes the scaling ambiguity and ensures that the factorization has a well-defined form.\nThe first \\(K\\) columns of the principal components and the associated rotation matrix provide estimates of \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\), respectively. In other words, PCA can be viewed as a special case of latent factor modeling where the latent factors are chosen to be orthogonal and ordered by the variance they explain. This explains why PCA naturally recovers interpretable patterns, such as genre preferences or actor-specific effects, without us explicitly coding them into the model.\nAnother way to see this connection is through optimization. PCA can be formulated as the solution to a least squares problem: among all possible \\(K\\)-dimensional projections of the data, PCA finds the one that minimizes the reconstruction error, that is, the sum of squared differences between the original data \\(\\mathbf{Y}\\) and its approximation \\(\\mathbf{P}\\mathbf{Q}^\\top\\). In this sense, PCA provides the best rank-\\(K\\) approximation of \\(\\mathbf{Y}\\) in the least squares sense.\nThis dual interpretation—both as a factor model and as a least squares optimizer—highlights why PCA is such a powerful tool for uncovering hidden structure in high-dimensional data: it compresses the data efficiently while also providing factors that capture the dominant sources of variation.\n\n\n\n\n\n\nEven with the orthogonality constraint, the solution is not completely unique.\nThere remains a sign indeterminacy: we can flip the sign of any column in \\(\\mathbf{P}\\) and the corresponding column in \\(\\mathbf{Q}\\) (multiply one by \\(-1\\) and the other by \\(-1\\)) without changing the product \\(\\mathbf{P}\\mathbf{Q}^\\top\\).\nThus, the factorization is identifiable only up to these sign changes.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Latent Factor Models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#case-study-movie-recommendations",
    "href": "highdim/latent-factor-models.html#case-study-movie-recommendations",
    "title": "25  Latent Factor Models",
    "section": "\n25.3 Case study: movie recommendations",
    "text": "25.3 Case study: movie recommendations\nIf we examine the correlation structure of the movies we simulated earlier, we again observe clear patterns:\n\n#&gt;                         Goodfellas Godfather: Part II, The\n#&gt; Goodfellas                  1.0000                   0.486\n#&gt; Godfather: Part II, The     0.4856                   1.000\n#&gt; You've Got Mail            -0.2657                  -0.246\n#&gt; Scent of a Woman            0.0789                   0.235\n#&gt; Sleepless in Seattle       -0.4302                  -0.474\n#&gt; Godfather, The              0.4977                   0.837\n#&gt;                         You've Got Mail Scent of a Woman\n#&gt; Goodfellas                       -0.266           0.0789\n#&gt; Godfather: Part II, The          -0.246           0.2348\n#&gt; You've Got Mail                   1.000          -0.3571\n#&gt; Scent of a Woman                 -0.357           1.0000\n#&gt; Sleepless in Seattle              0.415          -0.5178\n#&gt; Godfather, The                   -0.348           0.5479\n#&gt;                         Sleepless in Seattle Godfather, The\n#&gt; Goodfellas                            -0.430          0.498\n#&gt; Godfather: Part II, The               -0.474          0.837\n#&gt; You've Got Mail                        0.415         -0.348\n#&gt; Scent of a Woman                      -0.518          0.548\n#&gt; Sleepless in Seattle                   1.000         -0.441\n#&gt; Godfather, The                        -0.441          1.000\n\nThis suggests that we can improve the predictions from Chapter 24 by leveraging these correlations. For instance, ratings for The Godfather should inform ratings for The Godfather II. But what other patterns might help?\nTo account for interaction such as these we use a latent factor model. Specifically, we extend the model from Chapter 24 to include factors that capture similarities between movies:\n\\[\nY_{ij} = \\mu + \\alpha_i + \\beta_j + \\sum_{k=1}^K p_{ik}q_{jk} + \\varepsilon_{ij}\n\\]\nRecall that, as explained in the previous chapter, we do not observe all \\((i,j)\\) combinations. Writing the data as an \\(I \\times J\\) matrix \\(\\mathbf{Y}\\), with \\(I\\) users and \\(J\\) movies, would produce many missing values.\nHowever, the sum \\(\\sum_{k=1}^K p_{ik}q_{jk}\\) can be expressed as the matrix product \\(\\mathbf{P}\\mathbf{Q}^\\top\\), where \\(\\mathbf{P}\\) is an \\(I \\times K\\) matrix and \\(\\mathbf{Q}\\) is a \\(J \\times K\\) matrix. Estimating all parameters fills in every cell of the \\(I \\times J\\) matrix, giving predictions for all \\((i,j)\\) pairs.\nGiven the large number of parameters and the sparsity of the data, especially for movies with few ratings, it is appropriate to use penalized least squares. We minimize:\n\\[\n\\frac{1}{N}\n\\sum_{i,j} \\left[Y_{ij} - \\left(\\mu + \\alpha_i + \\beta_j + \\sum_{k=1}^K p_{ik}q_{jk}\\right)\\right]^2 +\n\\lambda_1 \\left(\n\\|\\boldsymbol{\\alpha}\\|^2 +\n\\|\\boldsymbol{\\beta}\\|^2\n\\right) +\n\\lambda_2 \\left(\n\\sum_{k=1}^K \\|\\mathbf{p}_k\\|^2+\n\\sum_{k=1}^K \\|\\mathbf{q}_k\\|^2\n\\right)\n\\]\nHere, \\(N\\) is the number of observed ratings, \\(I\\) the number of users, and \\(J\\) the number of movies. The vectors \\(\\boldsymbol{\\alpha}\\) and \\(\\boldsymbol{\\beta}\\) are the user and movie effects, \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_I)^\\top\\) and \\(\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_J)^\\top\\). The vectors \\(\\mathbf{p}_k = (p_{1,k}, \\dots, p_{ik})^\\top\\) and \\(\\mathbf{q}_k = (q_{1,k}, \\dots, q_{jk})^\\top\\) are the \\(k\\)-th latent factor components. Recall that \\(|\\boldsymbol{\\alpha}|^2\\) denotes the sum of squares, \\(\\sum_{i=1}^I \\alpha_i^2\\).\nWe use two penalties: \\(\\lambda_1\\) for the linear effects (\\(\\alpha\\)s and \\(\\beta\\)s), and \\(\\lambda_2\\) for the latent factors. This lets us regularize the two components differently, reflecting their distinct roles in the model.\nHow do we estimate the parameters in this model? As mentioned, the challenge comes from the fact that both the \\(p\\)s and \\(q\\)s are unknown and appear multiplied together, making the model nonlinear in its parameters. In earlier sections, we highlighted the connection between factor analysis and Principal Component Analysis (PCA), and showed that in settings with complete data, PCA can be used to estimate the latent factors of a factor analysis model.\nHowever, recommendation systems present a critical complication: the rating matrix is extremely sparse. Most users only rate a small subset of movies, so the data matrix has many missing entries. PCA and related approaches require a fully observed matrix, so they cannot be applied directly in this context. In addition, while PCA provides least squares estimates, here we want to use penalized least squares, which allows us to regularize the parameters and avoid overfitting when the data for some users or movies are limited.\nTo address these challenges, we turn to an iterative optimization method called Alternating Least Squares (ALS). The key idea is to alternately estimate \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\): fix one matrix and solve for the other using penalized least squares, then switch. This alternating approach also extends easily to penalized versions of the model, where user/movie effects and latent factors are regularized separately. For these reasons, ALS has become one of the standard approaches for fitting latent factor models in recommendation systems, where sparsity makes direct application of PCA infeasible.\nThe dslabs package provides the fit_recommender_model function, which implements this approach which we use here:\n\nfit &lt;- with(train_set, fit_recommender_model(rating, userId, movieId, min_ratings = 20, reltol = 1e-5))\n\nWe can now build a prediction for the test set based on the estimated parameters and note that it improves prediction:\n\nresid  &lt;- with(test_set, rating - \n                 clamp(fit$mu + fit$a[as.character(userId)] + fit$b[as.character(movieId)] + \n                 rowSums(fit$p[as.character(userId),] * fit$q[as.character(movieId),])))\nrmse(resid)\n#&gt; [1] 0.86\n\n\n\n\n\n\n\nNote that further gains are possible by optimizing the penalty terms and tuning the number of latent factors. The team from BellKor, who ultimately won the Netflix Prize, had another key insight that further improved prediction: they accounted for the information hidden in the missing ratings. In practice, users tend to avoid movies they expect to dislike, so the absence of a rating is not random but itself carries signal. Incorporating this additional structure into the model helped them achieve state-of-the-art performance.\n\n\n\nVisualizing factors\nExamining the estimated movie factors \\(\\mathbf{q}_k\\) reveals that they are interpretable.\nThe estimates for these factors are contained in fit$q. Because some movies do not have enough ratings to stably estimate latent factors, the fit_recommender_model function excludes these movies from the estimation. We therefore obtain the estimated movie factors \\(\\mathbf{q}_k\\) using:\n\nfactors &lt;- fit$q[fit$n_item &gt;= fit$min_ratings,] \n\nTo make interpretation easier, we replace the row names (which are movie IDs) with movie titles:\n\nrownames(factors) &lt;- movielens[match(rownames(factors), movieId)]$title\n\nPlotting the first two factors reveals several insights. As shown below, the movies highlighted in blue demonstrate that mob films cluster together, while romance films without Al Pacino also cluster together:\n\n\n\n\n\n\n\n\nLooking at the movies with the most extreme factor values confirms that the dimensions are interpretable. The first factor separates critically acclaimed films:\n\nnames(sort(factors[,1], decreasing = TRUE)[1:5])\n#&gt; [1] \"2001: A Space Odyssey\"                                               \n#&gt; [2] \"Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb\"\n#&gt; [3] \"Clockwork Orange, A\"                                                 \n#&gt; [4] \"Taxi Driver\"                                                         \n#&gt; [5] \"Silence of the Lambs, The\"\n\nfrom Hollywood blockbusters:\n\nnames(sort(factors[,1])[1:5])\n#&gt; [1] \"Armageddon\"                               \n#&gt; [2] \"Pearl Harbor\"                             \n#&gt; [3] \"Mission: Impossible II\"                   \n#&gt; [4] \"Con Air\"                                  \n#&gt; [5] \"Star Wars: Episode I - The Phantom Menace\"\n\nThe second factor contrasts cult or offbeat films:\n\nnames(sort(factors[,2], decreasing = TRUE)[1:5])\n#&gt; [1] \"Showgirls\"           \"Harold and Maude\"    \"Leaving Las Vegas\"  \n#&gt; [4] \"Bone Collector, The\" \"Scary Movie\"\n\nwith epics:\n\nnames(sort(factors[,2])[1:5])\n#&gt; [1] \"Lord of the Rings: The Two Towers, The\"            \n#&gt; [2] \"Lord of the Rings: The Return of the King, The\"    \n#&gt; [3] \"Lord of the Rings: The Fellowship of the Ring, The\"\n#&gt; [4] \"Dances with Wolves\"                                \n#&gt; [5] \"Matrix, The\"\n\nThese results demonstrate that the latent factors capture meaningful distinctions in film genres and styles, showing how matrix factorization can uncover interpretable structure from sparse rating data.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Latent Factor Models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#singular-value-decomposition",
    "href": "highdim/latent-factor-models.html#singular-value-decomposition",
    "title": "25  Latent Factor Models",
    "section": "\n25.4 Singular Value Decomposition",
    "text": "25.4 Singular Value Decomposition\nA related technique often used in latent factor analysis is the Singular Value Decomposition (SVD). It states any \\(N \\times p\\) matrix can be written:\n\\[\n\\mathbf{Y} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^\\top\n\\]\nwhere \\(\\mathbf{U}\\) is an orthogonal \\(N \\times p\\) matrix, \\(\\mathbf{V}\\) an orthogonal \\(p \\times p\\) matrix, and \\(\\mathbf{D}\\) diagonal with \\(d_{1,1} \\geq d_{2,2} \\geq \\dots \\geq d_{p,p}\\). SVD is connected to PCA because \\(\\mathbf{V}\\) provides the rotations for the principal components, while \\(\\mathbf{U}\\mathbf{D}\\) are the principal components themselves. Squaring the diagonal entries of \\(\\mathbf{D}\\) gives the sums of squares:\n\\[\n\\mathbf{U}^\\top \\mathbf{D} \\mathbf{U} = \\mathbf{D}^2\n\\]\nIn R, we can compute the SVD with svd and confirm its relationship to PCA:\n\nx &lt;- matrix(rnorm(1000), 100, 10)\npca &lt;- prcomp(x, center = FALSE)\ns &lt;- svd(x)\n\nall.equal(pca$rotation, s$v, check.attributes = FALSE)\n#&gt; [1] TRUE\nall.equal(pca$sdev^2, s$d^2/(nrow(x) - 1))\n#&gt; [1] TRUE\nall.equal(pca$x, s$u %*% diag(s$d), check.attributes = FALSE)\n#&gt; [1] TRUE\n\nAs an optimization, note that s$u %*% diag(s$d) can be written more efficiently as:\n\nsweep(s$u, 2, s$d, \"*\")",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Latent Factor Models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#exercises",
    "href": "highdim/latent-factor-models.html#exercises",
    "title": "25  Latent Factor Models",
    "section": "\n25.5 Exercises",
    "text": "25.5 Exercises\nIn this exercise set, we use the singular value decomposition (SVD) to estimate factors in an example related to the first application of factor analysis: finding factors related to student performance in school.\nWe construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage points each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:\n\nset.seed(1987)\nn &lt;- 100\nk &lt;- 8\nSigma &lt;- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) \nm &lt;- MASS::mvrnorm(n, rep(0, 3), Sigma)\nm &lt;- m[order(rowMeans(m), decreasing = TRUE),]\ny &lt;- m %x% matrix(rep(1, k), nrow = 1) +\n  matrix(rnorm(matrix(n * k * 3)), n, k * 3)\ncolnames(y) &lt;- c(paste(rep(\"Math\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Science\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Arts\",k), 1:k, sep=\"_\"))\n\nOur goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all simply random independent numbers. Are all students just about as good? Does being good in one subject imply one will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors, we can explain much of the variability in this \\(100 \\times 24\\) dataset.\nYou can visualize the 24 test scores for the 100 students by plotting an image:\n\nmy_image &lt;- function(x, zlim = range(x), ...){\n  colors = rev(RColorBrewer::brewer.pal(9, \"RdBu\"))\n  cols &lt;- 1:ncol(x)\n  rows &lt;- 1:nrow(x)\n  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = \"n\", yaxt = \"n\",\n        xlab=\"\", ylab=\"\",  col = colors, zlim = zlim, ...)\n  abline(h=rows + 0.5, v = cols + 0.5)\n  axis(side = 1, cols, colnames(x), las = 2)\n}\n\nmy_image(y)\n\n1. How would you describe the data based on this figure?\n\nThe test scores are all independent of each other.\nThe students that test well are at the top of the image and there seems to be three groupings by subject.\nThe students that are good at math are not good at science.\nThe students that are good at math are not good at humanities.\n\n2. You can examine the correlation between the test scores directly like this:\n\nmy_image(cor(y), zlim = c(-1,1))\nrange(cor(y))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWhich of the following best describes what you see?\n\nThe test scores are independent.\nMath and science are highly correlated, but the humanities are not.\nThere is high correlation between tests in the same subject, but no correlation across subjects.\nThere is a correlation among all tests, but higher if the tests are in science and math and even higher within each subject.\n\n3. Remember that orthogonality means that \\(U^{\\top}U\\) and \\(V^{\\top}V\\) are equal to the identity matrix. This implies that we can also rewrite the decomposition as:\n\\[ \\mathbf{Y V} = \\mathbf{U D} \\mbox{ or } \\mathbf{U}^{\\top}\\mathbf{Y} = \\mathbf{D V}^{\\top}\\]\nWe can think of \\(\\mathbf{YV}\\) and \\(\\mathbf{U}^{\\top}\\mathbf{V}\\) as two transformations of \\(\\mathbf{Y}\\) that preserve the total variability.\nUse the function svd to compute the SVD of y. This function will return \\(U\\), \\(V\\) and the diagonal entries of \\(D\\).\n\ns &lt;- svd(y)\nnames(s)\n\nYou can check that the SVD works by typing:\n\ny_svd &lt;- sweep(s$u, 2, s$d, FUN = \"*\") %*% t(s$v)\nmax(abs(y - y_svd))\n\nCompute the sum of squares of the columns of \\(Y\\) and store them in ss_y. Then compute the sum of squares of columns of the transformed \\(\\mathbf{YV}\\) and store them in ss_yv. Confirm that sum(ss_y) is equal to sum(ss_yv).\n4. We see that the total sum of squares is preserved. This is because \\(\\mathbf{V}\\) is orthogonal. Now to start understanding how \\(\\mathbf{YV}\\) is useful, plot ss_y against the column number and then do the same for ss_yv. What do you observe?\n5. We see that the variability of the columns of \\(\\mathbf{YV}\\) is decreasing. Furthermore, we see that, relative to the first three, the variability of the columns beyond the third is almost 0. Now notice that we didn’t have to compute ss_yv because we already have the answer. How? Remember that \\(\\mathbf{YV} = \\mathbf{UD}\\) and because \\(\\mathbf{U}\\) is orthogonal, we know that the sum of squares of the columns of \\(\\mathbf{UD}\\) are the diagonal entries of \\(\\mathbf{D}\\) squared. Confirm this by plotting the square root of ss_yv versus the diagonal entries of \\(\\mathbf{D}\\).\n6. From the above we know that the sum of squares of the columns of \\(\\mathbf{Y}\\) (the total sum of squares) add up to the sum of s$d^2, and that the transformation \\(\\mathbf{YV}\\) gives us columns with sums of squares equal to s$d^2. Now compute what percent of the total variability is explained by just the first three columns of \\(\\mathbf{YV}\\).\n7. We see that almost 99% of the variability is explained by the first three columns of \\(\\mathbf{YV}  = \\mathbf{UD}\\). So we get the sense that we should be able to explain much of the variability and structure we found while exploring the data with a few columns. Before we continue, let’s show a useful computational trick to avoid creating the matrix diag(s$d). To motivate this, we note that if we write \\(\\mathbf{U}\\) out in its columns \\([\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_p]\\), then \\(\\mathbf{UD}\\) is equal to:\n\\[\n\\mathbf{UD} = [\\mathbf{u}_1 d_{1,1}, \\mathbf{u}_2 d_{2,2}, \\dots, \\mathbf{u}_p d_{p,p}]\n\\]\nUse the sweep function to compute \\(UD\\) without constructing diag(s$d) and without using matrix multiplication.\n8. We know that \\(\\mathbf{u}_1 d_{1,1}\\), the first column of \\(\\mathbf{UD}\\), has the most variability of all the columns of \\(\\mathbf{UD}\\). Earlier we saw an image of \\(Y\\):\n\nmy_image(y)\n\nin which we can see that the student to student variability is quite large and that it appears that students that are good in one subject are good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student and plot it against \\(\\mathbf{u}_1 d_{1,1}\\), and describe what you find.\n9. We note that the signs in SVD are arbitrary because:\n\\[ \\mathbf{U D V}^{\\top} = (-\\mathbf{U}) D (-\\mathbf{V})^{\\top} \\]\nWith this in mind, we see that the first column of \\(\\mathbf{UD}\\) is almost identical to the average score for each student except for the sign.\nThis implies that multiplying \\(\\mathbf{Y}\\) by the first column of \\(\\mathbf{V}\\) must be performing a similar operation to taking the average. Make an image plot of \\(\\mathbf{V}\\) and describe the first column relative to others and how this relates to taking an average.\n10. We already saw that we can rewrite \\(UD\\) as:\n\\[\n\\mathbf{u}_1 d_{1,1} + \\mathbf{u}_2 d_{2,2} + \\dots + \\mathbf{u}_p d_{p,p}\n\\]\nwith \\(\\mathbf{u}_j\\) the j-th column of \\(\\mathbf{U}\\). This implies that we can rewrite the entire SVD as:\n\\[\\mathbf{Y} = \\mathbf{u}_1 d_{1,1} \\mathbf{v}_1 ^{\\top} + \\mathbf{u}_2 d_{2,2} \\mathbf{v}_2 ^{\\top} + \\dots + \\mathbf{u}_p d_{p,p} \\mathbf{v}_p ^{\\top}\\]\nwith \\(\\mathbf{V}_j\\) the jth column of \\(\\mathbf{V}\\). Plot \\(\\mathbf{u}_1\\), then plot \\(\\mathbf{v}_1^{\\top}\\) using the same range for the y-axis limits. Then make an image of \\(\\mathbf{u}_1 d_{1,1} \\mathbf{v}_1 ^{\\top}\\) and compare it to the image of \\(\\mathbf{Y}\\). Hint: Use the my_image function defined above and use the drop=FALSE argument to assure the subsets of matrices are matrices.\n11. We see that with just a vector of length 100, a scalar, and a vector of length 24, we actually come close to reconstructing the original \\(100 \\times 24\\) matrix. This is our first matrix factorization:\n\\[\n\\mathbf{Y} \\approx d_{1,1} \\mathbf{u}_1 \\mathbf{v}_1^{\\top}\n\\] We know it explains s$d[1]^2/sum(s$d^2) * 100 percent of the total variability. Our approximation only explains the observation that good students tend to be good in all subjects. But another aspect of the original data that our approximation does not explain was the higher similarity we observed within subjects. We can see this by computing the difference between our approximation and original data and then computing the correlations. You can see this by running this code:\n\nresid &lt;- y - with(s,(u[,1, drop=FALSE]*d[1]) %*% t(v[,1, drop=FALSE]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nNow that we have removed the overall student effect, the correlation plot reveals that we have not yet explained the within subject correlation nor the fact that math and science are closer to each other than to the arts. So let’s explore the second column of the SVD. Repeat the previous exercise but for the second column: Plot \\(\\mathbf{u}_2\\), then plot \\(\\mathbf{v}_2^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(\\mathbf{u}_2 d_{2,2} \\mathbf{v}_2 ^{\\top}\\) and compare it to the image of resid.\n12. The second column clearly relates to a student’s difference in ability in math/science versus the arts. We can see this most clearly from the plot of s$v[,2]. Adding the matrix we obtain with these two columns will help with our approximation:\n\\[\n\\mathbf{Y} \\approx d_{1,1} \\mathbf{u}_1 \\mathbf{v}_1^{\\top} + d_{2,2} \\mathbf{u}_2 \\mathbf{v}_2^{\\top}\n\\]\nWe know it will explain:\n\nsum(s$d[1:2]^2)/sum(s$d^2) * 100\n\npercent of the total variability. We can compute new residuals like this:\n\nresid &lt;- y - with(s,sweep(u[,1:2], 2, d[1:2], FUN=\"*\") %*% t(v[,1:2]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nand see that the structure that is left is driven by the differences between math and science. Confirm this by plotting \\(\\mathbf{u}_3\\), then plot \\(\\mathbf{v}_3^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(\\mathbf{u}_3 d_{3,3} \\mathbf{v}_3 ^{\\top}\\) and compare it to the image of resid.\n13. The third column clearly relates to a student’s difference in ability in math and science. We can see this most clearly from the plot of s$v[,3]. Adding the matrix we obtain with these two columns will help with our approximation:\n\\[\n\\mathbf{Y} \\approx d_{1,1} \\mathbf{u}_1 \\mathbf{v}_1^{\\top} + d_{2,2} \\mathbf{u}_2 \\mathbf{v}_2^{\\top} + d_{3,3} \\mathbf{u}_3 \\mathbf{v}_3^{\\top}\n\\]\nWe know it will explain:\n\nsum(s$d[1:3]^2)/sum(s$d^2) * 100\n\npercent of the total variability. We can compute new residuals like this:\n\nresid &lt;- y - with(s,sweep(u[,1:3], 2, d[1:3], FUN=\"*\") %*% t(v[,1:3]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWe no longer see structure in the residuals: they seem to be independent of each other. This implies that we can describe the data with the following model:\n\\[\n\\mathbf{Y} =  \\mathbf{u}_1 \\mathbf{v}_1^{\\top} + d_{2,2} \\mathbf{u}_2 \\mathbf{v}_2^{\\top} + d_{3,3} \\mathbf{u}_3 \\mathbf{v}_3^{\\top} + \\varepsilon\n\\]\nwith \\(\\varepsilon\\) a matrix of independent identically distributed errors. This model is useful because we summarize \\(100 \\times 24\\) observations with \\(3 \\times (100+24+1) = 375\\) numbers. Furthermore, the three components of the model have useful interpretations: 1) the overall ability of a student, 2) the difference in ability between the math/sciences and arts, and 3) the remaining differences between the three subjects. The sizes \\(d_{1,1}, d_{2,2}\\) and \\(d_{3,3}\\) tell us the variability explained by each component. Finally, note that the components \\(d_{j,j} \\mathbf{u}_j \\mathbf{v}_j^{\\top}\\) are equivalent to the jth principal component.\nFinish the exercise by plotting an image of \\(Y\\), an image of \\(d_{1,1} \\mathbf{u}_1 \\mathbf{v}_1^{\\top} + d_{2,2} \\mathbf{u}_2 \\mathbf{v}_2^{\\top} + d_{3,3} \\mathbf{u}_3 \\mathbf{v}_3^{\\top}\\) and an image of the residuals, all with the same zlim.",
    "crumbs": [
      "High Dimensional Data",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Latent Factor Models</span>"
    ]
  },
  {
    "objectID": "highdim/reading-highdim.html",
    "href": "highdim/reading-highdim.html",
    "title": "Recommended reading",
    "section": "",
    "text": "Hefferon, J. (2017). Linear Algebra.\nA free and accessible open-source textbook designed for beginners, with clear explanations, worked examples, and plenty of exercises. Ideal for self-learners or students seeking to build both intuition and practical skills.\nAvailable online\nStrang, G. (2019). Linear Algebra and Learning from Data.\nFocuses on the role of linear algebra in modern machine learning and data science. A practical companion to statistical and algorithmic methods.\nSeber, G. A. F. (2003). Linear Regression Analysis (2nd ed.).\nA concise and mathematically rigorous exploration of regression, grounded in matrix algebra. Offers expanded coverage of diagnostics, model fitting, selection, and prediction.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning (2nd ed.).\nA highly accessible introduction to statistical learning, including clear explanations and examples of regularization techniques such as ridge regression and the lasso. Available online for free.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.).\nThe authoritative reference on regularization and other machine learning techniques. More theoretical and in-depth, suited for readers looking for a deeper mathematical understanding.\nKoren, Y., Bell, R., & Volinsky, C. (2009). The BellKor Solution to the Netflix Prize. A technical yet accessible explanation of the ensemble of models that won the Netflix Prize, detailing matrix factorization, neighborhood models, and hybrid ensemble techniques.\nAvailable online",
    "crumbs": [
      "High Dimensional Data",
      "Recommended reading"
    ]
  },
  {
    "objectID": "ml/intro-ml.html",
    "href": "ml/intro-ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine learning has achieved remarkable successes in a variety of applications. These range from the postal service’s use of machine learning for reading handwritten zip codes to the development of voice recognition systems like Apple’s Siri. Other significant advances include spam and malware detection, housing price prediction algorithms, and the ongoing development of autonomous vehicles.\nThe field of Artificial Intelligence (AI) has been evolving for several decades. Traditional AI systems, including some chess-playing machines, often relied on decision-making based on preset rules and knowledge representation. However, with the advent of data availability, machine learning has gained prominence. It focuses on decision-making through algorithms trained with data. In recent years, the terms AI and Machine Learning have been used interchangeably in many contexts, though they have distinct meanings. AI broadly refers to systems or applications that exhibit intelligent behavior, encompassing both rule-based approaches and machine learning. Machine Learning specifically involves learning from data to make decisions or predictions.\nIn this part of the book, we will delve into the concepts, ideas, and methodologies of machine learning. We will also demonstrate their practical application, using the example of recognizing handwritten digits, a classic problem that exemplifies the power and utility of machine learning techniques.",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml/notation-and-terminology.html",
    "href": "ml/notation-and-terminology.html",
    "title": "\n26  Notation and Terminology\n",
    "section": "",
    "text": "26.1 Terminology\nIn Section 21.2, we introduced the MNIST handwritten digits dataset. Here we describe how the task of automatically reading these digits can be framed as a machine learning challenge. In doing so, we introduce machine learning mathematical notation and terminology used throughout this part of the book.\nOriginally, mail sorting in the post office involved humans reading zip codes written on the envelopes. Today, thanks to machine learning algorithms, a computer can read zip codes and then a robot sorts the letters. We will learn how to build algorithms that can read a digitized handwritten digit.\nIn machine learning, data comes in the form of the outcome we want to predict and the features that we will use to predict the outcome. We build algorithms that take feature values as input and returns a prediction for the outcome when we don’t know the outcome. The machine learning approach is to train an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don’t know the outcome.\nPrediction problems can be divided into categorical and continuous outcomes. For categorical outcomes can be any one of \\(K\\) classes. The number of classes can vary greatly across applications. For example, in the digit reader data, \\(K=10\\) with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In speech recognition, the outcomes are all possible words or phrases we are trying to detect. Spam detection has two outcomes: spam or not spam. In this book, we denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\). However, for binary data we will use \\(k=0,1\\) for mathematical conveniences that we demonstrate later.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Notation and Terminology</span>"
    ]
  },
  {
    "objectID": "ml/notation-and-terminology.html#notation",
    "href": "ml/notation-and-terminology.html#notation",
    "title": "\n26  Notation and Terminology\n",
    "section": "\n26.2 Notation",
    "text": "26.2 Notation\nWe will use \\(Y\\) to denote the outcome and \\(X_1, \\dots, X_p\\) to denote the features. These features are also sometimes referred to as predictors or covariates, and we will treat these terms as synonyms.\nThe first step in building a machine learning algorithm is to clearly identify what the outcomes and features are. In Section 21.2, we showed that each digitized image \\(i\\) is associated with a categorical outcome \\(Y_i\\) and a set of features \\(X_{i,1}, \\dots, X_{i,p}\\), with \\(p=784\\). For convenience, we often use boldface notation \\(\\mathbf{X}_i = (X_{i,1}, \\dots, X_{i,p})^\\top\\) to represent the vector of predictors, following the notation introduced in Section 21.1. When referring to an arbitrary set of features rather than a specific image, we drop the index \\(i\\) and simply use \\(Y\\) and \\(\\mathbf{X} = (X_1, \\dots, X_p)\\). We use uppercase to emphasize that these are random variables. Observed values are denoted in lowercase, such as \\(\\mathbf{X} = \\mathbf{x}\\). In practice, when writing code, we typically use lowercase.\nThe machine learning task is to build an algorithm that predicts the outcome given any combination of features. At first glance, this might seem impossible, but we will start with very simple examples and gradually build toward more complex cases. We begin with one predictor, then extend to two predictors, and eventually tackle real-world challenges involving thousands of predictors.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Notation and Terminology</span>"
    ]
  },
  {
    "objectID": "ml/notation-and-terminology.html#the-machine-learning-challenge",
    "href": "ml/notation-and-terminology.html#the-machine-learning-challenge",
    "title": "\n26  Notation and Terminology\n",
    "section": "\n26.3 The machine learning challenge",
    "text": "26.3 The machine learning challenge\nThe general setup is as follows. We have a series of features and an unknown outcome we want to predict:\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature p\n\n\n?\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(\\dots\\)\n\\(X_p\\)\n\n\n\n\nTo build a model that predicts outcomes from observed features \\(X_1=x_1, X_2=x_2, \\dots, X_p=x_p\\), we need a dataset where the outcomes are known:\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature 5\n\n\n\n\\(y_{1}\\)\n\\(x_{1,1}\\)\n\\(x_{1,2}\\)\n\\(x_{1,3}\\)\n\\(\\dots\\)\n\\(x_{1,p}\\)\n\n\n\\(y_{2}\\)\n\\(x_{2,1}\\)\n\\(x_{2,2}\\)\n\\(x_{2,3}\\)\n\\(\\dots\\)\n\\(x_{2,p}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\\(y_n\\)\n\\(x_{n,1}\\)\n\\(x_{n,2}\\)\n\\(x_{n,3}\\)\n\\(\\dots\\)\n\\(x_{n,p}\\)\n\n\n\n\n\nWhen the outcome is continuous, we refer to the task as prediction. The model returns a function \\(f\\) that produces a prediction \\(\\hat{y} = f(x_1, x_2, \\dots, x_p)\\) for any feature vector. We call \\(y\\) the actual outcome. Predictions \\(\\hat{y}\\) are rarely exact, so we measure accuracy by the error, defined as \\(y - \\hat{y}\\).\nWhen the outcome is categorical, the task is called classification. The model produces a decision rule that prescribes which of the \\(K\\) classes should be predicted. Typically, models output functions \\(f_k(x_1, \\dots, x_p)\\), one for each class \\(k\\). In the binary case, a common rule is: if \\(f_1(x_1, \\dots, x_p) &gt; C\\), predict class 1, otherwise predict class 2, with \\(C\\) a chosen cutoff. Here, predictions are either right or wrong.\nIt is worth noting that terminology varies across textbooks and courses. Sometimes prediction is used for both categorical and continuous outcomes. The term regression is also used for continuous outcomes, but here we avoid it to prevent confusion with linear regression. In most contexts, whether outcomes are categorical or continuous will be clear, so we will simply use prediction or classification as appropriate.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Notation and Terminology</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html",
    "href": "ml/evaluation-metrics.html",
    "title": "27  Evaluation Metrics",
    "section": "",
    "text": "27.1 Training and test sets\nBefore we start describing approaches to optimize the way we build algorithms, we first need to define what we mean when we say one approach is better than another. In this section, we focus on describing ways in which machine learning algorithms are evaluated. Specifically, we need to quantify what we mean by “better”.\nFor our first introduction to machine learning concepts, we will start with a boring and simple example: how to predict sex using height. As we explain how to build a prediction algorithm with this example, we will start to set down the first building block needed to understand machine learning. Soon enough, we will be undertaking more interesting challenges.\nWe introduce the caret package, which provides useful functions to facilitate machine learning in R, and we describe it in more detail in Section 33.1. For our first example, we use the height data provided by the dslabs package.\nWe start by defining the outcome and predictors:\nIn this case, we have only one predictor, height, and y is clearly a categorical outcome since observed values are either Male or Female. We know that we will not be able to predict \\(Y\\) very accurately based on \\(X\\) because male and female average heights are not that different relative to within group variability. But can we do better than guessing? To answer this question, we need a quantitative definition of “better”.\nUltimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it. We refer to the group for which we know the outcome, and that we use to develop the algorithm, as the training set. We refer to the group for which we pretend we don’t know the outcome as the test set.\nA standard way of generating the training and test sets is by randomly splitting the data. The caret package includes the function createDataPartition that helps us generate indexes for randomly splitting the data into training and test sets:\nset.seed(2007)\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\nThe argument times is used to define how many random samples of indexes to return, the argument p is used to define what proportion of the data is represented by the index, and the argument list is used to decide if we want the indexes returned as a list or not. We can use the result of the createDataPartition function call to define the training and test sets as follows:\ntest_set &lt;- heights[test_index, ]\ntrain_set &lt;- heights[-test_index, ]\nWe will now develop an algorithm using only the training set. Once we are done developing the algorithm, we will freeze it and evaluate it using the test set. The simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted in the test set. This metric is usually referred to as overall accuracy.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#overall-accuracy",
    "href": "ml/evaluation-metrics.html#overall-accuracy",
    "title": "27  Evaluation Metrics",
    "section": "\n27.2 Overall accuracy",
    "text": "27.2 Overall accuracy\nTo demonstrate the use of overall accuracy, we will build two competing algorithms and compare them.\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\n\nNote that we are completely ignoring the predictor and simply guessing the sex.\nIn machine learning applications, it is useful to use factors to represent the categorical outcomes because R functions developed for machine learning, such as those in the caret package, require or recommend that categorical outcomes be coded as factors. So convert y_hat to factors using the factor function:\n\ny_hat &lt;- factor(y_hat, levels = levels(test_set$sex))\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\nmean(y_hat == test_set$sex)\n#&gt; [1] 0.516\n\nNot surprisingly, our accuracy is about 50%. We are guessing!\nCan we do better? Exploratory data analysis suggests we can because, on average, males are slightly taller than females:\n\nlibrary(tidyverse)\nheights |&gt; group_by(sex) |&gt; summarize(avg = mean(height), sd = sd(height))\n#&gt; # A tibble: 2 × 3\n#&gt;   sex      avg    sd\n#&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Female  64.9  3.76\n#&gt; 2 Male    69.3  3.61\n\nBut how do we make use of this insight? Let’s try another simple approach: predict Male if height is within two standard deviations from the average male.\n\ny_hat &lt;- factor(ifelse(x &gt; 62, \"Male\", \"Female\"), levels(test_set$sex))\n\nThe accuracy goes up from 0.50 to about 0.80:\n\nmean(y == y_hat)\n#&gt; [1] 0.793\n\nBut can we do even better? In the example above, we used a cutoff of 62, but we can examine the accuracy obtained for other cutoffs and then pick the value that provides the best results. But remember, it is important that we optimize the cutoff using only the training set: the test set is only for evaluation. Although for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to overfitting, which often results in over-optimistic assessments.\nHere we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:\n\ncutoff &lt;- seq(61, 70)\naccuracy &lt;- sapply(cutoff, function(x){\n  y_hat &lt;- factor(ifelse(train_set$height &gt; x, \"Male\", \"Female\"), levels = levels(test_set$sex))\n  mean(y_hat == train_set$sex)\n})\n\nWe can make a plot showing the accuracy obtained on the training set for males and females:\n\n\n\n\n\n\n\n\nWe see that the maximum value is:\n\nmax(accuracy)\n#&gt; [1] 0.85\n\nwhich is much higher than 0.5. The cutoff resulting in this accuracy is:\n\nbest_cutoff &lt;- cutoff[which.max(accuracy)]\nbest_cutoff\n#&gt; [1] 64\n\nWe can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:\n\ny_hat &lt;- factor(ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\"), levels = levels(test_set$sex))\ny_hat &lt;- factor(y_hat)\nmean(y_hat == test_set$sex)\n#&gt; [1] 0.804\n\nWe see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing. And by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#the-confusion-matrix",
    "href": "ml/evaluation-metrics.html#the-confusion-matrix",
    "title": "27  Evaluation Metrics",
    "section": "\n27.3 The confusion matrix",
    "text": "27.3 The confusion matrix\nThe prediction rule we developed in the previous section predicts Male if the student is taller than 64 inches. Given that the average female is about 64 inches, this prediction rule seems wrong. What happened? If a student is the height of the average female, shouldn’t we predict Female?\nGenerally speaking, overall accuracy can be a deceptive measure. To see this, we will start by constructing what is referred to as the confusion matrix, which basically tabulates each combination of prediction and actual value. We can do this in R simply using table(predicted = y_hat, actual = test_set$sex), but the confusionMatrix function in the caret package computes the confusion matrix and much more:\n\ncm &lt;- confusionMatrix(data = y_hat, reference = test_set$sex)\ncm$table\n#&gt;           Reference\n#&gt; Prediction Female Male\n#&gt;     Female     48   32\n#&gt;     Male       71  374\n\nIf we study this table closely, it reveals a problem. If we compute the accuracy separately for each sex, we get (in the next section, we explain that, in this contxt, sensitivity and specificity a are equivalent to accuracy for females and males, respectively):\n\ncm$byClass[c(\"Sensitivity\", \"Specificity\")]\n#&gt; Sensitivity Specificity \n#&gt;       0.403       0.921\n\nWe notice an imbalance: too many females are predicted to be male. We are calling almost half of the females male! How can our overall accuracy be so high then? This is because the prevalence of males in this dataset is high. These heights were collected from three data sciences courses, two of which had higher male enrollment:\n\ncm$byClass[\"Prevalence\"]\n#&gt; Prevalence \n#&gt;      0.227\n\nSo when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. This type of bias can actually be a big problem in practice. If your training data is biased in some way, you are likely to develop algorithms that are biased as well. The fact that we used a test set does not matter because it is also derived from the original biased dataset. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.\nThere are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix. A general improvement to using overall accuracy is to study sensitivity and specificity separately.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#sec-senistivity-and-specificity",
    "href": "ml/evaluation-metrics.html#sec-senistivity-and-specificity",
    "title": "27  Evaluation Metrics",
    "section": "\n27.4 Sensitivity and specificity",
    "text": "27.4 Sensitivity and specificity\nTo define sensitivity and specificity, we need a binary outcome. When outcomes are categorical, we can still use these terms by focusing on one specific category of interest. For example, in a digit recognition task, we might ask: What is the specificity of correctly identifying the digit 2 as opposed to any other digit? Once we choose a specific category, we treat observations in that category as positive cases (\\(Y=1\\)) and all others as negative cases (\\(Y=0\\)). This binary framing allows us to compute sensitivity and specificity in the usual way.\nIn general, sensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: \\(\\hat{Y}=1\\) when \\(Y=1\\). Because an algorithm that calls everything positive (\\(\\hat{Y}=1\\) no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm. For this reason, we also examine specificity, which is generally defined as the ability of an algorithm to predict a negative \\(\\hat{Y}=0\\) when the actual outcome is a negative \\(Y=0\\). We can summarize in the following way:\n\nHigh sensitivity: \\(Y=1 \\implies \\hat{Y}=1\\)\n\nHigh specificity: \\(Y=0 \\implies \\hat{Y} = 0\\)\n\n\nAlthough the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive:\n\nHigh specificity: \\(\\hat{Y}=1 \\implies Y=1\\).\n\nTo provide precise definitions, we name the four entries of the confusion matrix:\n\n\n\n\n\nActually Positive\nActually Negative\n\n\n\nPredicted positive\nTrue positives (TP)\nFalse positives (FP)\n\n\nPredicted negative\nFalse negatives (FN)\nTrue negatives (TN)\n\n\n\n\n\nSensitivity is typically quantified by \\(TP/(TP+FN)\\), the proportion of actual positives (the first column = \\(TP+FN\\)) that are called positives (\\(TP\\)). This quantity is referred to as the true positive rate (TPR) or recall.\nSpecificity is defined as \\(TN/(TN+FP)\\) or the proportion of negatives (the second column = \\(FP+TN\\)) that are called negatives (\\(TN\\)). This quantity is also called the true negative rate (TNR). There is another way of quantifying specificity which is \\(TP/(TP+FP)\\) or the proportion of outcomes called positives (the first row or \\(TP+FP\\)) that are actually positives (\\(TP\\)). This quantity is referred to as positive predictive value (PPV) and also as precision. Note that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing.\nThe multiple names can be confusing, so we include a table to help us remember the terms. The table includes a column that shows the definition if we think of the proportions as probabilities.\n\n\n\n\n\n\n\n\n\nMeasure of\nName 1\nName 2\nDefinition\nProbability representation\n\n\n\nsensitivity\nTPR\nRecall\n\\(\\frac{\\mbox{TP}}{\\mbox{TP} + \\mbox{FN}}\\)\n\\(\\mathrm{Pr}(\\hat{Y}=1 \\mid Y=1)\\)\n\n\nspecificity\nTNR\n1-FPR\n\\(\\frac{\\mbox{TN}}{\\mbox{TN}+\\mbox{FP}}\\)\n\\(\\mathrm{Pr}(\\hat{Y}=0 \\mid Y=0)\\)\n\n\nspecificity\nPPV\nPrecision\n\\(\\frac{\\mbox{TP}}{\\mbox{TP}+\\mbox{FP}}\\)\n\\(\\mathrm{Pr}(Y=1 \\mid \\hat{Y}=1)\\)\n\n\n\nThe caret function confusionMatrix computes all these metrics for us once we define which category is the “positive” (\\(Y=1\\)). The function expects factors as input, and the first level is considered the positive outcome, though it can be redefined with the positive argument. In our example, Female is the first level because it comes before Male alphabetically. If you type this into R, you will see several metrics including accuracy, sensitivity, specificity, and PPV.\nYou can access these directly, for example, like this:\n\ncm$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.804\ncm$byClass[c(\"Sensitivity\",\"Specificity\", \"Prevalence\")]\n#&gt; Sensitivity Specificity  Prevalence \n#&gt;       0.403       0.921       0.227\n\nWe can see that the high overall accuracy is possible despite relatively low sensitivity. As we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low. Because prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the overall accuracy as much as failing to predict actual males as males (low specificity). This is an example of why it is important to examine sensitivity and specificity and not just accuracy. Before applying this algorithm to general datasets, we need to ask ourselves if prevalence in the general population will be the same as in our training dataset.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "href": "ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "title": "27  Evaluation Metrics",
    "section": "\n27.5 Balanced accuracy and \\(F_1\\) score",
    "text": "27.5 Balanced accuracy and \\(F_1\\) score\nAlthough we usually recommend studying both specificity and sensitivity, it is often useful to have a one-number summary, for example, for optimization purposes. One metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as balanced accuracy. Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average. In fact, the \\(F_1\\)-score, a widely used one-number summary, is the harmonic average of precision and recall:\n\\[\n\\frac{1}{\\frac{1}{2}\\left(\\frac{1}{\\mbox{recall}} +\n    \\frac{1}{\\mbox{precision}}\\right) }\n\\]\nBecause it is easier to write, you often see this harmonic average rewritten as:\n\\[\n2 \\times \\frac{\\mbox{precision} \\cdot \\mbox{recall}}\n{\\mbox{precision} + \\mbox{recall}}\n\\]\nwhen defining \\(F_1\\).\nDepending on the context, some types of errors are more costly than others. For instance, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition. In a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person. The \\(F_1\\)-score can be adapted to weigh specificity and sensitivity differently. To do this, we define \\(\\beta\\) to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:\n\\[\n\\frac{1}{\\frac{\\beta^2}{1+\\beta^2}\\frac{1}{\\mbox{recall}} +\n    \\frac{1}{1+\\beta^2}\\frac{1}{\\mbox{precision}} }\n\\]\nThe F_meas function in the caret package computes this summary with \\(\\beta\\) defaulting to 1.\nLet’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:\n\ncutoff &lt;- seq(61, 70)\nF_1 &lt;- sapply(cutoff, function(x){\n  y_hat &lt;- ifelse(train_set$height &gt; x, \"Male\", \"Female\")\n  F_meas(data = factor(y_hat, levels(test_set$sex)), \n         reference = factor(train_set$sex))\n})\n\nAs before, we can plot these \\(F_1\\) measures versus the cutoffs:\n\n\n\n\n\n\n\n\nWe see that it is maximized at \\(F_1\\) value of:\n\nmax(F_1)\n#&gt; [1] 0.647\n\nThis maximum is achieved when we use the following cutoff:\n\nbest_cutoff &lt;- cutoff[which.max(F_1)]\nbest_cutoff\n#&gt; [1] 66\n\nA cutoff of 66 makes more sense than 64 because it falls closer to the midpoint between the average heights of males and females. Additionally, it provides a better balance between sensitivity and specificity in the resulting confusion matrix.\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt; \n  factor(levels = levels(test_set$sex))\nsensitivity(data = y_hat, reference = test_set$sex)\n#&gt; [1] 0.63\nspecificity(data = y_hat, reference = test_set$sex)\n#&gt; [1] 0.833\n\nWe now see that we do much better than guessing, that both sensitivity and specificity are relatively high.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#prevalence-matters-in-practice",
    "href": "ml/evaluation-metrics.html#prevalence-matters-in-practice",
    "title": "27  Evaluation Metrics",
    "section": "\n27.6 Prevalence matters in practice",
    "text": "27.6 Prevalence matters in practice\nA machine learning algorithm with very high TPR and TNR may not be useful in practice when prevalence is close to either 0 or 1. To see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease.\nThe doctor shares data with about 1/2 cases and 1/2 controls and some predictors. You then develop an algorithm with TPR=0.99 and TNR = 0.99. You are excited to explain to the doctor that this means that if a patient has the disease, the algorithm is very likely to predict correctly. The doctor is not impressed and explains that your TNR is too low for this algorithm to be used in practice. This is because this is a rare disease with a prevalence in the general population of 0.5%. The doctor reminds you of Bayes formula:\n\\[\n\\begin{aligned}\n&\\mathrm{Pr}(Y = 1\\mid \\hat{Y}=1) = \\mathrm{Pr}(\\hat{Y}=1 \\mid Y=1) \\frac{\\mathrm{Pr}(Y=1)}{\\mathrm{Pr}(\\hat{Y}=1)} \\implies\\\\\n&\\text{Precision} = \\text{TPR} \\times \\frac{\\text{Prevalence}}{\\text{TPR}\\times \\text{Prevalence} + \\text{FPR}\\times(1-\\text{Prevalence})} \\approx 0.33  \n\\end{aligned}\n\\]\nHere is plot of precision as a function of prevalence with TPR and TNR both equal to 95%:\n\n\n\n\n\n\n\n\nAlthough your algorithm has a precision of about 95% on the data you train on, with prevalence of 50%, if applied to the general population, the algorithm’s precision would be just 33%. The doctor can’t use an algorithm with 33% of people receiving a positive test actually not having the disease. Note that even if your algorithm had perfect sensitivity, the precision would still be around 33%. So you need to greatly decrease your FPR for the algorithm to be useful in practice.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#roc-and-precision-recall-curves",
    "href": "ml/evaluation-metrics.html#roc-and-precision-recall-curves",
    "title": "27  Evaluation Metrics",
    "section": "\n27.7 ROC and precision-recall curves",
    "text": "27.7 ROC and precision-recall curves\nWhen comparing the two methods (guessing versus using a height cutoff), we looked at accuracy and \\(F_1\\). The second method clearly outperformed the first. However, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability. Be aware that guessing Male with higher probability would give us higher accuracy due to the bias in the sample. But, as described above, this would come at the cost of lower sensitivity. The curves we describe in this section will help us see this.\nRemember that for each of these parameters, we can get a different sensitivity and specificity. For this reason, a very common approach to evaluating methods is to compare them graphically by plotting both.\nA widely used plot that does this is the receiver operating characteristic (ROC) curve. If you are wondering about this name, consult the ROC Wikipedia page1.\nThe ROC curve plots sensitivity, represented as the TPR, versus 1 - specificity represented as the false positive rate (FPR). Here we compute the TPR and FPR needed for different probabilities of guessing male:\n\nprobs &lt;- seq(0, 1, length.out = 10)\nguessing &lt;- sapply(probs, function(p){\n  y_hat &lt;- \n    sample(c(\"Male\", \"Female\"), nrow(test_set), TRUE, c(p, 1 - p)) |&gt; \n    factor(levels = c(\"Female\", \"Male\"))\n  c(FPR = 1 - specificity(y_hat, test_set$sex),\n    TPR = sensitivity(y_hat, test_set$sex))\n})\n\nWe can use similar code to compute these values for our our second approach. By plotting both curves together, we are able to compare sensitivity for different values of specificity:\n\n\n\n\n\n\n\n\nWe see that we obtain higher sensitivity with the cutoff approach for all values of specificity, which implies it is in fact a better method than guessing. Keep in mind that ROC curves for guessing always fall on the identity line. Also, note that when making ROC curves, it is often nice to add the cutoff associated with each point.\nThe packages pROC and plotROC are useful for generating these plots.\nROC curves have one weakness and it is that neither of the measures plotted depends on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot. The idea is similar, but we instead plot precision against recall:\n\n\n\n\n\n\n\n\nFrom the plot on the left, we immediately see that the precision of guessing is not high. This is because the prevalence is low. From the plot on the right, we also see that if we change \\(Y=1\\) to mean Male instead of Female, the precision increases. Note that the ROC curve would remain the same.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#sec-mse",
    "href": "ml/evaluation-metrics.html#sec-mse",
    "title": "27  Evaluation Metrics",
    "section": "\n27.8 Mean Squared Error",
    "text": "27.8 Mean Squared Error\nUp to now we have described evaluation metrics that apply exclusively to categorical data. Specifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and \\(F_1\\) can be used as quantification. However, these metrics are not useful for continuous outcomes.\nIn this section, we describe how the general approach to defining “best” in machine learning is to define a loss function, which can be applied to both categorical and continuous data.\nThe most commonly used loss function is the squared loss function. If \\(\\hat{y}\\) is our predictor and \\(y\\) is the observed outcome, the squared loss function is simply: \\((\\hat{y} - y)^2\\).\nBecause we often model \\(y\\) as the outcome of a random process, theoretically, it does not make sense to compare algorithms based on \\((\\hat{y} - y)^2\\) as the minimum can change from sample to sample. For this reason, we minimize mean squared error (MSE):\n\\[\n\\text{MSE} \\equiv \\mathrm{E}\\{(\\hat{Y} - Y)^2 \\}\n\\]\nConsider that if the outcomes are binary, the MSE is equivalent to one minus expected accuracy, since \\((\\hat{y} - y)^2\\) is 0 if the prediction was correct and 1 otherwise.\nDifferent algorithms will result in different predictions \\(\\hat{Y}\\), and therefore different MSE. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.\nHowever, note that the mean squared error (MSE) is a theoretical quantity that depends on the unknown data-generating process. How do we estimate it in practice? Because we typically have a test set consisting of many independent observations \\(y_1, \\dots, y_N\\), a natural and widely used estimate of the MSE is based on the average of the squared errors over this test set:\n\\[\n\\hat{\\mbox{MSE}} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n\\]\nwith the \\(\\hat{y}_i\\) generated completely independently from the the \\(y_i\\).\nHowever, the estimate \\(\\hat{\\text{MSE}}\\) is a random variable. In fact, \\(\\text{MSE}\\) and \\(\\hat{\\text{MSE}}\\) are often referred to as the true error and apparent error, respectively. Due to the complexity of some machine learning, it is difficult to derive the statistical properties of how well the apparent error estimates the true error. In Chapter 30, we introduce cross-validation an approach to estimating the MSE.\nWe conclude this chapter by noting that squared loss is not the only possible choice of loss function. For instance, the mean absolute error (MAE) replaces squared errors \\((\\hat{Y}_i - Y_i)^2\\) with their absolute values \\(|\\hat{Y}_i - Y_i|\\). Other loss functions are also possible, depending on the context and goals of the analysis. In this book, however, we focus on squared loss, since it is by far the most widely used and provides important mathematical conveniences that simplify both theory and computation.\n\n\n\n\n\n\nIn practice, we often report the root mean squared error (RMSE), which is simply \\(\\sqrt{\\mbox{MSE}}\\), because it is in the same units as the outcomes.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#exercises",
    "href": "ml/evaluation-metrics.html#exercises",
    "title": "27  Evaluation Metrics",
    "section": "\n27.9 Exercises",
    "text": "27.9 Exercises\nThe reported_height and height datasets were collected from three classes taught in the Departments of Computer Science and Biostatistics, as well as remotely through the Extension School. The Biostatistics class was taught in 2016 along with an online version offered by the Extension School. On 2016-01-25 at 8:15 AM, during one of the lectures, the instructors asked students to fill in the sex and height questionnaire that populated the reported_height dataset. The online students filled the survey during the next few days, after the lecture was posted online. We can use this insight to define a variable, call it type, to denote the type of student: inclass or online:\n\nlibrary(lubridate)\ndat &lt;- mutate(reported_heights, date_time = ymd_hms(time_stamp)) |&gt;\n  filter(date_time &gt;= make_date(2016, 01, 25) & \n           date_time &lt; make_date(2016, 02, 1)) |&gt;\n  mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & \n                         between(minute(date_time), 15, 30),\n                       \"inclass\", \"online\")) |&gt; select(sex, type)\nx &lt;- dat$type\ny &lt;- factor(dat$sex, c(\"Female\", \"Male\"))\n\n1. Show summary statistics that indicate that type is predictive of sex.\n2. Instead of using height to predict sex, use the type variable.\n3. Show the confusion matrix.\n4. Use the confusionMatrix function in the caret package to report accuracy.\n5. Now use the sensitivity and specificity functions to report specificity and sensitivity.\n6. What is the prevalence (% of females) in the dat dataset defined above?",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#footnotes",
    "href": "ml/evaluation-metrics.html#footnotes",
    "title": "27  Evaluation Metrics",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html",
    "href": "ml/conditionals.html",
    "title": "28  Conditional Probabilities and Expectations",
    "section": "",
    "text": "28.1 Conditional probabilities\nIn machine learning applications, we rarely can predict outcomes perfectly. For example, spam detectors often miss emails that are clearly spam, Siri often misunderstands the words we are saying, and sometimes your bank thinks your card was stolen when it was not. The most common reason for not being able to build perfect algorithms is that it is impossible. To see this, consider that most datasets will include groups of observations with the same exact observed values for all predictors, but with different outcomes.\nBecause our prediction rules are functions, equal inputs (the predictors) implies equal outputs (the predictions). Therefore, for a challenge in which the same predictors are associated with different outcomes across different individual observations, it is impossible to predict correctly for all these cases. We saw a simple example of this in the previous section: for any given height \\(x\\), you will have both males and females that are \\(x\\) inches tall.\nHowever, none of this means that we can’t build useful algorithms that are much better than guessing, and in some cases better than expert opinions. To achieve this in an optimal way, we make use of probabilistic representations of the problem based on the ideas presented in Section 15.3. Observations with the same observed values for the predictors may not all be the same, but we can assume that they all have the same probability of this class or that class. We will write this idea out mathematically for the case of categorical data.\nWe use the notation \\((X_1 = x_1,\\dots,X_p=x_p)\\) to represent the fact that we have observed values \\(x_1,\\dots,x_p\\) for covariates \\(X_1, \\dots, X_p\\). This does not imply that the outcome \\(Y\\) will take a specific value. Instead, it implies a specific probability. In particular, we denote the conditional probabilities for each class \\(k\\) with:\n\\[\n\\mathrm{Pr}(Y=k \\mid X_1 = x_1,\\dots,X_p=x_p), \\, \\mbox{for}\\,k=1,\\dots,K.\n\\]\nTo avoid writing out all the predictors, we will use bold letters like this: \\(\\mathbf{X} \\equiv (X_1,\\dots,X_p)^\\top\\) and \\(\\mathbf{x} \\equiv (x_1,\\dots,x_p)^\\top\\). We will also use the following notation for the conditional probability of being class \\(k\\):\n\\[\np_k(\\mathbf{x}) = \\mathrm{Pr}(Y=k \\mid \\mathbf{X}=\\mathbf{x}), \\, \\mbox{for}\\, k=1,\\dots,K.\n\\] Notice that the \\(p_k(\\mathbf{x})\\) have to add up to 1 for each \\(\\mathbf{x}\\), so once we know \\(K-1\\), we know all. When the outcome is binary, we only need to know 1, so we drop the \\(k\\) and use the notation \\(p(\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\).\nThese probabilities guide the construction of an algorithm that makes the best prediction: for any given \\(\\mathbf{x}\\), we will predict the class \\(k\\) with the largest probability among \\(p_1(\\mathbf{x}), p_2(\\mathbf{x}), \\dots p_K(\\mathbf{x})\\). In mathematical notation, we write it like this:\n\\[\\hat{y} = \\max_k p_k(\\mathbf{x})\\]\nIn machine learning, we refer to this as Bayes’ Rule. But this is a theoretical rule since, in practice, we don’t know \\(p_k(\\mathbf{x}), k=1,\\dots,K\\). In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. The better our probability estimates \\(\\hat{p}_k(\\mathbf{x})\\), the better our predictor.\nSo how well we predict depends on two things: 1) how close are the \\(\\max_k p_k(\\mathbf{x})\\) to 1 or 0 (perfect certainty) and 2) how close our estimates \\(\\hat{p}_k(\\mathbf{x})\\) are to \\(p_k(\\mathbf{x})\\). We can’t do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities.\nThe first restriction does imply that we have limits as to how well even the best possible algorithm can perform. You should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, with digit readers for example, in others, our success is restricted by the randomness of the process, such as medical diagnosis from biometric data.\nKeep in mind that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context. As discussed in Chapter 27, sensitivity and specificity may differ in importance. But even in these cases, having a good estimate of the \\(p_k(x), k=1,\\dots,K\\) will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. For instance, we can simply change the cutoffs used to predict one outcome or the other. In the plane example, we may ground the plane anytime the probability of malfunction is lower than 1 in a million as opposed to the default 1/2 used when error types are equally undesired.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Conditional Probabilities and Expectations</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html#conditional-probabilities",
    "href": "ml/conditionals.html#conditional-probabilities",
    "title": "28  Conditional Probabilities and Expectations",
    "section": "",
    "text": "Do not be confused by the fact that we use \\(p\\) for two different things: the conditional probability \\(p(\\mathbf{x})\\) and the number of predictors \\(p\\).",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Conditional Probabilities and Expectations</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html#conditional-expectations",
    "href": "ml/conditionals.html#conditional-expectations",
    "title": "28  Conditional Probabilities and Expectations",
    "section": "\n28.2 Conditional expectations",
    "text": "28.2 Conditional expectations\nFor binary data, you can think of the probability \\(\\mathrm{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\) as the proportion of 1s in the stratum of the population for which \\(\\mathbf{X}=\\mathbf{x}\\). Many of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between conditional probabilities and conditional expectations.\nBecause the expectation is the average of values \\(y_1,\\dots,y_n\\) in the population, in the case in which the \\(y\\)s are 0 or 1, the expectation is equivalent to the probability of randomly picking a one since the average is simply the proportion of ones:\n\\[\n\\mathrm{E}(Y \\mid \\mathbf{X}=\\mathbf{x})=\\mathrm{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}).\n\\]\nAs a result, we often only use the expectation to denote both the conditional probability and conditional expectation.\nJust like with categorical outcomes, in most applications the same observed predictors do not guarantee the same continuous outcomes. Instead, we assume that the outcome follows the same conditional distribution. We will now explain why we use the conditional expectation to define our predictors.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Conditional Probabilities and Expectations</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html#conditional-expectations-minimize-squared-loss-function",
    "href": "ml/conditionals.html#conditional-expectations-minimize-squared-loss-function",
    "title": "28  Conditional Probabilities and Expectations",
    "section": "\n28.3 Conditional expectations minimize squared loss function",
    "text": "28.3 Conditional expectations minimize squared loss function\nWhy do we care about the conditional expectation in machine learning? This is because the expected value has an attractive mathematical property: it minimizes the MSE. Specifically, of all possible predictions \\(\\hat{Y}\\),\n\\[\n\\hat{Y} = \\mathrm{E}(Y \\mid \\mathbf{X}=\\mathbf{x}) \\, \\mbox{ minimizes } \\, \\mathrm{E}\\{ (\\hat{Y} - Y)^2  \\mid  \\mathbf{X}=\\mathbf{x} \\}\n\\]\nDue to this property, a succinct description of the main task of machine learning is that we use data to estimate:\n\\[\nf(\\mathbf{x}) \\equiv \\mathrm{E}( Y  \\mid  \\mathbf{X}=\\mathbf{x} )\n\\]\nfor any set of features \\(\\mathbf{x} = (x_1, \\dots, x_p)^\\top\\).\nThis is easier said than done, since this function can take any shape and \\(p\\) can be very large. Consider a case in which we only have one predictor \\(x\\). The expectation \\(\\mathrm{E}(Y  \\mid  X=x )\\) can be any function of \\(x\\): a line, a parabola, a sine wave, a step function, anything. It gets even more complicated when we consider instances with large \\(p\\), in which case \\(f(\\mathbf{x})\\) is a function of a multidimensional vector \\(\\mathbf{x}\\). For example, in our digit reader example \\(p = 784\\)!\nThe main way in which competing machine learning algorithms differ is in their approach to estimating this conditional expectation.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Conditional Probabilities and Expectations</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html#exercises",
    "href": "ml/conditionals.html#exercises",
    "title": "28  Conditional Probabilities and Expectations",
    "section": "\n28.4 Exercises",
    "text": "28.4 Exercises\n1. Compute conditional probabilities for being Male for the heights dataset. Round the heights to the closest inch. Plot the estimated conditional probability \\(P(x) = \\mathrm{Pr}(\\mbox{Male} | \\mbox{height}=x)\\) for each \\(x\\).\n2. In the plot we just made, we see high variability for low values of height. This is because we have few data points in these strata. This time use the quantile function for quantiles \\(0.1,0.2,\\dots,0.9\\) and the cut function to assure each group has the same number of points. Hint: For any numeric vector x, you can create groups based on quantiles as we demonstrate below.\n\ncut(x, quantile(x, seq(0, 1, 0.1)), include.lowest = TRUE)\n\n3. Generate data from a bivariate normal distribution using the MASS package like this:\n\nSigma &lt;- 9*matrix(c(1,0.5,0.5,1), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 10000, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nYou can make a quick plot of the data using plot(dat). Use an approach similar to the previous exercise to estimate the conditional expectations and make a plot.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Conditional Probabilities and Expectations</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html",
    "href": "ml/smoothing.html",
    "title": "29  Smoothing",
    "section": "",
    "text": "29.1 Example: Is it a 2 or a 7?\nBefore continuing with machine learning algorithms, we introduce the important concept of smoothing. Smoothing is a widely used technique across data analysis, also known as curve fitting or low-pass filtering. Its purpose is to uncover underlying trends in noisy data when the exact shape of the trend is unknown. The key idea is to assume that the trend itself is smooth, like a gently curving surface, while the noise consists of unpredictable, irregular fluctuations. By leveraging this smoothness assumption, smoothing methods help us separate the systematic pattern from the random wobbles.\nPart of what we explain in this section are the assumptions that permit us to extract the trend from the noise.\nTo motivate the need for smoothing and make the connection with machine learning, we will construct a simplified version of the MNIST dataset with just two classes for the outcome and two predictors. Specifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the proportion of dark pixels in the upper left quadrant (\\(X_1\\)) and the lower right quadrant (\\(X_2\\)). We also selected a random sample of 1,000 digits, 800 in the training set and 200 in the test set. Both sets are almost evenly distributed with 2s and 7s. We provide this dataset in the mnist_27 object in the dslabs package. For the training data, we have \\(n=800\\) observed outcomes \\(y_1,\\dots,y_n\\), with \\(Y\\) indicating whether the digit is 7 or 2, and \\(n=800\\) features \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\), with each feature a two-dimensional point \\(\\mathbf{x}_i = (x_{i,1}, x_{i,2})^\\top\\).\nTo illustrate how to interpret \\(X_1\\) and \\(X_2\\), we include four example images. On the left are the original images of the two digits with the largest and smallest values for \\(X_1\\) and on the right we have the images corresponding to the largest and smallest values of \\(X_2\\):\nHere is a plot of the observed \\(X_2\\) versus observed \\(X_1\\) with color determining if \\(y\\) is 1 (blue) or 0 (red):\nlibrary(caret)\nlibrary(dslabs)\nmnist_27$train |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point()\nWe can immediately see some patterns. For example, if \\(x_1\\) (the upper left panel) is very large, then the digit is probably a 7. Also, for smaller values of \\(x_1\\), the 2s appear to be in the mid range values of \\(x_2\\).\nWe can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging.\nWe haven’t really learned any algorithms yet, so let’s try building an algorithm using multivariable regression. The model is simply:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) =\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nWe fit can fit this model using least squares and obtain an estimate \\(\\hat{p}(\\mathbf{x})\\) by using the least square estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\). We define a decision rule by predicting \\(\\hat{y}=1\\) if \\(\\hat{p}(\\mathbf{x})&gt;0.5\\) and 0 otherwise.\nWe get an accuracy of 0.775, well above 50%. Not bad for our first try. But can we do better?\nBecause we constructed the mnist_27 example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the true conditional distribution \\(p(\\mathbf{x})\\). Keep in mind that in practice we don’t have access to the true conditional distribution. We include it in this educational example because it permits the comparison of \\(\\hat{p}(\\mathbf{x})\\) to the true \\(p(\\mathbf{x})\\). This comparison teaches us the limitations of different algorithms.\nWe have stored the true \\(p(\\mathbf{x})\\) in the mnist_27 and can plot it as an image. We draw a curve that separates values of \\(\\mathbf{x}\\) for which \\(p(\\mathbf{x}) &gt; 0.5\\) and those for which \\(p(\\mathbf{x}) &lt; 0.5\\):\nTo start understanding the limitations of regression, first note that with regression \\(\\hat{p}(\\mathbf{x})\\), has to be a plane. As a result the boundary defined by the decision rule is given by: \\(\\hat{p}(\\mathbf{x}) = 0.5\\):\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5 \\implies\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5  \\implies\nx_2 = (0.5-\\hat{\\beta}_0)/\\hat{\\beta}_2  -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\]\nThis implies that for the boundary, \\(x_2\\) is a linear function of \\(x_1\\), which suggests that our regression approach has no chance of capturing the non-linear nature of the true \\(p(\\mathbf{x})\\). Below is a visual representation of \\(\\hat{p}(\\mathbf{x})\\) which clearly shows how it fails to capture the shape of \\(p(\\mathbf{x})\\):\nWe need something more flexible: a method that permits estimates with shapes other than a plane. Smoothing techniques permit this flexibility. We will start by describing nearest neighbor and kernel approaches. To understand why we cover this topic, remember that the concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#signal-plus-noise-model",
    "href": "ml/smoothing.html#signal-plus-noise-model",
    "title": "29  Smoothing",
    "section": "\n29.2 Signal plus noise model",
    "text": "29.2 Signal plus noise model\nTo explain these concepts, we will focus first on a problem with just one predictor. Specifically, we try to estimate the time trend in the 2008 US popular vote poll margin (the difference between Obama and McCain). Later we will learn how to extend smoothing ideas to higher dimensions.\n\npolls_2008 |&gt; ggplot(aes(day, margin)) + geom_point()\n\n\n\n\n\n\n\nFor the purposes of the popular vote example, do not think of it as a forecasting problem. Instead, we are simply interested in learning the shape of the trend after the election is over.\nWe assume that for any given day \\(x\\), there is a true preference among the electorate \\(f(x)\\), but due to the uncertainty introduced by the polling, each data point comes with an error \\(\\varepsilon\\). A mathematical model for the observed poll margin is:\n\\[\nY_i = f(x_i) + \\varepsilon_i\n\\]\nTo think of this as a machine learning problem, consider that we want to predict \\(Y\\) given a day \\(x\\). If we knew the conditional expectation \\(f(x) = \\mathrm{E}(Y \\mid X=x)\\), we would use it. But since we don’t know this conditional expectation, we have to estimate it. Let’s use regression, since it is the only method we have learned up to now.\n\n\n\n\n\n\n\n\nThe fitted regression line does not appear to describe the trend very well. For example, on September 4 (day -62), the Republican Convention was held and the data suggest that it gave John McCain a boost in the polls. However, the regression line does not capture this potential trend. To see the lack of fit more clearly, we note that points above the fitted line (blue) and those below (red) are not evenly distributed across days. We therefore need an alternative, more flexible approach.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#bin-smoothing",
    "href": "ml/smoothing.html#bin-smoothing",
    "title": "29  Smoothing",
    "section": "\n29.3 Bin smoothing",
    "text": "29.3 Bin smoothing\nThe general idea of smoothing is to group data points into strata in which the value of \\(f(x)\\) can be assumed to be constant. We can make this assumption when we think \\(f(x)\\) changes slowly and, as a result, \\(f(x)\\) is almost constant in small windows of \\(x\\). An example of this idea for the poll_2008 data is to assume that public opinion remained approximately the same within a week’s time. With this assumption in place, we have several data points with the same expected value.\nIf we fix a day to be in the center of our week, call it \\(x_0\\), then for any other day \\(x\\) such that \\(|x - x_0| \\leq h\\), with \\(h = 3.5\\), we assume \\(f(x)\\) is a constant \\(f(x) = \\mu\\). This assumption implies that:\n\\[\nE[Y_i | X_i = x_i ] \\approx \\mu \\mbox{   if   }  |x_i - x_0| \\leq 3.5\n\\]\nIn smoothing, we call the size of the interval satisfying \\(|x_i - x_0| \\leq 3.5\\) the window size, bandwidth or span. Later we will learn how to optimize this parameter.\nThis assumption implies that a good estimate for \\(f(x_0)\\) is the average of the \\(y_i\\) values in the window. If we define \\(A_0\\) as the set of indexes \\(i\\) such that \\(|x_i - x_0| \\leq 3.5\\) and \\(N_0\\) as the number of indexes in \\(A_0\\), then our estimate is:\n\\[\n\\hat{f}(x_0) = \\frac{1}{N_0} \\sum_{i \\in A_0}  y_i\n\\]\nWe make this calculation with each value of \\(x\\) as the center. In the poll example, for each day, we would compute the average of the values within a week with that day in the center. Here are two examples: \\(x_0 = -125\\) and \\(x_0 = -55\\). The blue segment represents the resulting average.\n\n\n\n\n\n\n\n\nBy computing this mean for every point, we form an estimate of the underlying curve \\(f(x)\\). Below we show the procedure happening as we move from the -155 up to 0. At each value of \\(x_0\\), we keep the estimate \\(\\hat{f}(x_0)\\) and move on to the next point:\n\n\n\n\n\n\n\n\nThe final code and resulting estimate look like this:\n\nspan &lt;- 7 \nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"box\", bandwidth = span))\n\npolls_2008 |&gt; mutate(fit = fit$y) |&gt;\n  ggplot(aes(x = day)) +\n  geom_point(aes(y = margin), size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(y = fit), color = \"red\")",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#kernels",
    "href": "ml/smoothing.html#kernels",
    "title": "29  Smoothing",
    "section": "\n29.4 Kernels",
    "text": "29.4 Kernels\nThe bin smoother’s estimate can look quite wiggly. A main reason is that as the window slides, points abruptly enter or leave the bin, causing jumps in the average. We can reduce these discontinuities with a kernel smoother. A kernel smoother assigns a weight to each data point according to its distance from the target location \\(x_0\\), then forms a weighted average.\nFormally, let \\(K\\) be a nonnegative kernel function and let \\(h&gt;0\\) be a bandwidth or window width. Define weights\n\\[\nw_{x_0}(x_i) = K\\!\\left(\\frac{x_i - x_0}{h}\\right),\n\\]\nand estimate the trend at \\(x_0\\) with\n\\[\n\\hat{f}(x_0) \\;=\\; \\frac{\\sum_{i=1}^N w_{x_0}(x_i)\\,y_i}{\\sum_{i=1}^N w_{x_0}(x_i)}.\n\\]\nThe bin smoother is a special case with the boxcar (or uniform) kernel \\(K(u) = 1\\) if \\(|u| \\leq 1\\) and 0 otherwise, which corresponds to assigning weight 1 inside the window and 0 outside. This is why, in the code above, we used kernel = \"box\" with ksmooth. To attenuate the wiggles caused by abrupt point entry and exit, we can use a smooth kernel that gives more weight to points near \\(x_0\\) and rapidly decays for distant points. The option kernel = \"normal\" in ksmooth does exactly this by using the standard normal density for \\(K\\).\nBelow we visualize the box and normal kernels for \\(x_0 = -125\\) and \\(h = 3.5\\), showing how the boxcar kernel weighs all in-bin points equally, while the normal kernel downweights points near the edges.\n\n\n\n\n\n\n\n\nThe final code and resulting plot for the normal kernel look like this:\n\nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"normal\", bandwidth = 7))\n\npolls_2008 |&gt; mutate(smooth = fit$y) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth), color = \"red\")\n\n\n\n\n\n\n\nNotice that this version looks smoother.\nThere are several functions in R that implement bin smoothers. One example is ksmooth, shown above. In practice, however, we typically prefer methods that use slightly more complex models than fitting a constant. The final result above, for example, is still somewhat wiggly in parts we don’t expect it to be (between -125 and -75, for example). Methods such as loess, which we explain next, improve on this.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#local-weighted-regression-loess",
    "href": "ml/smoothing.html#local-weighted-regression-loess",
    "title": "29  Smoothing",
    "section": "\n29.5 Local weighted regression (loess)",
    "text": "29.5 Local weighted regression (loess)\nA limitation of the bin smoother approach just described is that we need small windows for the approximately constant assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates \\(\\hat{f}(x)\\). Here we describe how local weighted regression (loess) permits us to consider larger window sizes. To do this, we will use a mathematical result, referred to as Taylor’s theorem, which tells us that if you look closely enough at any smooth function \\(f(x)\\), it will look like a line. To see why this makes sense, consider the curved edges gardeners make using straight-edged spades:\n\n(“Downing Street garden path edge”1 by Flickr user Number 102. CC-BY 2.0 license3.)\nInstead of assuming the function is approximately constant in a window, we assume the function is locally linear. We can consider larger window sizes with the linear assumption than with a constant. Instead of the one-week window, we consider a larger one in which the trend is approximately linear. We start with a three-week window and later consider and evaluate other options:\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) \\mbox{   if   }  |x_i - x_0| \\leq 21\n\\]\nFor every point \\(x_0\\), loess defines a window and fits a line within that window. Here is an example showing the fits for \\(x_0=-125\\) and \\(x_0 = -55\\):\n\n\n\n\n\n\n\n\nThe fitted value at \\(x_0\\) becomes our estimate \\(\\hat{f}(x_0)\\). Below we show the procedure happening as we move from the -155 up to 0:\n\n\n\n\n\n\n\n\nThe final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters:\n\ntotal_days &lt;- diff(range(polls_2008$day))\nspan &lt;- 21/total_days\nfit &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008)\npolls_2008 |&gt; mutate(smooth = fit$fitted) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") +\n  geom_line(aes(day, smooth), color = \"red\")\n\n\n\n\n\n\n\nDifferent spans give us different estimates. We can see how different window sizes lead to different estimates:\n\n\n\n\n\n\n\n\nHere are the final estimates:\n\n\n\n\n\n\n\n\nThere are three other differences between loess and the typical bin smoother.\n1. Rather than keeping the bin size the same, loess keeps the number of points used in the local fit the same. This number is controlled via the span argument, which expects a proportion. For example, if N is the number of data points and span=0.5, then for a given \\(x\\), loess will use the 0.5*N closest points to \\(x\\) for the fit.\n2. When fitting a line locally, loess uses a weighted approach. Basically, instead of using least squares, we minimize a weighted version:\n\\[\n\\sum_{i=1}^N w_0(x_i) \\left[y_i - \\left\\{\\beta_0 + \\beta_1 (x_i-x_0)\\right\\}\\right]^2\n\\]\nHowever, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight:\n\\[\nK(u)= \\left( 1  - |u|^3\\right)^3 \\mbox{ if } |u| \\leq 1 \\mbox{ and } K(u) = 0 \\mbox{ if } |u| &gt; 1\n\\]\nTo define the weights, we denote \\(2h\\) as the window size and define \\(w_0(x_i)\\) as above: \\(w_0(x_i) = K\\left(\\frac{x_i - x_0}{h}\\right)\\).\nThis kernel differs from the Gaussian kernel in that more points get values closer to the max:\n\n\n\n\n\n\n\n\n3. loess has the option of fitting the local model robustly. An iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration. To use this option, we use the argument family=\"symmetric\".\nFitting parabolas\nTaylor’s theorem also tells us that if you look at any mathematical function closely enough, it looks like a parabola. The theorem also states that you don’t have to look as closely when approximating with parabolas as you do when approximating with lines. This means we can make our windows even larger and fit parabolas instead of lines.\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) + \\beta_2 (x_i-x_0)^2 \\mbox{   if   }  |x_i - x_0| \\leq h\n\\]\nYou may have noticed that when we showed the code for using loess, we set degree = 1. This tells loess to fit polynomials of degree 1, a fancy name for lines. If you read the help page for loess, you will see that the argument degree defaults to 2. By default, loess fits parabolas not lines. Here is a comparison of the fitting lines (red dashed) and fitting parabolas (orange solid):\n\ntotal_days &lt;- diff(range(polls_2008$day))\nspan &lt;- 28/total_days\nfit_1 &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008)\nfit_2 &lt;- loess(margin ~ day, span = span, data = polls_2008)\n\npolls_2008 |&gt; mutate(smooth_1 = fit_1$fitted, smooth_2 = fit_2$fitted) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") +\n  geom_line(aes(day, smooth_1), color = \"red\", lty = 2) +\n  geom_line(aes(day, smooth_2), color = \"orange\", lty = 1) \n\n\n\n\n\n\n\nThe degree = 2 gives us more wiggly results. In general, we actually prefer degree = 1 as it is less prone to this kind of noise.\nBeware of default smoothing parameters\nThe geom_smooth function in the ggplot2 package supports a variety of smoothing methods. By default, it uses loess or a related method called Generalized Additive Model, the latter if any window of data exceeds 1000 observations. We can request that loess is used using the method function:\n\npolls_2008 |&gt; ggplot(aes(day, margin)) +\n  geom_point() + \n  geom_smooth(method = loess, formula = y ~ x)\n\n\n\n\n\n\n\nBut be careful with default parameters as they are rarely optimal. However, you can conveniently change them:\n\npolls_2008 |&gt; ggplot(aes(day, margin)) +\n  geom_point() + \n  geom_smooth(method = loess, formulat = y ~ x, method.args = list(span = 0.15, degree = 1))",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#sec-smoothing-ml-connection",
    "href": "ml/smoothing.html#sec-smoothing-ml-connection",
    "title": "29  Smoothing",
    "section": "\n29.6 Connecting smoothing to machine learning",
    "text": "29.6 Connecting smoothing to machine learning\nTo see how smoothing relates to machine learning with a concrete example, consider again our Section 29.1 example. If we define the outcome \\(Y = 1\\) for digits that are seven and \\(Y=0\\) for digits that are 2, then we are interested in estimating the conditional probability:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2).\n\\]\nwith \\(x_1\\) and \\(x_2\\) the two predictors defined in Section 29.1. In this example, the 0s and 1s we observe are “noisy” because for some regions the probabilities \\(p(\\mathbf{x})\\) are not that close to 0 or 1. We therefore need to estimate \\(p(\\mathbf{x})\\). Smoothing is an alternative to accomplishing this. In Section 29.1, we saw that linear regression was not flexible enough to capture the non-linear nature of \\(p(\\mathbf{x})\\), thus smoothing approaches provide an improvement. In Section 30.1, we describe a popular machine learning algorithm, k-nearest neighbors, which is based on the concept of smoothing.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#exercises",
    "href": "ml/smoothing.html#exercises",
    "title": "29  Smoothing",
    "section": "\n29.7 Exercises",
    "text": "29.7 Exercises\n1. The dslabs package provides the following dataset with mortality counts for Puerto Rico for 2015-2018.\n\nlibrary(dslabs)\nhead(pr_death_counts)\n\nRemove data from before May 2018, then use the loess function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long.\n2. Plot the smooth estimates against day of the year, all on the same plot but with different colors.\n3. Suppose we want to predict 2s and 7s in our mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power. In fact, if we fit a regular logistic regression, the coefficient for x_2 is not significant!\n\nlibrary(dslabs)\nglm(y ~ x_2, family = \"binomial\", data = mnist_27) \n\nPlotting a scatterplot here is not useful since y is binary:\n\nwith(mnist_27$train, plot(x_2, y)\n\nFit a loess line to the data above and plot the results. Notice that there is predictive power, except the conditional probability is not linear.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#footnotes",
    "href": "ml/smoothing.html#footnotes",
    "title": "29  Smoothing",
    "section": "",
    "text": "https://www.flickr.com/photos/49707497@N06/7361631644↩︎\nhttps://www.flickr.com/photos/number10gov/↩︎\nhttps://creativecommons.org/licenses/by/2.0/↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html",
    "href": "ml/resampling-methods.html",
    "title": "30  Resampling Methods",
    "section": "",
    "text": "30.1 Motivation with k-nearest neighbors\nIn this chapter, we introduce resampling, one of the most important ideas in machine learning. Here we focus on the conceptual and mathematical aspects. We will describe how to implement resampling methods in practice with the caret package later in Section 33.1. To motivate the concept, we will use the two predictor digits data presented in Section 29.1 and introduce k-nearest neighbors (kNN), to demonstrate the ideas.\nWe are interested in estimating the conditional probability function:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y = 1 \\mid X_1 = x_1 , X_2 = x_2).\n\\]\nas defined in Section 29.6.\nWith k-nearest neighbors (kNN) we estimate \\(p(\\mathbf{x})\\) in a similar way to bin smoothing. First, we define the distance between all observations based on the features. Then, for any point \\(\\mathbf{x}_0\\), we estimate \\(p(\\mathbf{x})\\) by identifying the \\(k\\) nearest points to \\(\\mathbf{x}_0\\) and afterwards taking an average of the \\(y\\)s associated with these points. We refer to the set of points used to compute the average as the neighborhood.\nDue to the connection we described earlier between conditional expectations and conditional probabilities, this gives us \\(\\hat{p}(\\mathbf{x}_0)\\), just like the bin smoother gave us an estimate of a trend. As with bin smoothers, we can control the flexibility of our estimate through the \\(k\\) parameter: larger \\(k\\)s result in smoother estimates, while smaller \\(k\\)s result in more flexible and wiggly estimates.\nTo implement the algorithm, we can use the knn3 function from the caret package. Looking at the help file for this package, we see that we can call it in one of two ways. We will use the first way in which we specify a formula and a data frame. The data frame contains all the data to be used. The formula has the form outcome ~ predictor_1 + predictor_2 + predictor_3 and so on. Therefore, we type y ~ x_1 + x_2. If we are going to use variables in the data frame, we can use the . like this y ~ .. We also need to pick \\(k\\), which is set to k = 5 by default. The final call looks like this:\nlibrary(dslabs)\nlibrary(caret)\nknn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 5)\nIn this case, since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance.\nThe predict function for knn3 produces a probability for each class. We can keep the probability of being a 7 as the estimate \\(\\hat{p}(\\mathbf{x})\\) using type = \"prob\". Here we obtain the actual prediction using type = \"class\":\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.815\nWe see that kNN, with the default parameter, already beats regression. To see why this is the case, we plot \\(\\hat{p}(\\mathbf{x})\\) and compare it to the true conditional probability \\(p(\\mathbf{x})\\):\nWe see that kNN adapts better to the non-linear shape of \\(p(\\mathbf{x})\\). However, our estimate has some islands of blue in the red area, which intuitively does not make much sense. We notice that we have higher accuracy in the train set compared to the test set:\ny_hat_knn &lt;- predict(knn_fit, mnist_27$train, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$train$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.859\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.815\nThis is due to what we call over-training.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#over-training",
    "href": "ml/resampling-methods.html#over-training",
    "title": "30  Resampling Methods",
    "section": "\n30.2 Over-training",
    "text": "30.2 Over-training\nWith kNN, over-training is at its worst when we set \\(k = 1\\). With \\(k = 1\\), the estimate for each \\(\\mathbf{x}\\) in the training set is obtained with just the \\(y\\) corresponding to that point. In this case, if the \\(x_1\\) and \\(x_2\\) are unique, we will obtain perfect accuracy in the training set because each point is used to predict itself (if the predictors are not unique and have different outcomes for at least one set of predictors, then it is impossible to predict perfectly).\nHere we fit a kNN model with \\(k = 1\\) and confirm we get near perfect accuracy in the training set:\n\nknn_fit_1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 1)\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$train, type = \"class\")\nconfusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[[\"Accuracy\"]]\n#&gt; [1] 0.996\n\nBut in the test set, accuracy is actually worse than what we obtained with regression:\n\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.81\n\nWe can see the over-fitting problem by plotting the decision rule boundaries produced by \\(\\hat{p}(\\mathbf{x})\\):\n\n\n\n\n\n\n\n\nThe estimate \\(\\hat{p}(\\mathbf{x})\\) follows the training data too closely (left). You can see that, in the training set, boundaries have been drawn to perfectly surround a single red point in a sea of blue. Because most points \\(\\mathbf{x}\\) are unique, the prediction is either 1 or 0 and the prediction for that point is the associated label. However, once we introduce the test set (right), we see that many of these small islands now have the opposite color and we end up making several incorrect predictions.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#over-smoothing",
    "href": "ml/resampling-methods.html#over-smoothing",
    "title": "30  Resampling Methods",
    "section": "\n30.3 Over-smoothing",
    "text": "30.3 Over-smoothing\nAlthough not as badly as with \\(k=1\\), we saw that with \\(k = 5\\) we also over-trained. Hence, we should consider a larger \\(k\\). Let’s try, as an example, a much larger number: \\(k = 401\\).\n\nknn_fit_401 &lt;- knn3(y ~ ., data = mnist_27$train, k = 401)\ny_hat_knn_401 &lt;- predict(knn_fit_401, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.76\n\nThe estimate turns out to be similar to the one obtained with regression:\n\n\n\n\n\n\n\n\nIn this case, \\(k\\) is so large that it does not permit enough flexibility. We call this over-smoothing.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#tuning-parameter",
    "href": "ml/resampling-methods.html#tuning-parameter",
    "title": "30  Resampling Methods",
    "section": "\n30.4 Tuning parameter",
    "text": "30.4 Tuning parameter\nIt is very common for machine learning algorithms to require that we set one or more values before fitting the model. A simple example is the choice of \\(k\\) in k-Nearest Neighbors (kNN). In Chapter 31, we will see additional examples. These values are referred to as tuning parameters, and an important part of applying machine learning in practice is choosing them, often called tuning the model.\nSo how do we pick tuning parameters? For instance, how do we decide on the best \\(k\\) in kNN? In principle, we want the value of \\(k\\) that maximizes accuracy or, equivalently, minimizes the expected MSE as defined in Section 27.8. The challenge is that we do not know the true expected error. The goal of resampling methods is to estimate this error for any given algorithm and set of tuning parameters, such as \\(k\\).\nTo see why we need resampling, let’s repeat what we did earlier: compare training and test set accuracy, but now for a range of \\(k\\) values. We can then plot the accuracy estimates for each choice of \\(k\\):\n\n\n\n\n\n\n\n\nFirst, note that the estimate obtained from the training set is generally more optimistic, higher accuracy, than the estimate from the test set, with the gap being larger for smaller values of \\(k\\). This is a classic symptom of overfitting.\nSo should we simply pick the \\(k\\) that maximizes accuracy and report this value? There are two key problems with this approach:\n\nThe accuracy-versus-\\(k\\) plot is quite jagged. We do not expect small changes in \\(k\\) to cause large swings in accuracy or MSE. The jaggedness arises because the accuracy is estimated from a finite sample, making it a random variable.\nAlthough we used the test set to estimate accuracy for each \\(k\\), we also used the same test set to select the best \\(k\\). As a result, the reported minimum test set error is overly optimistic and will not generalize to new, unseen data.\n\nResampling methods provide a principled solution to both problems by reducing variability and ensuring that test data are not used twice—once for evaluation and again for tuning.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#mathematical-description-of-resampling-methods",
    "href": "ml/resampling-methods.html#mathematical-description-of-resampling-methods",
    "title": "30  Resampling Methods",
    "section": "\n30.5 Mathematical description of resampling methods",
    "text": "30.5 Mathematical description of resampling methods\nIn the previous section, we introduced kNN as an example to motivate the topic of this chapter. In this particular case, there is just one parameter, \\(k\\), that affects the performance of the algorithm. However, in general, machine algorithms may have multiple parameters so we use the notation \\(\\lambda\\) to represent any set of parameters needed to define a machine learning algorithm. We also introduce notation to distinguish the predictions you get with each set of parameters with \\(\\hat{y}(\\lambda)\\) and the MSE for this choice with \\(\\text{MSE}(\\lambda)\\). Our goal is to find the \\(\\lambda\\) that minimizes \\(\\text{MSE}(\\lambda)\\). Resampling methods help us estimate \\(\\text{MSE}(\\lambda)\\).\nAn intuitive first attempt is the apparent error defined in Section 27.8 and used in the previous section:\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{N}\\sum_{i = 1}^N \\left\\{\\hat{y}_i(\\lambda) - y_i\\right\\}^2\n\\]\nAs noted in the previous section, this estimate is a random error, based on just one test set, with enough variability to affect the choice of the best \\(\\lambda\\) substantially.\nNow imagine a world in which we could obtain data repeatedly, say from new random samples. We could take a very large number \\(B\\) of new samples, split them into training and test sets, and define:\n\\[\n\\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left\\{\\hat{y}_i^b(\\lambda) - y_i^b\\right\\}^2\n\\]\nwith \\(y_i^b\\) the \\(i\\)th observation in sample \\(b\\) and \\(\\hat{y}_{i}^b(\\lambda)\\) the prediction obtained with the algorithm defined by parameter \\(\\lambda\\) and trained independently of \\(y_i^b\\). The law of large numbers tells us that as \\(B\\) becomes larger, this quantity gets closer and closer to \\(MSE(\\lambda)\\). This is of course a theoretical consideration as we rarely get access to more than one dataset for algorithm development, but the concept inspires the idea behind resampling methods.\nThe general idea is to generate a series of different random samples from the data at hand. There are several approaches to doing this, but all randomly generate several smaller datasets that are not used for training, and instead are used to estimate MSE. Next, we describe cross validation, one of the most widely used resampling methods.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#cross-validation",
    "href": "ml/resampling-methods.html#cross-validation",
    "title": "30  Resampling Methods",
    "section": "\n30.6 Cross validation",
    "text": "30.6 Cross validation\nOverall, we are provided a dataset (blue) and we need to build an algorithm, using this dataset, that will eventually be used in completely independent datasets (yellow) that we might not even see.\n\n\n\n\n\n\n\n\nSo to imitate this situation, we start by carving out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a training set (blue) and a test set (red). We will train the entirety of our algorithm, including the choice of parameter \\(\\lambda\\), exclusively on the training set and use the test set only for evaluation purposes.\nWe usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the MSE without fitting an impractical number of models. Typical choices are to use 10%-20% of the data for testing.\n\n\n\n\n\n\n\n\nLet’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting features, not for anything!\nBut then how do we optimize \\(\\lambda\\)? In cross validation, we achieve this by splitting the training set into two: the training and validation sets.\n\n\n\n\n\n\n\n\nWe will do this many times assuring that the estimates of MSE obtained in each dataset are independent from each other. There are several proposed methods for doing this. Here we describe one of these approaches, K-fold cross validation, in detail to provide the general idea used in all approaches.\nK-fold cross validation\nAs a reminder, we are going to imitate the concept used when introducing this version of the MSE:\n\\[\n\\mbox{MSE}(\\lambda) \\approx\\frac{1}{B} \\sum_{b = 1}^B \\frac{1}{N}\\sum_{i = 1}^N \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2\n\\]\nWe want to generate a dataset that can be thought of as independent random sample, and do this \\(B\\) times. The K in K-fold cross validation, represents the number of time \\(B\\). In the illustrations, we are showing an example that uses \\(B = 5\\).\nWe will eventually end up with \\(B\\) samples, but let’s start by describing how to construct the first: we simply pick \\(M = N/B\\) observations at random (we round if \\(M\\) is not a round number) and think of these as a random sample \\(y_1^b, \\dots, y_M^b\\), with \\(b = 1\\). We call this the validation set.\nNow we can fit the model in the training set, then compute the apparent error on the independent set:\n\\[\n\\hat{\\mbox{MSE}}_b(\\lambda) = \\frac{1}{M}\\sum_{i = 1}^M \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2\n\\]\nAs a reminder, this is just one sample and will therefore return a noisy estimate of the true error. In K-fold cross validation, we randomly split the observations into \\(B\\) non-overlapping sets:\n\n\n\n\n\n\n\n\nNow we repeat the calculation above for each of these sets \\(b = 1,\\dots,B\\) and obtain \\(\\hat{\\mbox{MSE}}_1(\\lambda),\\dots, \\hat{\\mbox{MSE}}_B(\\lambda)\\). Then, for our final estimate, we compute the average:\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\mbox{MSE}}_b(\\lambda)\n\\]\nand obtain an estimate of our loss. A final step would be to select the \\(\\lambda\\) that minimizes the MSE.\nHow many folds?\nNow how do we pick the cross validation fold? Large values of \\(B\\) are preferable because the training data better imitates the original dataset. However, larger values of \\(B\\) will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of \\(B = 5\\) and \\(B = 10\\) are popular.\nOne way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick \\(B\\) sets of some size at random.\nEstimate MSE of our optimized algorithm\nWe have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and we therefore need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on:\n\n\n\n\n\n\n\n\nWe can actually do cross validation again:\n\n\n\n\n\n\n\n\nand obtain a final estimate of our expected loss. However, note that last cross validation iteration means that our entire compute time gets multiplied by \\(K\\). You will soon learn that fitting each algorithm takes time because we are performing many complex computations. As a result, we are always looking for ways to reduce this time. For the final evaluation, we often just use the one test set.\nOnce we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#boostrap-resampling",
    "href": "ml/resampling-methods.html#boostrap-resampling",
    "title": "30  Resampling Methods",
    "section": "\n30.7 Boostrap resampling",
    "text": "30.7 Boostrap resampling\nTypically, cross-validation involves partitioning the original dataset into a training set to train the model and a testing set to evaluate it. With the bootstrap approach, based on the ideas described in Chapter 14, you can create multiple different training datasets via bootstrapping. This method is sometimes called bootstrap aggregating or bagging.\nIn bootstrap resampling, we create a large number of bootstrap samples from the original training dataset. Each bootstrap sample is created by randomly selecting observations with replacement, usually the same size as the original training dataset. For each bootstrap sample, we fit the model and compute the MSE estimate on the observations not selected in the random sampling, referred to as the out-of-bag observations. These out-of-bag observations serve a similar role to a validation set in standard cross-validation.\nWe then average the MSEs obtained in the out-of-bag observations from each bootstrap sample to estimate the model’s performance.\nThis approach is actually the default approach in the caret package. We describe how to implement resampling methods with the caret package in the next chapter.\nComparison of MSE estimates\nIn Section 30.1, we computed an estimate of MSE based just on the provided test set (shown in red in the plot below). Here we show how the cross-validation techniques described above help reduce variability. The green curve below shows the results of applying K-fold cross validation with 10 folds, leaving out 10% of the data for validation. We can see that the variance is reduced substantially. The blue curve is the result of using 100 bootstrap samples to estimate MSE. The variability is reduced even further, but at the cost of a 10 fold increase in computation time.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#exercises",
    "href": "ml/resampling-methods.html#exercises",
    "title": "30  Resampling Methods",
    "section": "\n30.8 Exercises",
    "text": "30.8 Exercises\nGenerate a set of random predictors and outcomes like this:\n\nset.seed(1996)\nn &lt;- 1000\np &lt;- 10000\nx &lt;- matrix(rnorm(n * p), n, p)\ncolnames(x) &lt;- paste(\"x\", 1:ncol(x), sep = \"_\")\ny &lt;- rbinom(n, 1, 0.5) |&gt; factor()\n\nx_subset &lt;- x[ ,sample(p, 100)]\n\n1. Because x and y are completely independent, you should not be able to predict y using x with accuracy larger than 0.5. Confirm this by running cross validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model. Hint: use the caret train function. The results component of the output of train shows you the accuracy. Ignore the warnings.\n2. Now instead of a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the \\(y = 1\\) group to those in the \\(y = 0\\) group, for each predictor, using a t-test. You can perform this step as follows:\n\ndevtools::install_bioc(\"genefilter\")\ninstall.packages(\"genefilter\")\nlibrary(genefilter)\ntt &lt;- colttests(x, y)\n\nCreate a vector of the p-values and call it pvals.\n3. Create an index ind with the column numbers of the predictors that were “statistically significantly” associated with y. Use a p-value cutoff of 0.01 to define “statistically significant”. How many predictors survive this cutoff?\n4. Re-run the cross validation but after redefining x_subset to be the subset of x defined by the columns showing “statistically significant” association with y. What is the accuracy now?\n5. Re-run the cross validation again, but this time using kNN. Try out the following grid of tuning parameters: k = seq(101, 301, 25). Make a plot of the resulting accuracy.\n6. In exercises 3 and 4, we see that despite the fact that x and y are completely independent, we were able to predict y with accuracy higher than 70%. We must be doing something wrong then. What is it?\n\nThe function train estimates accuracy on the same data it uses to train the algorithm.\nWe are over-fitting the model by including 100 predictors.\nWe used the entire dataset to select the columns used in the model. This step needs to be included as part of the algorithm. The cross validation was done after this selection.\nThe high accuracy is just due to random variability.\n\n7. Advanced. Re-do the cross validation but this time include the selection step in the cross validation. The accuracy should now be close to 50%.\n8. Load the tissue_gene_expression dataset. Use the train function to predict tissue from gene expression. Use kNN. What k works best?\n9. The createResample function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the mnist_27 dataset like this:\n\nset.seed(1995)\nindexes &lt;- createResample(mnist_27$train$y, 10)\n\nHow many times do 3, 4, and 7 appear in the first re-sampled index?\n10. We see that some numbers appear more than once and others appear no times. This must be so for each dataset to be independent. Repeat the exercise for all the re-sampled indexes.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html",
    "href": "ml/algorithms.html",
    "title": "31  Examples of Algorithms",
    "section": "",
    "text": "31.1 Logistic regression\nThere are hundreds of machine learning algorithms. Here we provide a few examples spanning rather different approaches. Throughout the chapter, we will be using the two predictor digits data introduced in Section 29.1 to demonstrate how the algorithms work. We focus on the concepts and ideas behind the algorithms using illustrative datasets from the dslabs package.\nLater, in Chapter 33, we show an efficient way to implement these ideas using the caret package.\nIn Section 29.1, we used linear regression to predict classes by fitting the model:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) =\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\] using least squares after assigning numeric values of 0 and 1 to the outcomes \\(y\\), and applied regression as if the data were continuous. An obvious problem with this approach is that \\(\\hat{p}(\\mathbf{x})\\) can be negative and larger than 1:\nfit_lm &lt;- lm(y ~ x_1 + x_2, data = mutate(mnist_27$train,y = ifelse(y == 7, 1, 0)))\nrange(fit_lm$fitted)\n#&gt; [1] -0.22  1.92\nTo avoid this, we can apply the approach described in Section 18.2 that is more appropriate for binary data. We write the model like this:\n\\[\n\\log \\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nWe can then find the maximum likelihood estimates (MLE) of the model parameters and predict using the estimate \\(p(\\mathbf{x})\\) to obtain an accuracy of 0.775. We see that logistic regression performs similarly to regression. This is not surprising given that the estimate of \\(\\hat{p}(\\mathbf{x})\\) looks similar as well:\nJust like regression, the decision rule is a line, a fact that can be corroborated mathematically. Defining \\(g(x) = \\log \\{x/(1-x)\\}\\), we have:\n\\[\ng^{-1}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2) = 0.5 \\implies\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = g(0.5) = 0 \\implies\nx_2 = -\\hat{\\beta}_0/\\hat{\\beta}_2 -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\]\nThus, much like with regression, \\(x_2\\) is a linear function of \\(x_1\\). This implies that our logistic regression approach has no chance of capturing the non-linear nature of the true \\(p(\\mathbf{x})\\). We now describe some techniques that estimate the conditional probability in a more flexible way.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Examples of Algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#logistic-regression",
    "href": "ml/algorithms.html#logistic-regression",
    "title": "31  Examples of Algorithms",
    "section": "",
    "text": "You are ready to do exercises 1 - 11.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Examples of Algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#k-nearest-neighbors",
    "href": "ml/algorithms.html#k-nearest-neighbors",
    "title": "31  Examples of Algorithms",
    "section": "\n31.2 k-nearest neighbors",
    "text": "31.2 k-nearest neighbors\nWe introduced the kNN algorithm in Section 30.1. In Section 30.7.1, we noted that \\(k=31\\) provided the highest accuracy in the test set. Using \\(k=31\\), we obtain an accuracy 0.825, an improvement over regression. A plot of the estimated conditional probability shows that the kNN estimate is flexible enough and does indeed capture the shape of the true conditional probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou are ready to do exercises 12 - 13.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Examples of Algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#generative-models",
    "href": "ml/algorithms.html#generative-models",
    "title": "31  Examples of Algorithms",
    "section": "\n31.3 Generative models",
    "text": "31.3 Generative models\nWe have described how, when using squared loss, the conditional expectation provides the best approach to developing a decision rule. In a binary case, the smallest true error we can achieve is determined by Bayes’ rule, which is a decision rule based on the true conditional probability:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y = 1 \\mid \\mathbf{X}=\\mathbf{x})\n\\]\nWe have described several approaches to estimating \\(p(\\mathbf{x})\\). In all these approaches, we estimate the conditional probability directly and do not consider the distribution of the predictors. In machine learning, these are referred to as discriminative approaches.\nHowever, Bayes’ theorem tells us that knowing the distribution of the predictors \\(\\mathbf{X}\\) may be useful. Methods that model the joint distribution of \\(Y\\) and \\(\\mathbf{X}\\) are referred to as generative models (we model how the entire data, \\(\\mathbf{X}\\) and \\(Y\\), are generated). We start by describing the most general generative model, Naive Bayes, and then proceed to describe two specific cases, quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA).\nNaive Bayes\nRecall that Bayes rule tells us that we can rewrite \\(p(\\mathbf{x})\\) as follows:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y = 1|\\mathbf{X}=\\mathbf{x}) = \\frac{f_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\mathrm{Pr}(Y = 1)}\n{ f_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\mathrm{Pr}(Y = 0)  + f_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\mathrm{Pr}(Y = 1) }\n\\]\nwith \\(f_{\\mathbf{X}|Y = 1}\\) and \\(f_{\\mathbf{X}|Y = 0}\\) representing the distribution functions of the predictor \\(\\mathbf{X}\\) for the two classes \\(Y = 1\\) and \\(Y = 0\\). The formula implies that if we can estimate these conditional distributions, we can develop a powerful decision rule. However, this is a big if.\nAs we go forward, we will encounter examples in which \\(\\mathbf{X}\\) has many dimensions and we do not have much information about the distribution. In these cases, Naive Bayes will be practically impossible to implement. However, there are instances in which we have a small number of predictors (not much more than 2) and many categories in which generative models can be quite powerful. We describe two specific examples and use our previously described case studies to illustrate them.\nLet’s start with a very simple and uninteresting, yet illustrative, case: the example related to predicting sex from height.\n\nset.seed(1995)\ny &lt;- heights$height\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntrain_set &lt;- heights |&gt; slice(-test_index)\ntest_set &lt;- heights |&gt; slice(test_index)\n\nIn this case, the Naive Bayes approach is particularly appropriate because we know that the normal distribution is a good approximation for the conditional distributions of height given sex for both classes \\(Y = 1\\) (female) and \\(Y = 0\\) (male). This implies that we can approximate the conditional distributions \\(f_{X|Y = 1}\\) and \\(f_{X|Y = 0}\\) by simply estimating averages and standard deviations from the data:\n\nparams &lt;- train_set |&gt; group_by(sex) |&gt; \n  summarize(avg = mean(height), sd = sd(height))\nparams\n#&gt; # A tibble: 2 × 3\n#&gt;   sex      avg    sd\n#&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Female  64.8  4.14\n#&gt; 2 Male    69.2  3.57\n\nThe prevalence, which we will denote with \\(\\pi = \\mathrm{Pr}(Y = 1)\\), can be estimated from the data with:\n\npi &lt;- train_set |&gt; summarize(pi = mean(sex == \"Female\")) |&gt; pull(pi)\npi\n#&gt; [1] 0.212\n\nNow we can use our estimates of average and standard deviation to get an actual rule:\n\nx &lt;- test_set$height\nf0 &lt;- dnorm(x, params$avg[2], params$sd[2])\nf1 &lt;- dnorm(x, params$avg[1], params$sd[1])\np_hat_bayes &lt;- f1*pi / (f1*pi + f0*(1 - pi))\n\nOur Naive Bayes estimate \\(\\hat{p}(x)\\) looks a lot like a logistic regression estimate:\n\n\n\n\n\n\n\n\nIn fact, we can show that the Naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as the Elements of Statistical Learning1. We can see that they are similar empirically by comparing the two resulting curves.\nControlling prevalence\nOne useful feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence. Using our sample, we estimated \\(f_{X|Y = 1}\\), \\(f_{X|Y = 0}\\) and \\(\\pi\\). If we use hats to denote the estimates, we can write \\(\\hat{p}(x)\\) as:\n\\[\n\\hat{p}(x)= \\frac{\\hat{f}_{X|Y = 1}(x) \\hat{\\pi}}\n{ \\hat{f}_{X|Y = 0}(x)(1-\\hat{\\pi}) + \\hat{f}_{X|Y = 1}(x)\\hat{\\pi} }\n\\]\nAs we discussed earlier, our sample has a much lower prevalence, 0.21, than the general population. So if we use the rule \\(\\hat{p}(x) &gt; 0.5\\) to predict females, our accuracy will be affected due to the low sensitivity:\n\ny_hat_bayes &lt;- ifelse(p_hat_bayes &gt; 0.5, \"Female\", \"Male\")\nsensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))\n#&gt; [1] 0.213\n\nAgain, this is because the algorithm gives more weight to specificity to account for the low prevalence:\n\nspecificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))\n#&gt; [1] 0.967\n\nThis is mainly due to the fact that \\(\\hat{\\pi}\\) is substantially less than 0.5, so we tend to predict Male more often. It makes sense for a machine learning algorithm to do this in our sample because we do have a higher percentage of males. But if we were to extrapolate this to a general population, our overall accuracy would be affected by the low sensitivity.\nThe Naive Bayes approach gives us a direct way to correct this since we can simply force \\(\\hat{\\pi}\\) to be whatever value we want it to be. So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule, we could simply change \\(\\hat{\\pi}\\) to 0.5 like this:\n\np_hat_bayes_unbiased &lt;- f1 * 0.5 / (f1 * 0.5 + f0 * (1 - 0.5)) \ny_hat_bayes_unbiased &lt;- ifelse(p_hat_bayes_unbiased &gt; 0.5, \"Female\", \"Male\")\n\nNote the difference in sensitivity with a better balance:\n\nsensitivity(factor(y_hat_bayes_unbiased), factor(test_set$sex))\n#&gt; [1] 0.693\nspecificity(factor(y_hat_bayes_unbiased), factor(test_set$sex))\n#&gt; [1] 0.832\n\nThe new rule also gives us a very intuitive cutoff between 66-67, which is about the middle of the female and male average heights:\n\nplot(x, p_hat_bayes_unbiased)\nabline(h = 0.5, lty = 2) \nabline(v = 67, lty = 2)\n\n\n\n\n\n\n\nQuadratic discriminant analysis\nQuadratic Discriminant Analysis (QDA) is a version of Naive Bayes in which we assume that the distributions \\(p_{\\mathbf{X}|Y = 1}(x)\\) and \\(p_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\) are multivariate normal. The simple example we described in the previous section is actually QDA. Let’s now look at a slightly more complicated case: the 2 or 7 example.\nIn this example, we have two predictors so we assume each one is bivariate normal. This implies that we need to estimate two averages, two standard deviations, and a correlation for each case \\(Y = 1\\) and \\(Y = 0\\). Once we have these, we can approximate the distributions \\(f_{X_1,X_2|Y = 1}\\) and \\(f_{X_1, X_2|Y = 0}\\). We can easily estimate parameters from the data:\n\nparams &lt;- mnist_27$train |&gt; \n  group_by(y) |&gt; \n  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), \n            sd_1= sd(x_1), sd_2 = sd(x_2), \n            r = cor(x_1, x_2))\nparams\n#&gt; # A tibble: 2 × 6\n#&gt;   y     avg_1 avg_2   sd_1   sd_2     r\n#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2     0.136 0.287 0.0670 0.0600 0.415\n#&gt; 2 7     0.238 0.290 0.0745 0.104  0.468\n\nWith these estimates in place, all we need are the prevalence \\(\\pi\\) to compute:\n\\[\n\\hat{p}(\\mathbf{x})= \\frac{\\hat{f}_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\hat{\\pi}}\n{ \\hat{f}_{\\mathbf{X}|Y = 0}(x)(1-\\hat{\\pi}) + \\hat{f}_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\hat{\\pi} }\n\\]\nNote that the densities \\(f\\) are bivariate normal distributions. Here we provide a visual way of showing the approach. We plot the data and use contour plots to give an idea of what the two estimated normal densities look like (we show the curve representing a region that includes 95% of the points):\n\n\n\n\n\n\n\n\nWe can fit QDA using the qda function the MASS package:\n\ntrain_qda &lt;- MASS::qda(y ~ ., data = mnist_27$train)\ny_hat &lt;- predict(train_qda, mnist_27$test)$class\n\nWe see that we obtain relatively good accuracy:\n\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"] \n#&gt; Accuracy \n#&gt;    0.815\n\nThe conditional probability looks relatively good, although it does not fit as well as the kernel smoothers:\n\n\n\n\n\n\n\n\nOne reason QDA does not work as well as the kernel methods is because the assumption of normality does not quite hold. Although for the 2s it seems reasonable, for the 7s it does seem to be off. Notice the slight curvature in the points for the 7s:\n\nmnist_27$train |&gt; mutate(y = factor(y)) |&gt; \n  ggplot(aes(x_1, x_2, fill = y, color = y)) + \n  geom_point(show.legend = FALSE) + \n  stat_ellipse(type = \"norm\") +\n  facet_wrap(~y)\n\n\n\n\n\n\n\nQDA can work well here, but it becomes harder to use as the number of predictors increases. Here we have 2 predictors and had to compute 4 means, 4 SDs, and 2 correlations. Notice that if we have 10 predictors, we have 45 correlations for each class. In general, the formula is \\(K\\times p(p-1)/2\\), which gets big fast. Once the number of parameters approaches the size of our data, the method becomes impractical due to overfitting.\nLinear discriminant analysis\nA relatively simple solution to QDA’s problem of having too many parameters is to assume that the correlation structure is the same for all classes, which reduces the number of parameters we need to estimate. In this case, the the distributions looks like this:\n\n\n\n\n\n\n\n\nWe can fit LDA using the MASS lda function:\n\ntrain_lda &lt;- MASS::lda(y ~ ., data = mnist_27$train)\ny_hat &lt;- predict(train_lda, mnist_27$test)$class\n\nNow the size of the ellipses as well as the angles are the same. This is because they are assumed to have the same standard deviations and correlations. Although this added constraint lowers the number of parameters, the rigidity lowers our accuracy to:\n\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.775\n\nWhen we force this assumption, we can show mathematically that the boundary is a line, just as with logistic regression. For this reason, we call the method linear discriminant analysis (LDA). Similarly, for QDA, we can show that the boundary must be a quadratic function.\n\n\n\n\n\n\n\n\nIn the case of LDA, the lack of flexibility does not permit us to capture the non-linearity in the true conditional probability function.\nConnection to distance\nThe normal density is:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left\\{ - \\frac{(x-\\mu)^2}{\\sigma^2}\\right\\}\n\\]\nIf we remove the constant \\(1/(\\sqrt{2\\pi} \\sigma)\\) and then take the log, we get:\n\\[\n- \\frac{(x-\\mu)^2}{\\sigma^2}\n\\]\nwhich is the negative of a distance squared scaled by the standard deviation. For higher dimensions, the same is true except the scaling is more complex and involves correlations.\n\n\n\n\n\n\nYou are now ready to do exercises 14-17.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Examples of Algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#sec-trees",
    "href": "ml/algorithms.html#sec-trees",
    "title": "31  Examples of Algorithms",
    "section": "\n31.4 Classification and regression trees (CART)",
    "text": "31.4 Classification and regression trees (CART)\nThe curse of dimensionality\nWe described how methods such as LDA and QDA are not meant to be used with many predictors \\(p\\) because the number of parameters that we need to estimate becomes too large. For example, with the digits example \\(p = 784\\), we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA. Kernel methods, such as kNN or local regression, do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the curse of dimensionality. The dimension here refers to the fact that when we have \\(p\\) predictors, the distance between two observations is computed in \\(p\\)-dimensional space.\nA useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility, and to be flexible we need to keep the neighborhoods small.\nTo see how this becomes an issue for higher dimensions, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1:\n\n\n\n\n\n\n\n\nNow for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to \\(\\sqrt{.10} \\approx .316\\):\n\n\n\n\n\n\n\n\nUsing the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is \\(\\sqrt[3]{.10} \\approx 0.464\\). In general, to include 10% of the data in a case with \\(p\\) dimensions, we need an interval with each side of size \\(\\sqrt[p]{.10}\\) of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.\n\n\n\n\n\n\n\n\nBy the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.\nHere we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on regression and decision trees and their extension to random forests.\nCART motivation\nTo motivate this section, we will use a new dataset that includes the breakdown of the composition of olive oil into 8 fatty acids:\n\nnames(olive)\n#&gt;  [1] \"region\"      \"area\"        \"palmitic\"    \"palmitoleic\"\n#&gt;  [5] \"stearic\"     \"oleic\"       \"linoleic\"    \"linolenic\"  \n#&gt;  [9] \"arachidic\"   \"eicosenoic\"\n\nFor illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.\n\ntable(olive$region)\n#&gt; \n#&gt; Northern Italy       Sardinia Southern Italy \n#&gt;            151             98            323\n\nWe remove the area column because we won’t use it as a predictor.\n\nolive &lt;- select(olive, -area)\n\nUsing kNN, we can achieve a test set accuracy of 0.9712321. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia.\n\n\n\n\n\n\n\n\nThis implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic.\n\n\n\n\n\n\n\n\nIn Section 22.3, we defined predictor spaces, which in this case consists of eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye, we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category.\nThis in turn can be used to define an algorithm with perfect accuracy. Specifically, we define the following decision rule: if eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linoleic is larger than \\(10.535\\), predict Sardinia, and if lower, predict Northern Italy. We can draw this decision tree as follows:\n\n\n\n\n\n\n\n\nDecision trees like this are often used in practice. For example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following:\n\n\n\n\n(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-1842.)\nA tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as nodes. Regression and decision trees operate by predicting an outcome variable \\(y\\) by partitioning the predictors.\nRegression trees\nWhen using trees, and the outcome is continuous, we call the approach a regression tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation \\(f(x) = \\mathrm{E}(Y | X = x)\\) with \\(Y\\) the poll margin and \\(x\\) the day.\n\n\n\n\n\n\n\n\nThe general idea here is to build a decision tree and, at the end of each node, obtain a predictor \\(\\hat{y}\\). A mathematical way to describe this is: we are partitioning the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\), and then for any predictor \\(x\\) that falls within region \\(R_j\\), estimate \\(f(x)\\) with the average of the training observations \\(y_i\\) for which the associated predictor \\(x_i\\) is also in \\(R_j\\).\nBut how do we decide on the partition \\(R_1, R_2, \\ldots, R_J\\) and how do we choose \\(J\\)? Here is where the algorithm gets a bit complicated.\nRegression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example, this space is the interval [-155, 1]. But after the first step, we will have two partitions. After the second step, we will split one of these partitions into two and will have three partitions, then four, and so on. Later we describe how we pick the partition to further partition, and when to stop.\nFor each existing partition, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, which we will call \\(R_1(j,s)\\) and \\(R_2(j,s)\\), that split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\):\n\\[\nR_1(j,s) = \\{\\mathbf{x} \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\}\n\\]\nIn our current example, we only have one predictor, so we will always choose \\(j = 1\\), but in general this will not be the case. Now after we define the new partitions \\(R_1\\) and \\(R_2\\), and we decide to stop the partitioning, we compute predictors by taking the average of all the observations \\(y\\) for which the associated \\(\\mathbf{x}\\) is in \\(R_1\\) and \\(R_2\\). We refer to these two as \\(\\hat{y}_{R_1}\\) and \\(\\hat{y}_{R_2}\\) respectively.\nBut how do we pick \\(j\\) and \\(s\\)? Basically we find the pair that minimizes the residual sum of squares (RSS):\n\\[\n\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 +\n\\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\nThis is then applied recursively to the new regions \\(R_1\\) and \\(R_2\\). We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.\nLet’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the rpart function in the rpart package.\n\nlibrary(rpart)\nfit &lt;- rpart(margin ~ ., data = polls_2008)\n\nIn this case, there is only one predictor. Thus we do not have to decide which predictor \\(j\\) to split by, we simply have to decide what value \\(s\\) we use to split. We can visually see where the splits were made:\n\nplot(fit, margin = 0.1)\ntext(fit, cex = 0.75)\n\n\n\n\n\n\n\n\n\nThe first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on. We end up with 8 partitions. The final estimate \\(\\hat{f}(x)\\) looks like this:\n\npolls_2008 |&gt; \n  mutate(y_hat = predict(fit)) |&gt; \n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_step(aes(day, y_hat), col = \"red\")\n\n\n\n\n\n\n\nNote that the algorithm stopped partitioning at 8 nodes. This decision is based on a measure called the complexity parameter (cp). Every time the data is split into two new partitions, the training set RSS decreases because the model gains flexibility to adapt to the data. If splitting continued until every observation was isolated in its own partition, the RSS would eventually reach 0—since the average of a single value is the value itself. To prevent such overfitting, the algorithm requires that each new split reduce the RSS by at least a factor of cp. Larger values of cp therefore stop the algorithm earlier, leading to fewer nodes.\nHowever, cp is not the only factor that controls partitioning. Another important parameter is the minimum number of observations required in a node before it can be split, set by the minsplit argument in rpart (default: 20). In addition, rpart allows users to specify the minimum number of observations allowed in a terminal node (leaf), controlled by the minbucket argument. By default, minbucket = round(minsplit / 3).\nAs expected, if we set cp = 0 and minsplit = 2, then our prediction is as flexible as possible and our predictor is our original data:\n\n\n\n\n\n\n\n\nIntuitively we know that this is not a good approach as it will generally result in over-training. These cp, minsplit, and minbucket parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restricts flexibility.\nSo how do we pick these parameters? We can use cross validation, just like with any tuning parameter. Here is the resulting tree when we use cross validation to choose cp:\n\n\n\n\n\n\n\n\nNote that if we already have a tree and want to apply a higher cp value, we can use the prune function. We call this pruning a tree because we are snipping off partitions that do not meet a cp criterion. Here is an example where we create a tree that used a cp = 0 and then we prune it back:\n\nfit &lt;- rpart(margin ~ ., data = polls_2008, control = rpart.control(cp = 0))\npruned_fit &lt;- prune(fit, cp = 0.01)\n\nClassification (decision) trees\nClassification trees, or decision trees, are used in prediction problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome.\nThe first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can’t take the average of categories).\nThe second is that we can no longer use RSS to choose the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the Gini Index and Entropy.\nIn a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The Gini Index is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define \\(\\hat{p}_{j,k}\\) as the proportion of observations in partition \\(j\\) that are of class \\(k\\). The Gini Index is defined as:\n\\[\n\\mbox{Gini}(j) = \\sum_{k = 1}^K \\hat{p}_{j,k}(1-\\hat{p}_{j,k})\n\\]\nIf you study the formula carefully, you will see that it is in fact 0 in the perfect scenario described above.\nEntropy is a related quantity, defined as:\n\\[\n\\mbox{entropy}(j) = -\\sum_{k = 1}^K \\hat{p}_{j,k}\\log(\\hat{p}_{j,k}), \\mbox{ with } 0 \\times \\log(0) \\mbox{ defined as }0\n\\]\nIf we use a classification tree on the 2 or 7 example, we achieve an accuracy of 0.81 which is better than regression, but is not as good as what we achieved with kernel methods.\nThe plot of the estimated conditional probability shows us the limitations of classification trees:\n\n\n\n\n\n\n\n\nNote that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity.\nClassification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are also easy to visualize (_if smal_Tl enough). Finally, they can model human decision processes and don’t require use of dummy predictors for categorical variables. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random forests, explained next, improve on several of these shortcomings.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Examples of Algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#random-forests",
    "href": "ml/algorithms.html#random-forests",
    "title": "31  Examples of Algorithms",
    "section": "\n31.5 Random forests",
    "text": "31.5 Random forests\nRandom forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees: a forest of trees constructed with randomness. It has two features that help accomplish this.\nThe first step is bootstrap aggregation or bagging. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest. The specific steps are as follows.\n1. Build \\(B\\) decision trees using the training set. We refer to the fitted models as \\(T_1, T_2, \\dots, T_B\\).\n2. For every observation in the test set, form a prediction \\(\\hat{y}_j\\) using tree \\(T_j\\).\n3. For continuous outcomes, form a final prediction with the average \\(\\hat{y} = \\frac{1}{B} \\sum_{j = 1}^B \\hat{y}_j\\). For categorical data classification, predict \\(\\hat{y}\\) with majority vote (most frequent class among \\(\\hat{y}_1, \\dots, \\hat{y}_B\\)).\nSo how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let \\(N\\) be the number of observations in the training set. To create \\(T_j, \\, j = 1,\\ldots,B\\) from the training set we do the following:\n1. Create a bootstrap training set by sampling \\(N\\) observations from the training set with replacement. This is the first way to induce randomness.\n2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.\nTo illustrate how the first steps can result in smoother estimates, we will fit a random forest to the 2008 polls data. We will use the randomForest function in the randomForest package:\n\nlibrary(randomForest)\nfit &lt;- randomForest(margin ~ ., data = polls_2008) \n\nNote that if we apply the function plot to the resulting object, we see how the error rate of our algorithm changes as we add trees:\n\nplot(fit)\n\n\n\n\n\n\n\n\n\nIn this case, the accuracy improves as we add more trees until we have used about 300 trees after which accuracy stabilizes.\nThe resulting estimate for this random forest, obtained with\n\ny_hat &lt;-  predict(fit, newdata = polls_2008)\n\nis shown with the red curve below:\n\n\n\n\n\n\n\n\nNotice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step functions can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure, you see each of the bootstrap samples for several values of \\(b\\) and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.\n\n\n\n\n\n\n\n\n\nlibrary(randomForest)\ntrain_rf &lt;- randomForest(y ~ ., data = mnist_27$train)\n\nThe accuracy for the random forest fit for our 2 or 7 example is 0.825. Here is what the conditional probabilities look like:\n\n\n\n\n\n\n\n\nVisualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum, the smoother the final estimate will be. If we use a node size of 31, the number of neighbors we used with kNN, we get an accuracy of 0.845. The selected model improves accuracy and provides a smoother estimate:\n\n\n\n\n\n\n\n\nVariable importance\nRandom forest performs better than trees in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine variable importance. To define variable importance, we count how often a predictor is used in the individual trees. You can learn more about variable importance in an advanced machine learning book3. The caret package includes the function varImp that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Examples of Algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#exercises",
    "href": "ml/algorithms.html#exercises",
    "title": "31  Examples of Algorithms",
    "section": "\n31.6 Exercises",
    "text": "31.6 Exercises\n1. Create a dataset using the following code:\n\nn &lt;- 100\nSigma &lt;- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nUse the caret package to partition into a test and training set of equal size. Train a linear model and report the RMSE. Repeat this exercise 100 times and make a histogram of the RMSEs and report the average and standard deviation. Hint: adapt the code shown earlier like this:\n\nlibrary(caret)\ny &lt;- dat$y\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntrain_set &lt;- dat |&gt; slice(-test_index)\ntest_set &lt;- dat |&gt; slice(test_index)\nfit &lt;- lm(y ~ x, data = train_set)\ny_hat &lt;- fit$coef[1] + fit$coef[2]*test_set$x\nsqrt(mean((y_hat - test_set$y)^2))\n\nand put it inside a call to replicate.\n2. Now we will repeat the above but using larger datasets. Repeat exercise 1 but for datasets with n &lt;- c(100, 500, 1000, 5000, 10000). Save the average and standard deviation of RMSE from the 100 repetitions. Hint: use the sapply or map functions.\n3. Describe what you observe with the RMSE as the size of the dataset becomes larger.\n\nOn average, the RMSE does not change much as n gets larger, while the variability of RMSE does decrease.\nBecause of the law of large numbers, the RMSE decreases: more data, more precise estimates.\n\nn = 10000 is not sufficiently large. To see a decrease in RMSE, we need to make it larger.\nThe RMSE is not a random variable.\n\n4. Now repeat exercise 1, but this time make the correlation between x and y larger by changing Sigma like this:\n\nn &lt;- 100\nSigma &lt;- 9*matrix(c(1, 0.95, 0.95, 1), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nRepeat the exercise and note what happens to the RMSE now.\n5. Which of the following best explains why the RMSE in exercise 4 is so much lower than exercise 1:\n\nIt is just luck. If we do it again, it will be larger.\nThe Central Limit Theorem tells us the RMSE is normal.\nWhen we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y. This correlation has a much bigger effect on RMSE than n. Large n simply provide us more precise estimates of the linear model coefficients.\nThese are both examples of regression, so the RMSE has to be the same.\n\n6. Create a dataset using the following code:\n\nn &lt;- 1000\nSigma &lt;- matrix(c(1, 3/4, 3/4, 3/4, 1, 0, 3/4, 0, 1), 3, 3)\ndat &lt;- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"y\", \"x_1\", \"x_2\"))\n\nNote that y is correlated with both x_1 and x_2, but the two predictors are independent of each other.\n\ncor(dat)\n\nUse the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2. Train a linear model and report the RMSE.\n7. Repeat exercise 6 but now create an example in which x_1 and x_2 are highly correlated:\n\nn &lt;- 1000\nSigma &lt;- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)\ndat &lt;- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"y\", \"x_1\", \"x_2\"))\n\nUse the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2 Train a linear model and report the RMSE.\n8. Compare the results in 6 and 7 and choose the statement you agree with:\n\nAdding extra predictors can improve RMSE substantially, but not when they are highly correlated with another predictor.\nAdding extra predictors improves predictions equally in both exercises.\nAdding extra predictors results in over fitting.\nUnless we include all predictors, we have no predicting power.\n\n9. Define the following dataset:\n\nmake_data &lt;- function(n = 1000, p = 0.5, \n                      mu_0 = 0, mu_1 = 2, \n                      sigma_0 = 1,  sigma_1 = 1){\n  y &lt;- rbinom(n, 1, p)\n  f_0 &lt;- rnorm(n, mu_0, sigma_0)\n  f_1 &lt;- rnorm(n, mu_1, sigma_1)\n  x &lt;- ifelse(y == 1, f_1, f_0)\n  test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\n  list(train = data.frame(x = x, y = as.factor(y)) |&gt; \n         slice(-test_index),\n       test = data.frame(x = x, y = as.factor(y)) |&gt; \n         slice(test_index))\n}\n\nNote that we have defined a variable x that is predictive of a binary outcome y.\n\ndat$train |&gt; ggplot(aes(x, color = y)) + geom_density()\n\nCompare the accuracy of linear regression and logistic regression.\n10. Repeat the simulation from exercise 9 100 times and compare the average accuracy for each method and notice they give practically the same answer.\n11. Generate 25 different datasets changing the difference between the two class: delta &lt;- seq(0, 3, len = 25). Plot accuracy versus delta.\n12. We can see what the data looks like if we add 1s to our 2 or 7 examples using this code:\n\nlibrary(dslabs)\nmnist_127$train |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point()\n\nFit QDA using the qda function in the MASS package then create a confusion matrix for predictions on the test. Which of the following best describes the confusion matrix:\n\nIt is a two-by-two table.\nBecause we have three classes, it is a two-by-three table.\nBecause we have three classes, it is a three-by-three table.\nConfusion matrices only make sense when the outcomes are binary.\n\n13. The byClass component returned by the confusionMatrix object provides sensitivity and specificity for each class. Because these terms only make sense when data is binary, each row represents sensitivity and specificity when a particular class is 1 (positives) and the other two are considered 0s (negatives). Based on the values returned by confusionMatrix, which of the following is the most common mistake:\n\nCalling 1s either a 2 or 7.\nCalling 2s either a 1 or 7.\nCalling 7s either a 1 or 2.\nAll mistakes are equally common.\n\n\nCreate a grid of x_1 and x_2 using:\n\n\nGS &lt;- 150\nnew_x &lt;- with(mnist_127$train,\n  expand.grid(x_1 = seq(min(x_1), max(x_1), len = GS),\n              x_2 = seq(min(x_2), max(x_2), len = GS)))\n\nthen visualize the decision rule by coloring the regions of the Cartesian plan to represent the label that would be called in that region.\n14. Repeat exercise 13 but for LDA. Which of the following explains why LDA has worse accuracy:\n\nLDA separates the space with lines making it too rigid.\nLDA divides the space into two and there are three classes.\nLDA is very similar to QDA the difference is due to chance.\nLDA can’t be used with more than one class.\n\n15. Now repeat exercise 13 for kNN with \\(k=31\\) and compute and compare the overall accuracy for all three methods.\n16. To understand how a simple method like kNN can outperform a model that explicitly tries to emulate Bayes’ rule, explore the conditional distributions of x_1 and x_2 to see if the normal approximation holds. Generative models can be very powerful, but only when we are able to successfully approximate the joint distribution of predictors conditioned on each class.\n17. Earlier we used logistic regression to predict sex from height. Use kNN to do the same. Use the code described in this chapter to select the \\(F_1\\) measure and plot it against \\(k\\). Compare to the \\(F_1\\) of about 0.6 we obtained with regression.\n18. Create a simple dataset where the outcome grows 0.75 units on average for every increase in a predictor:\n\nn &lt;- 1000\nsigma &lt;- 0.25\nx &lt;- rnorm(n, 0, 1)\ny &lt;- 0.75 * x + rnorm(n, 0, sigma)\ndat &lt;- data.frame(x = x, y = y)\n\nUse rpart to fit a regression tree and save the result to fit.\n19. Plot the final tree so that you can see where the partitions occurred.\n20. Make a scatterplot of y versus x along with the predicted values based on the fit.\n21. Now model with a random forest instead of a regression tree using randomForest from the randomForest package, and remake the scatterplot with the prediction line.\n22. Use the function plot to see if the random forest has converged or if we need more trees.\n23. It seems that the default values for the random forest result in an estimate that is too flexible (not smooth). Re-run the random forest but this time with nodesize set at 50 and maxnodes set at 25. Remake the plot.\n24. This dslabs dataset includes the tissue_gene_expression with a matrix x:\n\nlibrary(dslabs)\ndim(tissue_gene_expression$x)\n\nwith the gene expression measured on 500 genes for 189 biological samples representing seven different tissues. The tissue type is stored in tissue_gene_expression$y.\n\ntable(tissue_gene_expression$y)\n\nFit a random forest using the randomForest function in the package randomForest. Then use the varImp function to see which are the top 10 most predictive genes. Make a histogram of the reported importance to get an idea of the distribution of the importance values.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Examples of Algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#footnotes",
    "href": "ml/algorithms.html#footnotes",
    "title": "31  Examples of Algorithms",
    "section": "",
    "text": "https://web.stanford.edu/~hastie/Papers/ESLII.pdf↩︎\nhttps://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid = 1759289&mirid = 1&type = 2↩︎\nhttps://web.stanford.edu/~hastie/Papers/ESLII.pdf↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Examples of Algorithms</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html",
    "href": "ml/clustering.html",
    "title": "32  Clustering",
    "section": "",
    "text": "32.1 Hierarchical clustering\nThe algorithms we have described so far are examples of a general approach referred to as supervised machine learning. The term supervised comes from the fact that the outcomes in the training set are used to supervise the construction of a prediction algorithm.\nIn contrast, there is another branch of machine learning known as unsupervised learning. In this setting, the outcomes are not provided, and the goal is instead to uncover structure in the data, often in the form of groups. These methods are also referred to as clustering algorithms because they group observations into clusters based on similarities in their predictors.\nIn the two examples we have considered so far, clustering would not be very effective. For example, given only heights, we would not expect an algorithm to cleanly discover two groups (male and female) because the overlap is substantial. Similarly, in the digit classification example, the distributions of twos and sevens overlap enough to make clustering challenging.\nHowever, there are many applications in which unsupervised learning is a powerful tool, particularly for exploratory data analysis. For example, clustering can reveal hidden subgroups in biological, social, or market data that may not be obvious at first glance.\nA first step in any clustering algorithm is defining a distance (or similarity) between observations or groups of observations. Once a distance is defined, the next step is to decide how to join observations into clusters. There are many algorithms for doing this. In this chapter, we introduce two common approaches: hierarchical clustering and k-means clustering.\nA first step in any clustering algorithm is defining a distance between observations or groups of observations. Then we need to decide how to join observations into clusters. There are many algorithms for doing this. Here we introduce two as examples: hierarchical and k-means.\nWe startin by constructing a simple example based on movie ratings. Here we quickly construct a matrix y that has ratings for the 50 movies with the most ratings among users with at least 100 ratings:\nNotice that if we compute distances directly from the original ratings, movies with generally high ratings will tend to be close to each other, as will movies with generally low ratings. However, this is not what we are actually interested in. Instead, we want to capture how ratings correlate across users for the 263 different users. To achieve this, we center the ratings for each movie by removing the movie effect:\nAs described in Chapter 24, many of the entries are missing because not every user rates every movie, so we use the argument na.rm = TRUE.\nWith the centered data, we can now cluster movies based on their rating patterns. The first step is to compute distances between each pair of movies using the dist function:\nThe dist function automatically accounts for missing values and standardizes the distance measurements, ensuring that the computed distance between two movies does not depend on the number of ratings available.\nWith the distance between each pair of movies computed, we need an algorithm to define groups, based on these distances. Hierarchical clustering starts by defining each observation as a separate group, then the two closest groups are joined into new groups. We then continue joining the closest groups into new groups iteratively until there is just one group including all the observations. The hclust function implements this algorithm and takes a distance as input.\nh &lt;- hclust(d)\nWe can see the resulting groups using a dendrogram. The function plot applied to an hclust object creates a dendrogram:\nplot(h, cex = 0.65, main = \"\", xlab = \"\")\nThis graph gives us an approximation between the distance between any two movies. To find this distance, we find the first location, from top to bottom, where these movies split into two different groups. The height of this location is the distance between these two groups. So, for example, the distance between the three Star Wars movies is 8 or less, while the distance between Raiders of the Lost of Ark and Silence of the Lambs is about 17.\nTo generate actual groups, we can do one of two things: 1) decide on a minimum distance needed for observations to be in the same group or 2) decide on the number of groups you want and then find the minimum distance that achieves this. The function cutree can be applied to the output of hclust to perform either of these two operations and generate groups.\ngroups &lt;- cutree(h, k = 10)\nNote that the clustering provides some insights into types of movies. Group 2 appears to be critically aclaimed movies:\nnames(groups)[groups == 2]\n#&gt; [1] \"American Beauty\"      \"Fargo\"                \"Godfather, The\"      \n#&gt; [4] \"Pulp Fiction\"         \"Silence of the La...\" \"Usual Suspects, The\"\nAnd Group 9 appears to be nerd movies:\nnames(groups)[groups == 9]\n#&gt; [1] \"Lord of the Rings...\" \"Lord of the Rings...\" \"Lord of the Rings...\"\n#&gt; [4] \"Star Wars IV - A ...\" \"Star Wars V - The...\" \"Star Wars VI - Re...\"\nWe can change the size of the group by either making k larger or h smaller.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html#heatmaps",
    "href": "ml/clustering.html#heatmaps",
    "title": "32  Clustering",
    "section": "\n32.2 Heatmaps",
    "text": "32.2 Heatmaps\nA powerful visualization tool for discovering clusters or patterns in your data is the heatmap. The idea is simple: plot an image of your data matrix with colors used as the visual cue and both the columns and rows ordered according to the results of a clustering algorithm. We will demonstrate this with the tissue_gene_expression dataset in dslabs.\nWe start by scaling the columns of the gene expression matrix because we care about relative differences in gene expression. After scaling, we compute perform clustering on both the observations and the predictors:\n\nlibrary(dslabs)\ny &lt;- sweep(tissue_gene_expression$x, 2, colMeans(tissue_gene_expression$x))\nh_1 &lt;- hclust(dist(y))\nh_2 &lt;- hclust(dist(t(y)))\n\nNote that we have performed two clusterings: we clustered samples (h_1) and genes (h_2).\nNow we can use the results of this clustering to order the rows and columns:\n\nimage(y[h_1$order, h_2$order])\n\nThe heatmap function that does all this for us:\n\nheatmap(y, col = RColorBrewer::brewer.pal(11, \"Spectral\"))\n\nNote we do not show the results of the heatmap function because there are too many features for the plot to be useful. We will therefore filter some columns and remake the plots.\nIf only a few features are different between clusters, including all the features can add enough noise that making cluster detection challenging. A simple approach to avoid this is to assume low variability features are not informative and include only high variance features. For example, in the movie example, users with low variance in their ratings are not really distinguishing movies: all the movies seem about the same to them.\nHere is an example code showing how we can include only the features with high variance in a heatmap:\n\nlibrary(matrixStats)\nsds &lt;- colSds(y, na.rm = TRUE)\no &lt;- order(sds, decreasing = TRUE)[1:25]\nheatmap(y[,o], col = RColorBrewer::brewer.pal(11, \"Spectral\"))\n\n\n\n\n\n\n\nNote there are several other heatmap functions in R. A popular example is the heatmap.2 in the gplots package.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html#k-means",
    "href": "ml/clustering.html#k-means",
    "title": "32  Clustering",
    "section": "\n32.3 k-means",
    "text": "32.3 k-means\nWe have so far described just one clustering algorithm: hierarchical clustering. However, there are many others that take quite different approaches. To demonstrate another, we now introduce the k-means clustering algorithm.\nThe k-means algorithm requires us to predefine $k$, the number of clusters we want to create. Once $k$ is set, the algorithm proceeds iteratively:\n\n\nInitialization: Define \\(k\\) centers (chosen at random).\n\nAssignment step: Each observation is assigned to the cluster with the closest center.\n\nUpdate step: The centers are redefined by taking the mean of the observations in each cluster. These new centers are called centroids.\n\nIteration: Steps 2 and 3 are repeated until the centers stabilize (converge).\n\nWe can run k-means to see if it can discover the different tissues:\n\nset.seed(2001)\nk &lt;- kmeans(y, centers = 7)\n\nThe cluster assignments are stored in the cluster component:\n\nk$cluster[1:5]\n#&gt; cerebellum_1 cerebellum_2 cerebellum_3 cerebellum_4 cerebellum_5 \n#&gt;            7            7            7            7            7\n\nBecause the initial centers are chosen at random (hence the use of set.seed), the resulting clusters can vary. To improve stability, we can repeat the process multiple times with different random starting points and keep the best result. The number of random starts is set with the nstart argument:\n\nk &lt;- kmeans(y, centers = 7, nstart = 100)\n\nWe can evaluate how well the clusters match the known tissue labels:\n\ntable(tissue_gene_expression$y, k$cluster)\n#&gt;              \n#&gt;                1  2  3  4  5  6  7\n#&gt;   cerebellum  33  0  0  0  0  5  0\n#&gt;   colon        0 34  0  0  0  0  0\n#&gt;   endometrium  0  0  0 15  0  0  0\n#&gt;   hippocampus  0  0  0  0  0 31  0\n#&gt;   kidney       0  1  0  0 38  0  0\n#&gt;   liver        0  0 26  0  0  0  0\n#&gt;   placenta     0  0  0  0  0  0  6\n\nThe results show that k-means did a reasonably good job at recovering the tissue groups. For example, clusters 1, 3, 4, 5, and 7 correspond to cerebellum, liver, endometrium, kidney, and placenta, respectively. All hippocampus samples are grouped in cluster 6, though five cerebellum samples are incorrectly included there. Cluster 2 captures almost all colon samples, with just one kidney misclassified.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html#exercises",
    "href": "ml/clustering.html#exercises",
    "title": "32  Clustering",
    "section": "\n32.4 Exercises",
    "text": "32.4 Exercises\n1. Load the tissue_gene_expression dataset. Remove the row means and compute the distance between each observation. Store the result in d.\n2. Make a hierarchical clustering plot and add the tissue types as labels.\n3. Run a k-means clustering on the data with \\(K=7\\). Make a table comparing the identified clusters to the actual tissue types. Run the algorithm several times to see how the answer changes.\n4. Make a heatmap of the 50 most variable genes. Make sure the observations show up in the columns, that the predictors are centered, and add a color bar to show the different tissue types. Hint: use the ColSideColors argument to assign colors. Also, use col = RColorBrewer::brewer.pal(11, \"RdBu\") for a better use of colors.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html",
    "href": "ml/ml-in-practice.html",
    "title": "33  Machine Learning in Practice",
    "section": "",
    "text": "33.1 The caret package\nNow that we have learned several methods and explored them with simple examples, we will try them out on a real example: the MNIST digits.\nWe can load this data using the following dslabs package function:\nThe dataset includes two components, a training set and a test set:\nEach of these components includes a matrix with features in the columns:\nand vector with the classes as integers:\nBecause we want this example to run on a small laptop and in less than one hour, we will consider a subset of the dataset. We will sample 10,000 random rows from the training set and 1,000 random rows from the test set:\nWhen fitting models to large datasets, we recommend using matrices instead of data frames, as matrix operations tend to be faster. In the caret package, predictor matrices must have column names to track features accurately during prediction on the test set. If the matrices lack column names, you can assign names based on their position:\nWe have already learned about several machine learning algorithms. Many of these algorithms are implemented in R. However, they are distributed via different packages, developed by different authors, and often use different syntax. The caret package tries to consolidate these differences and provide consistency. It currently includes over 200 different methods which are summarized in the caret package manual1. Keep in mind that caret does not include the packages needed to run each possible algorithm. To apply a machine learning method through caret you still need to install the library that implement the method. The required packages for each method are described in the package manual.\nThe caret package also provides a function that performs cross validation for us. Here we provide some examples showing how we use this helpful package. We will first use the 2 or 7 example to illustrate and, in later sections, we use the package to run algorithms on the larger MNIST dataset.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#sec-caret",
    "href": "ml/ml-in-practice.html#sec-caret",
    "title": "33  Machine Learning in Practice",
    "section": "",
    "text": "The train function\nThe R functions that fit machine algorithms are all slightly different. Functions such as lm, glm, qda, lda, knn3, rpart and randomForest use different syntax, have different argument names and produce objects of different types.\nThe caret train function lets us train different algorithms using similar syntax. So, for example, we can type the following to train three different models:\n\nlibrary(caret)\ntrain_glm &lt;- train(y ~ ., method = \"glm\", data = mnist_27$train)\ntrain_qda &lt;- train(y ~ ., method = \"qda\", data = mnist_27$train)\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)\n\nAs we explain in more detail in Section 33.1.3, the train function selects parameters for you using a resampling method to estimate the MSE, with bootstrap as the default.\nThe predict function\nThe predict function is one of the most useful tools in R for machine learning and statistical modeling. It takes a fitted model object and a data frame containing the features \\(\\mathbf{x}\\) for which we want to make predictions, then returns the corresponding predicted values.\nHere is an example with logistic regression:\n\nfit &lt;- glm(y ~ ., data = mnist_27$train, family = \"binomial\")\np_hat &lt;- predict(fit, newdata = mnist_27$test)\n\nIn this case, the function is simply computing\n\\[\n\\hat{p}(\\mathbf{x}) = g^{-1}\\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 \\right) \\text{ with } g(p) = \\log\\frac{p}{1-p} \\implies g^{-1}(\\mu) = \\frac{1}{1-e^{-\\mu}}\n\\]\nfor the x_1 and x_2 in the test set mnist_27$test. With these estimates in place, we can make our predictions and compute our accuracy:\n\ny_hat &lt;- factor(ifelse(p_hat &gt; 0.5, 7, 2))\n\nHowever, note that predict does not always return objects of the same type; it depends on what type of object it is applied to. To learn about the specifics, you need to look at the help file specific for the type of fit object that is being used.\npredict is actually a special type of function in R called a generic function. Generic functions call other functions depending on what kind of object it receives. So if predict receives an object coming out of the lm function, it will call predict.lm. If it receives an object coming out of glm, it calls predict.glm. If the fit is from knn3, it calls predict.knn3, and so on. These functions are similar but not exactly. You can learn more about the differences by reading the help files:\n\n?predict.glm\n?predict.qda\n?predict.knn3\n\nThere are many other versions of predict and many machine learning algorithms define their own predict function.\nAs with train, the caret packages unifies the use of predict with the function predict.train. This function takes the output of train and produces prediction of categories or estimates of \\(p(\\mathbf{x})\\).\nThe code looks the same for all methods:\n\ny_hat_glm &lt;- predict(train_glm, mnist_27$test, type = \"raw\")\ny_hat_qda &lt;- predict(train_qda, mnist_27$test, type = \"raw\")\ny_hat_knn &lt;- predict(train_knn, mnist_27$test, type = \"raw\")\n\nThis permits us to quickly compare the algorithms. For example, we can compare the accuracy like this:\n\nfits &lt;- list(glm = y_hat_glm, qda = y_hat_qda, knn = y_hat_knn)\nsapply(fits, function(f) confusionMatrix(f, mnist_27$test$y)$overal[[\"Accuracy\"]])\n#&gt;   glm   qda   knn \n#&gt; 0.775 0.815 0.835\n\nResampling\nWhen an algorithm includes a tuning parameter, train automatically uses a resampling method to estimate MSE and decide among a few default candidate values. To find out what parameter or parameters are optimized, you can read the caret manual2 or study the output of:\n\nmodelLookup(\"knn\")\n\nTo obtain all the details of how caret implements kNN you can use:\n\ngetModelInfo(\"knn\")\n\nIf we run it with default values:\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)\n\nyou can quickly see the results of the cross validation using the ggplot function. The argument highlight highlights the max:\n\nggplot(train_knn, highlight = TRUE)\n\n\n\n\n\n\n\nBy default, the resampling is performed by taking 25 bootstrap samples, each comprised of 25% of the observations.\nFor the knn method, the default is to try \\(k=5,7,9\\). We change this using the tuneGrid argument. The grid of values must be supplied by a data frame with the parameter names as specified in the modelLookup output.\nHere we present an example where we try out 38 values between 1 and 75. To do this with caret, we need to define a column named k, so we use this: data.frame(k = seq(1, 75, 2)). Note that when running this code, we are fitting 38 versions of kNN to 25 bootstrapped samples. Since we are fitting \\(38 \\times 25 = 950\\) kNN models, running this code will take several seconds.\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(1, 75, 2)))\nggplot(train_knn, highlight = TRUE)\n\n\n\n\n\n\n\n\nBecause resampling methods are random procedures, the same code can result in different results. To assure reproducible results you should set the seed, as we did at the start of this chapter.\n\nTo access the parameter that maximized the accuracy, you can use this:\n\ntrain_knn$bestTune\n#&gt;     k\n#&gt; 37 73\n\nand the best performing model like this:\n\n#&gt; 73-nearest neighbor model\n#&gt; Training set outcome distribution:\n#&gt; \n#&gt;   2   7 \n#&gt; 401 399\n\nThe function predict will use this best performing model. Here is the accuracy of the best model when applied to the test set, which we have not yet used because the cross validation was done on the training set:\n\nconfusionMatrix(predict(train_knn, mnist_27$test, type = \"raw\"),\n                mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.825\n\nBootstrapping is not always the best approach to resampling (see Section 33.4, for an example). If we want to change our resampling method, we can use the trainControl function. For example, the code below runs 10-fold cross validation. This means we have 10 samples using 90% of the observations to train in each sample. We accomplish this using the following code:\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10, p = .9)\ntrain_knn_cv &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(1, 71, 2)),\n                   trControl = control)\n\n\n\n\n\n\n\nThe results component of the train output includes several summary statistics related to the variability of the cross validation estimates:\n\nnames(train_knn$results)\n#&gt; [1] \"k\"          \"Accuracy\"   \"Kappa\"      \"AccuracySD\" \"KappaSD\"\n\nYou can learn many more details about the caret package, from the manual3.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#preprocessing",
    "href": "ml/ml-in-practice.html#preprocessing",
    "title": "33  Machine Learning in Practice",
    "section": "\n33.2 Preprocessing",
    "text": "33.2 Preprocessing\nWe often transform predictors before running the machine algorithm. We also remove predictors that are clearly not useful. We call these steps preprocessing.\nExamples of preprocessing include standardizing the predictors, taking the log transform of some predictors, removing predictors that are highly correlated with others, and removing predictors with very few non-unique values or close to zero variation.\nFor example, we can run the nearZeroVar function from the caret package to see that several features do not vary much from observation to observation. We can see that there is a large number of features with close to 0 variability:\n\nlibrary(matrixStats)\nsds &lt;- colSds(x)\nhist(sds, breaks = 256)\n\n\n\n\n\n\n\nThis is expected because there are parts of the image that rarely contain writing (dark pixels).\nThe caret packages includes a function that recommends features to be removed due to near zero variance:\n\nnzv &lt;- nearZeroVar(x)\n\nWe can see the columns recommended for removal are the near the edges:\n\nimage(matrix(1:784 %in% nzv, 28, 28))\n\n\n\n\n\n\n\n\n\nSo we end up removing length(nzv) = 532 redictors.\nThe caret package features the preProcess function, which allows users to establish a predefined set of preprocessing operations based on a training set. This function is designed to apply these operations to new datasets without recalculating anything on the test set, ensuring that all preprocessing steps are consistent and derived solely from the training data.\nBelow is an example demonstrating how to remove predictors with near-zero variance and then center the remaining predictors:\n\npp &lt;- preProcess(x, method = c(\"nzv\", \"center\"))\ncentered_subsetted_x_test &lt;- predict(pp, newdata = x_test)\ndim(centered_subsetted_x_test)\n#&gt; [1] 1000  252\n\nAdditionally, the train function in caret includes a preProcess argument that allows users to specify which preprocessing steps to apply automatically during model training. We will explore this functionality further in the context of a k-Nearest Neighbors model in Section 33.4.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#parallelization",
    "href": "ml/ml-in-practice.html#parallelization",
    "title": "33  Machine Learning in Practice",
    "section": "\n33.3 Parallelization",
    "text": "33.3 Parallelization\nDuring cross-validation or bootstrapping, the process of fitting models to different samples or using varying parameters can be performed independently. Imagine you are fitting 100 models; if you had access to 100 computers, you could theoretically speed up the process by a factor of 100 by fitting each model on a separate computer and then aggregating the results. In reality, most modern computers, including many personal computers, are equipped with multiple processors that allow for such parallel execution. This method, known as parallelization, leverages these multiple processors to conduct several computational tasks simultaneously, significantly accelerating the model training process. By distributing the workload across different processors, parallelization makes it feasible to manage large datasets and complex modeling procedures efficiently.\nThe caret package is designed to take advantage of parallel processing, but you need to explicitly tell R to run tasks in parallel. To do this, we use the doParallel package:\n\nlibrary(doParallel)\n#&gt; Loading required package: foreach\n#&gt; Loading required package: iterators\n#&gt; Loading required package: parallel\nnc &lt;- detectCores() - 1   # it is convention to leave 1 core for the OS\ncl &lt;- makeCluster(nc)\nregisterDoParallel(cl)\n\nIf you do use parallelization, make sure to let R know you are done with the following lines of code:\n\nstopCluster(cl)\nstopImplicitCluster()\n\n\n\n\n\n\n\nWhen parallelizing tasks across multiple processors, it’s important to consider the risk of running out of memory. Each processor might require a copy of the data or substantial portions of it, which can multiply overall memory demands. This is especially challenging if the data or models are large.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#sec-knn-in-practice",
    "href": "ml/ml-in-practice.html#sec-knn-in-practice",
    "title": "33  Machine Learning in Practice",
    "section": "\n33.4 k-nearest neighbors",
    "text": "33.4 k-nearest neighbors\nIn Section Section 30.1, we introduced the k-nearest neighbors (kNN) algorithm and described its intuition and mathematical formulation. Here, we focus on the practical aspects of applying kNN to real data. We will explore how to train and test kNN models in R, select an appropriate value of k, and evaluate performance using cross-validation and tuning techniques.\n\n\n\n\n\n\nBefore starting this section, note that the first two calls to the train function in the code below can take several hours to run. This is a common challenge when training machine learning algorithms since we have to run the algorithm for each cross validation split and each set of tuning parameters being considered. In the next section, we will provide some suggestions on how to predict the duration of the process and ways to reduce.\n\n\n\nLimitations of Bootstrapping for kNN\nAs we will see soon, the optimal \\(k\\) for the MNIST data is between 1 and 7. For small values of \\(k\\), bootstrapping can be problematic for estimating MSE for a k-Nearest Neighbors (kNN). This is because bootstrapping involves sampling with replacement from the original dataset which implies the closest neighbor will often appear twice but considered two independent observations by kNN. This is a unrealistic scenario that can distort the estimate MSE when, for example, \\(k=3\\). We therefore use cross-validation to estimate our MSE.\nThe first step is to optimize for \\(k\\).\n\ntrain_knn &lt;- train(x, y, method = \"knn\", \n                   preProcess = \"nzv\",\n                   trControl = trainControl(\"cv\", number = 20, p = 0.95),\n                   tuneGrid = data.frame(k = seq(1, 7, 2)))\n\nOnce we optimize our algorithm, the predict function defaults to using the best performing algorithm fit with the entire training data:\n\ny_hat_knn &lt;- predict(train_knn, x_test, type = \"raw\")\n\nWe achieve relatively high accuracy:\n\nconfusionMatrix(y_hat_knn, factor(y_test))$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.953\n\nDimension reduction with PCA\nAn alternative to removing low variance columns directly is to use dimension reduction on the feature matrix before applying the algorithms.\n\npca &lt;- prcomp(x)\n\nWe can actually explain, say, 75% of the variability in the predictors with a small number of dimensions:\n\np &lt;- which(cumsum(pca$sdev^2)/sum(pca$sdev^2) &gt;= 0.75)[1]\n\nWe can then re-run our algorithm on these 33 features:\n\nfit_knn_pca &lt;- knn3(pca$x[,1:p], y, k = train_knn$bestTune)\n\nWhen predicting, it is important that we not use the test set when finding the PCs nor any summary of the data, as this could result in overtraining. We therefore compute the averages needed for centering and the rotation on the training set:\n\nnewdata &lt;-  sweep(x_test, 2, colMeans(x)) %*% pca$rotation[,1:p]\ny_hat_knn_pca &lt;- predict(fit_knn_pca, newdata, type = \"class\")\n\nWe obtain similar accuracy, while using only 33 dimensions:\n\nconfusionMatrix(y_hat_knn_pca, factor(y_test))$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.959\n\nIn this example, we used the \\(k\\) optimized for the raw data, not the principal components. Note that to obtain an unbiased MSE estimate we have to recompute the PCA for each cross-validation sample and apply to the validation set. Because the train function includes PCA as one of the available preprocessing operations we can achieve this with this modification of the code above:\n\ntrain_knn_pca &lt;- train(x, y, method = \"knn\", \n                       preProcess = c(\"nzv\", \"pca\"),\n                       trControl = trainControl(\"cv\", number = 20, p = 0.95,\n                                                preProcOptions = list(pcaComp = p)),\n                       tuneGrid = data.frame(k = seq(1, 7, 2)))\ny_hat_knn_pca &lt;- predict(train_knn_pca, x_test, type = \"raw\")\nconfusionMatrix(y_hat_knn_pca, factor(y_test))$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.962\n\nA limitation of this approach is that we don’t get to optimize the number of PCs used in the analysis. To do this we need to write our own method. The caret manual4 describe how to do this.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#random-forest",
    "href": "ml/ml-in-practice.html#random-forest",
    "title": "33  Machine Learning in Practice",
    "section": "\n33.5 Random Forest",
    "text": "33.5 Random Forest\nWith the random forest algorithm several parameters can be optimized, but the main one is mtry, the number of predictors that are randomly selected for each tree. This is also the only tuning parameter that the caret function train permits when using the default implementation from the randomForest package.\n\nlibrary(randomForest)\ntrain_rf &lt;- train(x, y, method = \"rf\", \n                  preProcess = \"nzv\",\n                  tuneGrid = data.frame(mtry = seq(5, 15)))\ny_hat_rf &lt;- predict(train_rf, x_test, type = \"raw\")\n\nNow that we have optimized our algorithm, we are ready to fit our final model:\n\ny_hat_rf &lt;- predict(train_rf, x_test, type = \"raw\")\n\nAs with kNN, we also achieve high accuracy:\n\nconfusionMatrix(y_hat_rf, y_test)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.951\n\nBy optimizing some of the other algorithm parameters, we can achieve even higher accuracy.\nTesting and improving computation time\nThe default method for estimating accuracy used by the train function is to test prediction on 25 bootstrap samples. This can result in long compute times. For example, if we are considering several values, say 10, of the tuning parameters, we will fit the algorithm 250 times. We can use the system.time function to estimate how long it takes to run the algorithm once:\n\nnzv &lt;- nearZeroVar(x)\nsystem.time({fit_rf &lt;- randomForest(x[, -nzv], y,  mtry = 9)})\n#&gt;    user  system elapsed \n#&gt;  60.948   0.665  61.719\n\nand use this to estimate the total time for the 250 iterations. In this case it will be several hours.\nOne way to reduce run time is to use k-fold cross validation with a smaller number of test sets. A popular choice is leaving out 5 test sets with 20% of the data. To use this we set the trControl argument in train to trainControl(method = \"cv\", number = 5, p = .8).\nFor random forest, we can also speed up the training step by running less trees per fit. After running the algorithm once, we can use the plot function to see how the error rate changes as the number of trees grows.\nHere we can see that error rate stabilizes after about 200 trees:\n\nplot(fit_rf)\n\n\n\n\n\n\n\nWe can use this finding to speed up the cross validation procedure. Specifically, because the default is 500, by adding the argument ntree = 200 to the call to train above, the procedure will finish 2.5 times faster.\nVariable importance\nWe described variable importance in Section 31.5.1. The following function computes the importance of each feature:\n\nimp &lt;- importance(fit_rf)\n\nWe can see which features are being used most by plotting an image:\n\nmat &lt;- rep(0, ncol(x))\nmat[-nzv] &lt;- imp\nimage(matrix(mat, 28, 28))",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#diagnostics",
    "href": "ml/ml-in-practice.html#diagnostics",
    "title": "33  Machine Learning in Practice",
    "section": "\n33.6 Diagnostics",
    "text": "33.6 Diagnostics\nAn important part of data analysis is visualizing results to determine why we are failing. How we do this depends on the application. Below we show the images of digits for which we made an incorrect prediction. Here are some errors for the random forest:\n\n\n\n\n\n\n\n\nBy examining errors like this, we often find specific weaknesses to algorithms or parameter choices and can try to correct them.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#ensembles",
    "href": "ml/ml-in-practice.html#ensembles",
    "title": "33  Machine Learning in Practice",
    "section": "\n33.7 Ensembles",
    "text": "33.7 Ensembles\nThe idea of an ensemble is similar to the idea of combining data from different pollsters to obtain a better estimate of the true support for each candidate.\nIn machine learning, one can usually greatly improve the final results by combining the results of different algorithms.\nHere is a simple example where we compute new class probabilities by taking the average of random forest and kNN. We can see that the accuracy improves:\n\np_rf &lt;- predict(fit_rf, x_test[,-nzv], type = \"prob\")  \np_rf &lt;- p_rf / rowSums(p_rf)\np_knn_pca  &lt;- predict(train_knn_pca, x_test, type = \"prob\")\np &lt;- (p_rf + p_knn_pca)/2\ny_pred &lt;- factor(apply(p, 1, which.max) - 1)\nconfusionMatrix(y_pred, y_test)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.968\n\nWe have just built an ensemble with just two algorithms. By combing more similarly performing, but uncorrelated, algorithms we can improve accuracy further.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#exercises",
    "href": "ml/ml-in-practice.html#exercises",
    "title": "33  Machine Learning in Practice",
    "section": "\n33.8 Exercises",
    "text": "33.8 Exercises\n1. In the exercises in Chapter 31 we saw that changing maxnodes or nodesize in the randomForest function improved our estimate. Let’s use the train function to help us pick these values. From the caret manual we see that we can’t tune the maxnodes parameter or the nodesize argument with randomForest, so we will use the Rborist package and tune the minNode argument. Use the train function to try values minNode &lt;- seq(5, 250, 25). See which value minimizes the estimated RMSE.\n2. This dslabs tissue_gene_expression dataset includes a matrix x:\n\nlibrary(dslabs)\ndim(tissue_gene_expression$x)\n\nwith the gene expression measured on 500 genes for 189 biological samples representing seven different tissues. The tissue type is stored in y:\n\ntable(tissue_gene_expression$y)\n\nSplit the data in training and test sets, then use kNN to predict tissue type and see what accuracy you obtain. Try it for \\(k = 1, 3, \\dots, 11\\).\n3. We are going to apply LDA and QDA to the tissue_gene_expression dataset. We will start with simple examples based on this dataset and then develop a realistic example.\nCreate a dataset with just the classes cerebellum and hippocampus (two parts of the brain) and a predictor matrix with 10 randomly selected columns. Estimate the accuracy of LDA.\n\nset.seed(1993)\ntissues &lt;- c(\"cerebellum\", \"hippocampus\")\nind &lt;- which(tissue_gene_expression$y %in% tissues)\ny &lt;- droplevels(tissue_gene_expression$y[ind])\nx &lt;- tissue_gene_expression$x[ind, ]\nx &lt;- x[, sample(ncol(x), 10)]\n\n4. In this case, LDA fits two 10-dimensional normal distributions. Look at the fitted model by looking at the finalModel component of the result of train. Notice there is a component called means that includes the estimate means of both distributions. Plot the mean vectors against each other and determine which predictors (genes) appear to be driving the algorithm.\n5. Repeat exercises 3 with QDA. Does it have a higher accuracy than LDA?\n6. Are the same predictors (genes) driving the algorithm? Make a plot as in exercise 4.\n7. One thing we see in the previous plot is that the value of predictors correlate in both groups: some predictors are low in both groups while others are high in both groups. The mean value of each predictor, colMeans(x), is not informative or useful for prediction and often, for interpretation purposes, it is useful to center or scale each column. This can be achieved with the preProcessing argument in train. Re-run LDA with preProcessing = \"scale\". Note that accuracy does not change but see how it is easier to identify the predictors that differ more between groups in the plot made in exercise 4.\n8. In the previous exercises, we saw that both approaches worked well. Plot the predictor values for the two genes with the largest differences between the two groups in a scatterplot to see how they appear to follow a bivariate distribution as assumed by the LDA and QDA approaches. Color the points by the outcome.\n9. Now we are going to increase the complexity of the challenge slightly: we will consider all the tissue types.\n\nset.seed(1993)\ny &lt;- tissue_gene_expression$y\nx &lt;- tissue_gene_expression$x\nx &lt;- x[, sample(ncol(x), 10)]\n\nWhat accuracy do you get with LDA?\n10. We see that the results are slightly worse. Use the confusionMatrix function to learn what type of errors we are making.\n11. Plot an image of the centers of the seven 10-dimensional normal distributions.\n12. Make a scatterplot along with the prediction from the best fitted model.\n13. Use the rpart function to fit a classification tree to the tissue_gene_expression dataset. Use the train function to estimate the accuracy. Try out cp values of seq(0, 0.05, 0.01). Plot the accuracy to report the results of the best model.\n14. Study the confusion matrix for the best fitting classification tree. What do you observe happening for placenta?\n15. Notice that placentas are called endometrium more often than placenta. Note also that the number of placentas is just six, and that, by default, rpart requires 20 observations before splitting a node. Thus it is not possible with these parameters to have a node in which placentas are the majority. Rerun the above analysis, but this time permit rpart to split any node by using the argument control = rpart.control(minsplit = 0). Does the accuracy increase? Look at the confusion matrix again.\n16. Plot the tree from the best fitting model obtained in exercise 11.\n17. We can see that with just six genes, we are able to predict the tissue type. Now let’s see if we can do even better with a random forest. Use the train function and the rf method to train a random forest. Try out values of mtry ranging from, at least, seq(50, 200, 25). What mtry value maximizes accuracy? To permit small nodesize to grow as we did with the classification trees, use the following argument: nodesize = 1. This will take several seconds to run. If you want to test it out, try using smaller values with ntree. Set the seed to 1990.\n18. Use the function varImp on the output of train and save it to an object called imp.\n19. The rpart model we ran above produced a tree that used just six predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was fit_rpart, we can extract the names like this:\n\nind &lt;- !(fit_rpart$finalModel$frame$var == \"&lt;leaf&gt;\")\ntree_terms &lt;- \n  fit_rpart$finalModel$frame$var[ind] |&gt;\n  unique() |&gt;\n  as.character()\ntree_terms\n\nWhat is the variable importance in the random forest call for these predictors? Where do they rank?\n20. Extract the top 50 predictors based on importance, take a subset of x with just these predictors and apply the function heatmap to see how these genes behave across the tissues. We will introduce the heatmap function in Chapter 32.\n21. Previously, we compared the conditional probability \\(p(\\mathbf{x})\\) given two predictors \\(\\mathbf{x} = (x_1, x_2)^\\top\\) to the fit \\(\\hat{p}(\\mathbf{x})\\) obtained with a machine learning algorithm by making image plots. The following code can be used to make these images and include a curve at the values of \\(x_1\\) and \\(x_2\\) for which the function is \\(0.5\\):\n\nplot_cond_prob &lt;- function(x_1, x_2, p){\n  data.frame(x_1 = x_1, x_2 = x_2, p = p) |&gt;\n    ggplot(aes(x_1, x_2)) +\n    geom_raster(aes(fill = p), show.legend = FALSE) +\n    stat_contour(aes(z = p), breaks = 0.5, color = \"black\") +\n    scale_fill_gradientn(colors = c(\"#F8766D\", \"white\", \"#00BFC4\"))\n}\n\nWe can see the true conditional probability for the 2 or 7 example like this:\n\nwith(mnist_27$true_p, plot_cond_prob(x_1, x_2, p))\n\nFit a kNN model and make this plot for the estimated conditional probability. Hint: Use the argument newdata = mnist_27$train to obtain predictions for a grid points.\n22. Notice that, in the plot made in exercise 1, the boundary is somewhat wiggly. This is because kNN, like the basic bin smoother, does not use a kernel. To improve this we could try loess. By reading through the available models part of the caret manual, we see that we can use the gamLoess method. We need to install the gam package, if we have not done so already. We see that we have two parameters to optimize:\n\nmodelLookup(\"gamLoess\")\n#&gt;      model parameter  label forReg forClass probModel\n#&gt; 1 gamLoess      span   Span   TRUE     TRUE      TRUE\n#&gt; 2 gamLoess    degree Degree   TRUE     TRUE      TRUE\n\nUse cross-validation to pick a span between 0.15 and 0.75. Keep degree = 1. What span does cross-validation select?\n23. Show an image plot of the estimate \\(\\hat{p}(x,y)\\) resulting from the model fit in exercise 22. How does the accuracy compare to that of kNN? Comment on the difference between the estimate obtained with kNN.\n24. Use the mnist_27 training set to build a model with several of the models available from the caret package. For example, you can try these:\n\nmodels &lt;- c(\"glm\", \"lda\",  \"naive_bayes\",  \"svmLinear\", \"gamboost\",  \n            \"gamLoess\", \"qda\", \"knn\", \"kknn\", \"loclda\", \"gam\", \"rf\", \n            \"ranger\",\"wsrf\", \"Rborist\", \"avNNet\", \"mlp\", \"monmlp\", \"gbm\", \n            \"adaboost\", \"svmRadial\", \"svmRadialCost\", \"svmRadialSigma\")\n\nWe have not explained many of these, but apply them anyway using train with all the default parameters. Keep the results in a list. You might need to install some packages. Keep in mind that you will likely get some warnings.\n25. Now that you have all the trained models in a list, use sapply or map to create a matrix of predictions for the test set. You should end up with a matrix with length(mnist_27$test$y) rows and length(models) columns.\n26. Compute accuracy for each model on the test set.\n27. Build an ensemble prediction by majority vote and compute its accuracy.\n28. Earlier we computed the accuracy of each method on the training set and noticed they varied. Which individual methods do better than the ensemble?\n29. It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from cross validation with the training data. Obtain these estimates and save them in an object.\n30. Now let’s only consider the methods with an estimated accuracy of 0.8 when constructing the ensemble. What is the accuracy now?\n31. Note that if two machine algorithms methods predict the same outcome, ensembling them will not change the prediction. For each pair of algorithms compare the percent of observations for which they make the same prediction. Use this to define a function and then use the heatmap function to visualize the results. Hint: use the method = \"binary\" argument in the dist function.\n32. Note that each method can also produce an estimated conditional probability. Instead of majority vote, we can take the average of these estimated conditional probabilities. For most methods, we can the use the type = \"prob\" in the train function. Note that some of the methods require you to use the argument trControl=trainControl(classProbs=TRUE) when calling train. Also, these methods do not work if classes have numbers as names. Hint: change the levels like this:\n\ndat$train$y &lt;- recode_factor(dat$train$y, \"2\"=\"two\", \"7\"=\"seven\")\ndat$test$y &lt;- recode_factor(dat$test$y, \"2\"=\"two\", \"7\"=\"seven\")\n\n33. In this chapter, we illustrated a couple of machine learning algorithms on a subset of the MNIST dataset. Try fitting a model to the entire dataset.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#footnotes",
    "href": "ml/ml-in-practice.html#footnotes",
    "title": "33  Machine Learning in Practice",
    "section": "",
    "text": "https://topepo.github.io/caret/available-models.html↩︎\nhttp://topepo.github.io/caret/available-models.html↩︎\nhttps://topepo.github.io/caret/available-models.html↩︎\nhttps://topepo.github.io/caret/using-your-own-model-in-train.html↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Machine Learning in Practice</span>"
    ]
  },
  {
    "objectID": "ml/reading-ml.html",
    "href": "ml/reading-ml.html",
    "title": "Recommended reading",
    "section": "",
    "text": "Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.\nA foundational text covering the theory and mathematics behind modern machine learning methods, including regression, classification, and ensemble techniques.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in R (3rd ed.). Springer.\nA more accessible companion to The Elements of Statistical Learning, focused on practical implementation in R with extensive code examples.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nA comprehensive reference for probabilistic and Bayesian approaches to machine learning, suitable for readers seeking a deeper mathematical understanding.\nRipley, B. D. (1996). Pattern Recognition and Neural Networks. Cambridge University Press.\nA classic text that bridges traditional statistical modeling with early neural network methods, emphasizing connections between machine learning and statistics.\nWood, S. N. (2017). Generalized Additive Models: An Introduction with R (2nd ed.). CRC Press.\nThe definitive reference for understanding and applying GAMs using the mgcv package. Wood introduces spline-based smoothing, penalization, model selection, and diagnostics with clear R examples.\nWand, M. P. & Jones, M. C. (1995). Kernel Smoothing. Chapman & Hall/CRC.\nA classic text offering a rigorous treatment of kernel smoothing methods, bandwidth selection, and bias-variance trade-offs, suitable for readers seeking a mathematical foundation.\nSimonoff, J. S. (1996). Smoothing Methods in Statistics. Springer.\nA comprehensive overview of smoothing techniques across regression, density estimation, and nonparametric modeling, with intuitive explanations and numerous examples.\nKaufman, L. & Rousseeuw, P. J. (2005).\nFinding Groups in Data: An Introduction to Cluster Analysis.\nWiley.\n— A foundational text on clustering, introducing key methods such as partitioning, hierarchical clustering, and density-based approaches, with practical insights and examples.\nMaechler, M., Rousseeuw, P., Struyf, A., Hubert, M., & Hornik, K. (2025).\ncluster: Cluster Analysis Basics and Extensions.\nR package version 2.1.8. Available at: https://cran.r-project.org/package=cluster\n— The main R package for clustering, providing implementations of many of the methods described in Kaufman & Rousseeuw (2005), along with practical extensions and tools for applied work.\nKuhn, M. & Johnson, K. (2013). Applied Predictive Modeling. Springer.\nA practical guide to building, tuning, and evaluating predictive models in R, with extensive coverage of the caret package.",
    "crumbs": [
      "Machine Learning",
      "Recommended reading"
    ]
  }
]