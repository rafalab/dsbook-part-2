[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preface\nThis is the website for the Statistics and Prediction Algorithms Through Case Studies part of Introduction to Data Science.\nThe website for the Data Wrangling and Visualization with R is here.\nThis book started out as part of the class notes used in the HarvardX Data Science Series1.\nA hardcopy version of the first edition of the book, which combined both parts, is available from CRC Press2.\nA free PDF of the October 24, 2019 version of the book, which combined both parts, is available from Leanpub3.\nThe Quarto code used to generate the book is available on GitHub4. Note that, the graphical theme used for plots throughout the book can be recreated using the ds_theme_set() function from dslabs package.\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0.\nWe make announcements related to the book on Twitter. For updates follow @rafalab.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "https://www.edx.org/professional-certificate/harvardx-data-science↩︎\nhttps://www.routledge.com/Introduction-to-Data-Science-Data-Analysis-and-Prediction-Algorithms-with/Irizarry/p/book/9780367357986?utm_source=author&utm_medium=shared_link&utm_campaign=B043135_jm1_5ll_6rm_t081_1al_introductiontodatascienceauthorshare↩︎\nhttps://leanpub.com/datasciencebook↩︎\nhttps://github.com/rafalab/dsbook-part-2↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The phrase data science began gaining significant popularity around 2012, thanks in part to the publication titled “Data Scientist: The Most Alluring Profession of the 21st Century”1(https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)]. This coincided with the rise of a new kind of endeavor in the technology sector and in academic projects during the 2000s: extracting insights from messy, complex, and large datasets, which had become increasingly prevalent with the advent of digital data storage.\nExamples include combining data from multiple political pollsters to improve election predictions, scraping athletic department websites to evaluate baseball prospects, analyzing movie ratings from millions of streaming service users to make personalized recommendations, developing software to read zip codes by digitizing handwritten digits, and using advanced measurement technologies to understand the molecular causes of diseases. This book is centered around these and other practical examples.\nAchieving success in these instances requires collaboration among experts with complementary skills. In this book, our primary focus is on data analysis. To understand how to analyze data effectively in these examples, we will cover key mathematical concepts. Many of these concepts are not new, some were originally developed for different purposes, but they have proven adaptable and useful across a wide range of applications.\nOver several decades, data analysts have developed ideas, concepts, and methodologies that apply broadly across projects. They have also identified common ways analysts can be misled by apparent patterns in the data, as well as important mathematical truths that are not immediately obvious. This collective wisdom has evolved into the field of Statistics, which offers a mathematical framework to articulate and rigorously assess these ideas. For a data analyst, having a strong foundation in Statistics is essential to avoid repeating mistakes and reinventing methods that are already well understood.\nThere is no shortage of excellent Statistics textbooks describing this framework. In fact, we reference several of them in Recommended Reading sections throughout the book. In this book, however, we emphasize bridging theory and practice, applying statistical concepts to real-world problems through case studies and worked examples. We provide representative case studies that mirror what a practicing data analyst encounters, and we present the R code used to solve each problem. To illustrate different styles of working with data, we use base R, data.table, and the tidyverse, choosing whichever tool is most appropriate for the task at hand.\nThe book is divided into six sections: Summary Statistics, Probability, Statistical Inference, Linear Models, High Dimensional Data, and Machine Learning. While the first two sections use data examples to illustrate concepts, the real-world case studies begin in the third section. Each section comprises several chapters, each designed to fit into a single lecture and accompanied by exercises. All data referenced in the book is available in the dslabs package, and the Quarto source code for the book is available on GitHub^[https://github.com/rafalab/dsbook-part-2].",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "summaries/intro-summaries.html",
    "href": "summaries/intro-summaries.html",
    "title": "Summary statistics",
    "section": "",
    "text": "We start by describing a simple yet powerful data analysis technique: constructing data summaries. Although the approach does not require mathematical models or probability, the motivation for the summaries we describe will later help us understand both these topics.\nIt is common to summarize numerical data using an average. For instance, the quality of a high school may be conveyed by a single figure: the average standardized test score attained by its students. Occasionally, a second number is reported: the standard deviation. For example, you might read a report stating that scores were 680 plus or minus 50, with 50 the standard deviation. The report has summarized the entirety of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list? In this part of the book, we answer these questions and motivate several useful summary statistics and plots, including the average, standard deviation, median, quartiles, histograms, density plots, boxplots, and quantile-quantile plots.",
    "crumbs": [
      "Summary statistics"
    ]
  },
  {
    "objectID": "summaries/distributions.html",
    "href": "summaries/distributions.html",
    "title": "1  Distributions",
    "section": "",
    "text": "1.1 Variable types\nBefore we begin, we clarify that this chapter will not cover distributions in the context of probability, which is the mathematical framework used to model uncertainty. We postpone that discussion until the next part of the book, where we will use it throughout. Here, we focus on distributions as a way to describe and summarize data we’ve already collected. Our goal is to understand the variation and patterns present in a dataset, without yet making probabilistic claims or predictions. The concepts introduced in this chapter, such as histograms, averages, and standard deviations, will later serve as the foundation for understanding probability distributions, where similar tools are used to describe the likely outcomes of random processes.\nTo illustrate the concepts needed to understand distribution and how they relate to summary statistics, we will pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame included in the dslabs package.\nOne way to convey the heights to ET is to simply send him this list of 1050 heights. However, there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights. We examine the female height data in Section 1.12.\nAt this stage, we are thinking of distributions purely as tools for summarizing and describing data. In the next part of the book, we will connect these same ideas to probability, where distributions help us model uncertainty. For now, our goal is simply to explore the patterns and variation present in the data we do have.\nIt turns out that, in some cases, the two summary statistics, the average and the standard deviation are all we need to understand the data. We will learn data visualization techniques that will help us determine when this two-number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.\nKnowing what variable type we are working with is important because it will help us determining the most effective way to summarize our data and display its distribution.\nWe will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.\nWhen data values represent groups instead of numbers, we refer to the data as categorical. Two simple examples are sex (male or female) and US regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers, such as spiciness (mild, medium, hot). These ordered categorical data are referred to as ordinal data.\nNumerical data can be continuous or discrete. A continuous variable can take on any value within a range. For example, if measured with sufficient precision, height is continuous because the difference between two individuals (even twins) can be an arbitrarily small fraction. However, if height is rounded to the nearest inch, it becomes discrete since values must be a whole number. Other examples of discrete data are the number of heads when tossing 10 coins, the number of cigarettes a person smokes a day, or the number of customers visiting a store in an hour. In these cases, the values are countable and cannot take on fractional values. Unlike continuous data, which can be measured with increasing precision, discrete data are limited to distinct, separate values.\nIn practice, discrete variables are often treated as continuous. For example, a digital measuring instrument might record data as 16-bit integers. However, because it is rare to obtain exactly the same reading more than once, it is often convenient to treat the data as continuous. Another example is country population size. Although population counts are always whole numbers, it is uncommon for two jurisdictions to have exactly the same population. For this reason, population size is also commonly treated as continuous.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#sec-pmf-intro",
    "href": "summaries/distributions.html#sec-pmf-intro",
    "title": "1  Distributions",
    "section": "\n1.2 Relative frequency distribution",
    "text": "1.2 Relative frequency distribution\nThe most basic statistical summary of a list of numbers or categories is its distribution. The simplest way to think of a distribution is as a compact description of values with many entries. This concept should not be new for readers of this book. For example, with categorical data, the relative frequency distribution simply describes the proportion of each unique category:\n\\[\np_k = \\frac{\\mbox{number of times category }k \\mbox{ appears in the list}}{n}\n\\]\nwith \\(n\\) the length of the list.\nHere is an example with US state regions:\n\nprop.table(table(state.region))\n#&gt; state.region\n#&gt;     Northeast         South North Central          West \n#&gt;          0.18          0.32          0.24          0.26\n\nWhen the data are discrete numerical values, the relative frequency distribution is defined in the same way, except it is more common to use this notation since it can be considered a function of numerical values:\n\\[\nf(x) = \\frac{\\mbox{number of times value } x \\mbox{ appears in the list}}{n}\n\\]\nHere is the distribution for US state populations rounded to the nearest million, which are discrete numerical values:",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#sec-ecdf-intro",
    "href": "summaries/distributions.html#sec-ecdf-intro",
    "title": "1  Distributions",
    "section": "\n1.3 Empirical cumulative distribution functions",
    "text": "1.3 Empirical cumulative distribution functions\nWhen the data is continuous, the task of constructing a summary based on a distribution is more challenging because reporting the frequency of each entry is not an effective summary: most entries are unique so the relative frequency distribution does not summarize much.\nHere are the relative frequencies of US state populations:\n\n\n\n\n\n\n\n\nSince there are 50 states, each with a unique population value \\(x\\), the plot shows \\(f(x) = \\frac{1}{50} = 0.02\\) for each state.\nTo define a distribution for continuous numeric data we define a function that reports the proportion of the data entries \\(x\\) that are below \\(a\\), for all possible values of \\(a\\). This function is called the empirical cumulative distribution function (eCDF) and often denoted with \\(F\\):\n\\[ F(a) = \\mbox{Proportion of data points that are less than or equal to }a\\]\nHere is the eCDF for state population data above:\n\n\n\n\n\n\n\n\nSimilar to how a frequency table summarizes categorical data, the empirical cumulative distribution function (eCDF) summarizes continuous data. The eCDF provides a clear picture of how the values are distributed and highlights key characteristics of the dataset. For instance, in the population data, we can see that about half of the states have populations greater than 5,000,000, while most states fall below 20,000,000. This type of summary helps us quickly identify medians, ranges, and other important distributional features.\nIn our heights case study, most students reported heights rounded to the nearest inch. However, some did not. For example, one student reported a height of 68.503937007874 inches, and another reported 68.8976377952756 inches. These values correspond to 174 cm and 175 cm converted to inches, respectively, suggesting that some students entered their heights in centimeters and relied on a conversion to inches.\nAs a result the data is continuos and the relative frequency distribution is somewhat confusing:\n\n\n\n\n\n\n\n\nAlthough this approach preserves all the information from the list of observations, it can give misleading impressions if extrapolating insights to a broader population. For example, it might suggest there are no people with heights between 74 and 74.5 inches or that significantly more people are exactly 72 inches tall than 71.5 inches, patterns we know aren’t accurate. In contrast, the eCDF provides a more useful summary:\n\n\n\n\n\n\n\n\nFrom the plot, we can see that 16% of the values are below 65, since \\(F(66)=\\) 0.16, or that 84% of the values are below 72, since \\(F(72)=\\) 0.84, and so on. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.\n\n\n\n\n\n\nThe reason we add the word empirical is because, as we will see in Section 4.1, the cumulative distribution function (CDF) can be defined mathematically, meaning without any data.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#histograms",
    "href": "summaries/distributions.html#histograms",
    "title": "1  Distributions",
    "section": "\n1.4 Histograms",
    "text": "1.4 Histograms\nAlthough the empirical cumulative distribution function (eCDF) is a fundamental concept in statistics, it is not commonly used in practice. The main reason is that it does not easily highlight key distribution characteristics, such as the central tendency, symmetry, or the range that contains 95% of the values. In contrast, histograms are widely preferred because they make these features much more interpretable. While histograms sacrifice a small amount of information, they provide a much clearer summary of the data.\nTo construct a histogram, we first divide the range of the data into non-overlapping bins. Each bin acts as a category, and its height is chosen so that the area of the bar, calculated as the height multiplied by the bin width, represents the relative frequency of observations within that bin. Although a histogram resembles a bar plot, it differs in that the x-axis represents numerical values rather than categorical ones.\nIf we round the reported heights to the nearest inch and create a relative frequency distribution, we obtain a histogram with bins defined in one-inch intervals:\n\\[(49.5, 50.5], (50.5, 51.5], (51.5, 52.5], (52.5, 53.5], \\dots, (82.5, 83.5]\\]\n\nheights |&gt; filter(sex == \"Male\") |&gt; \n  mutate(height = round(height)) |&gt;\n  count(height) |&gt; mutate(f = n / sum(n)) |&gt; \n  ggplot(aes(height, f)) + geom_col()\n\n\n\n\n\n\n\nThe hist function allows us quickly construct a histogram. We can define custom bin intervals which is particularly useful for grouping extreme values and avoiding empty bins:\n\nwith(heights, hist(height[sex == \"Male\"], breaks = c(50, 55, 60:80, 85)))\n\n\n\n\n\n\n\nIf we were to send either of these plots to an extraterrestrial unfamiliar with human height distributions, they would immediately gain key insights about the data. First, the height values range from 50 to 84 inches, with over 95% of observations falling between 63 and 75 inches. Second, the distribution is approximately symmetric around 69 inches. Additionally, by summing bin counts, one could estimate the proportion of data falling within any given interval. Thus, a histogram provides an intuitive summary that preserves most of the information from the original 812 height measurements using only about 30 bin counts.\nWhat information is lost? A histogram treats all values within a bin as if they were the same when computing bar heights. For instance, values of 64.0, 64.1, and 64.2 inches are grouped together. However, since these differences are barely perceptible, the practical impact is negligible, and we achieve a meaningful summary using just 23 numbers.\n\n\n\n\n\n\nBy default, histograms often display frequency on the y-axis instead of relative frequency. The ggplot2 package follows this convention:\n\nheights |&gt; filter(sex == \"Male\") |&gt; ggplot(aes(height)) + geom_histogram()\n\n\n\n\n\n\n\nWhen displaying frequencies, it is important to use equally spaced bins, as larger bins will naturally contain more observations and may distort the visual representation of the data. To instead display relative frequency, you can modify the code by setting y = after_stat(density):\n\nheights |&gt; filter(sex == \"Male\") |&gt; \n  ggplot(aes(x = height, y = after_stat(density))) + \n  geom_histogram(breaks = c(50, 55, 60:80, 85), fill = \"grey\", color = \"black\")\n\nWe use fill and color to mimic the behavior or hist.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#smoothed-density",
    "href": "summaries/distributions.html#smoothed-density",
    "title": "1  Distributions",
    "section": "\n1.5 Smoothed density",
    "text": "1.5 Smoothed density\nSmooth density plots are similar to histograms, but the data is not divided into bins. Here is what a smooth density plot looks like for our heights data:\n\nheights |&gt; filter(sex == \"Male\") |&gt; \n  ggplot(aes(height)) + geom_density(alpha = 0.2, fill = \"#00BFC4\")\n\n\n\n\n\n\n\nIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed.\nTo understand the smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics.\nThe main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any other list of values, and what we truly want to report to ET is this larger distribution, as it is much more general. Unfortunately, we don’t get to see it.\nHowever, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below, we present a hypothetical histogram with bins of size 1:\n\n\n\n\n\n\n\n\nThe smaller we make the bins, the smoother the histogram becomes. Below are the histograms with bin width of 1, 0.5, and 0.1:\n\n\n\n\n\n\n\n\nThe smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts:\n\n\n\n\n\n\n\n\nNow, back to reality. We don’t have millions of measurements. Instead, we have 812 and we can’t make a histogram with very small bins.\nTherefore, we make a histogram using bin sizes appropriate for our data, computing frequencies rather than counts. Additionally, we draw a smooth curve that passes through the tops of the histogram bars. The following plots demonstrate the steps that lead to a smooth density:\n\n\n\n\n\n\n\n\nHowever, remember that smooth is a relative term. We can actually control the smoothness of the curve that defines the smooth density through an option in the function that computes the smooth density curve. Here are two examples using different degrees of smoothness on the same histogram:\n\np &lt;- heights |&gt; filter(sex == \"Male\") |&gt; \n  ggplot(aes(height)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 1, alpha = 0.5) \n\np1 &lt;- p +  geom_line(stat = 'density', adjust = 0.5)\np2 &lt;- p +  geom_line(stat = 'density', adjust = 2) \n\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\nWe need to make this choice with care as the resulting summary can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71 than to the proportion that is 78 or 65. This implies that the curve should be relatively smooth, resembling the example on the right more than the one on the left.\nWhile the histogram is an assumption-free summary, the smoothed density is based on some assumptions.\nNote that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine that we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:\n\n\n\n\n\n\n\n\nThe proportion of this area is about 0.3, meaning that about 30% of male heights are between 65 and 68 inches.\nBy understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the material covered up to this point, you can complete exercises 1 through 10.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#sec-normal-distribution",
    "href": "summaries/distributions.html#sec-normal-distribution",
    "title": "1  Distributions",
    "section": "\n1.6 The normal distribution",
    "text": "1.6 The normal distribution\nHistograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. One reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for these occurrences, which we will describe later. Here we focus on how the normal distribution helps us summarize data.\nRather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mathrm{Pr}(a &lt; x \\leq b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma} \\right)^2} \\, dx\\]\nYou don’t need to memorize the formula to use the normal distribution in practics. The most important characterstics is that it is completely defined by just two parameters: \\(\\mu\\) and \\(\\sigma\\). The rest of the symbols in the formula represent the interval ends, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\). These two parameters, \\(\\mu\\) and \\(\\sigma\\), are referred to as the mean and the standard deviation (SD) of the distribution, respectively (and are the Greek letters for \\(m\\) and \\(s\\)).\nThe distribution is symmetric, centered at \\(\\mu\\), and most values (about 95%) are within \\(2\\sigma\\) from \\(\\mu\\). Here is what the normal distribution looks like when the \\(\\mu = 0\\) and \\(\\sigma = 1\\):\n\n\n\n\n\n\n\n\nThe fact that the distribution is defined by just two parameters implies that if a dataset’s distribution is approximated by the normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the mean and the standard deviation.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#the-average-and-standard-deviation",
    "href": "summaries/distributions.html#the-average-and-standard-deviation",
    "title": "1  Distributions",
    "section": "\n1.7 The average and standard deviation",
    "text": "1.7 The average and standard deviation\nDiscrete distributions also have means and standard deviations. We will discuss these in more detail in Section 5.1. For a list of values \\(x_1, \\dots, x_n\\) the mean, call it \\(\\mu_x\\) is defined by\n\\[\n\\mu_x = \\frac{1}{n}\\sum_{i=1}^n x_i\n\\] Note that this equivalent to the average of the \\(x\\)s.\nThe standard deviation is defined with\n\\[\n\\sigma_x = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu_x)^2}\n\\] which can thought of as the average distance of the points \\(x_i\\) to the mean \\(\\mu_x\\).\nLet’s compute the mean and stardard deviations for the height for males which we will store in the object x:\n\nx &lt;- with(heights, height[sex == \"Male\"])\n\nThe pre-built functions mean and sd can be used here:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n\n\n\n\n\n\n\nFor reasons explained in Section 9.2.1, sd(x) divides by length(x)-1 rather than length(x). But note that when length(x) is large, sd(x) and sqrt(sum((x-mu)^2) / length(x)) are practically equal.\n\n\n\nIf the distribution of the values stored in x is approximated with a normal distribution, it make sense that it is a normal distribution with mean m &lt;- mean(x) and stadnard deviation s &lt;- sd(x). This implies that the distribution of x can be summarized with just these two numbers!\nHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:\n\n\n\n\n\n\n\n\nThe normal distribution does appear to be quite a good approximation here. We will now see how well this approximation works at predicting the proportion of values within intervals.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#standard-units",
    "href": "summaries/distributions.html#standard-units",
    "title": "1  Distributions",
    "section": "\n1.8 Standard units",
    "text": "1.8 Standard units\nFor data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector X, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of X, respectively. Why is this convenient?\nFirst, revisit the formula for the normal distribution and observe that what is being exponentiated is \\(-z^2/2\\) with \\(z\\) equivalent to \\(x\\) in standard units. Because the maximum of \\(e^{-z^2/2}\\) is when \\(z = 0\\), this explains why the maximum of the distribution occurs at the mean. It also explains the symmetry since \\(- z^2/2\\) is symmetric around 0. Secondly, note that by converting the normally distributed data to standard units, we can quickly ascertain whether, for example, a person is about average (\\(z = 0\\)), one of the tallest (\\(z \\approx 2\\)), one of the shortest (\\(z \\approx -2\\)), or an extremely rare occurrence (\\(z &gt; 3\\) or \\(z &lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.\nIn R, we can obtain standard units using the function scale:\n\nz &lt;- scale(x)\n\nTo see how many males are within 2 SDs from the average, we simply type:\n\nmean(abs(z) &lt; 2)\n#&gt; [1] 0.95\n\nThe proportion is about 95%, which is what the normal distribution predicts! To further validate this approximation, we can use quantile-quantile plots.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#quantile-quantile-plots",
    "href": "summaries/distributions.html#quantile-quantile-plots",
    "title": "1  Distributions",
    "section": "\n1.9 Quantile-quantile plots",
    "text": "1.9 Quantile-quantile plots\nA systematic way to check whether one distribution is well approximated by another is to compare their cumulative distribution functions (CDFs). A practical way to do this is through their quantiles.\nFor a proportion \\(0 \\leq p \\leq 1\\), the \\(p\\)-th quantile of a distribution \\(F(x)\\) is the value \\(q\\) such that \\(F(q) = p\\), or \\(q = F^{-1}(p)\\). For example:\n\n\n\\(p = 0\\) gives the minimum,\n\n\\(p = 0.5\\) gives the median,\n\n\\(p = 1\\) gives the maximum.\n\nIf two distributions \\(F_1(x)\\) and \\(F_2(x)\\) are similar, then their quantiles at any \\(p\\) should also be similar.\nA quantile-quantile plot (qqplot) lets us visually check this by plotting the quantiles of one distribution against the quantiles of another. Specifically, we plot \\(F_1^{-1}(p_1), \\dots, F_1^{-1}(p_m)\\) versus \\(F_2^{-1}(p_1), \\dots, F_2^{-1}(p_m)\\) for a set of proportions \\(p_1, \\dots, p_m\\).\nA common choice for these proportions is\n\\[\np_i = \\frac{i - 0.5}{n}, \\quad i=1,\\dots,n\n\\]\nwith \\(n\\) the sample size. This avoids exactly \\(p=0\\) or \\(p=1\\), which would correspond to \\(-\\infty\\) or \\(+\\infty\\) in distributions like the normal.\nA common use of qqplots is to compare data to a theoretical normal distribution.\nThe CDF of the standard normal is usually written as \\(\\Phi(x)\\), which gives the probability that a normal random variable is less than \\(x\\). For example, \\(\\Phi(-1.96) = 0.025\\) and \\(\\Phi(1.96) = 0.975\\). In R we compute these with pnorm:\n\npnorm(-1.96)\n#&gt; [1] 0.025\n\nThe inverse of \\(\\Phi\\), denoted \\(\\Phi^{-1}(p)\\), gives the theoretical quantiles. For example, \\(\\Phi^{-1}(0.975) = 1.96\\). In R this is qnorm:\n\nqnorm(0.975)\n#&gt; [1] 1.96\n\nBy default pnorm and qnorm use the standard normal (mean 0, sd 1). You can change this with the mean and sd arguments:\n\nqnorm(0.975, mean = 5, sd = 2)\n#&gt; [1] 8.92\n\nWe can compute quantiles for a list of observations too. If we have a vector x, the quantile \\(q_p\\) is the value such that \\(p\\) proportion of the data is below it:\n\np &lt;- 0.5\nq_p &lt;- quantile(x, p)\n\nFor example, in the heights dataset:\n\nmean(x &lt;= 69.5)\n#&gt; [1] 0.515\n\nAbout 50% of men are shorter than 69.5 inches, so this is the median (\\(p=0.5\\)).\nTo compare make a qqplot to assess if our observations follow a normal distribution:\n\nDefine a vector of proportions \\(p_1, \\dots, p_m\\).\nCompute sample quantiles from the data.\nCompute theoretical quantiles from a normal distribution with the same mean and SD as the data.\nPlot them against each other.\n\nExample:\n\np &lt;- seq(0.05, 0.95, 0.05)\nsample_quantiles &lt;- quantile(x, p)\ntheoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x))\nplot(theoretical_quantiles, sample_quantiles)\nabline(0,1)\n\n\n\n\n\n\n\nIf the data are normally distributed, the points should fall along the 45° line.\nIt is even simpler if we first standardize the data:\n\nsample_quantiles &lt;- quantile(z, p)\ntheoretical_quantiles &lt;- qnorm(p) \nplot(theoretical_quantiles, sample_quantiles)\nabline(0,1)\n\nInstead of writing this by hand, R provides convenient functions:\n\nqqnorm(x)\nqqline(x)\n\nOr with ggplot2:\n\nheights |&gt; filter(sex == \"Male\") |&gt;\n  ggplot(aes(sample = scale(height))) + \n  geom_qq() +\n  geom_abline()\n\nBy default, qqnorm and geom_qq use all \\(n\\) quantiles, \\(p_i = (i-0.5)/n\\).\nqqplots can also compare two empirical distributions, not just data to a theoretical model. For example, to compare male and female heights:\n\nwith(heights, qqplot(height[sex == \"Female\"], height[sex == \"Male\"]))\nabline(0,1)\n\n\n\n\n\n\n\nIn summary: qqplots give a simple but powerful way to check whether two distributions match, most often by comparing data to a theoretical distribution such as the normal.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#percentiles",
    "href": "summaries/distributions.html#percentiles",
    "title": "1  Distributions",
    "section": "\n1.10 Percentiles",
    "text": "1.10 Percentiles\nBefore we move on, let’s define some terms that are commonly used in exploratory data analysis.\nPercentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). For example, we refer to the case of \\(p = 0.25\\) as the 25th percentile, representing a value below which 25% of the data falls. The most famous percentile is the 50th, also known as the median. Another special case that receives a name are the quartiles, which are obtained when setting \\(p = 0.25,0.50\\), and \\(0.75\\).",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#boxplots",
    "href": "summaries/distributions.html#boxplots",
    "title": "1  Distributions",
    "section": "\n1.11 Boxplots",
    "text": "1.11 Boxplots\nTo introduce boxplots, we will use a dataset of US murders by state. Suppose we want to summarize the murder rate distribution. Using the techniques we have learned, we can quickly see that the normal approximation does not apply in this case:\n\n\n\n\n\n\n\n\nIn this instance, the histogram above or a smooth density plot would serve as a relatively succinct summary.\nNow suppose those used to receiving just two numbers as summaries ask us for a more compact numerical summary.\nThe boxplot provides a five-number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). The boxplot often ignores outliers when computing the range and instead plots these as independent points. We will provide a detailed explanation of outliers in Chapter 2. Finally, we plot these numbers as a “box” with “whiskers” like this:\n\n\n\n\n\n\n\n\nThe box spans from the first quartile to the third quartile. The horizontal line inside the box marks the median (the 50th percentile). The whiskers extend to the smallest and largest values that are not considered outliers. Outliers are shown as individual points beyond the whiskers. Note that the height of the box is the difference between the third and first quartile. This difference is called the interquartile range (IQR), and it measures the spread of the middle 50% of the data.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#sec-stratification",
    "href": "summaries/distributions.html#sec-stratification",
    "title": "1  Distributions",
    "section": "\n1.12 Stratification",
    "text": "1.12 Stratification\nIn data analysis, we often divide observations into groups based on the values of one or more variables associated with those observations. For example, in the next section, we divide the height values into groups based on a sex variable: females and males. We call this procedure stratification and refer to the resulting groups as strata.\nStratification is common in data visualization because we are often interested in how the distribution of variables differs across different subgroups.\nUsing the histogram, density plots, and qqplots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.3 inches and a SD of 3.6 inches. With this information, ET will have a good idea of what to expect when he meets our male students. However, to provide a complete picture we need to also provide a summary of the female heights.\nWe learned that boxplots are useful when we want to quickly compare two or more distributions. Here are the heights for females and males:\n\nheights |&gt; ggplot(aes(sex, height, fill = sex)) + geom_boxplot()\n\n\n\n\n\n\n\nThe plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. But does the normal approximation also work for the female height data collected by the survey? We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful:\n\n\n\n\n\n\n\n\nWe see something we did not see for the males: the density plot has a second bump. Also, the qqplot shows that the highest points tend to be taller than expected by the normal distribution. Finally, we also see five points in the qqplot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights.\nWe have noticed what we didn’t expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, Female was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data.\nRegarding the five smallest values, note that these are:\n\nheights |&gt; filter(sex == \"Female\") |&gt; \n  top_n(5, desc(height)) |&gt;\n  pull(height)\n#&gt; [1] 51 53 55 52 52\n\nBecause these are reported heights, a possibility is that the student meant to enter 5'1\", 5'2\", 5'3\" or 5'5\".",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#recommended-reading",
    "href": "summaries/distributions.html#recommended-reading",
    "title": "1  Distributions",
    "section": "\n1.13 Recommended reading",
    "text": "1.13 Recommended reading\n\nFreedman, D., Pisani, R., & Purves, R. (2007). Statistics (4th ed.). A clear, concept-first introduction to distributions, variability, and summaries—excellent for intuition and real-data thinking.\nTukey, J. W. (1977). Exploratory Data Analysis. The classic source for boxplots, robust summaries, and EDA philosophy—foundational for this chapter’s emphasis on describing data.\nCleveland, W. S. (1993). Visualizing Data. A practical, graphics-first treatment of distributions and patterns; pairs well with histograms, density plots, and eCDFs.\nSpiegelhalter, D. (2019). The Art of Statistics. Gentle, story-driven coverage of describing data, uncertainty, and what summaries (like means, SDs, and percentiles) really tell us.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/distributions.html#exercises",
    "href": "summaries/distributions.html#exercises",
    "title": "1  Distributions",
    "section": "\n1.14 Exercises",
    "text": "1.14 Exercises\n1. In the murders dataset, the region is a categorical variable and the following is its distribution:\n\n\n\n\n\n\n\n\nTo the closest 5%, what proportion of the states are in the North Central region?\n2. Which of the following is true:\n\nThe graph above is a histogram.\nThe graph above shows only four numbers with a bar plot.\nCategories are not numbers, so it does not make sense to graph the distribution.\nThe colors, not the height of the bars, describe the distribution.\n\n3. The plot below shows the eCDF for male heights:\n\n\n\n\n\n\n\n\nBased on the plot, what percentage of males are shorter than 75 inches?\n\n100%\n95%\n80%\n72 inches\n\n4. To the closest inch, what height m has the property that 1/2 of the male students are taller than m and 1/2 are shorter?\n\n61 inches\n64 inches\n69 inches\n74 inches\n\n5. Here is an eCDF of the murder rates across states:\n\n\n\n\n\n\n\n\nKnowing that there are 51 states (counting DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people?\n\n1\n5\n10\n50\n\n6. Based on the eCDF above, which of the following statements are true:\n\nAbout half the states have murder rates above 7 per 100,000 and the other half below.\nMost states have murder rates below 2 per 100,000.\nAll the states have murder rates above 2 per 100,000.\nWith the exception of 4 states, the murder rates are below 5 per 100,000.\n\n7. Below is a histogram of male heights in our heights dataset:\n\n\n\n\n\n\n\n\nBased on this plot, how many males are between 63.5 and 65.5?\n\n10\n24\n47\n100\n\n8. About what percentage are shorter than 60 inches?\n\n1%\n10%\n25%\n50%\n\n9. Based on the density plot below, about what proportion of US states have populations larger than 10 million?\n\n\n\n\n\n\n\n\n\n0.02\n0.15\n0.50\n0.55\n\n10. Below are three density plots. Is it possible that they are from the same dataset?\n\n\n\n\n\n\n\n\nWhich of the following statements is true:\n\nIt is impossible that they are from the same dataset.\nThey are from the same dataset, but the plots are different due to code errors.\nThey are the same dataset, but the first and second plot undersmooth and the third oversmooths.\nThey are the same dataset, but the first is not in the log scale, the second undersmooths, and the third oversmooths.\n\n11. Define variables containing the heights of males and females as follows:\n\nlibrary(dslabs)\nmale &lt;- heights$height[heights$sex == \"Male\"]\nfemale &lt;- heights$height[heights$sex == \"Female\"]\n\nHow many measurements do we have for each?\n12. Suppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, 70th, & 90th percentiles for each sex. Then create a data frame with these two as columns.\n13. Study the following boxplots showing population sizes by country:\n\n\n\n\n\n\n\n\nWhich continent has the country with the biggest population size?\n14. Which continent has the largest median population size?\n15. What is median population size for Africa to the nearest million?\n16. What proportion of countries in Europe have populations below 14 million?\n\n0.99\n0.75\n0.50\n0.25\n\n17. If we use a log transformation, which continent shown above has the largest interquartile range?\n18. Load the height dataset and create a vector x with just the male heights:\n\nlibrary(dslabs)\nx &lt;- heights$height[heights$sex==\"Male\"]\n\nWhat proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean.\n19. Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the pnorm function to predict the proportions.\n20. Notice that the approximation calculated in question 19 is very close to the exact calculation in question 18. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81]. How many times larger is the actual proportion than the approximation?\n21. Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as seven-footers. Hint: use the pnorm function.\n22. There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many of these men (18-40 year olds) are seven feet tall or taller in the world?\n23. There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world’s 18-to-40-year-old seven-footers are in the NBA?\n24. Repeat the calculations performed in the previous question for Lebron James’ height: 6 feet 8 inches. There are about 150 players that are at least that tall.\n25. In answering the previous questions, we found that it is not uncommon for a seven-footer to become an NBA player. What would be a fair critique of our calculations:\n\nPractice and talent are what make a great basketball player, not height.\nThe normal approximation is not appropriate for heights.\nThe normal approximation tends to underestimate the extreme values. It’s possible that there are more seven-footers than we predicted.\nThe normal approximation tends to overestimate the extreme values. It’s possible that there are fewer seven-footers than we predicted.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "summaries/robust-summaries.html",
    "href": "summaries/robust-summaries.html",
    "title": "2  Robust summaries",
    "section": "",
    "text": "2.1 Outliers\nNote that the heights we explored in the Chapter 1 are not the original heights reported by students. A second challenge involves exploring the original reported heights, which are also included in the dslabs package in the reported_heights object. We will see that due to errors in reporting, using robust summaries are necessary to produce useful summaries.\nWe previously described how boxplots show outliers, but we did not provide a precise definition. Here we discuss outliers, approaches that can help detect them, and summaries that take into account their presence.\nOutliers are very common in real-world data analysis. Data recording can be complex and it is common to observe data points generated in error. For example, an old monitoring device may read out nonsensical measurements before completely failing. Human error is also a source of outliers, in particular when data entry is done manually. For example, an individual may mistakenly enter their height in centimeters instead of inches or put the decimal in the wrong place.\nHow do we distinguish an outlier from measurements that were too big or too small simply due to expected variability? This is not always an easy question to answer, but we try to provide some guidance. Let’s begin with a simple case.\nSuppose a colleague is charged with collecting demography data for a group of males. The data report height in feet and are stored in the object:\nlibrary(dslabs)\nstr(outlier_example)\n#&gt;  num [1:500] 5.59 5.8 5.54 6.15 5.83 5.54 5.87 5.93 5.89 5.67 ...\nOur colleague uses the fact that heights are usually well approximated by a normal distribution and summarizes the data with average and standard deviation:\nmean(outlier_example)\n#&gt; [1] 6.1\nOur colleage writes a report on the interesting fact that this group of males is much taller than usual. The average height is over six feet tall! However, using your data analysis skills you notice something else that is unexpected: the standard deviation is over 7 feet.\nsd(outlier_example)\n#&gt; [1] 7.8\nAdding and subtracting two standard deviations, you note that 95% of this population will have heights between -9.49, 21.7 feet, which does not make sense. A quick plot reveals the problem:\nboxplot(outlier_example)\nThere appears to be at least one value that is nonsensical, since we know that a height of 180 feet is impossible. The boxplot detects this point as an outlier.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Robust summaries</span>"
    ]
  },
  {
    "objectID": "summaries/robust-summaries.html#the-median",
    "href": "summaries/robust-summaries.html#the-median",
    "title": "2  Robust summaries",
    "section": "\n2.2 The median",
    "text": "2.2 The median\nWhen we have an outlier like this, the average can become very large. Mathematically, we can make the average as large as we want by simply changing one number: with 500 data points, we can increase the average by any amount \\(\\Delta\\) by adding \\(\\Delta \\times\\) 500 to a single observation. The median, defined as the value for which half the values are smaller and the other half are bigger, is robust to such outliers. No matter how large we make the largest point, the median remains the same.\nWith this data the median is:\n\nmedian(outlier_example)\n#&gt; [1] 5.74\n\nwhich is about 5 feet and 9 inches.\nThe median is what boxplots display as a horizontal line.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Robust summaries</span>"
    ]
  },
  {
    "objectID": "summaries/robust-summaries.html#the-inter-quartile-range-iqr",
    "href": "summaries/robust-summaries.html#the-inter-quartile-range-iqr",
    "title": "2  Robust summaries",
    "section": "\n2.3 The inter quartile range (IQR)",
    "text": "2.3 The inter quartile range (IQR)\nThe box in boxplots is defined by the first and third quartile. These are meant to provide an idea of the variability in the data: 50% of the data is within this range. The difference between the 3rd and 1st quartile (or 75th and 25th percentiles) is referred to as the inter quartile range (IQR). As is the case with the median, this quantity will be robust to outliers as large values do not affect it. We can do some math to see that for normally distributed data, the IQR divided by 1.349 approximates the standard deviation of the data had an outlier not been present. We can see that this works well in our example, since we get a standard deviation estimate of:\n\nIQR(outlier_example)/1.349\n#&gt; [1] 0.245\n\nwhich is about 3 inches.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Robust summaries</span>"
    ]
  },
  {
    "objectID": "summaries/robust-summaries.html#a-data-driven-definition-of-outliers",
    "href": "summaries/robust-summaries.html#a-data-driven-definition-of-outliers",
    "title": "2  Robust summaries",
    "section": "\n2.4 A data-driven definition of outliers",
    "text": "2.4 A data-driven definition of outliers\ndefinition of outlier used by the boxplot function was introduced by John Tukey. The top whisker ends at the 75th percentile plus 1.5 \\(\\times\\) IQR. Similarly the bottom whisker ends at the 25th percentile minus 1.5\\(\\times\\) IQR. If we define the first and third quartiles as \\(Q_1\\) and \\(Q_3\\), respectively, then an outlier is anything outside the range:\n\\[[Q_1 - 1.5 \\times (Q_3 - Q1), Q_3 + 1.5 \\times (Q_3 - Q1)].\\]\nWhen the data is normally distributed, the standard units of these values are:\n\nq3 &lt;- qnorm(0.75)\nq1 &lt;- qnorm(0.25)\niqr &lt;- q3 - q1\nr &lt;- c(q1 - 1.5*iqr, q3 + 1.5*iqr)\nr\n#&gt; [1] -2.7  2.7\n\nUsing the pnorm function, we see that 99.3% of the data falls in this interval.\nKeep in mind that this is not such an extreme event: if we have 1,000 data points that are normally distributed, we expect to see about 7 outside of this range. But these would not be outliers since we expect to see them under the typical variation.\nIf we want an outlier to be rarer, we can increase the 1.5 to a larger number. Tukey also used 3 and called these far out outliers. With a normal distribution, 99.9998% of the data falls in this interval. This translates into about 2 in a million chance of being outside the range. In the geom_boxplot function, this can be controlled by the coef argument, which defaults to 1.5.\nThe 180 inches measurement is well beyond the range of the height data:\n\nmax_height &lt;- quantile(outlier_example, 0.75) + 3*IQR(outlier_example)\nmax_height\n#&gt;  75% \n#&gt; 6.91\n\nIf we remove this value, we can see that the data is in fact normally distributed as expected:\n\nx &lt;- outlier_example[outlier_example &lt; max_height]\nqqnorm(x)\nqqline(x)",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Robust summaries</span>"
    ]
  },
  {
    "objectID": "summaries/robust-summaries.html#median-absolute-deviation",
    "href": "summaries/robust-summaries.html#median-absolute-deviation",
    "title": "2  Robust summaries",
    "section": "\n2.5 Median absolute deviation",
    "text": "2.5 Median absolute deviation\nAnother way to robustly estimate the standard deviation in the presence of outliers is to use the median absolute deviation (MAD). To compute the MAD, we first compute the median, and then for each value we compute the distance between that value and the median. The MAD is defined as the median of these distances. For technical reasons not discussed here, this quantity needs to be multiplied by 1.4826 to assure it approximates the actual standard deviation. The mad function already incorporates this correction. For the height data, we get a MAD of:\n\nmad(outlier_example)\n#&gt; [1] 0.237\n\nwhich is about 3 inches.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Robust summaries</span>"
    ]
  },
  {
    "objectID": "summaries/robust-summaries.html#recommended-reading",
    "href": "summaries/robust-summaries.html#recommended-reading",
    "title": "2  Robust summaries",
    "section": "\n2.6 Recommended reading",
    "text": "2.6 Recommended reading\n\nHoaglin, D. C., Mosteller, F., & Tukey, J. W. (1983). Understanding Robust and Exploratory Data Analysis. Wiley. A classic edited volume aimed at explaining robust methods in an accessible style; many chapters are practical and easy to dip into.\nWilcox, R. R. (2017). Introduction to Robust Estimation and Hypothesis Testing (4th ed.). Academic Press. A practical, applied text with many worked examples. It starts gently, with chapters on medians, trimmed means, MAD, and robust alternatives to SD.\nHuber, P. J., & Ronchetti, E. M. (2009). Robust Statistics (2nd ed.). Wiley. More advanced, but the opening chapters give a rigorous yet readable account of why robustness matters. Useful for readers that want to go a bit deeper.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Robust summaries</span>"
    ]
  },
  {
    "objectID": "summaries/robust-summaries.html#exercises",
    "href": "summaries/robust-summaries.html#exercises",
    "title": "2  Robust summaries",
    "section": "\n2.7 Exercises",
    "text": "2.7 Exercises\nWe are going to use the HistData package. Load the height data set and create a vector x, consisting solely of the male heights used in Galton’s data on the heights of parents and their children, part of his historic research on heredity.\n\nlibrary(HistData)\nx &lt;- Galton$child\n\n1. Compute the average and SD of these data.\n2. Compute the median and median absolute deviation of these data.\n3. Now suppose Galton made a mistake when entering the first value and forgot to use the decimal point. You can imitate this error by typing:\n\nx_with_error &lt;- x\nx_with_error[1] &lt;- x_with_error[1]*10\n\nHow many inches does the average grow after this mistake?\n4. How many inches does the SD grow after this mistake?\n5. How many inches does the median grow after this mistake?\n6. How many inches does the MAD grow after this mistake?\n7. How could you use exploratory data analysis to detect that an error was made?\n\nSince it is only one value out of many, we will not be able to detect this.\nWe would see an obvious shift in the distribution.\nA boxplot, histogram, or qqplot would reveal a clear outlier.\nA scatterplot would show high levels of measurement error.\n\n8. How much can the average accidentally grow with mistakes like this? Write a function called error_avg that takes a value k and returns the average of the vector x after the first entry changed to k. Show the results for k=10000 and k=-10000.\n9. Using the murders dataset in the dslabs package. Compute the murder rate for each state. Make a boxplot comparing the murder rates for each region of the United States.\n10. For the same dataset, compute the median and IQR murder rate for each region.\n11. The heights we have been looking at in the heights data frameare not the original heights reported by students. The original reported heights are also included in the dslabs package in the object reported_heights. Note that the height column in this data frame is a character, and if we try to create a new column with the numeric version:\n\nlibrary(tidyverse)  \nreported_heights &lt;- reported_heights |&gt;\n  mutate(original_heights = height, height = as.numeric(height))\n\nwe get a warnings about NAs. Examine the rows that result in NAs and describe why this is happening.\n12. Add a column to the reported_heights with the year the height was entered. You can use the year function in the lubridate package to extract the year from reported_heights$time_stamp. Change the height column from characters to numbers using as.numeric. Some of the heights will be converted to NA because they were incorrectly entered and include characters, for example 165cm. These heights were supposed to be reported in inches, but many clearly did not. Convert any entry with highly unlikely heights, below 54 or above 72, to NA using the na_if function from dplyr. Once you do this, stratify by sex and year and report the percentage of incorrectly entered heights, represented by the NA.\n13. Remove the entries that result in NAs when attempting to convert heights to numbers. Compute the mean, standard deviation, median, and MAD by sex. What do you notice?\n14. Generate boxplots summarizing the heights for males and females and describe what you see.\n15. Look at the largest 10 heights and provide a hypothesis for what you think is happening.\n16. Review all the nonsensical answers by looking at the data considered to be far out by Tukey and comment on the type of errors you see.",
    "crumbs": [
      "Summary statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Robust summaries</span>"
    ]
  },
  {
    "objectID": "prob/intro-to-prob.html",
    "href": "prob/intro-to-prob.html",
    "title": "Probability",
    "section": "",
    "text": "Probability is the mathematical foundation of most data analysis. Whenever our data involve uncertainty or variation, whether from sampling, measurement error, or random processes, probability provides the language and framework we need to reason about it. Every concept we use later in this book, from statistical inference to machine learning, rests on the basic ideas introduced here.\nThe study of probability began with games of chance, where its meaning was concrete and intuitive: rolling dice, drawing cards, or betting on outcomes. Understanding these games offered strategic advantage, and mathematicians such as Cardano, Fermat, and Pascal developed the first formal methods for computing odds. From this work, an entire mathematical discipline was born, Probability Theory.\nToday, probability underlies much more than gambling. We use it to describe the likelihood of rain, the risk of disease, or the uncertainty in a prediction. Yet, outside of games of chance, the meaning of probability is often less obvious. This section helps clarify what probability represents and how it connects to the kinds of data problems we face in practice.\nWe will not cover the mathematical theory of probability needed to be an expert data analysts, many excellent textbooks already do this, but rather focus on the essential concepts needed to understand data analysis. We introduce the basic building blocks: random variables, expected value, and standard error. Our emphasis is on intuition and computation rather than mathematical derivations.\nThroughout this part of the book, we will also connect probability theory to computer simulations. Using R code and Monte Carlo methods, we will learn how to estimate probabilities, explore random behavior, and develop intuition for uncertainty through simulation.\nThis practical, code-based approach will prepare you to see how probability connects directly to real data in the chapters that follow.",
    "crumbs": [
      "Probability"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html",
    "href": "prob/discrete-probability.html",
    "title": "\n3  Discrete probability\n",
    "section": "",
    "text": "3.1 Relative frequency\nWe begin by covering some basic principles related to categorical data. The specific area of probability which deals with categorical data is referred to as discrete probability. Understanding this topic, will help us comprehend the probability theory we will later introduce for numeric and continuous data, which is much more common in data analysis. Since discrete probability is invaluable in card games, we will use these as illustrative examples.\nThe term probability is used in everyday language. Yet answering questions about probability is often hard, if not impossible. In this section, we discuss a mathematical definition of probability that allows us to give precise answers to certain questions.\nFor example, if I have 2 red beads and 3 blue beads inside an urn1 (most probability books use this archaic term, so we do too) and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes, of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has an equal chance of occurring, we conclude that the probability is .4 for red and .6 for blue.\nA more intuitive way to think about the probability of an event is as the long-run proportion of times the event occurs when an experiment is repeated independently and under identical conditions, an infinite number of times. This interpretation naturally leads to one of the most powerful techniques in data science—Monte Carlo simulations, which we will explore later in this chapter.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#probability-distributions",
    "href": "prob/discrete-probability.html#probability-distributions",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.2 Probability distributions",
    "text": "3.2 Probability distributions\nProbability distributions are the foundation of most statistical models. They describe how likely different outcomes are, providing a mathematical summary of uncertainty. Every model we use in data analysis, from simple polling examples to complex multivariate models, is built on assumptions about underlying probability distributions. As we move to continuous and multidimensional settings, these distributions become more complex and abstract. For discrete cases, however, the idea is quite intuitive: each possible outcome, or category, is assigned a probability based on its relative frequency.\nIf we know the relative frequency of the different categories, defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases analogous to beads in an urn, for each bead type, their proportion defines the distribution.\nFor example, if we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% Undecided, and 2% Green Party, these proportions define the probability for each group in any given call. The probability distribution is:\n\n\nPr(picking a Republican)\n=\n0.44\n\n\nPr(picking a Democrat)\n=\n0.44\n\n\nPr(picking an Undecided)\n=\n0.10\n\n\nPr(picking a Green)\n=\n0.02",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#monte-carlo",
    "href": "prob/discrete-probability.html#monte-carlo",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.3 Monte Carlo",
    "text": "3.3 Monte Carlo\nComputers provide a way to actually perform the simple random long-run experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.\nAn example is the sample function in R. We demonstrate its use in the code below. First, we use the function rep to generate the urn:\n\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2,3))\n\nand then use sample to pick a bead at random:\n\nsample(beads, 1)\n#&gt; [1] \"red\"\n\nThis line of code produces a single random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat indefinitely. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating indefinitely. This is an example of a Monte Carlo simulation.\nMuch of what mathematical and theoretical statisticians study, topics that we do not cover in this book, relates to providing rigorous definitions of “practically equivalent”. Additionally, they explore how close a large number of experiments brings us to what happens in the limit. Later in this section, we provide a practical approach to determining what is “large enough”.\nTo perform our first Monte Carlo simulation, we use the replicate function, which allows us to repeat the same task any number of times. Here, we repeat the random event 10,000 times:\n\nset.seed(1986) \nevents &lt;- replicate(10000, sample(beads, 1))\n\n\n\n\n\n\n\nHere we use the replicate function for educational purposes, but there is a faster way to generate this simulation using:\n\nsample(beads, 10000, replace = TRUE)\n\nwhich we explain in the following section.\n\n\n\nWe can now verify if our definition actually is in agreement with this Monte Carlo simulation approximation. We use table to see the distribution:\n\ntable(events)\n#&gt; events\n#&gt; blue  red \n#&gt; 6014 3986\n\nand prop.table gives us the proportions:\n\nprop.table(table(events))\n#&gt; events\n#&gt;  blue   red \n#&gt; 0.601 0.399\n\nThe numbers above represent the estimated probabilities obtained by this Monte Carlo simulation. Statistical theory tells us that as \\(B\\) gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.\nThis is a simple and not very useful example, since we can easily compute the probabilities mathematically. Monte Carlo simulations are useful when it is hard, or impossible, to compute the exact probabilities mathematically. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.\n\n3.3.1 Setting the random seed\nBefore we continue, we will briefly explain the following important line of code:\n\nset.seed(1986) \n\nThroughout this book, we use random number generators. This implies that many of the results presented can potentially change by chance, indicating that a static version of the book may show a different result than what you obtain when following the code as presented. This is actually fine, given that the results are random and change by chance. However, if you want to ensure that results are consistent with each run, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed every time and we used a popular way to pick the seed which is year - month - day. For example, we chose 1986 on December 20, 2018: \\(2018 - 12 - 20 = 1986\\).\nYou can learn more about setting the seed by looking at the documentation:\n\n?set.seed\n\nIn the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.\n\n3.3.2 With and without replacement\nThe function sample has an argument that allows us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not returned to the bag. Notice what happens when we ask to randomly select five beads:\n\nsample(beads, 5)\n#&gt; [1] \"red\"  \"blue\" \"blue\" \"blue\" \"red\"\nsample(beads, 5)\n#&gt; [1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\nThis results in rearrangements that consistently comprise three blue and two red beads. If we ask that six beads be selected, we get an error.\nHowever, the sample function can therefore be used directly, without the use of replicate, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample with replacement: return the bead back to the urn after selecting it. We can tell sample to do this by changing the replace argument, which defaults to FALSE, to replace = TRUE:\n\nevents &lt;- sample(beads, 10000, replace = TRUE)",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#combinations-and-permutations",
    "href": "prob/discrete-probability.html#combinations-and-permutations",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.4 Combinations and permutations",
    "text": "3.4 Combinations and permutations\nMost university-level statistics courses begin with combinatorics, which provides the foundation for many probability calculations. These techniques let us count the number of outcomes that satisfy a condition, typically by computing the number of permutations (when order matters) or combinations (when order does not matter).\nTo make these ideas concrete, let’s return to a familiar setting: cards. Suppose we want to understand the probability of drawing specific hands from a deck. This example helps demonstrate how combinatorics works and how we can use R to perform these calculations.\nFirst, let’s build a deck of cards:\n\nsuits &lt;- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\")\nnumbers &lt;- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\",\n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\ndeck &lt;- expand.grid(number = numbers, suit = suits)\ndeck &lt;- paste(deck$number, deck$suit)\n\nIf we draw one card at random, the probability of getting a King is 1/13:\n\nmean(deck %in% paste(\"King\", suits))\n#&gt; [1] 0.0769\n\nNow let’s introduce the two key functions: permutations() and combinations() from the gtools package. These let us enumerate all possible selections from a list, depending on whether order matters.\nFor example:\n\nlibrary(gtools)\npermutations(3, 2)\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    1    3\n#&gt; [3,]    2    1\n#&gt; [4,]    2    3\n#&gt; [5,]    3    1\n#&gt; [6,]    3    2\n\nThis lists all ordered pairs from the numbers 1, 2, and 3. Notice that (1,2) and (2,1) are treated as distinct. If order does not matter, we use combinations():\n\ncombinations(3, 2)\n#&gt;      [,1] [,2]\n#&gt; [1,]    1    2\n#&gt; [2,]    1    3\n#&gt; [3,]    2    3\n\nNow (1,2) and (2,1) are treated as the same outcome.\nLet’s use these tools to compute the probability of a Natural 21 in Blackjack, which occurs when a player’s first two cards are an Ace and a face card (Jack, Queen, King, or Ten). Since order does not matter, we use combinations():\n\naces &lt;- paste(\"Ace\", suits)\nfacecard &lt;- c(\"King\", \"Queen\", \"Jack\", \"Ten\")\nfacecard &lt;- expand.grid(number = facecard, suit = suits)\nfacecard &lt;- paste(facecard$number, facecard$suit)\nhands &lt;- combinations(52, 2, v = deck)\nmean((hands[,1] %in% aces & hands[,2] %in% facecard) |\n       (hands[,2] %in% aces & hands[,1] %in% facecard))\n#&gt; [1] 0.0483\n\n\n3.4.1 Monte Carlo approach\nWe can obtain the same result through simulation. Instead of enumerating all combinations, we repeatedly draw two cards from the deck and check whether we have a Natural 21. This is a Monte Carlo approximation to the same probability.\n\nblackjack &lt;- function(){\n   x &lt;- sample(deck, 2)\n   (x[1] %in% aces & x[2] %in% facecard) | (x[2] %in% aces & x[1] %in% facecard)\n}\n\nresults &lt;- replicate(10000, blackjack())\nmean(results)\n#&gt; [1] 0.0453\n\nBoth approaches yield nearly the same result. The combinatoric functions give the exact probability, while the Monte Carlo simulation provides an approximation that converges to the same value as the number of repetitions increases.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#examples",
    "href": "prob/discrete-probability.html#examples",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.5 Examples",
    "text": "3.5 Examples\nIn this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We use R to help illustrate the mathematical concepts.\n\n3.5.1 Monty Hall problem\nIn the 1970s, there was a game show called “Let’s Make a Deal,” with Monty Hall as the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door, there was a prize, while the other doors had a goat behind them to show the contestant had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and reveal to the contestant that there was no prize behind that door. Then, he would ask, “Do you want to switch doors?” What would you do?\nWe can use probability to demonstrate that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This might seem counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy2 or read one on Wikipedia3. Below, we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.\nLet’s start with the stick strategy:\n\nmonty_hall &lt;- function(strategy = c(\"stick\", \"switch\")) {\n  strategy &lt;- match.arg(strategy)\n  doors &lt;- c(\"1\", \"2\", \"3\")\n  prize_door &lt;- sample(doors, 1)\n  my_pick &lt;- sample(doors, 1)\n  show &lt;- sample(setdiff(doors, c(my_pick, prize_door)), 1)\n  final_pick &lt;- if (strategy == \"stick\") my_pick else \n    setdiff(doors, c(my_pick, show))\n  final_pick == prize_door\n}\nB &lt;- 10000\nmean(replicate(B, monty_hall(\"stick\")))\n#&gt; [1] 0.326\nmean(replicate(B, monty_hall(\"switch\")))\n#&gt; [1] 0.669\n\nAs we write the code, we see that the lines starting with my_pick and show have no influence on the last logical operation, when we stick to our original choice. From this, we should realize that the chance is 1 in 3, as we initially considered. When we switch, the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by demonstrating that we are removing a door, show, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, we win 1 - 1/3 = 2/3 of the times.\n\n3.5.2 Birthday problem\nSuppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do that later. Here, we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29, which doesn’t significantly change the answer.\nFirst, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained as follows:\n\nn &lt;- 50\nbdays &lt;- sample(1:365, n, replace = TRUE)\n\nTo check if there are at least two people with the same birthday in this particular set of 50 people, we can use the duplicated function, which returns TRUE whenever an element of a vector is a duplicate:\n\nany(duplicated(bdays))\n#&gt; [1] TRUE\n\nIn this case, we see that it did happen; there were at least two people who had the same birthday.\nTo estimate the probability of a shared birthday in the group, we repeat this experiment by repeatedly sampling sets of 50 birthdays:\n\nsame_birthday &lt;- function(n) any(duplicated(sample(1:365, n, replace = TRUE)))\nresults &lt;- replicate(10000, same_birthday(50))\nmean(results)\n#&gt; [1] 0.969\n\nPeople tend to underestimate these probabilities so let’s say we want to use this knowledge to make bet with friends about the likelihood of two people sharing the same birthday in a group. At what group size do the chances become greater than 50%? Greater than 75%?\nLet’s create a look-up table. We can quickly create a function to compute this for any group size:\n\ncompute_prob &lt;- function(n, B = 10000) mean(replicate(B, same_birthday(n)))\n\nUsing the function sapply, we can perform element-wise operations on any function:\n\nn &lt;- seq(1,60)\nprob &lt;- sapply(n, compute_prob)\n\nWe can now make a plot of the estimated probabilities of two people having the same birthday in a group of size \\(n\\):\n\nplot(n, prob)\n\n\n\n\n\n\n\nNow, let’s compute the exact probabilities instead of relying on Monte Carlo approximations. Not only do we obtain the exact answer using math, but the computations are much faster since we did not have to generate experiments.\nTo make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule: The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one day, is 364/365, and so on. The chances of all \\(n\\) people having a unique birthday is therefore:\n\\[\n1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365}\n\\]\nWe can write a function that does this for any number:\n\nexact_prob &lt;- function(n) 1 - prod(seq(365, 365 - n + 1)/365)\neprob &lt;- sapply(n, exact_prob)\nplot(n, prob)\nlines(n, eprob, col = \"red\")\n\n\n\n\n\n\n\nThis plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had we not been able to compute the exact probabilities, we could still accurately estimate them.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#infinity-in-practice",
    "href": "prob/discrete-probability.html#infinity-in-practice",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.6 Infinity in practice",
    "text": "3.6 Infinity in practice\nThe theory described here requires repeating experiments over and over indefinitely. In practice, we can’t do this. In the examples above, we used \\(B=10,000\\) Monte Carlo experiments, yielding accurate estimates. The larger this number, the more accurate the estimate becomes, until the approximation is so good that your computer can’t tell the difference. However, in more complex calculations, 10,000 may not be nearly enough. Moreover, for some calculations, 10,000 experiments might not be computationally feasible.\nIn practical scenarios, we won’t know what the answer is beforehand, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger the \\(B\\), the better the approximation. But how large do we need it to be? This is actually a challenging question, and answering it often requires advanced theoretical statistics training.\nOne practical approach is to check for the stability of the estimate. The following example illustrates the birthday problem for a group of 25 people.\n\nB &lt;- 10^seq(1, 5, len = 100)\ncompute_prob &lt;- function(B, n = 25) mean(replicate(B, same_birthday(n)))\nprob &lt;- sapply(B, compute_prob)\nplot(log10(B), prob)\n\n\n\n\n\n\n\nIn this plot, we can see that the values start to stabilize at around 1000. Note that the exact probability, which is known in this case, is 0.5686997.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#recommended-reading",
    "href": "prob/discrete-probability.html#recommended-reading",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.7 Recommended reading",
    "text": "3.7 Recommended reading\n\n3.7.1 Recommended reading\nFor readers who want to explore discrete probability in more depth, here are a few classic and accessible resources:\n\nIntroduction to Probability by Joseph K. Blitzstein and Jessica Hwang - an engaging, example-driven introduction that emphasizes problem solving and intuition.\nA First Course in Probability by Sheldon Ross — a widely used undergraduate text that builds a solid foundation through clear explanations and a broad range of exercises.\nAn Introduction to Probability Theory and Its Applications, Volume 1 by William Feller — a rigorous classic that provides a deeper and more formal treatment of the subject.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#exercises",
    "href": "prob/discrete-probability.html#exercises",
    "title": "\n3  Discrete probability\n",
    "section": "\n3.8 Exercises",
    "text": "3.8 Exercises\n1. One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?\n2. What is the probability that the ball will not be cyan?\n3. Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability of the first draw being cyan and the second draw not being cyan?\n4. Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability of the first draw being cyan and the second draw not being cyan?\n5. Let’s say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one will be yellow?\n6. If you roll a 6-sided die six times, what is the probability of not seeing a 6?\n7. Two teams, let’s say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics will win at least one game?\n8. Create a Monte Carlo simulation to confirm your answer to the previous problem. Use B &lt;- 10000 simulations. Hint: use the following code to generate the results of the first four games:\n\nceltic_wins &lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))\n\nThe Celtics must win one of these 4 games.\n9. Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?\n10. Confirm the results of the previous question with a Monte Carlo simulation.\n11. Two teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p&gt;0.5\\) chance of winning each game. Given a value \\(p\\), the probability of winning the series for the underdog team \\(B\\) can be computed with the following function based on a Monte Carlo simulation:\n\nprob_win &lt;- function(p){\n  B &lt;- 10000\n  result &lt;- replicate(B, {\n    b_win &lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1 - p, p))\n    sum(b_win) &gt;= 4\n  })\n  mean(result)\n}\n\nUse the function sapply to compute the probability, call it Pr, of winning for p &lt;- seq(0.5, 0.95, 0.025). Then plot the result.\n12. Repeat the exercise above, but now keep the probability fixed at p &lt;- 0.75 and compute the probability for different series lengths: best of 1 game, best of 3 games, best of 5 games,… Specifically, N &lt;- seq(1, 25, 2). Hint: use the function below.\n\nprob_win &lt;- function(n, p = 0.75){\n  B &lt;- 10000\n  result &lt;- replicate(B, {\n    b_win &lt;- sample(c(1,0), n, replace = TRUE, prob = c(1 - p, p))\n    sum(b_win) &gt;= (n + 1)/2\n  })\n  mean(result)\n}",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/discrete-probability.html#footnotes",
    "href": "prob/discrete-probability.html#footnotes",
    "title": "\n3  Discrete probability\n",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Urn_problem↩︎\nhttps://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem↩︎\nhttps://en.wikipedia.org/wiki/Monty_Hall_problem↩︎",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Discrete probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html",
    "href": "prob/continuous-probability.html",
    "title": "\n4  Continuous probability\n",
    "section": "",
    "text": "4.1 Cumulative distribution functions\nIn Section 1.3, we discussed why it is not practical to assign a probability to every possible numeric outcome, such as an exact height, since there are infinitely many possible values. The same idea extends to outcomes that take values on a continuous scale: each individual value has probability zero. Instead, we describe their behavior through probability density functions, which let us compute probabilities for intervals of values rather than single points.\nIn this chapter, we introduce the mathematical framework for continuous probability distributions and present several useful approximations that frequently appear in data analysis.\nWe return to our example using the heights of adult male students:\nlibrary(tidyverse)\nlibrary(dslabs)\nx &lt;- heights %&gt;% filter(sex == \"Male\") %&gt;% pull(height)\nWe previously defined the empirical cumulative distribution function (eCDF) as\nF &lt;- function(a) mean(x &lt;= a)\nwhich, for any value a, gives the proportion of values in the list x that are less than or equal to a.\nTo connect the eCDF to probability, imagine randomly selecting one of the male students. What is the chance that he is taller than 70.5 inches? Because each student is equally likely to be chosen, this probability is simply the proportion of students taller than 70.5 inches. Using the eCDF, we can compute it as:\n1 - F(70.5)\n#&gt; [1] 0.363\nThe cumulative distribution function (CDF) is the theoretical counterpart of the eCDF. Rather than relying on observed data, it assigns probabilities to ranges of values for a random outcome \\(X\\). Specifically, the CDF gives, for any number \\(a\\), the probability that \\(X\\) is less than or equal to \\(a\\):\n\\[\nF(a) = \\Pr(X \\leq a)\n\\]\nOnce the CDF is defined, we can compute the probability that \\(X\\) falls within any interval. For example, the probability that a student’s height is between \\(a\\) and \\(b\\) is:\n\\[\n\\Pr(a &lt; X \\leq b) = F(b) - F(a)\n\\]\nBecause we can determine the probability of any event from the CDF, it fully defines the probability distribution of a continuous outcome.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Continuous probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#probability-density-function",
    "href": "prob/continuous-probability.html#probability-density-function",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.2 Probability density function",
    "text": "4.2 Probability density function\nFor most continuous distributions, we can describe the cumulative distribution function (CDF) in terms of another function, \\(f(x)\\), such that\n\\[\nF(b) - F(a) = \\int_a^b f(x)\\,dx\n\\]\nThis function \\(f(x)\\) is called the probability density function (PDF).\nThe PDF plays a role similar to the relative frequency distribution for discrete data. Instead of assigning probabilities to individual outcomes, which would all be zero for a continuous variable, the PDF describes how probability is distributed across values of \\(x\\). We can think of it as defining the shape of the distribution: wider regions under the curve correspond to more likely ranges of values.\nTo build intuition, imagine dividing the range of possible outcomes into many tiny intervals. The width of each interval forms the base of a rectangle, and \\(f(x)\\) determines its height. The total area of all rectangles approximates the probability of observing a value between \\(a\\) and \\(b\\). In the limit, this approximation becomes an integral:\n\n\n\n\n\n\n\n\nAn important example is the normal distribution, whose probability density function is\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - m}{s}\\right)^2\\right)\n\\]\nIntegrating this function gives the CDF of the normal distribution. In R, the corresponding function is pnorm. A random outcome is said to be normally distributed with mean m and standard deviation s if its CDF is defined by\n\nF(a) &lt;- pnorm(a, mean = m, sd = s)\n\nThis is particularly useful in practice. If we are willing to assume that a variable such as height follows a normal distribution, we can answer probability questions without needing the full dataset. For example, to find the probability that a randomly selected student is taller than 70 inches, we only need the sample mean and standard deviation:\n\nm &lt;- mean(x)\ns &lt;- sd(x)\n1 - pnorm(70.5, m, s)\n#&gt; [1] 0.371",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Continuous probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#theoretical-distributions-as-approximations",
    "href": "prob/continuous-probability.html#theoretical-distributions-as-approximations",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.3 Theoretical distributions as approximations",
    "text": "4.3 Theoretical distributions as approximations\nThe normal distribution is defined mathematically, without relying on data. In practice, however, almost all the quantities we analyze come from discrete observations. For instance, our height data can be viewed as categorical, with each unique height representing a category and its probability given by its relative frequency.\n\n\n\n\n\n\n\n\nWhile these reported values appear discrete, this discreteness arises from rounding. A few students reported exact metric conversions, such as 177 cm = 69.685 inches, while most rounded to the nearest inch. It is therefore more useful to treat height as a continuous variable, recognizing that no one is exactly 70 inches tall—the higher frequency at 70 simply reflects rounding.\nIn continuous distributions, individual points have probability zero. Instead, we work with intervals, asking questions such as: what is the probability that a height falls between 69.5 and 70.5 inches? For rounded data, this matches the natural interval corresponding to a single reported inch.\nThe normal distribution provides a convenient way to approximate these probabilities. For example:\n\nmean(x &lt; 70.5) - mean(x &lt;= 69.5)\n#&gt; [1] 0.119\npnorm(70.5, m, s) - pnorm(69.5, m, s)\n#&gt; [1] 0.108\n\nThe approximation is close for intervals aligned with the rounding, though the approximation deteriorates for smaller, uneven ranges. This discrepancy reflects discretization rather than a flaw in the normal model itself. As long as we are aware of this limitation, treating rounded data as continuous and using normal approximations remains an effective and practical approach.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Continuous probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#monte-carlo",
    "href": "prob/continuous-probability.html#monte-carlo",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.4 Monte Carlo",
    "text": "4.4 Monte Carlo\nR provides functions to generate normally distributed outcomes. Specifically, the rnorm function takes three arguments: size, mean (defaults to 0), and standard deviation (defaults to 1), and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:\n\nn &lt;- length(x)\nm &lt;- mean(x)\ns &lt;- sd(x)\nsimulated_heights &lt;- rnorm(n, m, s)\n\nNot surprisingly, the distribution looks normal:\n\n\n\n\n\n\n\n\nThis is one of the most useful functions in R because it lets us generate data that mimics natural variation and explore what outcomes might occur by chance through Monte Carlo simulations.\nFor instance, suppose we repeatedly sample 800 men at random and record the tallest person in each group. What does the distribution of these tallest heights look like? How rare is it to find a seven-footer among 800 men? The following Monte Carlo simulation helps us find out:\n\ntallest &lt;- replicate(10000, max(rnorm(800, m, s)))\n\nHaving a seven-footer is quite rare:\n\nmean(tallest &gt;= 7*12)\n#&gt; [1] 0.0175\n\nHere is the resulting distribution:\n\n\n\n\n\n\n\n\nNote that although the derivation is not straightforward, the distribution of the maximum can be computed analytically. Once derived, it provides a much faster and more efficient way to evaluate probabilities than relying on simulation. However, in cases where the derivation is too complex or not possible, either due to the form of the distribution or the nature of the problem, Monte Carlo simulation offers a practical alternative. By repeatedly generating random samples, we can approximate the distribution of the maximum, or any other statistic, and obtain reliable estimates even in analytically intractable situations.\nThe normal distribution is not the only useful theoretical model. Other continuous distributions that often appear in data analysis include the Student’s t, chi-square, exponential, gamma, and beta distributions. Their corresponding shorthand names in R are t, chisq, exp, gamma, and beta.\nR follows a simple and consistent naming convention for functions associated with these distributions. Each distribution has four related functions, which begin with the letters d, p, q, and r, indicating density, cumulative probability, quantile, and random generation, respectively. For example, for the Student’s t distribution (discussed later in Section 9.2.3), we use dt for the density, pt for the cumulative distribution function, qt for quantiles, and rt to generate random samples for Monte Carlo simulations.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Continuous probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#recommended-reading",
    "href": "prob/continuous-probability.html#recommended-reading",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.5 Recommended reading",
    "text": "4.5 Recommended reading\n\n4.5.1 Recommended reading\nFor readers who want to explore continuous probability and distributions in more depth, the following undergraduate-level textbooks provide clear explanations and practical examples:\n\nIntroduction to Probability by Joseph K. Blitzstein and Jessica Hwang — an engaging introduction that builds intuition for both discrete and continuous probability through real-world problems and visual reasoning.\nProbability and Statistics by Morris H. DeGroot and Mark J. Schervish — a comprehensive and widely used text that connects probability theory with its applications in data analysis and inference.\nA First Course in Probability by Sheldon Ross — a classic undergraduate textbook offering clear explanations, worked examples, and a strong foundation in both discrete and continuous probability models.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Continuous probability</span>"
    ]
  },
  {
    "objectID": "prob/continuous-probability.html#exercises",
    "href": "prob/continuous-probability.html#exercises",
    "title": "\n4  Continuous probability\n",
    "section": "\n4.6 Exercises",
    "text": "4.6 Exercises\n1. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?\n2. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?\n3. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?\n4. Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?\n5. Notice that the answer to the question does not change when you change units. This makes sense since the standard deviations from the average for an entry in a list are not affected by what units we use. In fact, if you look closely, you notice that 61 and 67 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.\n6. To understand the mathematical rationale that explains why the answers to exercises 3, 4, and 5 are the same, suppose we have a random variable with average \\(m\\) and standard error \\(s\\). Suppose we ask the probability of \\(X\\) being smaller or equal to \\(a\\). Remember that, by definition, \\(a\\) is \\((a - m)/s\\) standard deviations \\(s\\) away from the average \\(m\\). The probability is:\n\\[\n\\mathrm{Pr}(X \\leq a)\n\\]\nNow we subtract \\(\\mu\\) to both sides and then divide both sides by \\(\\sigma\\):\n\\[\n\\mathrm{Pr}\\left(\\frac{X-\\mu}{\\sigma} \\leq \\frac{a-\\mu}{\\sigma} \\right)\n\\]\nThe quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it \\(Z\\):\n\\[\n\\mathrm{Pr}\\left(Z \\leq \\frac{a-\\mu}{\\sigma} \\right)\n\\]\nSo, no matter the units, the probability of \\(X\\leq a\\) is the same as the probability of a standard normal variable being less than \\((a - \\mu)/\\sigma\\). If mu is the average and sigma the standard error, which of the following R code would give us the right answer in every situation?\n\nmean(X &lt;= a)\npnorm((a - m)/s)\npnorm((a - m)/s, m, s)\npnorm(a)\n\n7. Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm.\n8. The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating 10,000 IQ scores and keeping the highest. Make a histogram.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Continuous probability</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html",
    "href": "prob/random-variables-sampling-models-clt.html",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "",
    "text": "5.1 Random variables\nMost data we work with is affected by chance, whether it comes from a random sample, is subject to measurement error, or reflects outcomes that are inherently random. Being able to quantify the uncertainty introduced by this randomness is one of the most important responsibilities of a data analyst. Statistical inference provides a framework, along with practical tools, to accomplish this. The first step is learning how to mathematically describe random variables. In this chapter, we introduce random variables and their key properties, beginning with simple examples from games of chance.\nStatistical inference begins with the concept of a random variable, a numeric quantity whose value depends on the outcome of a random process. Random variables connect probability theory to data analysis by giving us a way to describe uncertainty mathematically. Once we define a random variable, we can study its distribution, compute its expected value and variability, and use it to make probability-based statements about future or unseen outcomes.\nIn this chapter, we focus on cases in which the probability distribution of the random variable is completely known. These are idealized settings, such as games of chance, where the probabilities are determined by the rules of the game rather than by data. Working with these simple, controlled examples allows us to understand the mathematical foundations of probability, expected values, standard errors, and sampling distributions.\nIn later chapters, we will turn to real-world data problems, where the underlying distributions are not known. In those cases, we use statistical inference to estimate or approximate these distributions from data. The probability concepts introduced here provide the theoretical foundation for those inferential methods.\nTo start, consider a simple discrete example. Suppose we draw one bead at random from an urn containing red and blue beads. Define the random variable as:\n\\[\nX =\n\\begin{cases}\n1 & \\text{if the bead is blue},\\\\\n0 & \\text{if the bead is red.}\n\\end{cases}\n\\]\nIn R, we can simulate this process:\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2, 3))\nx &lt;- ifelse(sample(beads, 1) == \"blue\", 1, 0)\nEach time we draw a bead, the value of \\(X\\) may change because the outcome is random. A variable like this, which takes only the values 0 and 1, is called a Bernoulli random variable. Bernoulli trials are the building blocks for many statistical models, since many outcomes, such as success/failure, yes/no, or heads/tails, can be represented in this way.\nNote that not all random variables are discrete. Some can take on a continuum of values. For example, the height of a randomly selected person or the result of a physical measurement can be viewed as a continuous random variable. We can simulate such a variable using the normal distribution:\nx &lt;- rnorm(10, mean = 70, sd = 3)\nHere each value of x represents one realization of a random variable drawn from a normal distribution with mean 70 and standard deviation 3. If we were to repeat the simulation, we would obtain slightly different numbers each time, reflecting the inherent randomness of the process.\nThese two examples, a discrete Bernoulli variable and a continuous normal variable, illustrate the main types of random variables we will study in this chapter. Understanding their behavior and summarizing their distributions are the key steps that lead us toward the ideas of expected value, variability, and, ultimately, statistical inference.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#sampling-models",
    "href": "prob/random-variables-sampling-models-clt.html#sampling-models",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.2 Sampling models",
    "text": "5.2 Sampling models\nMany data generation procedures can be effectively modeled as draws from an urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 codes for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing the outcomes for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can be modeled by draws from an urn, reflecting the way individuals are assigned into group; when getting assigned, individuals draw their group at random. Sampling models are therefore ubiquitous in data analysis. Casino games offer a plethora of real-world cases in which sampling models are used to answer specific questions. We will therefore start with these examples.\nSuppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play, and that the only game available on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will decide against installing roulette wheels.\nWe are going to define a random variable \\(S_n\\) that will represent the casino’s total winnings after \\(n\\) games. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn:\n\ncolor &lt;- rep(c(\"Black\", \"Red\", \"Green\"), c(18, 18, 2))\n\nThe 1,000 outcomes from people playing are independent draws from this urn. If red comes up, the gambler wins, and the casino loses a dollar, resulting in the observed random variable being -$1. Otherwise, the casino wins a dollar, and the random variable is $1. To construct our the 1,000 outcomes of the random variable \\(X\\) we can use this code:\n\nn &lt;- 1000\nx &lt;- sample(ifelse(color == \"Red\", -1, 1),  n, replace = TRUE)\n\nNote that the code above is shown for educational purposes only because we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining color:\n\nx &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))\n\nWe call this a sampling model, as it involves modeling the random behavior through the sampling of draws from an urn. The total winnings \\(S_n\\) is simply the sum of these 1,000 independent draws:\n\nn &lt;- 1000\nx &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))\ns &lt;- sum(x)\n\nThinking in terms of a sampling model is a powerful way to approach data analysis when uncertainty is involved. It provides a conceptual link between the random variables we study in probability and the datasets we encounter in practice. By imagining data as random draws from a well-defined process—an urn, a population, or an experiment—we gain clarity about what our models represent and the assumptions they depend on.\nThis perspective helps us interpret statistical procedures more thoughtfully. Many standard methods, such as confidence intervals, hypothesis tests, and regression models, often rely on assumptions that stem from a sampling models. Understanding these assumptions allows us to apply such methods in a principled way, recognize when they may fail, and develop models that more accurately reflect the uncertainty inherent in real data.\nWith this foundation, we can now turn to simple but instructive examples, like casino games, where the sampling model is fully known.\n\n\n\n\n\n\nIn this section, we use casino games as examples because their sampling models are completely known. The probabilities in these settings are defined by clear physical mechanisms,such as the number of red, black, and green pockets on a roulette wheel, which can be represented precisely as draws from an urn. These examples let us focus on the key ideas of probability, randomness, and sampling without the complications that arise in real-world data.\nHowever, it is important to begin connecting these examples to practical applications. In real data analysis, the urn often represents a population, with each individual corresponding to one bead and each bead having a value associated with it, such as height, income, or health outcome. When we collect data, we are effectively drawing a random sample from this population urn.\nThinking this way helps develop statistical intuition. It reminds us that every dataset represents a sample from some underlying process, and that the assumptions behind our models and methods, such as independence and random sampling, are rooted in this conceptual framework. Building this habit early makes it easier to reason clearly about uncertainty and to apply statistical methods thoughtfully in practice.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#the-probability-distribution-of-a-random-variable",
    "href": "prob/random-variables-sampling-models-clt.html#the-probability-distribution-of-a-random-variable",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.3 The probability distribution of a random variable",
    "text": "5.3 The probability distribution of a random variable\nIf you rerun the code above, you see that s changes every time. This is, of course, because \\(S_n\\) is a random variable. The probability distribution of a random variable informs us about the probability of the observed value falling in any given interval. For example, if we want to know the probability that we lose money, we are asking the probability that \\(S_n\\) is in the interval \\((-\\infty,0)\\).\nKeep in mind that if we can define a cumulative distribution function \\(F(a) = \\mathrm{Pr}(S_n\\leq a)\\), then we will be able to answer any question related to the probability of events defined by our random variable \\(S_n\\), including the event \\(S_n&lt;0\\). We call this \\(F\\) the random variable’s distribution function.\nWe can estimate the distribution function for the random variable \\(S\\) by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people repeatedly play roulette, specifically \\(B = 10,000\\) times:\n\nn &lt;- 1000\nB &lt;- 10000\nroulette_winnings &lt;- function(n){\n  x &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))\n  sum(x)\n}\ns &lt;- replicate(B, roulette_winnings(n))\n\nNow, we can ask the following: in our simulation, how often did we get sums less than or equal to a?\n\nmean(s &lt;= a)\n\nThis will be a very good approximation of \\(F(a)\\), allowing us to easily answer the casino’s question “how likely is it that we will lose money?”. We can see it is quite low:\n\nmean(s &lt; 0)\n#&gt; [1] 0.0439\n\nWe can visualize the distribution of \\(S\\) by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\):\n\n\n\n\n\n\n\n\nWe see that the distribution appears to be approximately normal. A qqplot will confirm that the normal approximation is close to a perfect approximation for this distribution.\nAs we have learned, if the distribution is normal, all we need to define it are the average and the standard deviation. Since we have the original values from which the distribution is generated, we can easily compute these with mean(s) and sd(s). The blue curve added to the histogram above is a normal density with this average and standard deviation.\nThis average and this standard deviation have special names; they are referred to as the expected value and standard error of the random variable \\(S_n\\). More details on these concepts will be provided in the next section.\nStatistical theory offers a method to derive the distribution of random variables defined as the sum of independent random draw of numbers from an urn. Specifically, in our example above, we can demonstrate that the number of successes, \\((S+n)/2\\), follows a binomial distribution. We therefore do not need to run Monte Carlo simulations to determine the probability distribution of \\(S\\). The simulations were conducted for illustrative purposes.\nWe can use the function dbinom and pbinom to compute the probabilities exactly. For example, to compute \\(\\mathrm{Pr}(S_n &lt; 0)\\), we note that:\n\\[\\mathrm{Pr}(S_n &lt; 0) =\n\\mathrm{Pr}((S_n + n)/2 &lt; n/2) = \\mathrm{Pr}((S_n + n)/2 \\leq n/2 - 1)\\]\nand we can use the pbinom to compute \\[\\mathrm{Pr}(S_n \\leq 0)\\]:\n\npbinom(n/2 - 1, size = n, prob = 10/19)\n#&gt; [1] 0.0448\n\nFor the details of the binomial distribution, you can consult any basic probability book or even Wikipedia1.\nWe do not delve into these details here. Instead, we will explore a useful theorem provided by mathematical theory, which generally applies to sums and averages of draws from any urn: the Central Limit Theorem (CLT).",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#distributions-versus-probability-distributions",
    "href": "prob/random-variables-sampling-models-clt.html#distributions-versus-probability-distributions",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.4 Distributions versus probability distributions",
    "text": "5.4 Distributions versus probability distributions\nBefore we continue, let’s establish an important distinction and connection between the distribution of a list of numbers and a probability distribution. As explained in Chapter 1, any list of numbers \\(x_1,\\dots,x_n\\) has a distribution. Given their usefulness as summaries when the distribution is approximately normal, we also define the average and standard deviation. These are determined with a straightforward operation involving the vector containing the list of numbers, denoted as x:\n\nm &lt;- sum(x)/length(x)\ns &lt;- sqrt(sum((x - m)^2)/length(x))\n\nA random variable \\(X\\) is associated with a probability distribution, which describes the likelihood of different outcomes. This distribution is a theoretical concept, it does not depend on having a list of numbers or data in hand. As seen above, we define the distribution function as \\(F(a) = \\Pr(X \\leq a)\\), which answers the question: What is the probability that \\(X\\) takes a value less than or equal to \\(a\\)?\nIf \\(X\\) represents the result of drawing a number at random from an urn, then the list of numbers inside the urn defines the possible outcomes. The probability distribution of \\(X\\) corresponds to the relative frequencies of these numbers. The average and standard deviation of the numbers in the urn define the expected value and standard error of the random variable. The practical use of these definitions become aparent later in this chapter.\nAnother way to visualize this idea is through simulation. We can generate a large number of realizations of \\(X\\) using a Monte Carlo simulation, producing a long list of outcomes. The distribution of this simulated list provides an increasingly accurate approximation of the true probability distribution of \\(X\\). As the number of simulated draws grows, the sample average and standard deviation of the list converge to the expected value and standard error of the random variable.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#notation-for-random-variables",
    "href": "prob/random-variables-sampling-models-clt.html#notation-for-random-variables",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.5 Notation for random variables",
    "text": "5.5 Notation for random variables\nIn statistical textbooks, upper case letters denote random variables, and we will adhere to this convention. Lower case letters are used for observed values. You will see some notation that include both. For example, you will see events defined as \\(X \\leq x\\). Here \\(X\\) is a random variable and \\(x\\) is an arbitrary value and not random. So, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see: 1, 2, 3, 4, 5, or 6. In this case, the probability of \\(X=x\\) is 1/6 regardless of the observed value \\(x\\).\nThis notation may seem a bit strange because when we inquire about probability, \\(X\\) is not an observed quantity; it’s a random quantity that we will encounter in the future. We can discuss what we expect \\(X\\) to be, what values are probable, but we can’t discuss what value \\(X\\) is. Once we have data, we do see a realization of \\(X\\). Therefore, data analysts often speak of what could have been after observing what actually happened.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#sec-mean-var-eqs",
    "href": "prob/random-variables-sampling-models-clt.html#sec-mean-var-eqs",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.6 The expected value and standard error",
    "text": "5.6 The expected value and standard error\nWe have described sampling models for draws. We will now review the mathematical theory that allows us to approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion, which we will need to understand how polls work.\nThe first key concept to understand is the expected value. This quantity represents the long-run average outcome of a random variable after many repetitions of the underlying random process. Inq other words, if we could observe the random variable \\(X\\) over and over again, its average value would approach the expected value.\nIn statistics textbooks, the expected value is commonly denoted as \\(\\mu_X\\) or as \\(\\mathrm{E}[X]\\), both meaning “the expected value of the random variable \\(X\\).”\nA random variable will vary around its expected value in a manner that if you take the average of many, many draws, the average will approximate the expected value. This approximation improves as you take more draws, making the expected value a useful quantity to compute.\nFor discrete random variable with possible outcomes \\(x_1,\\dots,x_n\\), the expected value is defined as:\n\\[\n\\mathrm{E}[X] = \\sum_{i=1}^n x_i \\,\\mathrm{Pr}(X = x_i)\n\\] If \\(X\\) is a continuous random variable with a range of values \\(a\\) to \\(b\\) and a probability density function \\(f(x)\\), this sum becomes an integral:\n\\[\n\\mathrm{E}[X] = \\int_a^b x f(x)\\, dx\n\\]\nNote that in the case that we are picking values from an urn, and each value \\(x_i\\) has an equal chance \\(1/n\\) of being selected, the above equation is simply the average of the \\(x_i\\)s.\n\\[\n\\mathrm{E}[X] = \\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\nIn the urn used to model betting on red in roulette, we have 20 one-dollar bills and 18 negative one-dollar bills, so the expected value is:\n\\[\n\\mathrm{E}[X] = (20 + -18)/38 = 1/19\n\\]\nwhich is about 5 cents. You might consider it a bit counterintuitive to say that \\(X\\) varies around 0.05 when it only takes the values 1 and -1. One way to make sense of the expected value in this context is by realizing that, if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this:\n\nB &lt;- 10^6\nx &lt;- sample(c(-1, 1), B, replace = TRUE, prob = c(9/19, 10/19))\nmean(x)\n#&gt; [1] 0.0518\n\nIn general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is:\n\\[\\mathrm{E}[X] = ap + b(1-p)\\]\nTo confirm this, observe that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s and \\(n(1-p)\\) \\(b\\)s, and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\).\nNow, the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sums. This, in turn, is useful for describing the distribution of averages and proportions. The first useful fact is that the expected value of the sum of the draws is the number of draws \\(\\times\\) the average of the numbers in the urn.\nTherefore, if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50.\nHowever, this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question.\nThe standard error (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use \\(\\sigma_X\\) or \\(\\mathrm{SE}[X]\\) to denote the standard error of a random variable.\n\n\n\n\n\n\nStatistical textbooks often use the Greek letters \\(\\mu\\) and \\(\\sigma\\) as shorthand for the expected value and the standard error, respectively. The notation comes from the fact that \\(\\mu\\) is the Greek equivalent of the letter m, the first letter of mean, which is another term for expected value. Likewise, \\(\\sigma\\) corresponds to the letter s, the first letter of standard deviation (and by extension, standard error). This convention helps keep mathematical expressions concise and consistent across statistical formulas.\nIn this book, however, we prefer the \\(\\mathrm{E}[X]\\) and \\(\\mathrm{SE}[X]\\) notation because it makes it clearer when we express expectations and standard errors of sums or other mathematical transformations of random variables. For example, when considering the sum of two random variables, we can write \\(\\mathrm{E}[X + Y]\\) directly.\n\n\n\nFor discrete random variable with possible outcomes \\(x_1,\\dots,x_n\\), the standard error is defined as:\n\\[\n\\mathrm{SE}[X] = \\sqrt{\\sum_{i=1}^n \\left(x_i - E[X]\\right)^2 \\,\\mathrm{Pr}(X = x_i)},\n\\] which you can think of as the expected average distance of \\(X\\) from the expected value.\nIf \\(X\\) is a continuous random variable, with range of values \\(a\\) to \\(b\\) and probability density function \\(f(x)\\), this sum becomes integral:\n\\[\n\\mathrm{SE}[X] = \\sqrt{\\int_a^b \\left(x-\\mathrm{E}[X]\\right)^2 f(x)\\,\\mathrm{d}x}\n\\]\n\n\n\n\n\n\nIn many statistics textbooks, you will see the variance of a random variable defined as the square of its standard error:\n\\[\n\\mathrm{SE}[X] = \\sqrt{\\mathrm{Var}[X]}.\n\\]\nThe variance is often used in mathematical derivations because it simplifies algebra by avoiding square roots. However, the standard error is more useful in practice, since it measures variability in the same units as the random variable itself. For example, if \\(X\\) represents height in inches, then \\(\\mathrm{SE}[X]\\) is also measured in inches, making it easier to interpret.\n\n\n\nNote that in the case that we are picking values from an un urn where each value \\(x_i\\) has an equal chance \\(1/n\\) of being selected, the above equation is simply the standard deviation of of the \\(x_i\\)s.\n\\[\n\\mathrm{SE}[X] = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\mathrm{E}[X])^2} \\mbox{ with }  \\mathrm{E}[X] =  \\frac{1}{n}\\sum_{i=1}^n x_i\n\\] Using the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\), respectively, the standard deviation is:\n\\[| b - a | \\sqrt{p(1-p)}.\\]\nSo in our roulette example, the standard deviation of the values inside the urn is \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) or:\n\n2*sqrt(90)/19\n#&gt; [1] 0.999\n\nThe standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we obtain either 1 or -1, with 1 slightly favored over -1.\nA widely used mathematical result is that if our draws are independent, then the standard error of the sum is given by the equation:\n\\[\n\\sqrt{\\mbox{number of draws}} \\times \\mbox{ standard deviation of the numbers in the urn}\n\\]\nUsing this formula, the sum of 1,000 people playing has standard error of about $32:\n\nsqrt(1000)*2*sqrt(90)/19\n#&gt; [1] 31.6\n\nAs a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet to install more roulette wheels. But we still haven’t answered the question: How likely is the casino to lose money? The CLT will help in this regard.\n\n\n\n\n\n\nThe exact probability for the casino winnings can be computed precisely, rather than approximately, using the binomial distribution. However, here we focus on the CLT, which can be applied more broadly to sums of random variables in a way that the binomial distribution cannot.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#sec-clt-prob",
    "href": "prob/random-variables-sampling-models-clt.html#sec-clt-prob",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.7 Central Limit Theorem",
    "text": "5.7 Central Limit Theorem\nThe Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Given that sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.\nPreviously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the expected value and standard error.\nWe previously ran this Monte Carlo simulation:\n\nn &lt;- 1000\nB &lt;- 10000\nroulette_winnings &lt;- function(n){\n  x &lt;- sample(c(-1, 1), n, replace = TRUE, prob = c(9/19, 10/19))\n  sum(x)\n}\ns &lt;- replicate(B, roulette_winnings(n))\n\nThe Central Limit Theorem (CLT) tells us that the sum s is approximated by a normal distribution. Using the formulas above, we know that the expected value and standard error are:\n\nn*(20 - 18)/38 \n#&gt; [1] 52.6\nsqrt(n)*2*sqrt(90)/19 \n#&gt; [1] 31.6\n\nThe theoretical values above match those obtained with the Monte Carlo simulation:\n\nmean(s)\n#&gt; [1] 53.1\nsd(s)\n#&gt; [1] 31.6\n\nUsing the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:\n\nmu &lt;- n*(20 - 18)/38\nse &lt;- sqrt(n)*2*sqrt(90)/19 \npnorm(0, mu, se)\n#&gt; [1] 0.0478\n\nwhich is also in very good agreement with our Monte Carlo result:\n\nmean(s &lt; 0)\n#&gt; [1] 0.0434\n\n\n5.7.1 How large is large in the Central Limit Theorem?\nThe CLT works when the number of draws is large, but large is a relative term. In many circumstances, as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, much larger sample sizes are needed.\nTo illustrate an important limitation of the Central Limit Theorem (CLT), consider a lottery. In most government-run lotteries, the chance of winning the grand prize is extremely small, often less than 1 in a million. Although thousands or even millions of people may buy tickets, the number of winners, the sum of all successes, is typically between 0 and a few individuals.\nIn this situation, the sum of the draws does not follow a shape that is well approximated by a normal distribution, even with such a large number of trials. The reason is that the probability of success is so small that the distribution of the total number of wins is highly skewed. In cases like this, when we have a large number of independent trials but each has a very low probability of success, the Poisson distribution provides a much better approximation than the normal.\nYou can explore the properties of the Poisson distribution using dpois and ppois. You can generate random variables following this distribution with rpois. However, we won’t cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia2.\nIn summary, the Central Limit Theorem is one of the most powerful results in probability and statistics. It explains why the normal distribution appears so often in data analysis and why many standard methods work well in practice, even when the underlying data are not normally distributed. When the number of draws is reasonably large and no single draw dominates the total, the CLT provides an accurate and remarkably robust approximation.\nThat said, rules of thumb such as “30 is enough” should always be treated as guidelines rather than guarantees. The CLT works under specific conditions, and when those conditions are not met, such as when probabilities are extremely small or the underlying population distribution is highly skewed, the normal approximation can fail badly. In these cases, other distributions, like the Poisson or binomial, may be more appropriate.\nDeveloping an intuition for when the CLT applies and when it does not is an important part of becoming a thoughtful data analyst. Recognizing the assumptions behind statistical methods and questioning whether they hold in your context is what separates mechanical computation from principled analysis.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#statistical-properties-of-averages",
    "href": "prob/random-variables-sampling-models-clt.html#statistical-properties-of-averages",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.8 Statistical properties of averages",
    "text": "5.8 Statistical properties of averages\nThere are several useful mathematical results that we used above and often employ when working with data. We list them below.\n1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:\n\\[\n\\mathrm{E}[X_1+X_2+\\dots+X_n] =  \\mathrm{E}[X_1] + \\mathrm{E}[X_2]+\\dots+\\mathrm{E}[X_n]\n\\]\nIf \\(X\\) represents independent draws from the urn, then they all have the same expected value. Let’s denote the expected value with \\(\\mu_X\\) and rewrite the equation as:\n\\[\n\\mathrm{E}[X_1+X_2+\\dots+X_n]=  n\\mu_X\n\\]\nwhich is another way of writing the result we show above for the sum of draws.\n2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:\n\\[\n\\mathrm{E}[aX] =  a\\times\\mathrm{E}[X]\n\\]\nTo understand why this is intuitive, consider changing units. If we change the units of a random variable, such as from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, denoted as \\(\\mu_X\\) again:\n\\[\n\\mathrm{E}[(X_1+X_2+\\dots+X_n) / n]=   \\mathrm{E}[X_1+X_2+\\dots+X_n] / n = n\\mu_X/n = \\mu_X\n\\]\n3. The variance of the sum of independent random variables is the sum of variances of each random variable:\n\\[\n\\mathrm{Var}[X_1+X_2+\\dots+X_n] =\\mathrm{Var}[X_1] + \\mathrm{Var}[X_2]+\\dots+\\mathrm{Var}[X_n]  \n\\] This implies that the following property for the standard error of the sum of independent random variables:\n\\[\n\\mathrm{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mathrm{SE}[X_1]^2 + \\mathrm{SE}[X_2]^2+\\dots+\\mathrm{SE}[X_n]^2  }\n\\]\nNote that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.\n4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation:\n\\[\n\\mathrm{SE}[aX] =  a \\times \\mathrm{SE}[X]\n\\]\nTo see why this is intuitive, again think of units.\nA consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma_X\\):\n\\[\n\\begin{aligned}\n\\mathrm{SE}[\\bar{X}] = \\mathrm{SE}[(X_1+X_2+\\dots+X_n) / n] &=   \\mathrm{SE}[X_1+X_2+\\dots+X_n]/n \\\\\n&= \\sqrt{\\mathrm{SE}[X_1]^2+\\mathrm{SE}[X_2]^2+\\dots+\\mathrm{SE}[X_n]^2}/n \\\\\n&= \\sqrt{\\sigma_X^2+\\sigma_X^2+\\dots+\\sigma_X^2}/n\\\\\n&= \\sqrt{n\\sigma_X^2}/n\\\\\n&= \\sigma_X / \\sqrt{n}    \n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe assumption of independence is important\n\n\n\nThe given equation reveals crucial insights for practical scenarios. Specifically, it suggests that the standard error can be minimized by increasing the sample size, \\(n\\), and we can quantify this reduction. However, this principle holds true only when the variables \\(X_1, X_2, ... X_n\\) are independent. If they are not, the estimated standard error can be significantly off.\nIn Section 14.2, we introduce the concept of correlation, which quantifies the degree to which variables are interdependent. If the correlation coefficient among the \\(X\\) variables is \\(\\rho\\), the standard error of their average is:\n\\[\n\\mathrm{SE}\\left(\\bar{X}\\right) = \\sigma_X \\sqrt{\\frac{1 + (n-1) \\rho}{n}}\n\\]\nThe key observation here is that as \\(\\rho\\) approaches its upper limit of 1, the standard error increases. Notably, in the situation where \\(\\rho = 1\\), the standard error, \\(\\mathrm{SE}(\\bar{X})\\), equals \\(\\sigma_X\\), and it becomes unaffected by the sample size \\(n\\).\n\n\n5. If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\).\n\n5.8.1 Law of large numbers\nAn important implication of result 4 above is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.\n\n\n\n\n\n\nMisinterpretation of the law of averages\n\n\n\nThe law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. Yet these events are independent so the chance of a coin landing heads is 50%, regardless of the previous 5. The same principle applies to the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses. Another funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#recommended-reading",
    "href": "prob/random-variables-sampling-models-clt.html#recommended-reading",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.9 Recommended reading",
    "text": "5.9 Recommended reading\n\nDavid Freedman, Robert Pisani & Roger Purves, Statistics.\nThis book emphasizes clear reasoning, real examples, and conceptual insight rather than formula memorization.\nDavid S. Moore, George P. McCabe, and Bruce A. Craig, Introduction to the Practice of Statistics — A clear and intuitive undergraduate text emphasizing sampling and inference.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#exercises",
    "href": "prob/random-variables-sampling-models-clt.html#exercises",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "\n5.10 Exercises",
    "text": "5.10 Exercises\n1. In American Roulette, you can also bet on green. There are 18 reds, 18 blacks, and 2 greens (0 and 00). What are the chances the green comes out?\n2. The payout for winning on green is $17 dollars. This means that if you bet a dollar and it lands on green, you get $17. Create a sampling model using sample to simulate the random variable \\(X\\) for your winnings. Hint: Refer to the example below for how it should look like when betting on red.\n3. Compute the expected value of \\(X\\).\n4. Compute the standard error of \\(X\\).\n5. Now create a random variable \\(S_n\\) that is the sum of your winnings after betting on green 1000 times. Hint: change the argument size and replace in your answer to exercise 2. Start your code by setting the seed to 1 with set.seed(1).\n6. What is the expected value of \\(S_n\\)?\n7. What is the standard error of \\(S_n\\)?\n8. What is the probability that you end up winning money? Hint: Use the CLT.\n9. Create a Monte Carlo simulation that generates 1,000 outcomes of \\(S_n\\). Compute the average and standard deviation of the resulting list to confirm the results of 6 and 7. Start your code by setting the seed to 1 with set.seed(1).\n10. Now check your answer to 8 using the Monte Carlo result.\n11. The Monte Carlo result and the CLT approximation are close, but not that close. What could account for this?\n\n1,000 simulations is not enough. If we do more, they match.\nThe CLT does not work as well when the probability of success is small. In this case, it was 1/19. If we make the number of roulette plays bigger, they will match better.\nThe difference is within rounding error.\nThe CLT only works for averages.\n\n12. Now create a random variable \\(Y\\) that is your average winnings per bet, after playing off your winnings after betting on green 1,000 times.\n13. What is the expected value of \\(Y\\)?\n14. What is the standard error of \\(Y\\)?\n15. What is the probability that you end up with winnings per game that are positive? Hint: Use the CLT.\n16. Create a Monte Carlo simulation that generates 2,500 outcomes of \\(Y\\). Compute the average and standard deviation of the resulting list to confirm the results of 13 and 14. Start your code by setting the seed to 1 with set.seed(1).\n17. Now compare your answer to 15 using the Monte Carlo result.\n18. The Monte Carlo result and the CLT approximation are now much closer. What could account for this?\n\nWe are now computing averages instead of sums.\n2,500 Monte Carlo simulations is not better than 1,000.\nThe CLT works better when the sample size is larger. We increased from 1,000 to 2,500.\nIt is not closer. The difference is within rounding error.\n\nThe following exercises are inspired by the events surrounding the financial crisis of 2007-20083. This financial crisis was in part caused by underestimating the risk of certain securities4 sold by financial institutions. Specifically, the risks of mortgage-backed securities (MBS) and collateralized debt obligations (CDO) were grossly underestimated. These assets were sold at prices that assumed most homeowners would make their monthly payments, and the probability of this not occurring was calculated as being low. A combination of factors resulted in many more defaults than were expected, which led to a price crash of these securities. As a consequence, banks lost so much money that they required government bailouts to avoid complete closure.\n19. More complex versions of the sampling models we have discussed are also used by banks to determine interest rates and insurance companies to determine premiums. To understand this, suppose you run a small bank that has a history of identifying potential homeowners that can be trusted to make payments. In fact, historically, only 2% of your customers default in a given year, meaning that they don’t pay back the money that you lent them. Suppose your bank will give out $n=$1,000 loans for $180,000 this year. Also, after adding up all costs, suppose your bank loses \\(l\\)=$200,000 per foreclosure. For simplicity, we assume this includes all operational costs. What is the expected profit \\(S_n\\) for you bank under this scenario?\n20. Note that the total loss defined by the final sum in the previous exercise is a random variable. Every time you run the sampling model code, you obtain a different number of people defaulting which results in a different loss. Code a sampling model for the random variable representing your banks profit \\(S_n\\) under scenario described in 19.\n21. The previous exercise demonstrates that if you simply loan money to everybody without interest, you will end up losing money due to the 2% that defaults. Although you know 2% of your clients will probably default, you don’t know which ones, so you can’t remove them. Yet by charging everybody just a bit extra in interest, you can make up the losses incurred due to that 2%, and also cover your operating costs. What quantity \\(x\\) would you have to charge each borrower so that your bank’s expected profit is 0? Assume that you don’t get \\(x\\) from the borrowers that default. Also, note \\(x\\) is not the interest rate, but the total you add meaning \\(x/180000\\) is the interest rate.\n22. Rewrite the sample model from exercise 20 and run a Monte Carlo simulation to get an idea of the distribution of your profit when you charge interest rates.\n23. We don’t actually need a Monte Carlo simulation. Based on what we have learned, the CLT informs us that, since our losses are a sum of independent draws, its distribution is approximately normal. What are the expected value and standard errors of the profit \\(S_n\\)? Write these as functions of the probability of foreclosure \\(p\\), the number of loans \\(n\\), the loss per foreclosure \\(l\\), and the quantity you charge each borrower \\(x\\).\n24. If you set \\(x\\) to assure your bank breaks even (expected profit is 0), what is the probability that your bank loses money?\n25. Suppose that if your bank has negative profit, it has to close. Therefore, you need to increase \\(x\\) to minimize this risk. However, setting the interest rates too high may lead your clients to choose another bank. So, let’s say that we want our chances of losing money to be 1 in 100. What does the \\(x\\) quantity need to be now? Hint: We want \\(\\mathrm{Pr}(S_n&lt;0) = 0.01\\). Note that you can add subtract constants to both side of an inequality, and the probability does not change: \\(\\mathrm{Pr}(S_n&lt;0) = \\mathrm{Pr}(S_n+k&lt;0+k)\\), Similarly, with division of positive constants: \\(\\mathrm{Pr}(S_n+k&lt;0+k) = \\mathrm{Pr}((S_n+k)/m &lt;k/m)\\). Use this fact and the CLT to transform the left side of the inequality in \\(\\mathrm{Pr}(S_n&lt;0)\\) into a standard normal.\n26. Our interest rate now increases. But it is still a very competitive interest rate. For the \\(x\\) you obtained in 25, what is expected profit per loan and the expected total profit?\n27. Run run a Monte Carlo simulation to double check the theoretical approximation used in 25 and 26.\n28. One of your employees points out that, since the bank is making a profit per loan, the bank should give out more loans! Why limit it to just \\(n\\)? You explain that finding those \\(n\\) clients was hard. You need a group that is predictable and that keeps the chances of defaults low. The employee then points out that even if the probability of default is higher, as long as our expected value is positive, you can minimize your chances of losses by increasing \\(n\\) and relying on the law of large numbers. Suppose the default probability is twice as high, or 4%, and you set the interest rate to 5%, or \\(x=\\)$9,000, what is your expected profit per loan?\n29. How much do we have to increase \\(n\\) by to assure the probability of losing money is still less than 0.01?\n30. Confirm the result in exercise 29 with a Monte Carlo simulation.\n31. According to this equation, giving out more loans increases your expected profit and lowers the chances of losing money! Giving out more loans seems like a no-brainier. As a result, your colleague decides to leave your bank and start his own high-risk mortgage company. A few months later, your colleague’s bank has gone bankrupt. A book is written, and eventually, the movies “The Big Short” and “Margin Call” are made, recounting the mistake your friend, and many others, made. What happened?\nYour colleague’s scheme was mainly based on this mathematical formula \\(\\mathrm{SE}\\left(\\bar{X}\\right) = \\sigma_X / \\sqrt{n}\\). By making \\(n\\) large, we minimize the standard error of our per-loan profit. However, for this rule to hold, the \\(X\\)s must be independent draws: one person defaulting must be independent of others defaulting.\nTo construct a more realistic simulation than the original one your colleague ran, let’s assume there is a global event affecting everybody with high-risk mortgages and altering their probability simultaneously. We will assume that with a 50-50 chance all the default probabilities slightly increase or decrease to somewhere between 0.03 and 0.05. However, this change occurs universally, impacting everybody at once, not just one person. As these draws are no longer independent, our equation for the standard error of the sum of random variables does not apply. Write a Monte Carlo simulation for your total profit with this model.\n32. Use the simulation results to report the expected profit, the probability of losing money, and the probability of losing more than $10,000,000. Study the distribution of profit and discuss how making the wrong assumption lead to a catastrophic result.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "prob/random-variables-sampling-models-clt.html#footnotes",
    "href": "prob/random-variables-sampling-models-clt.html#footnotes",
    "title": "\n5  Foundations of statistical inference\n",
    "section": "",
    "text": "https://en.wikipedia.org/w/index.php?title=Binomial_distribution↩︎\nhttps://en.wikipedia.org/w/index.php?title=Poisson_distribution↩︎\nhttps://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008↩︎\nhttps://en.wikipedia.org/w/index.php?title=Security_(finance)↩︎",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundations of statistical inference</span>"
    ]
  },
  {
    "objectID": "inference/intro-inference.html",
    "href": "inference/intro-inference.html",
    "title": "Statistical inference",
    "section": "",
    "text": "Statistical Inference is the branch of statistics dedicated to distinguishing patterns arising from signal versus those arising from chance. It is a broad topic and, in this section, we review the basics using polls as a motivating example. To illustrate the concepts, we supplement mathematical formulas with Monte Carlo simulations and R code. We demonstrate the practical value of these concepts by using election forecasting as a case study.\nThe day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. Political commentator Joe Scarborough said during his show1:\nTo which Nate Silver responded via Twitter:\nObama won the election.\nIn 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. In contrast, many other forecasters were almost certain she would win. She lost. But 71% is still more than 50%, so was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed or cards being dealt somewhere?\nIn this part of the book, we will demonstrate how the probability concepts covered in the previous part can be applied to develop statistical approaches that render polls effective tools. Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and straightforward example to introduce the main concepts of statistical inference. Forecasting an election is a more complex process that involves combining results from 50 states and DC. We will delve into this subject after we cover all the basic concepts. Specifically, we will learn the statistical concepts necessary to define estimates and margins of errors for the popular vote, and show how these are used to construct confidence intervals. Once we grasp these ideas, we will be able to understand statistical power and p-values, concepts that are ubiquitous in, for example, the academic literature. We will then aggregate data from different pollsters to highlight the shortcomings of the models used by traditional pollsters and present a method for improving these models. To understand probabilistic statements about the chances of a candidate winning, we will introduce Bayesian modeling. Finally, we put it all together using hierarchical models to recreate a simplified version of the FiveThirtyEight model and apply it to the 2016 election.\nThe final two chapters explore two important inference topics—hypothesis testing and bootstrapping—that are widely used in practice but were not required for the case study.",
    "crumbs": [
      "Statistical inference"
    ]
  },
  {
    "objectID": "inference/intro-inference.html#footnotes",
    "href": "inference/intro-inference.html#footnotes",
    "title": "Statistical inference",
    "section": "",
    "text": "https://www.youtube.com/watch?v=TbKkjm-gheY↩︎",
    "crumbs": [
      "Statistical inference"
    ]
  },
  {
    "objectID": "inference/parameters-estimates.html",
    "href": "inference/parameters-estimates.html",
    "title": "\n6  Parameters and Estimates\n",
    "section": "",
    "text": "6.1 The sampling model for polls\nOpinion polling has been conducted since the 19th century. The general aim is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive in the US during presidential elections. Polls are useful when interviewing every member of a specific population is logistically impossible. The general strategy involves interviewing a smaller, randomly chosen group and then inferring the opinions of the entire population from those of this subset. Statistical theory, known as inference, is used to justify the process and is the primary focus of this part of the book.\nPerhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists extensively use polls to decide, among other things, where to allocate resources, such as determining the geographical locations to focus their “get out the vote” efforts.\nElections are a particularly interesting instances of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an real election, which makes polling a cost-effective strategy for those seeking to forecast the results. In addition to strategist, news organizations are also interested in forecasting elections due to the apparent demand for what they reveal.\nWe start by connecting probability theory to the task of using polls to learn about a population.\nAlthough typically the results of polls run by political candidates are kept private, polls are also conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at these public datasets.\nReal Clear Politics1 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results, reporting estimates of the popular vote for the 2016 presidential election2:\nPoll\nDate\nSample\nMoE\nClinton\nTrump\nSpread\n\n\n\nRCP Average\n10/31 - 11/7\n--\n--\n47.2\n44.3\nClinton +2.9\n\n\nBloomberg\n11/4 - 11/6\n799 LV\n3.5\n46.0\n43.0\nClinton +3\n\n\nEconomist\n11/4 - 11/7\n3669 LV\n--\n49.0\n45.0\nClinton +4\n\n\nIBD\n11/3 - 11/6\n1026 LV\n3.1\n43.0\n42.0\nClinton +1\n\n\nABC\n11/3 - 11/6\n2220 LV\n2.5\n49.0\n46.0\nClinton +3\n\n\nFOX News\n11/3 - 11/6\n1295 LV\n2.5\n48.0\n44.0\nClinton +4\n\n\nMonmouth\n11/3 - 11/6\n748 LV\n3.6\n50.0\n44.0\nClinton +6\n\n\nCBS News\n11/2 - 11/6\n1426 LV\n3.0\n47.0\n43.0\nClinton +4\n\n\nLA Times\n10/31 - 11/6\n2935 LV\n4.5\n43.0\n48.0\nTrump +5\n\n\nNBC News\n11/3 - 11/5\n1282 LV\n2.7\n48.0\n43.0\nClinton +5\n\n\nNBC News\n10/31 - 11/6\n30145 LV\n1.0\n51.0\n44.0\nClinton +7\n\n\nMcClatchy\n11/1 - 11/3\n940 LV\n3.2\n46.0\n44.0\nClinton +2\n\n\nReuters\n10/31 - 11/4\n2244 LV\n2.2\n44.0\n40.0\nClinton +4\n\n\nGravisGravis\n10/31 - 10/31\n5360 RV\n1.3\n50.0\n50.0\nTie\nLet’s make some observations about the table above. First, observe that different polls, all conducted days before the election, report different spreads: the estimated difference between support for the two candidates. Notice that the reported spreads hover around what eventually became the actual result: Clinton won the popular vote by 2.1%. Additionally, we see a column titled MoE which stands for margin of error.\nTo help us understand the connection between polls and what we have learned, let’s construct a situation similar to what pollsters face. To simulate the challenge pollsters encounter in terms of competing with other pollsters for media attention, we will use an urn filled with beads to represent voters, and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar):\nBefore making a prediction, you can take a sample (with replacement) from the urn. To reflect the fact that running polls is expensive, it costs you $0.10 for each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you would have paid $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you receive half what you paid and proceed to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\nlibrary(tidyverse)\nlibrary(dslabs)\ntake_poll(25)\nThink about how you would construct your interval based on the data shown above.\nWe have just described a simple sampling model for opinion polls. In this model, the beads inside the urn represent individuals who will vote on election day. The red beads represent those voting for the Republican candidate, while the blue beads represent the Democrats. For simplicity, let’s assume there are no other colors; that is, that there are just two parties: Republican and Democratic.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parameters and Estimates</span>"
    ]
  },
  {
    "objectID": "inference/parameters-estimates.html#populations-samples-parameters-and-estimates",
    "href": "inference/parameters-estimates.html#populations-samples-parameters-and-estimates",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.2 Populations, samples, parameters, and estimates",
    "text": "6.2 Populations, samples, parameters, and estimates\nWe want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The goal of statistical inference is to predict the parameter \\(p\\) based on the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it seems unlikely that \\(p\\) &gt; .9 or \\(p\\) &lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?\nWe want to construct an estimate of \\(p\\) using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.\n\n\n\n\n\n\n\n\nObserve that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can improve it.\n\n6.2.1 The sample average as an estimate\nConducting an opinion poll is being modeled as taking a random sample from an urn. We propose using the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can easily report an estimate for the spread \\(2p-1\\). However, for simplicity, we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to justify our use of the sample proportion and to quantify its proximity to the population proportion \\(p\\).\nWe start by defining the random variable \\(X\\) as \\(X=1\\), if we pick a blue bead at random, and \\(X=0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads, and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In statistics textbooks, a bar on top of a symbol typically denotes the average. The theory we just covered about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\\[\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i\n\\]\nFor simplicity, let’s assume that the draws are independent; after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nHere, we encounter an important difference compared to what we did in the probability part of the book: we don’t know the composition of the urn. While we know there are blue and red beads, we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\).\n\n6.2.2 Parameters\nJust as we use variables to define unknowns in systems of equations, in statistical inference, we define parameters to represent unknown parts of our models. In the urn model, which we are using to simulate an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. Since our main goal is determining \\(p\\), we are going to estimate this parameter.\n\n\n\n\n\n\nIntroductory statistics textbooks typically begin by introducing the population average as the first example of a parameter. In our case, the parameter of interest \\(p\\) is defined as the proportion of 1s (blue) in the urn. Notice that this proportion is also equal to the average of all the numbers in the urn, since the 1s and 0s can be treated as numeric values.\nThis means that our parameter \\(p\\) can be interpreted as a population average. For this reason, we will use \\(\\bar{X}\\) to denote its estimate, the average computed from a sample of draws from the urn. Although many textbooks use the notation \\(\\hat{p}\\) for this estimate, the symbol \\(\\bar{X}\\) better emphasizes the connection between sample averages and population means, a concept that extends naturally to situations beyond binary data.\n\n\n\nThe concepts presented here on how we estimate parameters, and provide insights into how good these estimates are, extend to many data analysis tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group, investigate the health effects of smoking on a population, analyze the differences in racial groups of fatal shootings by police, or assess the rate of change in life expectancy in the US during the last 10 years. All these questions can be framed as a task of estimating a parameter from a sample.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parameters and Estimates</span>"
    ]
  },
  {
    "objectID": "inference/parameters-estimates.html#estimate-properties-expected-value-and-standard-error",
    "href": "inference/parameters-estimates.html#estimate-properties-expected-value-and-standard-error",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.3 Estimate properties: expected value and standard error",
    "text": "6.3 Estimate properties: expected value and standard error\nTo understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered Section 5.6 apply.\nApplying the concepts we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, denoted as \\(p\\). Dividing by the non-random constant \\(N\\) yields the expected value of the average \\(\\bar{X}\\) as \\(p\\). We can write it using our mathematical notation:\n\\[\n\\mathrm{E}(\\bar{X}) = p\n\\]\nWe can also use what we learned to determine the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error of the average:\n\\[\n\\mathrm{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\n\\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\), and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough sample, our estimate converges to \\(p\\).\nIf we take a large enough sample to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t compute the standard error. However, for illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):\n\n\n\n\n\n\n\n\nThe plot shows that we would need a poll of over 10,000 people to achieve a standard error that low. We rarely see polls of this size due in part to the associated costs. According to the Real Clear Politics table, sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is:\n\nsqrt(p*(1 - p))/sqrt(1000)\n#&gt; [1] 0.0158\n\nor 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\) and we do that in Chapter 7.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parameters and Estimates</span>"
    ]
  },
  {
    "objectID": "inference/parameters-estimates.html#polling-versus-forecasting",
    "href": "inference/parameters-estimates.html#polling-versus-forecasting",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.4 Polling versus forecasting",
    "text": "6.4 Polling versus forecasting\nBefore we continue, it’s important to clarify a practical issue related to forecasting an election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment, and not for election day. The \\(p\\) for election night might be different, as people’s opinions tend to fluctuate through time. Generally, the polls conducted the night before the election tend to be the most accurate, since opinions do not change significantly in a day. However, forecasters try to develop tools that model how opinions vary over time and aim to predict the election night results by taking into consideration these fluctuations. We will explore some approaches for doing this in a later section.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parameters and Estimates</span>"
    ]
  },
  {
    "objectID": "inference/parameters-estimates.html#exercises",
    "href": "inference/parameters-estimates.html#exercises",
    "title": "\n6  Parameters and Estimates\n",
    "section": "\n6.5 Exercises",
    "text": "6.5 Exercises\n1. Suppose you poll a population in which a proportion \\(p\\) of voters are Democrats and \\(1-p\\) are Republicans. Your sample size is \\(N=25\\). Consider the random variable \\(S\\), which is the total number of Democrats in your sample. What is the expected value of this random variable? Hint: It’s a function of \\(p\\).\n2. What is the standard error of \\(S\\) ? Hint: It’s a function of \\(p\\).\n3. Consider the random variable \\(S/N\\). This is equivalent to the sample average, which we have been denoting as \\(\\bar{X}\\). What is the expected value of the \\(\\bar{X}\\)? Hint: It’s a function of \\(p\\).\n4. What is the standard error of \\(\\bar{X}\\)? Hint: It’s a function of \\(p\\).\n5. Write a line of code that gives you the standard error se for the problem above for several values of \\(p\\), specifically for p &lt;- seq(0, 1, length = 100). Make a plot of se versus p.\n6. Copy the code above and put it inside a for-loop to make the plot for \\(N=25\\), \\(N=100\\), and \\(N=1000\\).\n7. If we are interested in the difference in proportions, \\(\\mu = p - (1-p)\\), our estimate is \\(\\hat{\\mu} = \\bar{X} - (1-\\bar{X})\\). Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of \\(\\hat{\\mu}\\).\n8. What is the standard error of \\(\\hat{\\mu}\\)?\n9. If the actual \\(p=.45\\), it means the Republicans are winning by a relatively large margin, since \\(\\mu = -.1\\), which is a 10% margin of victory. In this case, what is the standard error of \\(2\\hat{X}-1\\) if we take a sample of \\(N=25\\)?\n10. Given the answer to exercise 9, which of the following best describes your strategy of using a sample size of \\(N=25\\)?\n\nThe expected value of our estimate \\(2\\bar{X}-1\\) is \\(\\mu\\), so our prediction will be accurate.\nOur standard error is larger than the difference, so the chances of \\(2\\bar{X}-1\\) representing a large margin are not small. We should pick a larger sample size.\nThe difference is 10% and the standard error is about 0.2, therefore much smaller than the difference.\nBecause we don’t know \\(p\\), we have no way of knowing that making \\(N\\) larger would actually improve our standard error.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parameters and Estimates</span>"
    ]
  },
  {
    "objectID": "inference/parameters-estimates.html#footnotes",
    "href": "inference/parameters-estimates.html#footnotes",
    "title": "\n6  Parameters and Estimates\n",
    "section": "",
    "text": "http://www.realclearpolitics.com↩︎\nhttp://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html↩︎",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parameters and Estimates</span>"
    ]
  },
  {
    "objectID": "inference/clt.html",
    "href": "inference/clt.html",
    "title": "7  Central Limit Theorem",
    "section": "",
    "text": "7.1 Margin of Error\nIn Section 5.7 of the Probability part of the book, we introduced the Central Limit Theorem (CLT) as one of the most powerful results in statistics. We learned that, when the sample size is large, the sum or average of independent draws from a population has a distribution that is approximately normal, regardless of the shape of the original population distribution.\nIn this chapter, we revisit the CLT from an inferential perspective. In probability, we used it to describe the behavior of random variables such as the total winnings in a roulette game. Now, our goal is to use it to quantify the uncertainty in estimates computed from data. In our poll example, the CLT tells us that the sample average \\(\\bar{X}\\) follows an approximately normal distribution centered at the true population parameter, with a standard error that depends on the sample size.\nThis approximation allows us to answer practical questions such as:\nIn the remainder of this chapter, we will see how the CLT helps answer this question and provides the foundation for confidence intervals, hypothesis testing, and many of the inferential tools used throughout statistics.\nIn practice, pollsters summarize uncertainty using a single, easy-to-interpret number called the margin of error. The margin of error, together with the estimate of the parameter, allows them to report an interval that they are very confident contains the true value. But what does “very confident” actually mean? Using the CLT, we can quantify this statement and connect it directly to probability.\nTo understand how the margin of error arises, let’s return to the question above and compute:\n\\[\n\\Pr(| \\bar{X} - p| \\leq 0.02) =\n\\Pr(\\bar{X} \\leq p + 0.02) - \\Pr(\\bar{X} \\leq p - 0.02).\n\\]\nWe start by standardizing \\(\\bar{X}\\), subtracting its expected value and dividing by its standard error, to obtain a standard normal random variable, denoted by \\(Z\\):\n\\[\nZ = \\frac{\\bar{X} - \\mathrm{E}[\\bar{X}]}{\\mathrm{SE}[\\bar{X}]}.\n\\]\nSince \\(p\\) is the expected value and \\(\\mathrm{SE}(\\bar{X}) = \\sqrt{p(1 - p)/N}\\) is the standard error, we can rewrite the probability above as:\n\\[\n\\Pr(\\bar{X}\\leq p + 0.02) - \\Pr(\\bar{X} \\leq p - 0.02)\n=\n\\Pr\\left(Z \\leq \\frac{0.02}{\\mathrm{SE}(\\bar{X})} \\right)\n-\n\\Pr\\left(Z \\leq -\\frac{0.02}{\\mathrm{SE}(\\bar{X})} \\right).\n\\]\nThe challenge is that we do not know \\(p\\), and therefore we do not know \\(\\mathrm{SE}(\\bar{X})\\). However, the CLT still works if we estimate the standard error by substituting \\(\\bar{X}\\) for \\(p\\). We call this a plug-in estimate:\n\\[\n\\hat{\\mathrm{SE}}(\\bar{X}) = \\sqrt{\\bar{X}(1 - \\bar{X})/N}.\n\\]\nIn statistics, a hat symbol indicates an estimate obtained from data. Using the observed sample and \\(N\\), we can compute this estimated standard error.\nLet’s continue our calculation using \\(\\hat{\\mathrm{SE}}(\\bar{X})\\). In our first sample, we had 12 blue and 13 red balls, giving \\(\\bar{X} = 0.48\\). The estimated standard error is:\nx_hat &lt;- 0.48\nse &lt;- sqrt(x_hat*(1 - x_hat)/25)\nse\n#&gt; [1] 0.0999\nNow, we can compute the probability that our estimate is within 2% of the true value:\npnorm(0.02/se) - pnorm(-0.02/se)\n#&gt; [1] 0.159\nThere is only a small chance of being this close to \\(p\\). A poll of just \\(N = 25\\) people is therefore not very informative, especially in a close election.\nThis reasoning naturally leads to the concept of the margin of error. Conventionally, pollsters report an interval that they are 95% confident contains the true parameter. Under the CLT, this 95% probability corresponds to approximately 1.96 standard errors on each side of the estimate. Hence, the margin of error is defined as:\n1.96*se\n#&gt; [1] 0.196\nWhy 1.96? Because if we ask for the probability of being within 1.96 standard errors of \\(p\\), we get:\n\\[\n\\Pr(Z \\leq 1.96) - \\Pr(Z \\leq -1.96),\n\\]\nwhich equals about 95%:\npnorm(1.96) - pnorm(-1.96)\n#&gt; [1] 0.95\nThus, there is a 95% probability that \\(\\bar{X}\\) will fall within \\(1.96 \\times \\hat{\\mathrm{SE}}(\\bar{X})\\) of \\(p\\). In our case, this corresponds to roughly 0.2. The choice of 95% is somewhat arbitrary, other levels are used in practice, but it is the most common convention. For simplicity, the factor 1.96 is often rounded to 2.\nIn summary, the CLT tells us that our poll based on \\(N = 25\\) is not particularly useful: the margin of error is too large to draw meaningful conclusions. We can only say that the race is unlikely to be a landslide. This is why pollsters rely on much larger sample sizes.\nFrom the Real Clear Politics table in the previous chapter, we saw that typical sample sizes range from 700 to 3,500. To see why this matters, consider that if we had obtained \\(\\bar{X} = 0.48\\) with a sample size of 2,000, our estimated standard error would have been 0.0111714. The result would be an estimate of 48% with a margin of error of 2%. This much smaller margin of error would yield a far more informative result—suggesting that red slightly outnumbers blue. Of course, this is hypothetical; we did not conduct a poll of 2,000 to avoid spoiling the competition.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "inference/clt.html#a-monte-carlo-simulation",
    "href": "inference/clt.html#a-monte-carlo-simulation",
    "title": "7  Central Limit Theorem",
    "section": "\n7.2 A Monte Carlo simulation",
    "text": "7.2 A Monte Carlo simulation\nSuppose we want to use a Monte Carlo simulation to corroborate the tools we have developed using probability theory. The problem is, of course, that we don’t know p. We could construct an urn, similar to the one pictured above, and conduct an analog simulation (without a computer). While time-consuming, we could take 10,000 samples, count the beads, and track the proportions of blue. We can use the function take_poll(n = 1000), instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.\nTherefore, one approach we can use to corroborate theoretical results is to pick one or several values of p and run simulations. Let’s set p=0.45 and simulate a poll:\n\np &lt;- 0.45\nN &lt;- 1000\nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\nx_hat &lt;- mean(x)\n\nIn this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:\n\nB &lt;- 10000\nx_hat &lt;- replicate(B, {\n  x &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\n  mean(x)\n})\n\nTo review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45, and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321. The simulation confirms this:\n\nmean(x_hat)\n#&gt; [1] 0.45\nsd(x_hat)\n#&gt; [1] 0.0157\n\nA histogram and qqplot confirm that the normal approximation is also accurate:\n\n\n\n\n\n\n\n\nOf course, in real life, we would never be able to run such an experiment because we don’t know \\(p\\). However, we can run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by rerunning the code above after changing the values of p and N.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "inference/clt.html#bias-why-not-run-a-very-large-poll",
    "href": "inference/clt.html#bias-why-not-run-a-very-large-poll",
    "title": "7  Central Limit Theorem",
    "section": "\n7.3 Bias: Why not run a very large poll?",
    "text": "7.3 Bias: Why not run a very large poll?\nFor realistic values of \\(p\\), let’s say ranging from 0.35 to 0.65, if we conduct a very large poll with 100,000 people, theory tells us that we would predict the election perfectly, as the largest possible margin of error is around 0.3%:\n\n\n\n\n\n\n\n\nSo why are sample sizes these large not seen? One reason is that conducting such a poll is very expensive. Another, and possibly more important reason, is that theory has its limitations. Polling is much more complicated than simply picking beads from an urn. Some people might lie to pollsters, and others might not have phones. However, perhaps the most important way an actual poll differs from an urn model is that we don’t actually know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\). We call this bias. Historically, we observe that polls are indeed biased, although not by a substantial amount. The typical bias for the popular vote, for example, appears to be about 2-3%. This makes election forecasting a bit more interesting, and we will explore how to model this in a later section.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "inference/clt.html#exercises",
    "href": "inference/clt.html#exercises",
    "title": "7  Central Limit Theorem",
    "section": "\n7.4 Exercises",
    "text": "7.4 Exercises\n1. Write an urn model function that takes the proportion of Democrats \\(p\\) and the sample size \\(N\\) as arguments, and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function take_sample.\n2. Now assume p &lt;- 0.45 and that your sample size is \\(N=100\\). Take a sample 10,000 times and save the vector of mean(X) - p into an object called errors. Hint: Use the function you wrote for exercise 1 to write this in one line of code.\n3. The vector errors contains, for each simulated sample, the difference between the actual \\(p\\) and our estimate \\(\\bar{X}\\). We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation, and select which of the following best describes their distributions:\n\nmean(errors)\nhist(errors)\n\n\nThe errors are all about 0.05.\nThe errors are all about -0.05.\nThe errors are symmetrically distributed around 0.\nThe errors range from -1 to 1.\n\n4. The error \\(\\bar{X}-p\\) is a random variable. In practice, the error is not observed because we do not know \\(p\\). Here, we observe it since we constructed the simulation. What is the average size of the error if we define the size by taking the absolute value \\(\\mid \\bar{X} - p \\mid\\)?\n5. The standard error is related to the typical size of the error we make when predicting. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of errors, rather than the average of the absolute values, to quantify the typical size. What is this standard deviation of the errors?\n6. The theory we just learned tells us what this standard deviation is going to be because it is the standard error of \\(\\bar{X}\\). What does theory tell us is the standard error of \\(\\bar{X}\\) for a sample size of 100?\n7. In practice, we don’t know \\(p\\), so we construct an estimate of the theoretical prediction by plugging in \\(\\bar{X}\\) for \\(p\\). Compute this estimate. Set the seed at 1 with set.seed(1).\n8. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict \\(p\\) with \\(\\bar{X}\\). Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier, we learned that the largest standard errors occur for \\(p=0.5\\). Create a plot of the largest standard error for \\(N\\) ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?\n\n100\n500\n2,500\n4,000\n\n9. For sample size \\(N=100\\), the Central Limit Theorem tells us that the distribution of \\(\\bar{X}\\) is:\n\npractically equal to \\(p\\).\napproximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\napproximately normal with expected value \\(\\bar{X}\\) and standard error \\(\\sqrt{\\bar{X}(1-\\bar{X})/N}\\).\nnot a random variable.\n\n10. Based on the answer from exercise 8, the error \\(\\bar{X} - p\\) is:\n\npractically equal to 0.\napproximately normal with expected value \\(0\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\napproximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\nnot a random variable.\n\n11. To corroborate your answer to exercise 9, make a qq-plot of the errors you generated in exercise 2 to see if they follow a normal distribution.\n12. If \\(p=0.45\\) and \\(N=100\\) as in exercise 2, use the CLT to estimate the probability that \\(\\bar{X}&gt;0.5\\). Assume you know \\(p=0.45\\) for this calculation.\n13. Assume you are in a practical situation and you don’t know \\(p\\). Take a sample of size \\(N=100\\) and obtain a sample average of \\(\\bar{X} = 0.51\\). What is the CLT approximation for the probability that your error is equal to or larger than 0.01?",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "inference/confidence-intervals.html",
    "href": "inference/confidence-intervals.html",
    "title": "\n8  Confidence intervals\n",
    "section": "",
    "text": "8.1 Constructing confidence intervals\nConfidence intervals are a very useful concept widely employed by data analysts.\nIn the competition described in Section 6.1, you were asked to give an interval. If the interval you submitted includes the \\(p\\), you receive half the money you spent on your poll back and proceed to the next stage of the competition. One way to pass to the second round is to report a very large interval. For example, the interval \\([0,1]\\) is guaranteed to include \\(p\\). However, with an interval this big, we have no chance of winning the competition. Similarly, if you are an election forecaster and predict the spread will be between -100% and 100%, you will be ridiculed for stating the obvious. Even a smaller interval, such as saying the spread will be between -10 and 10%, will not be considered serious.\nOn the other hand, the smaller the interval we report, the smaller our chances are of winning the prize. Likewise, a bold pollster that reports very small intervals and misses the mark most of the time will not be considered a good pollster. We want to be somewhere in between.\nWe can use the statistical theory we have learned to compute the probability of any given interval including \\(p\\). If we are asked to create an interval with, say, a 95% chance of including \\(p\\), we can do that as well; these are called 95% confidence intervals.\nWhen a pollster reports an estimate and a margin of error, they are, indirectly, reporting a 95% confidence interval. Let’s now see how this works mathematically.\nIn Section 7.1, we used the Central Limit Theorem (CLT) to derive the margin of error and showed that the interval\\([\\bar{X} - 1.96\\,\\hat{\\mathrm{SE}}(\\bar{X}), \\bar{X} + 1.96\\,\\hat{\\mathrm{SE}}(\\bar{X})]\\)\nhas a 95% probability of containing the true proportion \\(p\\). This type of interval is known as a 95% confidence interval. We now take a closer look at what this statement really means.\nIt is important to remember that the endpoints of this interval are not fixed numbers, they are random variables that depend on the sample we happen to draw. Each time we collect a new sample, we obtain a different \\(\\bar{X}\\) and therefore a different interval. To illustrate this, we can repeat our sampling process many times using the same parameters as in an earlier Monte Carlo simulation:\np &lt;- 0.45\nN &lt;- 1000\nx &lt;- sample(c(0, 1), size = N, replace = TRUE, prob = c(1 - p, p))\nx_hat &lt;- mean(x)\nse_hat &lt;- sqrt(x_hat*(1 - x_hat)/N)\nc(x_hat - 1.96*se_hat, x_hat + 1.96*se_hat)\n#&gt; [1] 0.422 0.484\nand notice it the intreval different if we run it again:\nx &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p))\nx_hat &lt;- mean(x)\nse_hat &lt;- sqrt(x_hat*(1 - x_hat)/N)\nc(x_hat - 1.96*se_hat, x_hat + 1.96*se_hat)\n#&gt; [1] 0.415 0.477\nKeep sampling and creating intervals, and you will see the random variation.\nTo determine the probability that the interval includes \\(p\\), we need to compute the following:\n\\[\n\\mathrm{Pr}\\left(\\bar{X} - 1.96\\,\\hat{\\mathrm{SE}}(\\bar{X}) \\leq p \\leq \\bar{X} + 1.96\\,\\hat{\\mathrm{SE}}(\\bar{X})\\right)\n\\]\nBy subtracting and dividing the same quantities in all parts of the equation, we find that the above is equivalent to:\n\\[\n\\mathrm{Pr}\\left(-1.96 \\leq \\frac{\\bar{X}- p}{\\hat{\\mathrm{SE}}(\\bar{X})} \\leq  1.96\\right)\n\\]\nThe term in the middle is an approximately normal random variable with expected value 0 and standard error 1, which we have been denoting with \\(Z\\), so we have:\n\\[\n\\mathrm{Pr}\\left(-1.96 \\leq Z \\leq  1.96\\right)\n\\]\nwhich we can quickly compute using :\npnorm(1.96) - pnorm(-1.96)\n#&gt; [1] 0.95\nproving that we have a 95% probability.\nIf we want to have a larger probability, say 99%, we need to multiply by whatever z satisfies the following:\n\\[\n\\mathrm{Pr}\\left(-z \\leq Z \\leq  z\\right) = 0.99\n\\]\nUsing:\nz &lt;- qnorm(0.995)\nz\n#&gt; [1] 2.58\nwill achieve this because by definition pnorm(qnorm(0.995)) is 0.995, and by symmetry pnorm(1-qnorm(0.995)) is 1 - 0.995. As a consequence, we have that:\npnorm(z) - pnorm(-z)\n#&gt; [1] 0.99\nis 0.995 - 0.005 = 0.99.\nWe can use this approach for any probability, not just 0.95 and 0.99. In statistics textbooks, confidence interval formulas are given for arbitrary probabilities written as \\(1-\\alpha\\). We can obtain the \\(z\\) for the equation above using z = qnorm(1 - alpha / 2) because \\(1 - \\alpha/2 - \\alpha/2 = 1 - \\alpha\\). So, for example, for \\(\\alpha=0.05\\), \\(1 - \\alpha/2 = 0.975\\) and we get the \\(z=1.96\\) we used above:\nqnorm(0.975)\n#&gt; [1] 1.96",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "inference/confidence-intervals.html#a-monte-carlo-simulation",
    "href": "inference/confidence-intervals.html#a-monte-carlo-simulation",
    "title": "\n8  Confidence intervals\n",
    "section": "\n8.2 A Monte Carlo simulation",
    "text": "8.2 A Monte Carlo simulation\nWe can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes \\(p\\) 95% of the time.\n\nN &lt;- 1000\nB &lt;- 10000\ninside &lt;- replicate(B, {\n  x &lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p))\n  x_hat &lt;- mean(x)\n  se_hat &lt;- sqrt(x_hat*(1 - x_hat)/N)\n  p &gt;= x_hat - 1.96*se_hat & p &lt;= x_hat + 1.96*se_hat\n})\nmean(inside)\n#&gt; [1] 0.948\n\nThe following plot shows the first 100 confidence intervals. In this case, we created the simulation so the black line denotes the parameter we are trying to estimate:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen applying the theory we described above, it’s important to remember that it’s the intervals that are random, not \\(p\\). In the plot above, we can see the random intervals moving around, while the proportion of blue beads in the urn \\(p\\), represented with the vertical line, remains in the same place. So the 95% relates to the probability that the random interval falls on top of \\(p\\). Interpreting a confidence interval as “\\(p\\) has a 95% chance of being between this or that” is technically incorrect because \\(p\\) is a fixed value, not a random variable.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "inference/confidence-intervals.html#exercises",
    "href": "inference/confidence-intervals.html#exercises",
    "title": "\n8  Confidence intervals\n",
    "section": "\n8.3 Exercises",
    "text": "8.3 Exercises\nFor these exercises, we will use actual polls from the 2016 election. You can load the data from the dslabs package.\n\nlibrary(dslabs)\n\nSpecifically, we will use all the national polls that ended within one week prior to the election.\n\nlibrary(tidyverse)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\") \n\n1. For the first poll, you can obtain the samples size and estimated Clinton percentage with:\n\nN &lt;- polls$samplesize[1]\nx_hat &lt;- polls$rawpoll_clinton[1]/100\n\nAssume there are only two candidates and construct a 95% confidence interval for the election night proportion \\(p\\).\n2. Now use dplyr to add a confidence interval as two columns, call them lower and upper, to the object polls. Then, use select to show the pollster, enddate, x_hat,lower, upper variables. Hint: Define temporary columns x_hat and se_hat.\n3. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion \\(p=0.482\\) or not.\n4. For the table you just created, what proportion of confidence intervals included \\(p\\)?\n5. If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include \\(p\\)?\n6. A much smaller proportion of the polls than expected produce confidence intervals containing \\(p\\). If you look closely at the table, you will see that most polls that fail to include \\(p\\) are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates, \\(\\mu\\), which in this election was \\(0. 482 - 0.461 = 0.021\\). Assume that there are only two parties and that \\(\\theta = 2p - 1\\). Redefine polls as below and re-do exercise 2, but for the difference.\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\")  |&gt;\n  mutate(theta_hat = rawpoll_clinton/100 - rawpoll_trump/100)\n\n7. Now repeat exercise 3, but for the difference.\n8. Now repeat exercise 4, but for the difference.\n9. Although the proportion of confidence intervals increases substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual \\(\\theta=0.021\\). Stratify by pollster.\n10. Redo the plot that you made for exercise 9, but only for pollsters that took five or more polls.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "inference/models.html",
    "href": "inference/models.html",
    "title": "9  Data-driven models",
    "section": "",
    "text": "9.1 Case study: poll aggregators\nSo far, our analysis of poll-related results has been based on a simple sampling model. This model assumes that each voter has an equal chance of being selected for the poll, similar to picking beads from an urn with two colors. However, in this section, we explore real-world data and discover that this model is incorrect. Instead, we propose a more effective approach in which we directly model the outcomes of pollsters rather than the individual polls.\nA more recent development since the original invention of opinion polls, is the use of computers to aggregate publicly available data from different sources and develop data-driven forecasting models. Here, we explore how poll aggregators collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the statistical models used to improve election forecasts beyond the power of individual polls. Specifically, we introduce a useful model for constructing a confidence interval for the difference in popular vote.\nIt is important to emphasize that this chapter offers only a brief introduction to the vast field of statistical modeling. The model presented here, for example, does not allow us to assign probabilities to outcomes such as a particular candidate winning the popular vote, as is done by poll aggregators like FiveThirtyEight. In the next chapter, we introduce Bayesian models, which provide the mathematical framework for such probabilistic statements. Later, in the Linear Models part of the book, we explore widely used approaches for analyzing data in practice. Still, this introduction only scratches the surface. Readers interested in statistical modeling are encouraged to consult additional references to gain a deeper and broader perspective.\nA few weeks before the 2012 election, Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had, and which others missed. To do this, we simulate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls, construct, and report 95% confidence intervals for the spread for each of the 12 polls:\nNot surprisingly, all 12 polls report confidence intervals that include the result we eventually saw on election night (dashed line). However, all 12 polls also include 0 (solid black line). Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up.\nPoll aggregators recognized that combining results from multiple polls can greatly improve precision. One straightforward approach is to reverse engineer the original data from each poll using the reported estimates and sample sizes. From these, we can reconstruct the total number of successes and failures across all polls, effectively combining them into a single, larger dataset. We then compute a new overall estimate and its corresponding standard error using the combined sample size, which is much larger than that of any individual poll.\nWhen we do this, the resulting margin of error is 0.0184545.\nOur combined estimate predicts a spread of 3.1 percentage points, plus or minus 1.8. This interval not only includes the actual result observed on election night but is also far from including zero. By aggregating the twelve polls—assuming no systematic bias—we obtain a much more precise estimate and can be quite confident that Obama will win the popular vote.\nHowever, this example was just a simplified simulation to illustrate the basic idea. In real-world settings, combining data from multiple polls is more complicated than treating them as one large random sample. Differences in methodology, timing, question wording, and even sampling biases can all affect the results. We will later explore how these factors make real poll aggregation more challenging and how statistical models can account for them.\nLet’s look at real data from the 2016 presidential election.\nSpecifically, the following subset of the polls_us_election_2016 data in dslabs includes results for national polls as well as state polls taken during the year prior to the election and organized by FiveThirtyEight. In this first example, we will filter the data to include national polls conducted on likely voters (lv) within the last week leading up to the election and consider their estimate of the spread:\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-11-02\" & population == \"lv\") |&gt;\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\nNote we will focus on predicting the spread, not the proportion \\(p\\). Since we are assuming there are only two parties, we know that the spread is \\(\\theta = p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(\\theta\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\hat{\\mathrm{SE}}(\\bar{X})\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\hat{\\mathrm{SE}}(\\bar{X})\\). Note that subtracting 1 does not add any variability, so it does not affect the standard error. Also, the CLT applies here because our estimate is a linear combination of a sample average, which itself follows approximately normal distributions.\nWe have 50 estimates of the spread. The theory we learned from sampling models tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(\\theta\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\).\nAssuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:\ntheta_hat &lt;- with(polls, sum(spread*samplesize)/sum(samplesize))\nand the standard error is:\np_hat &lt;- (theta_hat + 1)/2 \nmoe &lt;- 1.96*2*sqrt(p_hat*(1 - p_hat)/sum(polls$samplesize))\nSo we report a spread of 3.59% with a margin of error of 0.38%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?\nA histogram of the reported spreads reveals a problem:\npolls |&gt; ggplot(aes(spread)) + geom_histogram(color = \"black\", binwidth = .01)\nThe estimates do not appear to be normally distributed, and the standard error appears to be larger than 0.003809. The theory is not working here.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data-driven models</span>"
    ]
  },
  {
    "objectID": "inference/models.html#case-study-poll-aggregators",
    "href": "inference/models.html#case-study-poll-aggregators",
    "title": "9  Data-driven models",
    "section": "",
    "text": "We use \\(\\theta\\) to denote the spread here because this is a common notation used in statistical textbooks for the parameter of interest.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data-driven models</span>"
    ]
  },
  {
    "objectID": "inference/models.html#sample-avg-model",
    "href": "inference/models.html#sample-avg-model",
    "title": "9  Data-driven models",
    "section": "\n9.2 Beyond the simple sampling model",
    "text": "9.2 Beyond the simple sampling model\nNotice that data come from various pollsters, and some are taking several polls a week:\n\npolls |&gt; count(pollster) |&gt; arrange(desc(n)) |&gt; head(5)\n#&gt;                   pollster n\n#&gt; 1                 IBD/TIPP 6\n#&gt; 2 The Times-Picayune/Lucid 6\n#&gt; 3    USC Dornsife/LA Times 6\n#&gt; 4 ABC News/Washington Post 5\n#&gt; 5     CVOTER International 5\n\nLet’s visualize the data for the pollsters that are regularly polling:\n\n\n\n\n\n\n\n\nThis plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll is between 0.018 and 0.033:\n\npolls |&gt; group_by(pollster) |&gt; \n  filter(n() &gt;= 5) |&gt;\n  summarize(se = 2*sqrt(p_hat*(1 - p_hat)/median(samplesize)))\n#&gt; # A tibble: 5 × 2\n#&gt;   pollster                     se\n#&gt;   &lt;fct&gt;                     &lt;dbl&gt;\n#&gt; 1 ABC News/Washington Post 0.0243\n#&gt; 2 CVOTER International     0.0260\n#&gt; 3 IBD/TIPP                 0.0333\n#&gt; 4 The Times-Picayune/Lucid 0.0197\n#&gt; 5 USC Dornsife/LA Times    0.0183\n\nThis agrees with the within-poll variation we see. However, there appear to be differences across the polls. Observe, for example, how the USC Dornsife/LA Times pollster is predicting a 4% lead for Trump, while Ipsos is predicting a lead larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values, instead it assumes all the polls have the same expected value. FiveThirtyEight refers to these differences as house effects. We also call them pollster bias. Nothing in our simple urn model provides an explanation for these pollster-to-pollster differences.\nThis model misspecification led to an overconfident interval that ended up not including the election night result. So, rather than modeling the process generating these values with an urn model, we instead model the pollster results directly. To do this, we collect data. Specifically, for each pollster, we look at the last reported result before the election:\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt; \n  filter(enddate == max(enddate)) |&gt;ungroup()\n\nHere is a histogram of the calculated spread for these 20 pollsters’ final polls:\n\nhist(one_poll_per_pollster$spread, breaks = 10)\n\n\n\n\n\n\n\nAlthough we are no longer using a model with red (Republicans) and blue (Democrats) beads in an urn, our new model can also be thought of as an urn model, but containing poll results from all possible pollsters. Think of our $N=$20 data points \\(Y_1,\\dots Y_N\\) as a random sample from this urn. To develop a useful model, we assume that the expected value of our urn is the actual spread \\(\\theta = 2p - 1\\), which implies that the sample average has expected value \\(\\theta\\).\nNow, because instead of 0s and 1s, our urn contains continuous numbers, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\). Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter which we will denote with \\(\\sigma\\).\nSo our new statistical model is that \\(Y_1, \\dots, Y_N\\) are a random sample with expected \\(\\theta\\) and standard deviation \\(\\sigma\\). The distribution, for now, is unspecified. But we consider \\(N\\) to be large enough to assume that the sample average \\(\\bar{Y} = \\sum_{i=1}^N Y_i\\) follows a normal distribution with expected value \\(\\theta\\) and standard error \\(\\sigma / \\sqrt{N}\\). We write:\n\\[\n\\bar{Y} \\sim \\mbox{N}(\\theta, \\sigma / \\sqrt{N})\n\\]\nHere the \\(\\sim\\) symbol tells us that the random variable on the left of the symbol follows the distribution on the right. We use the notation \\(N(a,b)\\) to represent the normal distribution with mean \\(a\\) and standard deviation \\(b\\).\n\nThis model for the sample average will be used again the next chapter.\n\n\n9.2.1 Estimating the standard deviation\nThe model we have specified has two unknown parameters: the expected value \\(\\theta\\) and the standard deviation \\(\\sigma\\). We know that the sample average \\(\\bar{Y}\\) will be our estimate of \\(\\theta\\). But what about \\(\\sigma\\)?\nOur task is to estimate \\(\\theta\\). Given that we model the observed values \\(Y_1,\\dots Y_N\\) as a random sample from the urn, for a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{Y}\\) is approximately normal with expected value \\(\\theta\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider $N=$20 large enough, we can use this to construct confidence intervals.\nTheory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as:\n\\[\ns = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N (Y_i - \\bar{Y})^2 }\n\\]\nKeep in mind that, unlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we do not cover it here.\nThe sd function in R computes the sample standard deviation:\n\nsd(one_poll_per_pollster$spread)\n#&gt; [1] 0.0222\n\n\n9.2.2 Computing a confidence interval\nWe are now ready to form a new confidence interval based on our new data-driven model:\n\nresults &lt;- one_poll_per_pollster |&gt; \n  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) |&gt; \n  mutate(start = avg - 1.96*se, end = avg + 1.96*se) \nround(results*100, 2)\n#&gt;    avg  se start end\n#&gt; 1 3.03 0.5  2.06   4\n\nOur confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.\n\n9.2.3 The t-distribution\nAbove, we made use of the CLT with a sample size of 20. Because we are estimating a second parameter \\(\\sigma\\), further variability is introduced into our confidence interval, which results in intervals that are too small. For very large sample sizes, this extra variability is negligible, but in general, especially for \\(N\\) smaller than 30, we need to be cautious about using the CLT.\nHowever, if the data in the urn is known to follow a normal distribution, then we actually have mathematical theory that tells us how much bigger we need to make the intervals to account for the estimation of \\(\\sigma\\). Applying this theory, we can construct confidence intervals for any \\(N\\). But again, this works only if the data in the urn is known to follow a normal distribution. So for the 0, 1 data of our previous urn model, this theory definitely does not apply.\n\n\n\n\n\n\nNote that 30 is a very general rule of thumb based on the case when the data come from a normal distribution. There are cases when a large sample size is needed as well as cases when smaller sample sizes are good enough.\n\n\n\nThe statistic on which confidence intervals for \\(\\theta\\) are based is:\n\\[\nZ = \\frac{\\bar{Y} - \\theta}{\\sigma/\\sqrt{N}}\n\\] Here, \\(\\theta\\) is the true population spread, and \\(\\sigma\\) is the standard deviation of the pollster-level urn.\nCLT tells us that Z is approximately normally distributed with expected value 0 and standard error 1. But in practice we don’t know \\(\\sigma\\), so we use:\n\\[\nt = \\frac{\\bar{Y} - \\theta}{s/\\sqrt{N}}\n\\]\nThis is referred to as a t-statistic. By substituting \\(\\sigma\\) with \\(s\\), we introduce some variability. The theory tells us that \\(t\\) follows a student t-distribution with \\(N-1\\) degrees of freedom. The degrees of freedom is a parameter that controls the variability via fatter tails:\n\n\n\n\n\n\n\n\nIf we are willing to assume the pollster effect data is normally distributed, based on the sample data \\(Y_1, \\dots, Y_N\\),\n\none_poll_per_pollster |&gt; ggplot(aes(sample = spread)) + stat_qq()\n\n\n\n\n\n\n\nthen \\(t\\) follows a t-distribution with \\(N-1\\) degrees of freedom. To construct a 95% confidence interval we simply use qt instead of qnorm: This results in a slightly larger confidence interval than we obtained before:\n\nn &lt;- length(one_poll_per_pollster$spread)\nttest_ci &lt;- one_poll_per_pollster |&gt; \n  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) |&gt; \n  mutate(start = avg - qt(0.975, n - 1)*se, end = avg + qt(0.975, n - 1)*se) |&gt;\n  select(start, end)\nround(ttest_ci*100, 2)\n#&gt;   start  end\n#&gt; 1  1.99 4.07\n\nThis interval is a bit larger than the one using normal because the t-distribution has large tails, as seen in the densities. Specifically, the quantile value for the t distribution\n\nqt(0.975, n - 1)\n#&gt; [1] 2.09\n\nis larger than that for the normal distribution\n\nqnorm(0.975)\n#&gt; [1] 1.96\n\n\n\n\n\n\n\nUsing the t-distribution and the t-statistic is the basis for t-tests, a widely used approach for computing p-values. To learn more about t-tests, you can consult any statistics textbook.\n\n\n\nThe t-distribution can also be used to model errors in bigger deviations that are more likely than with the normal distribution. FiveThirtyEight uses the t-distribution to generate errors that better model the deviations we see in election data. For example, in Wisconsin, the average of six high quality polls was 7% in favor of Clinton with a standard deviation of 3%, but Trump won by 0.8%. Even after taking into account the overall bias, this 7.7% residual is more in line with t-distributed data than the normal distribution.\n\npolls_us_election_2016 |&gt;\n  filter(state == \"Wisconsin\" & enddate &gt; \"2016-11-01\" & \n            population == \"lv\" & grade %in% c(\"A+\",\"A\",\"A-\",\"B+\",\"B\")) |&gt;\n  left_join(results_us_election_2016, by = \"state\") |&gt;\n  mutate(spread = rawpoll_clinton - rawpoll_trump, actual = clinton - trump) |&gt;\n  summarize(actual = first(actual), estimate = mean(spread), sd = sd(spread)) \n#&gt;   actual estimate sd\n#&gt; 1 -0.764     7.04  3",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data-driven models</span>"
    ]
  },
  {
    "objectID": "inference/models.html#exercises",
    "href": "inference/models.html#exercises",
    "title": "9  Data-driven models",
    "section": "\n9.3 Exercises",
    "text": "9.3 Exercises\nWe have been using urn models to motivate the use of probability models. Yet, most data science applications are not related to data obtained from urns. More common are data that come from individuals. The reason probability plays a role here is because the data come from a random sample. The random sample is taken from a population, and the urn serves as an analogy for the population.\nDefine the males that replied to the height survey as the population\n\nlibrary(dslabs)\nx &lt;- heights |&gt; filter(sex == \"Male\") |&gt;\n  pull(height)\n\nto answer the following questions.\n1. Mathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it. What are the average and standard deviation of our population?\n2. Call the population average computed above \\(\\mu\\) and the standard deviation \\(\\sigma\\). Now take a sample of size 50, with replacement, and construct an estimate for \\(\\mu\\) and \\(\\sigma\\).\n3. What does the theory tell us about the sample average \\(\\bar{X}\\) and how it is related to \\(\\mu\\)?\n\nIt is practically identical to \\(\\mu\\).\nIt is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\).\nIt is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma\\).\nContains no information.\n\n4. So, how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we can only measure 50 of the 708. We will use \\(\\bar{X}\\) as our estimate. We know from the answer to exercise 3 that the standard error of our estimate \\(\\bar{X}-\\mu\\) is \\(\\sigma/\\sqrt{N}\\). We want to compute this, but we don’t know \\(\\sigma\\). Based on what is described in this section, show your estimate of \\(\\sigma\\).\n5. Now that we have an estimate of \\(\\sigma\\), let’s call our estimate \\(s\\). Construct a 95% confidence interval for \\(\\mu\\).\n6. Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include \\(\\mu\\)?\n7. Use the qnorm and qt functions to generate quantiles. Compare these quantiles for different degrees of freedom for the t-distribution. Use this to motivate the sample size of 30 rule of thumb.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data-driven models</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html",
    "href": "inference/bayes.html",
    "title": "10  Bayesian statistics",
    "section": "",
    "text": "10.1 Bayes theorem\nIn 2016, FiveThirtyEight showed this chart depicting distributions for the percent of the popular vote for each candidate:\nBut what does this mean in the context of the theory we have previously covered, in which these percentages are considered fixed? Furthermore, election forecasters make probabilistic statements such “Obama has a 90% chance of winning the election.” Note that in the context of an urn model, this would be equivalent to stating that the probability of \\(p&gt;0.5\\) is 90%. However, in the urn model \\(p\\) is a fixed parameter and it does not make sense to talk about probability. With Bayesian statistics, we assume \\(p\\) is a random variable, and thus, a statement such as “90% chance of winning” is consistent with the mathematical approach. Forecasters also use models to describe variability at different levels. For example, sampling variability, pollster to pollster variability, day to day variability, and election to election variability. One of the most successful approaches used for this are hierarchical models, which can be explained in the context of Bayesian statistics.\nIn this chapter, we will briefly describe Bayesian statistics. We use three cases studies: 1) interpreting diagnostic tests for a rare disease, and 2) estimating the probability of Hillary Clinton winning in 2016 using pre-election poll data. For an in-depth treatment of this topic, we recommend one of the following textbooks:\nWe start by describing Bayes theorem, using a hypothetical cystic fibrosis test as an example.\nSuppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation:\n\\[\n\\mathrm{Pr}(+ \\mid D=1)=0.99, \\mathrm{Pr}(- \\mid D=0)=0.99\n\\]\nwith \\(+\\) meaning a positive test and \\(D\\) representing if you actually have the disease (1) or not (0).\nImagine we select a random person and they test positive. What is the probability that they have the disease? We write this as \\(\\mathrm{Pr}(D=1 \\mid +)?\\) The cystic fibrosis rate is 1 in 3,900, which implies that \\(\\mathrm{Pr}(D=1)=0.00025\\). To answer this question, we will use Bayes theorem, which tells us that:\n\\[\n\\mathrm{Pr}(A \\mid B)  =  \\frac{\\mathrm{Pr}(B \\mid A)\\mathrm{Pr}(A)}{\\mathrm{Pr}(B)}\n\\]\nThis equation, when applied to our problem, becomes:\n\\[\n\\begin{aligned}\n\\mathrm{Pr}(D=1 \\mid +) & =  \\frac{ \\mathrm{Pr}(+ \\mid D=1) \\, \\mathrm{Pr}(D=1)} {\\mathrm{Pr}(+)} \\\\\n& =  \\frac{\\mathrm{Pr}(+ \\mid D=1) \\, \\mathrm{Pr}(D=1)} {\\mathrm{Pr}(+ \\mid D=1) \\, \\mathrm{Pr}(D=1) + \\mathrm{Pr}(+ \\mid D=0) \\mathrm{Pr}( D=0)}\n\\end{aligned}\n\\]\nPlugging in the numbers, we get:\n\\[\n\\mathrm{Pr}(D=1 \\mid +)  = \\frac{0.99 \\cdot 0.00025}{0.99 \\cdot 0.00025 + 0.01 \\cdot (.99975)}  \\approx  0.02\n\\]\nAccording to the above, despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02. This might seem counter-intuitive to some, but it is because we must factor in the very rare probability that a randomly chosen person has the disease. To illustrate this, we run a Monte Carlo simulation.\nWe start by randomly selecting 100,000 people from a population in which the disease in question has a 1 in 4,000 prevalence.\np &lt;- 0.00025\nN &lt;- 100000\nD &lt;- sample(c(1, 0), N, replace = TRUE, prob = c(p, 1 - p))\nNote that, as expected, there are very few people with the disease and many people without the disease,\nN_1 &lt;- sum(D == 1)\nN_0 &lt;- sum(D == 0)\ncat(N_1, \"with disease, and\", N_0, \"without.\")\n#&gt; 23 with disease, and 99977 without.\nThis makes it more probable that we will see some false positives given that the test is not perfect. Now, each person gets the test, which is correct 99% of the time:\nacc &lt;- 0.99\ntest &lt;- vector(\"character\", N)\ntest[D == 1]  &lt;- sample(c(\"+\", \"-\"), N_1, replace = TRUE, prob = c(acc, 1 - acc))\ntest[D == 0]  &lt;- sample(c(\"-\", \"+\"), N_0, replace = TRUE, prob = c(acc, 1 - acc))\nSince the number of healthy individuals is much larger than the number of individuals with the disease, even a low false positive rate results in more healthy individuals testing positive than actual cases.\ntable(D, test)\n#&gt;    test\n#&gt; D       -     +\n#&gt;   0 99012   965\n#&gt;   1     0    23\nFrom this table, we see that the proportion of positive tests that have the disease is 23 out of 988. We can run this over and over again to see that, in fact, the probability converges to about 0.02.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html#priors-posteriors-and-and-credible-intervals",
    "href": "inference/bayes.html#priors-posteriors-and-and-credible-intervals",
    "title": "10  Bayesian statistics",
    "section": "\n10.2 Priors, posteriors and and credible intervals",
    "text": "10.2 Priors, posteriors and and credible intervals\nIn the previous chapter, we computed an estimate and margin of error for the difference in popular votes between Hillary Clinton and Donald Trump. We denoted the parameter, the difference in popular votes, with \\(\\theta\\). The estimate was between 2 and 3 percent, and the confidence interval did not include 0. A forecaster would use this to predict Hillary Clinton would win the popular vote. But to make a probabilistic statement about winning the election, we need to use a Bayesian approach.\nWe start the Bayesian approach by quantifying our knowledge before seeing any data. This is done using a probability distribution referred to as a prior. For our example, we could write:\n\\[\n\\theta \\sim N(\\theta_0, \\tau)\n\\]\nWe can think of \\(\\theta_0\\) as our best guess for the popular vote difference had we not seen any polling data, and we can think of \\(\\tau\\) as quantifying how certain we feel about this guess. Generally, if we have expert knowledge related to \\(\\theta\\), we can try to quantify it with the prior distribution. In the case of election polls, experts use fundamentals, which include, for example, the state of the economy, to develop prior distributions.\nThe data is used to update our initial guess or prior belief. This can be done mathematically if we define the distribution for the observed data for any given \\(\\theta\\). In our particular example, we would write down a model for the average of our polls. If we fixed \\(\\theta\\), this model is the same we used in the previous chapter:\n\\[\n\\bar{Y} \\mid \\theta \\sim N(\\theta, \\sigma/\\sqrt{N})\n\\]\nAs before, \\(\\sigma\\) describes randomness due to sampling and the pollster effects. In the Bayesian contexts, this is referred to as the sampling distribution. Note that we write the conditional \\(\\bar{Y} \\mid \\theta\\) because \\(\\theta\\) is now considered a random variable.\nWe do not show the derivations here, but we can now use calculus and a version of Bayes’ Theorem to derive the conditional distribution of \\(\\theta\\) given the observed data, referred to as the posterior distribution. Specifically, we can show that \\(\\theta \\mid \\bar{Y}\\) follows a normal distribution with expected value:\n\\[\n\\mathrm{E}[\\theta \\mid \\bar{Y}] = B \\theta_0 + (1-B) \\,\\bar{Y} \\mbox{ with } B = \\frac{\\sigma^2/N}{\\sigma^2/N+\\tau^2}\n\\]\nand standard error :\n\\[\n\\mathrm{SE}[\\mu \\mid \\bar{X}] = \\sqrt{\\frac{1}{N/\\sigma^2+1/\\tau^2}}.\n\\]\nNote that the expected value is a weighted average of our prior guess \\(\\theta_0\\) and the observed data \\(\\bar{Y}\\). The weight depends on how certain we are about our prior belief, quantified by \\(\\tau^2\\), and the variance \\(\\sigma^2/N\\) of the summary of our observed data.\nThis weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior value. To see this note that we can rewrite the weighted average as:\n\\[\nB \\theta_0 + (1-B) \\bar{Y}= \\theta_0 + (1-B)(\\bar{Y}-\\theta_0)\\\\\n\\] The closer \\(B\\) is to 1, the more we shrink our estimate toward \\(\\theta_0\\).\nThese formulas are useful way quantifying how we update our beliefs.\nWe can also report intervals with high probability of occurring given our model. Specifically, for any probability value \\(\\alpha\\) we can use the posterior distribution to construct intervals centered at our estimate and with \\(\\alpha\\) chance of occurring. These are called credible intervals.\nAs an example, we compute a posterior distribution and construct a credible interval for the popular vote, after defining a prior distribution with mean 0% and standard error 5%. This prior distribution can be interpreted as follows: before seeing polling data, we don’t think any candidate has the advantage, and a difference of up to 10% either way is possible.\n\ntheta_0 &lt;- 0\ntau &lt;- 0.05\n\nWe can then compute the posterior distribution by applying the equations above to the one_poll_per_pollster data defined in Chapter 9:\n\nres &lt;- one_poll_per_pollster |&gt; \n  summarise(y_bar = mean(spread), sigma = sd(spread), n = n())\nB &lt;- with(res, sigma^2/n / (sigma^2/n + tau^2))\nposterior_mean &lt;- B*theta_0 + (1 - B)*res$y_bar\nposterior_se &lt;- with(res, sqrt(1/(n/sigma^2 + 1/tau^2)))\nposterior_mean + c(-1, 1)*qnorm(0.975)*posterior_se\n#&gt; [1] 0.0203 0.0397\n\nFurthermore, we can now make the probabilistic statement that we could not make with the frequentists approach. Specifically, \\(\\mathrm{Pr}(\\mu&gt;0 \\mid \\bar{X})\\) can be computed as follows:\n\n1 - pnorm(0, posterior_mean, posterior_se)\n#&gt; [1] 1\n\nAccording to the calculation above, we would be almost 100% certain that Clinton will win the popular vote, an estimate that feels overly confident. Moreover, it does not align with FiveThirtyEight’s reported probability of 81.4%. What accounts for this difference? Our current model does not yet capture all sources of uncertainty. We will revisit this issue and address the missing variability in Chapter 11.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html#recommended-reading",
    "href": "inference/bayes.html#recommended-reading",
    "title": "10  Bayesian statistics",
    "section": "\n10.3 Recommended reading",
    "text": "10.3 Recommended reading\n\nBolstad, W. M. (2013). Introduction to Bayesian Statistics (3rd ed.). Wiley.\nA clear, methodical introduction that bridges classical and Bayesian inference. It covers conjugate priors, credible intervals, and basic hierarchical models, with R-based examples and exercises.\nDowney, A. B. (2013). Think Bayes: Bayesian Statistics Made Simple. O’Reilly Media.\nA modern, computation-focused introduction that emphasizes simulation and intuition over calculus. Freely available online, this text is popular in data science courses and ideal for readers learning through Python.\nBerger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis (2nd ed.). Springer-Verlag.\nA classic and comprehensive reference that develops Bayesian methods from first principles of decision theory. Recommended for readers who wish to explore the mathematical foundations in more depth.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html#exercises",
    "href": "inference/bayes.html#exercises",
    "title": "10  Bayesian statistics",
    "section": "\n10.4 Exercises",
    "text": "10.4 Exercises\n1. In 1999, in England, Sally Clark1 was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants, so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500, and then calculating that the chance of two SIDS cases was 8,500 \\(\\times\\) 8,500 \\(\\approx\\) 73 million. Which of the following do you agree with?\n\nSir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: \\(\\mathrm{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) &gt; \\mathrm{Pr}(\\mbox{first case of SIDS})\\).\nNothing. The multiplication rule always applies in this way: \\(\\mathrm{Pr}(A \\mbox{ and } B) =\\mathrm{Pr}(A)\\mathrm{Pr}(B)\\)\n\nSir Meadow is an expert and we should trust his calculations.\nNumbers don’t lie.\n\n2. Let’s assume that there is, in fact, a genetic component to SIDS and the probability of \\(\\mathrm{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) = 1/100\\), is much higher than 1 in 8,500. What is the probability of both of her sons dying of SIDS?\n3. Many press reports stated that the expert claimed the probability of Sally Clark being innocent was 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written as the probability of a mother is a son-murdering psychopath given that two of her children are found dead with no evidence of physical harm. According to Bayes’ rule, what is this?\n4. Assume that the chance of a son-murdering psychopath finding a way to kill her children, without leaving evidence of physical harm, is:\n\\[\n\\mathrm{Pr}(A \\mid B) = 0.50\n\\]\nwith A = two of her children are found dead with no evidence of physical harm, and B = a mother is a son-murdering psychopath = 0.50. Assume that the rate of son-murdering psychopath mothers is 1 in 1,000,000. According to Bayes’ Theorem, what is the probability of \\(\\mathrm{Pr}(B \\mid A)\\) ?\n5. After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clark was acquitted in June 2003. What did the expert miss?\n\nHe made an arithmetic error.\nHe made two mistakes. First, he misused the multiplication rule and did not take into account how rare it is for a mother to murder her children. After using Bayes’ rule, we found a probability closer to 0.5 than 1 in 73 million.\nHe mixed up the numerator and denominator of Bayes’ rule.\nHe did not use R.\n\n6. Florida is one of the most closely watched states in U.S. elections because it has many electoral votes. In past elections, Florida was a swing state where both Republicans and Democrats won implying it could affect a close elections.\nCreate the following table with the polls taken during the last two weeks:\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state == \"Florida\" & enddate &gt;= \"2016-11-04\" ) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nTake the average spread of these polls. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called results.\n7. Now assume a Bayesian model that sets the prior distribution for Florida’s election night spread \\(\\theta\\) to follow a normal distribution with expected value \\(\\theta_0\\) and standard deviation \\(\\tau\\). What are the interpretations of \\(\\theta\\) and \\(\\tau\\)?\n\n\n\\(\\theta_0\\) and \\(\\tau\\) are arbitrary numbers that let us make probability statements about \\(\\theta\\).\n\n\\(\\theta_0\\) and \\(\\tau\\) summarize what we would predict for Florida before seeing any polls. Based on past elections, we would set \\(\\theta\\) close to 0, because both Republicans and Democrats have won, and \\(\\tau\\) at about \\(0.02\\), because these elections tend to be close.\n\n\\(\\theta_0\\) and \\(\\tau\\) summarize what we want to be true. We therefore set \\(\\theta_0\\) at \\(0.10\\) and \\(\\tau\\) at \\(0.01\\).\nThe choice of prior has no effect on Bayesian analysis.\n\n8. The CLT tells us that our estimate of the spread \\(\\hat{\\theta}\\) has normal distribution with expected value \\(\\theta\\) and standard deviation \\(\\sigma\\) calculated in exercise 6. Use the formulas we provided for the posterior distribution to calculate the expected value of the posterior distribution if we set \\(\\theta_0 = 0\\) and \\(\\tau = 0.01\\).\n9. Now compute the standard deviation of the posterior distribution.\n10. Using the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals.\n11. According to this analysis, what was the probability that Trump wins Florida?\n12. Now use sapply function to change the prior variance from seq(0.005, 0.05, len = 100) and observe how the probability changes by making a plot.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "inference/bayes.html#footnotes",
    "href": "inference/bayes.html#footnotes",
    "title": "10  Bayesian statistics",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Sally_Clark↩︎",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html",
    "href": "inference/hierarchical-models.html",
    "title": "11  Hierarchichal Models",
    "section": "",
    "text": "11.1 Case study: election forecasting\nHierarchical models are useful for quantifying different levels of variability or uncertainty. One can use them using a Bayesian or Frequentist framework. However, because in the Frequentist framework they often extend a model with a fixed parameter by assuming the parameter is actually random, the model description includes two distributions that look like the prior and a sampling distribution used in the Bayesian framework. This makes the resulting summaries very similar or even equal to what is obtained with a Bayesian context. A key difference between the Bayesian and the Frequentist hierarchical model approach is that, in the latter, we use data to construct priors rather than treat priors as a quantification of prior expert knowledge. In this section, we illustrate the use of hierarchical models by describing a simplified version of the approach used by FiveThirtyEight to forecast the 2016 election.\nSince the 2008 elections, organizations other than FiveThirtyEight have started their own election forecasting groups that also aggregate polling data and uses statistical models to make predictions. However, in 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election, the New York Times reported1 the following probabilities for Hillary Clinton winning the presidency:\nNYT\n538\nHuffPost\nPW\nPEC\nDK\nCook\nRoth\n\n\nWin Prob\n85%\n71%\n98%\n89%\n&gt;99%\n92%\nLean Dem\nLean Dem\nNote that the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, substantially higher than the others. In fact, four days before the election, FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton2.\nSo why did FiveThirtyEight’s model fair so much better than others? How could PEC and Huffington Post get it so wrong if they were using the same data? In this chapter, we describe how FiveThirtyEight used a hierarchical model to correctly account for key sources of variability and outperform all other forecasters. For illustrative purposes, we will continue examining our popular vote example. In the final section, we will describe the more complex approach used to forecast the electoral college result.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#sec-general-bias",
    "href": "inference/hierarchical-models.html#sec-general-bias",
    "title": "11  Hierarchichal Models",
    "section": "\n11.2 Systematic polling error",
    "text": "11.2 Systematic polling error\nIn the previous chapter, we computed the posterior probability of Hillary Clinton winning the popular vote with a standard Bayesian analysis and found it to be very close to 100%. However, FiveThirtyEight gave her a 81.4% chance3. What explains this difference? Below, we describe the systemic polling error, another source of variability, included in the FiveThirtyEight model, that accounts for the difference.\nAfter elections are over, one can look at the difference between the polling averages and the actual results. An important observation, that our initial models did not take into account, is that it is common to see a systemic polling error that affects most pollsters in the same way. Statisticians refer to this as a bias. The cause of this bias is unclear, but historical data shows it fluctuates. In one election, polls may favor Democrats by 2%, the next Republicans by 1%, then show no bias, and later favor Republicans by 3%. In 2016, polls favored Democrats by 1-2%.\nAlthough we know this systematic polling error affects our polls, we have no way of knowing what this bias is until election night. So we can’t correct our polls accordingly. What we can do is include a term in our model that accounts for the variability.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#mathematical-representations-of-the-hierarchical-model",
    "href": "inference/hierarchical-models.html#mathematical-representations-of-the-hierarchical-model",
    "title": "11  Hierarchichal Models",
    "section": "\n11.3 Mathematical representations of the hierarchical model",
    "text": "11.3 Mathematical representations of the hierarchical model\nSuppose we are collecting data from one pollster and we assume there is no systematic error. The pollster collects several polls with a sample size of \\(N\\), so we observe several measurements of the spread \\(Y_1, \\dots, Y_J\\). Suppose the real proportion for Hillary is \\(p\\) and the spread is \\(\\theta\\). The urn model theory tells us that these random variables are approximately normally distributed, with expected value \\(\\theta\\) and standard error \\(2 \\sqrt{p(1-p)/N}\\):\n\\[\nY_j \\sim \\mbox{N}\\left(\\theta, \\, 2\\sqrt{p(1-p)/N}\\right)\n\\]\nWe use \\(j\\) as an index to label polls, so that the \\(j\\)th poll corresponds to the results reported by the \\(j\\)th poll.\nBelow is a simulation for six polls assuming the spread is 2.1 and \\(N\\) is 2,000:\n\nset.seed(3)\nJ &lt;- 6\nN &lt;- 2000\ntheta &lt;- .021\np &lt;- (theta + 1)/2\ny &lt;- rnorm(J, theta, 2*sqrt(p*(1 - p)/N))\n\nNow, suppose we have \\(J=6\\) polls from each of \\(I=5\\) different pollsters. For simplicity, let’s say all polls had the same sample size \\(N\\). The urn model tell us the distribution is the same for all pollsters, so to simulate data, we use the same model for each:\n\nI &lt;- 5\ny &lt;- sapply(1:I, function(i) rnorm(J, theta, 2*sqrt(p*(1 - p)/N)))\n\nAs expected, the simulated data does not really seem to capture the features of the actual data because it does not account for pollster-to-pollster variability:\n\n\n\n\n\n\n\n\nTo fix this, we need to represent the two levels of variability and we need two indexes, one for pollster and one for the polls each pollster takes. We use \\(Y_{ij}\\) with \\(i\\) representing the pollster and \\(j\\) representing the \\(j\\)th poll from that pollster. The model is now augmented to include pollster effects \\(h_i\\), referred to as house effects by FiveThirtyEight, with standard deviation \\(\\sigma_h\\):\n\\[\n\\begin{aligned}\nh_i &\\sim \\mbox{N}\\left(0, \\sigma_h\\right)\\\\\nY_{ij} \\mid h_i &\\sim \\mbox{N}\\left(\\theta + h_i, \\, 2\\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\]\nTo simulate data from a specific pollster, we first need to draw an \\(h_i\\), and then generate individual poll data after adding this effect. In the simulation below we assume \\(\\sigma_h\\) is 0.025 and generate the \\(h\\) using rnorm:\n\nh &lt;- rnorm(I, 0, 0.025)\ny &lt;- sapply(1:I, function(i) theta + h[i] + rnorm(J, 0, 2*sqrt(p*(1 - p)/N)))\n\nThe simulated data now looks more like the observed data:\n\n#&gt; Warning: Removed 5 rows containing missing values or values outside the scale\n#&gt; range (`geom_point()`).\n\n\n\n\n\n\n\nNote that \\(h_i\\) is common to all the observed spreads from pollster \\(i\\). Different pollsters have a different \\(h_i\\), which explains why we can see the groups of points shift up and down from pollster to pollster.\nNow, in the model above, we assume the average house effect is 0: we generate it with rnorm(I, 0, 0.025). We think that for every pollster biased in favor of one party, there is another one in favor of the other in way that the error averages out when computing the average across all polls. In this case the polling average is unbiased. However, as mentioned above, systematic polling error is observed when we study past elections.\nWe can’t observe this bias with just the 2016 data, but if we collect historical data, we see that the average of polls miss the actual result by more than what is prediced by models like the one above. We don’t show the data here, but if we took the average of polls for each past election and compare it to the actual election night result, we would observe difference with a standard deviation of between 2-4%.\nAlthough we can’t observe the bias, we can define a model that accounts for its variability. We do this by adding another level to the model as follows:\n\\[\n\\begin{aligned}\nb &\\sim \\mbox{N}\\left(0, \\sigma_b\\right)\\\\\nh_j \\mid \\, b &\\sim \\mbox{N}\\left(b, \\sigma_h\\right)\\\\\nY_{ij} | \\, h_j, b &\\sim \\mbox{N}\\left(\\theta + h_j, \\, 2\\sqrt{p(1-p)/N}\\right)\n\\end{aligned}\n\\] This model captures three distinct sources of variability:\n\n\nSystematic error across elections, represented by the random variable \\(b\\), with variability quantified by \\(\\sigma_b\\).\n\nPollster-to-pollster variability, often called the house effect, quantified by \\(\\sigma_h\\).\n\nSampling variability within each poll, which arises from the random sampling of voters and is given by \\(2\\sqrt{p(1-p)/N}\\), where \\(p = (\\theta + 1)/2\\).\n\nFailing to include a term like \\(b\\), the election-level bias, led many forecasters to be overly confident in their predictions. The key point is that \\(b\\) changes from one election to another but remains constant across all pollsters and polls within a single election. Because \\(b\\) has no index, we cannot estimate \\(\\sigma_b\\) using data from only one election. Furthermore, this shared \\(b\\) implies that all random variables \\(Y_{ij}\\) within the same election are correlated, since they are influenced by the same underlying election-level bias.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#computing-a-posterior-probability",
    "href": "inference/hierarchical-models.html#computing-a-posterior-probability",
    "title": "11  Hierarchichal Models",
    "section": "\n11.4 Computing a posterior probability",
    "text": "11.4 Computing a posterior probability\nNow, let’s fit a model like the above to data. We will use one_poll_per_pollster data defined in Chapter 9:\n\none_poll_per_pollster &lt;- polls |&gt; group_by(pollster) |&gt; \n  filter(enddate == max(enddate)) |&gt;\n  ungroup()\n\nHere, we have just one poll per pollster, so we will drop the \\(j\\) index and represent the data as before with \\(X_1, \\dots, X_I\\). As a reminder, we have data from $I=$20 pollsters. Based on the model assumptions described above, we can mathematically show that the average \\(\\bar{X}\\):\n\ny_bar &lt;- mean(one_poll_per_pollster$spread)\n\nhas expected value \\(\\theta\\); thus, in the long run, it provides an unbiased estimate of the outcome of interest. But how precise is this estimate? Can we use the observed sample standard deviation to construct an estimate of the standard error of \\(\\bar{Y}\\)?\nIt turns out that, because the \\(X_i\\) are correlated, estimating the standard error is more complex than what we have described up to now. Specifically, we can show that the standard error can be estimated with:\n\nsigma_b &lt;- 0.03\ns2 &lt;- with(one_poll_per_pollster, sd(spread)^2/length(spread))\nse &lt;- sqrt(s2 + sigma_b^2)\n\nAs mentioned earlier, estimating \\(\\sigma_b\\) requires data from past elections. However, collecting this data is a complex process and beyond the scope of this book. To provide a practical example, we set \\(\\sigma_b\\) to 3%.\nWe can now redo the Bayesian calculation to account for this additional variability. This adjustment yields a result that closely aligns with FiveThirtyEight’s estimates:\n\ntheta_0 &lt;- 0\ntau &lt;- 0.05\nB &lt;- se^2/(se^2 + tau^2)\nposterior_mean &lt;- B*theta_0 + (1 - B)*y_bar\nposterior_se &lt;- sqrt(1/(1/se^2 + 1/tau^2))\n1 - pnorm(0, posterior_mean, posterior_se)\n#&gt; [1] 0.803",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#predicting-the-electoral-college",
    "href": "inference/hierarchical-models.html#predicting-the-electoral-college",
    "title": "11  Hierarchichal Models",
    "section": "\n11.5 Predicting the electoral college",
    "text": "11.5 Predicting the electoral college\nUp to now, we have focused on the popular vote. However, in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college. Each state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state. In 2016 California, the largest state, had 55 electoral votes, while the smallest seven states and the District of Columbia had 3.\nWith the exception of two states, Maine and Nebraska, electoral votes in U.S. presidential elections are awarded on a winner-takes-all basis. This means that if a candidate wins the popular vote in a state by even a single vote, they receive all of that state’s electoral votes. For example, winning California by just one vote would secure all 55 of its electoral votes.\nThis system can lead to scenarios where a candidate wins the national popular vote but loses the electoral college, as was the case in 1876, 1888, 2000, and 2016.\n\n\n\n\n\n\nThe electoral college was designed to balance the influence of populous states and protect the interests of smaller ones in presidential elections. As a federation, the U.S. included states wary of losing power to larger states. During the Constitutional Convention of 1787, smaller states negotiated this system to ensure their voices remained significant, receiving electors based on their senators and representatives. This compromise helped secure their place in the union.\n\n\n\nWe are now ready to predict the electoral college result for 2016. We start by creating a data frame with the electoral votes for each state:\n\nresults &lt;- results_us_election_2016 |&gt; select(state, electoral_votes)\n\nWe then aggregate results from polls taken during the last weeks before the election and include only those taken on registered and likely voters. We define spread as the estimated difference in proportion:\n\npolls &lt;- polls_us_election_2016 |&gt;\n  filter(state != \"U.S.\" & enddate &gt;= \"2016-11-02\" &  population != \"a\") |&gt;\n  mutate(spread = (rawpoll_clinton - rawpoll_trump)/100)\n\nIf a pollster ran more than one poll in this period, we keep only the latest.\n\npolls &lt;- polls |&gt;\n  arrange(population, desc(enddate)) |&gt;\n  group_by(state, pollster) |&gt;\n  slice(1) |&gt;\n  ungroup() \n\nInstead of simply averaging polls, FiveThirtyEight assigns weights to pollsters based on a letter grade, stored in the grade column. This grade reflects the pollster’s past accuracy and reliability.\n\ngrades &lt;- unique(sort(polls$grade))\nweights &lt;- data.frame(grade = grades, weight = seq(0.3, 1, length = length(grades)))\n\nHigher-graded pollsters are assigned more weight in the model, giving their results greater influence, while lower-graded or less reliable pollsters contribute less. This is achieved mathematically through a weighted average:\n\\[\n\\bar{Y}_w = \\frac{\\sum_{i=1}^N w_i Y_i}{\\sum_{i=1}^N w_i}\n\\]\nwhere \\(Y_i\\) represents the result from pollster \\(i\\), \\(w_i\\) is the weight assigned to pollster \\(i\\),\nand \\(N\\) is the total number of pollsters.\nTo understand this intuitively, think of the weights as fractions of a full data point. For example, if the maximum weight is 1, a pollster with a weight of \\(0.5\\) contributes half as much as a fully trusted pollster, while a weight of \\(0.25\\) can be interpreted as contributing a quarter of a poll.\nWe use a similar equation to estimate the standard deviation:\n\\[\ns_w = \\sqrt{\\frac{\\sum_{i=1}^N w_i (Y_i - \\bar{Y}_w)}{\\frac{N-1}{N}\\sum_{i=1}^N w_i}}\n\\]\n\n\n\n\n\n\nFiveThirtyEight considers various factors when determining the weight of each poll, including the proximity to election day. However, for simplicity, this analysis focuses solely on pollster grades.\n\n\n\nWe can use the formula learned in Section 5.6 to derive the standard error for our weighted estimate:\n\\[\n\\mathrm{SE}(\\bar{X}_w) = \\frac{\\sigma_h}{\\sqrt{N_{\\mbox{eff}}}} \\mbox{ with } N_{\\mbox{eff}} = \\frac{\\left(\\sum_{i=1}^N w_i\\right)^2}{\\sum_{i=1}^N w_i^2}\n\\]\nBecause it occupies the same position in the equation for the sample average, \\(N_{\\mbox{eff}}\\) is referred to as the effective sample size. Note that it is equal to \\(N\\) when all the weights are 1 and gets smaller the closer weights get to 0.\nWe now use these equations to compute the weighted averages:\n\npolls &lt;- polls |&gt; \n  filter(!is.na(grade)) |&gt;\n  left_join(weights, by = \"grade\") |&gt; \n  group_by(state) |&gt;\n  summarize(n = n(),\n            avg = sum(weight*spread)/sum(weight), \n            sd = sqrt(sum(weight*(spread - avg)^2)/((n - 1)/n*sum(weight))),\n            n_eff = sum(weight)^2/sum(weight^2))\n\nWe assume the pollster or house effect standard deviation \\(\\sigma_h\\) is the same across states and take the median of those based on an effective sample size larger or equal to 5:\n\npolls$sd &lt;- with(polls, median(sd[n_eff &gt;= 5], na.rm = TRUE))\n\n\n\n\n\n\n\nIn the Linear Models section of the book, we explore a more statistically rigorous method for estimating standard deviations.\n\n\n\nNext we use the command left_join to combine the results data frame containing the electoral votes for each state and the polls data frame containing the summary statistics:\n\nresults &lt;- left_join(results, polls, by = \"state\")\n\nTo make probabilistic arguments, we will use a Bayesian model. To do this we need means and standard deviations for the prior distribution of each state. We will assume a normal distribution for the prior and use 2012 election results to construct priors means. Specifically we compute the difference between Democrat (Obama) and Republican (Romney) candidates:\n\nresults &lt;- results_us_election_2012 |&gt; \n  mutate(theta_0 = (obama - romney)/100) |&gt; \n  select(state, theta_0) |&gt; \n  right_join(results, by = \"state\")\n\nAs done for the popular vote prior, we assign a prior standard deviation of 5% or \\(\\tau=0.05\\).\n\nresults$tau &lt;- 0.05\n\nWith the prior distribution established, we can calculate the posterior distributions for each state. In some states, the outcome is considered highly predictable, with one party (Republican or Democrat) almost certain to win. As a result, there are no polls conducted in these states. In such cases, the posterior distribution remains the same as the prior distribution.\n\nresults &lt;- results |&gt;\n  mutate(B = sd^2/n_eff/((sd^2/n_eff) + tau^2),\n         posterior_mean = if_else(is.na(avg), theta, B*theta_0 + (1 - B)*avg),\n         posterior_se = if_else(is.na(avg), tau, sqrt(1/(n_eff/sd^2 + 1/tau^2))))\n\nWe then use Monte Carlo to generate 50,000 election day results \\(\\theta\\) for each state. For each iteration, we examine \\(\\theta\\) for each iteration. If \\(\\theta&lt;0\\) Trump receives all the electoral votes for that state in that iteration. We assume that the poll results from each state are independent.\n\nset.seed(2024-12-29)\nB &lt;- 50000\nn_states &lt;- nrow(results)\ntrump_ev &lt;- replicate(B,{\n  theta &lt;- with(results, rnorm(n_states, posterior_mean, posterior_se))\n  sum(results$electoral_votes[theta &lt; 0])\n})\nmean(trump_ev &gt; 269)\n#&gt; [1] 0.00322\n\nThis model gives Trump less than a 1% chance of winning, similar to the prediction made by the Princeton Election Consortium. We now know it was quite off. What happened?\nThe model above ignores the possibility of a systematic polling error and, incorrectly, assumes the results from different states are independent. To correct this, we assume that the systematic error term has a standard deviation of 3%. For each iteration we generate systematic error at random and add it to result for all states.\n\nsigma_b &lt;- 0.03\ntrump_ev_2 &lt;- replicate(B, {\n  bias &lt;- rnorm(1, 0, sigma_b) \n  mu &lt;- with(results, rnorm(n_states, posterior_mean, posterior_se))\n  mu &lt;- mu + bias\n  sum(results$electoral_votes[mu &lt; 0])\n})\nmean(trump_ev_2 &gt; 269)\n#&gt; [1] 0.243\n\nThis gives Trump almost a 25% change of winning, which turned out to be a much more sensible estimate, and closer to the prediction made by FiveThirtyEight of 29%.\nPolling faced significant criticism after the 2016 election for “getting it wrong”. However, the polling average actually predicted the final result with notable accuracy, as is typically the case.\n\n\n\n\n\n\n\n\nIn fact, the polling average got the sign of the difference wrong in only five out of the 50 states.\n\ntmp |&gt; filter(sign(spread) != sign(avg)) |&gt;\n  select(state, avg, spread) |&gt;\n  mutate(across(-state, ~round(.,1))) |&gt;\n  setNames(c(\"State\", \"Polling average\", \"Actual result\"))\n#&gt;            State Polling average Actual result\n#&gt; 1        Florida             0.1          -1.2\n#&gt; 2   Pennsylvania             2.0          -0.7\n#&gt; 3       Michigan             3.3          -0.2\n#&gt; 4 North Carolina             0.9          -3.7\n#&gt; 5      Wisconsin             5.6          -0.8\n\nNotice that the errors were all in the same direction, suggesting a systematic polling error. However, the scatter plot reveals several points above the identity line, indicating deviations from the assumption of uniform bias across all states. A closer inspection shows that the bias varies by geographical region, with some areas experiencing stronger effects than others. This pattern is further confirmed by plotting the differences across regions directly:\n\n\n\n\n\n\n\n\nAdvanced forecasting models, like FiveThirtyEight’s, recognize that systematic polling errors often vary by region. To reflect this in a model, we can group states into regions and introduce a regional error term. Since states within the same region share this error, it creates correlation between their outcomes.\nMore sophisticated models also account for variability by using distributions that allow for more extreme events than a normal distribution. For example, a t-distribution with small number of degrees of freedom can capture these rare but impactful outcomes.\nBy incorporating these adjustments—regional errors and heavy-tailed distributions—our model produces a probability for Trump similar to FiveThirtyEight’s reported 29%. Simulations reveal that factoring in regional polling errors and correlations increases the overall variability in the results.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#forecasting",
    "href": "inference/hierarchical-models.html#forecasting",
    "title": "11  Hierarchichal Models",
    "section": "\n11.6 Forecasting",
    "text": "11.6 Forecasting\nForecasters aim to predict election outcomes well before election day, updating their projections as new polls are released. However, a key question remains: How informative are polls conducted weeks before the election about the final outcome? To address this, we examine how poll results vary over time and how this variability affects forecasting accuracy.\nTo make sure the variability we observe is not due to pollster effects, let’s study data from one pollster:\n\none_pollster &lt;- polls_us_election_2016 |&gt; \n  filter(pollster == \"Ipsos\" & state == \"U.S.\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nSince there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation. But the empirical standard deviation is higher than the highest possible theoretical estimate:\n\none_pollster |&gt; \n  summarize(empirical = sd(spread), \n            theoretical = 2*sqrt(mean(spread)*(1 - mean(spread))/min(samplesize)))\n#&gt;   empirical theoretical\n#&gt; 1    0.0403      0.0326\n\nFurthermore, the spread data does not look normal as the theory would predict:\n\n\n\n\n\n\n\n\nThe models we have described include pollster-to-pollster variability and sampling error. But this plot is for one pollster and the variability we see is certainly not explained by sampling error. Where is the extra variability coming from? The following plots make a strong case that it comes from time fluctuations not accounted for by the theory that assumes \\(p\\) is fixed:\n\n\n\n\n\n\n\n\nSome of the peaks and valleys we see coincide with events such as the party conventions, which tend to give the candidate a boost. We can see that the peaks and valleys are consistent across several pollsters:\n\n\n\n\n\n\n\n\nThis implies that if we are going to forecast, our model must include a term to accounts for the time effect. The standard deviation of this effect would depend on time since the closer we get to election day, the closer to 0 this term should be.\nPollsters also try to estimate trends from these data and incorporate them into their predictions. We can model the time trend with a smooth function. There is a variety of methods for estimating trends which we discuss in the section on Machine Learning.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#recommended-reading",
    "href": "inference/hierarchical-models.html#recommended-reading",
    "title": "11  Hierarchichal Models",
    "section": "\n11.7 Recommended reading",
    "text": "11.7 Recommended reading\n\nNate Silver (2016). How FiveThirtyEight’s Election Forecast Works.\nFiveThirtyEight’s official explanation of the 2016 model, including poll weighting, correlated errors, and simulation methodology.https://fivethirtyeight.com/features/how-fivethirtyeights-election-forecast-works/\nFiveThirtyEight (2020). How Our Presidential Forecast Works (2020 Edition).\nDetails updates to the 2020 model, including adjustments for state and national polling correlation, house effects, and uncertainty in turnout and the Electoral College.https://fivethirtyeight.com/features/how-our-presidential-forecast-works/\nElliott Morris (2020). How to Build an Election Forecast Model.\nA comparative explanation of different forecasting approaches, including FiveThirtyEight’s, written for The Economist readers.https://projects.economist.com/us-2020-forecast/methodology\nAndrew Gelman (2020). Election Forecasting: Why We’re Not as Sure as We Think.\nA statistical perspective on model uncertainty, correlated polling errors, and the interpretation of forecast probabilities.https://statmodeling.stat.columbia.edu/2020/11/02/election-forecasting-why-were-not-as-sure-as-we-think/\n\nTogether, these readings provide both a practical and theoretical understanding of how modern election forecasting models, such as FiveThirtyEight’s, combine polling data, uncertainty modeling, and simulation to produce probabilistic predictions.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#exercises",
    "href": "inference/hierarchical-models.html#exercises",
    "title": "11  Hierarchichal Models",
    "section": "\n11.8 Exercises",
    "text": "11.8 Exercises\n1. Create this table:\n\nlibrary(tidyverse)\nlibrary(dslabs)\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(state != \"U.S.\" & enddate &gt;= \"2016-10-31\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\nNow, for each poll, use the CLT to create a 95% confidence interval for the spread reported by each poll. Call the resulting object cis with columns lower and upper for the limits of the confidence intervals. Use the select function to keep the columns state, startdate, enddate, pollster, grade, spread, lower, upper.\n2. You can add the final result to the cis table you just created using the left_join function like this:\n\nadd &lt;- results_us_election_2016 |&gt; \n  mutate(actual_spread = clinton/100 - trump/100) |&gt; \n  select(state, actual_spread)\ncis &lt;- cis |&gt; \n  mutate(state = as.character(state)) |&gt; \n  left_join(add, by = \"state\")\n\nNow, determine how often the 95% confidence interval includes the election night result stored in actual_spread.\n3. Repeat this, but show the proportion of hits for each pollster. Consider only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster. Hint: Use n=n(), grade = grade[1] in the call to summarize.\n4. Repeat exercise 3, but instead of pollster, stratify by state. Note that here we can’t show grades.\n5. Make a barplot based on the result of exercise 4. Use coord_flip.\n6. Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same. Hint: Use the function sign. Call the object resids.\n7. Create a plot like in exercise 5, but for the proportion of times the sign of the spread agreed with the election night result.\n8. In exercise 7, we see that for most states the polls had it right 100% of the time. For only 9 states did the polls miss more than 25% of the time. In particular, notice that in Wisconsin every single poll got it wrong. In Pennsylvania and Michigan, more than 90% of the polls had the signs wrong. Make a histogram of the errors. What is the median of these errors?\n9. We see that at the state level, the median error was 3% in favor of Clinton. The distribution is not centered at 0, but at 0.03. This related to the general bias described in Section 11.2. Create a boxplot to see if the bias was general to all states or it affected some states differently. Use filter(grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) to only include pollsters with high grades.\n10. Some of these states only have a few polls. Repeat exercise 9, but only include states with 5 good polls or more. Hint: Use group_by, filter then ungroup. You will see that the West (Washington, New Mexico, California) underestimated Hillary’s performance, while the Midwest (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) overestimated it. In our simulation, we did not model this behavior since we added general bias, rather than a regional bias. Note that some pollsters may now be modeling correlation between similar states and estimating this correlation from historical data. To learn more about this, you can explore concepts related to random effects and mixed models.\n11. In April 2013, José Iglesias, a professional baseball player was starting his career. He was performing exceptionally well, with an excellent batting average (AVG) of .450. The batting average statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. José had 9 successes out of 20 tries. An AVG of .450 means José has been successful 45% of the times he has batted, which is rather high historically speaking. In fact, no one has finished a season with an AVG of .400 or more since Ted Williams did it in 1941! We want to predict José’s batting average at the end of the season after players have had about 500 tries or at bats. With the frequentist techniques, we have no choice but to predict that his AVG will be .450 at the end of the season. Compute a confidence interval for the success rate.\n12. Despite the frequentist prediction of \\(.450\\), not a single baseball enthusiast would make this prediction. Why is this? One reason is that they know the estimate has much uncertainty. However, the main reason is that they are implicitly using a hierarchical model that factors in information from years of following baseball. Use the following code to explore the distribution of batting averages in the three seasons prior to 2013, and describe what this tells us.\n\nwarning=FALSE, echo=FALSE, eval=FALSE}\nlibrary(tidyverse)\nlibrary(Lahman)\nfilter(Batting, yearID %in% 2010:2012) |&gt; \n  mutate(AVG = H/AB) |&gt; \n  filter(AB &gt; 500) |&gt; \n  ggplot(aes(AVG)) +\n  geom_histogram(color = \"black\", binwidth = .01) +\n  facet_wrap( ~ yearID)\n\n13. So is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both luck and talent. But how much of each? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential. The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, \\(\\theta\\). Then, we see 20 random outcomes with success probability \\(\\theta\\). What model would you use for the first level of your hierarchical model?\n14. Describe the second level of the hierarchical model.\n15. Apply the hierarchical model to José’s data. Suppose we want to predict his innate ability in the form of his true batting average \\(\\theta\\). Write down the distributions of the hierarchical model.\n16. We now are ready to compute a the distribution of \\(\\theta\\) conditioned on the observed data \\(\\bar{Y}\\). Compute the expected value of \\(\\theta\\) given the current average \\(\\bar{Y}\\), and provide an intuitive explanation for the mathematical formula.\n17. We started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 \\(\\pm\\) 0.220. Construct a credible interval for \\(\\theta\\) based on the hierarchical model.\n18. The credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José, as we are predicting he will be just slightly above average. Interestingly, the Red Sox traded José to the Detroit Tigers in July. Here are José Iglesias’ batting averages for the next five months:\n\n\nMonth\nAt Bat\nHits\nAVG\n\n\n\nApril\n20\n9\n.450\n\n\nMay\n26\n11\n.423\n\n\nJune\n86\n34\n.395\n\n\nJuly\n83\n17\n.205\n\n\nAugust\n85\n25\n.294\n\n\nSeptember\n50\n10\n.200\n\n\nTotal w/o April\n330\n97\n.293\n\n\n\nWhich of the two approaches provided a better prediction?",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hierarchical-models.html#footnotes",
    "href": "inference/hierarchical-models.html#footnotes",
    "title": "11  Hierarchichal Models",
    "section": "",
    "text": "https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html↩︎\nhttps://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/↩︎\nhttps://projects.fivethirtyeight.com/2016-election-forecast/↩︎",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hierarchichal Models</span>"
    ]
  },
  {
    "objectID": "inference/hypothesis-testing.html",
    "href": "inference/hypothesis-testing.html",
    "title": "12  Hypothesis testing",
    "section": "",
    "text": "12.1 p-values\nIn scientific studies, you’ll often see phrases like “the results are statistically significant”. This points to a technique called hypothesis testing, where we use p-values, a type of probability, to test our initial assumption or hypothesis.\nIn hypothesis testing, rather than providing an estimate of the parameter we’re studying, we provide a probability that serves as evidence supporting or contradicting a specific hypothesis. The hypothesis usually involves whether a parameter is different from a predetermined value (often 0).\nHypothesis testing is used when you can phrase your research question in terms of whether a parameter differs from this predetermined value. It’s applied in various fields, asking questions such as: Does a medication extend the lives of cancer patients? Does an increase in gun sales correlate with more gun violence? Does class size affect test scores?\nTake, for instance, the previously used example with colored beads. We might not be concerned about the exact proportion of blue beads, but instead ask: Are there more blue beads than red ones? This could be rephrased as asking if the proportion of blue beads is more than 0.5.\nThe initial hypothesis that the parameter equals the predetermined value is called the null hypothesis. It’s popular because it allows us to focus on the data’s properties under this null scenario. Once data is collected, we estimate the parameter and calculate the p-value, which is the probability of the estimate being as extreme as observed if the null hypothesis is true. If the p-value is small, it indicates the null hypothesis is unlikely, providing evidence against it.\nWe will see more examples of hypothesis testing in Chapter 16.\nSuppose we take a random sample of \\(N=100\\) and we observe \\(52\\) blue beads, which gives us \\(\\bar{X} = 0.52\\). This seems to be pointing to there being more blue than red beads since 0.52 is larger than 0.5. However, we know there is chance involved in this process and we could get a 52 even when the actual proability is 0.5. We call the assumption that the probability is 0.5, \\(\\pi = 0.5\\), a null hypothesis. The null hypothesis is the skeptic’s hypothesis.\nWe have observed a random variable \\(\\bar{X} = 0.52\\), and the p-value is the answer to the question: How likely is it to see a value this large, when the null hypothesis is true? If the p-value is small enough, we reject the null hypothesis and say that the results are statistically significant.\nTo obtain a p-value for our example, we write:\n\\[\\mathrm{Pr}(\\mid \\bar{X} - 0.5 \\mid &gt; 0.02 ) \\]\nassuming the \\(\\pi=0.5\\). Under the null hypothesis we know that:\n\\[\n\\sqrt{N}\\frac{\\bar{X} - 0.5}{\\sqrt{0.5(1-0.5)}}\n\\]\nis standard normal. We, therefore, can compute the probability above, which is the p-value.\n\\[\\mathrm{Pr}\\left(\\sqrt{N}\\frac{\\mid \\bar{X} - 0.5\\mid}{\\sqrt{0.5(1-0.5)}} &gt; \\sqrt{N} \\frac{0.02}{ \\sqrt{0.5(1-0.5)}}\\right)\\]\nN &lt;- 100\nz &lt;- sqrt(N)*0.02/0.5\n1 - (pnorm(z) - pnorm(-z))\n#&gt; [1] 0.689\nIn this case, there is actually a large chance of seeing 52 or larger under the null hypothesis.\nKeep in mind that there is a close connection between p-values and confidence intervals. In our example, if a 95% confidence interval does not include 0.5, we know that the p-value must be smaller than 0.05. In general, we can show mathematically that if a \\((1-\\alpha)\\times 100\\)% confidence interval does not contain the null hypothesis value, the null hypothesis is rejected with a p-value as small or smaller than \\(\\alpha\\). So statistical significance can be determined from confidence intervals.\nTo learn more about p-values, you can consult any statistics textbook. However, in general, we prefer reporting confidence intervals over p-values because it gives us an idea of the size of the estimate. If we just report the p-value, we provide no information about the significance of the finding in the context of the problem. For this reason, we recommend avoiding p-values whenever you can compute a confidence interval.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference/hypothesis-testing.html#p-values",
    "href": "inference/hypothesis-testing.html#p-values",
    "title": "12  Hypothesis testing",
    "section": "",
    "text": "We use \\(\\pi\\) to represent the probability of drawing a blue bead, instead of \\(p\\) as in previous sections, to avoid confusion with between the parameter \\(p\\) and the p in p-value.\n\n\n\n\n\nThe p-value of 0.05 as a threshold for statistical significance is conventionally used in many areas of research. A cutoff of 0.01 is also used to define highly significance. The choice of 0.05 is somewhat arbitrary and was popularized by the British statistician Ronald Fisher in the 1920s. We do not recommend using these cutoff without justification and recommend avoiding the phrase statistically significant.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference/hypothesis-testing.html#power",
    "href": "inference/hypothesis-testing.html#power",
    "title": "12  Hypothesis testing",
    "section": "\n12.2 Power",
    "text": "12.2 Power\nPollsters are not successful at providing correct confidence intervals, but rather at predicting who will win. When we took a 25 bead sample size, the confidence interval for the spread:\n\nN &lt;- 25\nx_hat &lt;- 0.48\n(2*x_hat - 1) + c(-1.96, 1.96)*2*sqrt(x_hat*(1 - x_hat)/N)\n#&gt; [1] -0.432  0.352\n\nincluded 0. If this were a poll and we were forced to make a declaration, we would have to say it was a “toss-up”.\nOne problem with our poll results is that, given the sample size and the value of \\(\\pi\\), we would have to sacrifice the probability of an incorrect call to create an interval that does not include 0.\nThis does not mean that the election is close. It only means that we have a small sample size. In statistical textbooks, this is called lack of power. In the context of polls, power is the probability of detecting spreads different from 0.\nBy increasing our sample size, we lower our standard error, and thus, have a much better chance of detecting the direction of the spread.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference/hypothesis-testing.html#exercises",
    "href": "inference/hypothesis-testing.html#exercises",
    "title": "12  Hypothesis testing",
    "section": "\n12.3 Exercises",
    "text": "12.3 Exercises\n\nGenerate a sample of size \\(N=1000\\) from an urn model with 50% blue beads:\n\n\nN &lt;- 1000\npi0 &lt;- 0.5 #we use pi0 to avoid the reserved constant pi\nx &lt;- rbinom(N, 1, pi0)\n\nthen, compute a p-value if \\(\\pi=0.5\\). Repeat this 10,000 times and report how often the p-value is lower than 0.05? How often is it lower than 0.01?\n\nMake a histogram of the p-values you generated in exercise 1.\n\n\nThe p-values are all 0.05.\nThe p-values are normally distributed; CLT seems to hold.\nThe p-values are uniformly distributed.\nThe p-values are all less than 0.05.\n\n3.Demonstrate, mathematically, why we see the histogram in exercise 2. Hint: To compute the p-value, we need to calculate a test statistic, \\(Z\\). We can approximate \\(Z\\) using the CLT, which tells us that \\(Z\\) approximately follows a standard normal distribution. The p-value is calculated as: \\(p = 2\\{1 - \\Phi(|z|)\\}\\) where:\\(z\\) is the observed value of \\(Z\\) and \\(\\Phi(z)\\) is the CDF of the standard normal distribution (pnorm(z) in R). To understand the distribution of the p-values, consider the probability that the p-value is less than or equal to some threshold \\(a\\) between 0 and 1: \\(\\mathrm{Pr}(p \\leq a) = \\mathrm{Pr}(\\{2\\{1 - \\Phi(|z|)\\} \\leq a)\\). Remember that \\(p\\) follows a uniform distribution if \\(\\mathrm{Pr}(p \\leq a) =a\\).\n\nGenerate a sample of size \\(N=1000\\) from an urn model with 52% blue beads:\n\n\nN &lt;- 1000 \npi0 &lt;- 0.52\nx &lt;- rbinom(N, 1, pi0)\n\nCompute a p-value to test if \\(\\pi=0.5\\). Repeat this 10,000 times and report how often the p-value is larger than 0.05? Note that you are computing 1 - power.\n\nRepeat exercise for but for the following values:\n\n\nvalues &lt;- expand.grid(N = c(25, 50, 100, 500, 1000), pi = seq(0.51 ,0.75, 0.01))\n\nPlot power as a function of \\(N\\) with a different color curve for each value of pi.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference/bootstrap.html",
    "href": "inference/bootstrap.html",
    "title": "13  Bootstrap",
    "section": "",
    "text": "13.1 Example: median income\nCLT provides a useful approach to building confidence intervals and performing hypothesis testing. However, it does not always apply. Here we provide a short introduction to an alternative approach to estimating the distribution of an estimate that does not rely on CLT.\nSuppose the income distribution of your population is as follows:\nset.seed(1995)\nn &lt;- 10^6\nincome &lt;- 10^(rnorm(n, log10(45000), log10(3)))\nhist(income/10^3, nclass = 1000)\nThe population median is:\nmedian(income)\n#&gt; [1] 44939\nSuppose we don’t have access to the entire population but want to estimate the population median, denoted by \\(m\\). We take a random sample of 100 observations and use the sample median, \\(M\\), as our estimate of \\(m\\):\nN &lt;- 100\nx &lt;- sample(income, N)\nmedian(x)\n#&gt; [1] 38461\nThe question now becomes: how can we assess the uncertainty in this estimate? In other words, how do we compute a standard error and construct a confidence interval for \\(m\\) based on our sample?\nIn the following sections, we introduce the bootstrap, a powerful resampling method that allows us to estimate variability and construct confidence intervals without relying on strong distributional assumptions.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "inference/bootstrap.html#confidence-intervals-for-the-median",
    "href": "inference/bootstrap.html#confidence-intervals-for-the-median",
    "title": "13  Bootstrap",
    "section": "\n13.2 Confidence intervals for the median",
    "text": "13.2 Confidence intervals for the median\nCan we construct a confidence interval? What is the distribution of \\(M\\)?\nBecause we are simulating the data, we can use a Monte Carlo simulation to learn the distribution of \\(M\\).\n\nB &lt;- 10^4\nm &lt;- replicate(B, {\n  x &lt;- sample(income, N)\n  median(x)\n})\nhist(m, nclass = 30)\nqqnorm(scale(m)); abline(0,1)\n\n\n\n\n\n\n\n\n\nIf we know this distribution, we can construct a confidence interval. The problem here is that, as we have already described, in practice we do not have access to the distribution. In previous sections we have used CLT, but what we learned applies to averages and here we are interested in the median. We can see that the 95% confidence interval based on CLT\n\nmedian(x) + 1.96*sd(x)/sqrt(N)*c(-1, 1)\n#&gt; [1] 21018 55905\n\nis quite different from the confidence interval we would generate if we know the actual distribution of \\(M\\):\n\nquantile(m, c(0.025, 0.975))\n#&gt;  2.5% 97.5% \n#&gt; 34438 59050\n\nThe bootstrap allows us to approximate a Monte Carlo simulation even when we do not have access to the full population distribution. The idea is straightforward: we treat the observed sample as if it were the population. From this sample, we draw new datasets of the same size, with replacement, and compute the statistic of interest, in this case, the median, for each resampled dataset. These resampled datasets are called bootstrap samples.\nIn many practical situations, the distribution of the statistics computed from bootstrap samples provides a good approximation to the sampling distribution of the original statistic. This approximation allows us to estimate variability, compute standard errors, and construct confidence intervals without knowing the true underlying distribution.\nThe following code demonstrates how to generate bootstrap samples and approximate the sampling distribution of the median:\n\nB &lt;- 10^4\nm_star &lt;- replicate(B, {\n  x_star &lt;- sample(x, N, replace = TRUE)\n  median(x_star)\n})\n\nNote a confidence interval constructed with the bootstrap is much closer to one constructed with the theoretical distribution:\n\nquantile(m_star, c(0.025, 0.975))\n#&gt;  2.5% 97.5% \n#&gt; 30253 56909",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "inference/bootstrap.html#recomended-reading",
    "href": "inference/bootstrap.html#recomended-reading",
    "title": "13  Bootstrap",
    "section": "\n13.3 Recomended reading",
    "text": "13.3 Recomended reading\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning (2nd ed.). Springer.\nUndergraduate friendly. See the chapter on resampling methods for a clear, concise introduction to the bootstrap with intuitive examples and code.\nEfron, B., & Tibshirani, R. J. (1993). An Introduction to the Bootstrap. Chapman & Hall/CRC.\nThe classic, authoritative treatment. Develops the bootstrap from first principles, with theory, practical guidance, and many examples.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "inference/bootstrap.html#exercises",
    "href": "inference/bootstrap.html#exercises",
    "title": "13  Bootstrap",
    "section": "\n13.4 Exercises",
    "text": "13.4 Exercises\n1. Generate a random dataset like this:\n\ny &lt;- rnorm(100, 0, 1)\n\nEstimate the 75th quantile, which we know is:\n\nqnorm(0.75)\n\nwith the sample quantile:\n\nquantile(y, 0.75)\n\nRun a Monte Carlo simulation to learn the expected value and standard error of this random variable.\n2. In practice, we can’t run a Monte Carlo simulation because we don’t know if rnorm is being used to simulate the data. Use the bootstrap to estimate the standard error using just the initial sample y. Use 10 bootstrap samples.\n3. Redo exercise 2, but with 10,000 bootstrap samples.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bootstrap</span>"
    ]
  },
  {
    "objectID": "linear-models/intro-to-linear-models.html",
    "href": "linear-models/intro-to-linear-models.html",
    "title": "Linear Models",
    "section": "",
    "text": "Up to this point, this book has focused mainly on datasets consisting of a single variable. However, in data analyses challenges, it is very common to be interested in the relationship between two or more variables. In this part of the book we introduce linear models, a general framework that unifies approaches used for analyzing association between variables, such as simple and multivariate regression, treatment effect models, and association tests. We will illustrate these using case studies related to understudying if height is hereditary, described in detail in 14  Regression, using data to build a baseball team on a budget, described in detail in 19  Multivariable Regression, determining if a high-fat diet makes mice heavier, described in detail in 16  Treatment effect models, and examining if their is gender bias in research funding in the Netherlands, described in detail in 17  Association tests.",
    "crumbs": [
      "Linear Models"
    ]
  },
  {
    "objectID": "linear-models/regression.html",
    "href": "linear-models/regression.html",
    "title": "14  Regression",
    "section": "",
    "text": "14.1 Case study: is height hereditary?\nTo understand the concepts of correlation and simple regression, we actually use the dataset from which regression was born. The example is from genetics. Francis Galton1 studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected, our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question, as well as many other circumstances.\nWe have access to Galton’s family height data through the HistData package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son from each family:\n#library(data.table)\nlibrary(tidyverse)\nlibrary(HistData)\n\nset.seed(1983)\n#setDT(GaltonFamilies)\n#x &lt;- data.table(GaltonFamilies)\n#galton_heights &lt;- x[gender == \"male\", .SD[sample(.N, 1)], by = family][, .(father, son = childHeight)]\n#galton_heights\n\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\nSuppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:\ngalton_heights |&gt; summarize(mean(father), sd(father), mean(son), sd(son))\n#&gt; # A tibble: 1 × 4\n#&gt;   `mean(father)` `sd(father)` `mean(son)` `sd(son)`\n#&gt;            &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1           69.1         2.55        69.2      2.71\nHowever, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.\ngalton_heights |&gt; ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5)\nWe will learn that the correlation coefficient is an informative summary of how two variables move together and then motivate simple regression by noting how this can be used to predict one variable using the other.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#sec-corr-coef",
    "href": "linear-models/regression.html#sec-corr-coef",
    "title": "14  Regression",
    "section": "\n14.2 The correlation coefficient",
    "text": "14.2 The correlation coefficient\nThe correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as the average of the product of the standardized values:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\n\\]\nwith \\(\\mu_x, \\mu_y\\) the averages of \\(x_1,\\dots, x_n\\) and \\(y_1, \\dots, y_n\\), respectively, and \\(\\sigma_x, \\sigma_y\\) the standard deviations. The Greek letter for \\(r\\), \\(\\rho\\) is commonly used in statistics books to denote the correlation. It is not a coincidence that \\(r\\) is the first letter in regression. Soon we learn about the connection between correlation and regression.\nWe can represent the formula above with R code using:\n\nrho &lt;- mean(scale(x) * scale(y))\n\nTo understand why this equation does in fact summarize how two variables move together, consider the \\(i\\)-th entry of \\(x\\) is \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\) SDs away from the average. Similarly, the \\(y_i\\) that is paired with \\(x_i\\), is \\(\\left( \\frac{y_1-\\mu_y}{\\sigma_y} \\right)\\) SDs away from the average \\(y\\). If \\(x\\) and \\(y\\) are unrelated, the product \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\\) will be positive ( \\(+ \\times +\\) and \\(- \\times -\\) ) as often as negative (\\(+ \\times -\\) and \\(- \\times +\\)) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products (\\(+ \\times +\\) and \\(- \\times -\\)) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.\nThe correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and, in this case, the correlation is:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\frac{1}{n} \\sum_{i=1}^n \\left( x_i-\\mu_x \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\sigma^2_x =\n1\n\\]\nA similar derivation, but with \\(x\\) and its exact opposite, proves the correlation has to be bigger or equal to -1.\nFor other pairs, the correlation is between -1 and 1. The correlation, computed with the function cor, between father and son’s heights is about 0.5:\n\ngalton_heights |&gt; summarize(r = cor(father, son)) |&gt; pull(r)\n#&gt; [1] 0.433\n\n\n\n\n\n\n\nThe function cor(x, y) computes the sample correlation, which divides the sum of products by length(x)-1 rather than length(x). The rationale for this is akin to the reason we divide by length(x)-1 when computing the sample standard deviation sd(x). Namely, this adjustment helps account for the degrees of freedom in the sample, which is necessary for unbiased estimates.\n\n\n\nTo see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99:\n\n\n\n\n\n\n\n\n\n14.2.1 Sample correlation is a random variable\nBefore we continue connecting correlation to regression, let’s remind ourselves about random variability.\nIn most data analysis projects, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest, but rather for a random sample. As with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.\nBy way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:\n\nr &lt;- sample_n(galton_heights, 25, replace = TRUE) |&gt; \n  summarize(r = cor(father, son)) |&gt; pull(r)\n\n`r`` is a random variable. We can run a Monte Carlo simulation to see its distribution:\n\nB &lt;- 1000\nN &lt;- 25\nr &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    summarize(r = cor(father, son)) |&gt; pull(r)\n})\nhist(r, breaks = 20)\n\n\n\n\n\n\n\nWe see that the expected value of `r`` is the population correlation:\n\nmean(r)\n#&gt; [1] 0.431\n\nand that it has a relatively high standard error relative to the range of values `r`` can take:\n\nsd(r)\n#&gt; [1] 0.161\n\nSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nAlso, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough \\(N\\), the distribution of R is approximately normal with expected value \\(\\rho\\). The standard deviation, which is somewhat complex to derive, is \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nIn our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one:\n\n\n\n\n\n\n\n\nIf you increase \\(N\\), you will see the distribution converging to normal.\n\n14.2.2 Correlation is not always a useful summary\nCorrelation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:\n\n\n\n\n\n\n\n\nCorrelation is only meaningful in a particular context. To help us understand when correlation is meaningful a s a summary statistic, we return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#sec-conditional-expectation",
    "href": "linear-models/regression.html#sec-conditional-expectation",
    "title": "14  Regression",
    "section": "\n14.3 Conditional expectations",
    "text": "14.3 Conditional expectations\nSuppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?\nIt turns out that, if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.\nIn general, we call this approach conditioning. The general idea is that we stratify a population into groups and compute summaries in each group. To provide a mathematical description of conditioning, consider that we have a population of pairs of values \\((x_1,y_1),\\dots,(x_n,y_n)\\), for example all father and son heights in England. In the previous chapter, we learned that if you take a random pair \\((X,Y)\\), the expected value and best predictor of \\(Y\\) is \\(\\mathrm{E}(Y) = \\mu_y\\), the population average \\(1/n\\sum_{i=1}^n y_i\\). However, we are no longer interested in the general population. Instead, we are interested in only the subset of a population with a specific \\(x_i\\) value, 72 inches in our example. This subset of the population is also a population, and thus, the same principles and properties we have learned apply. The \\(y_i\\) in the subpopulation have a distribution, referred to as the conditional distribution, and this distribution has an expected value referred to as the conditional expectation. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is:\n\\[\n\\mathrm{E}(Y \\mid X = x)\n\\]\nwith \\(x\\) representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with:\n\\[\n\\mbox{SD}(Y \\mid X = x) = \\sqrt{\\mathrm{Var}(Y \\mid X = x)}\n\\]\nBecause the conditional expectation \\(E(Y\\mid X=x)\\) is the best predictor for the random variable \\(Y\\) for an individual in the strata defined by \\(X=x\\), many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.\nIn the example we have been considering, we are interested in computing the average son height conditioned on the father being 72 inches tall. We want to estimate \\(E(Y|X=72)\\) using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that, for continuous data, we don’t have many data points matching exactly one value in our sample. For example, we have only:\n\nsum(galton_heights$father == 72)\n#&gt; [1] 8\n\nfathers that are exactly 72 inches. If we change the number to 72.5, we get even fewer data points:\n\nsum(galton_heights$father == 72.5)\n#&gt; [1] 1\n\nA practical way to improve these estimates of the conditional expectations is to define strata of observations with similar value of \\(x\\). In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:\n\nconditional_avg &lt;- galton_heights |&gt; \n  filter(round(father) == 72) |&gt;\n  summarize(avg = mean(son)) |&gt; \n  pull(avg)\nconditional_avg\n#&gt; [1] 70.5\n\nNote that a 72 inch father is taller than average, specifically (72.0 - 69.1)/2.5 = 1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72 inch fathers have regressed some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later section, this is not a coincidence.\nIf we want to make a prediction of any height, not just 72 inches, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:\n\ngalton_heights |&gt; mutate(father_strata = factor(round(father))) |&gt; \n  ggplot(aes(father_strata, son)) + \n  geom_boxplot() + \n  geom_point()\n\n\n\n\n\n\n\nNot surprisingly, the centers of the groups are increasing with height. Furthermore, these centers appear to follow a linear relationship. Below, we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:\n\n\n\n\n\n\n\n\nThe fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the regression line, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line, so we also describe Galton’s theoretical justification for using the regression line.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#the-regression-line",
    "href": "linear-models/regression.html#the-regression-line",
    "title": "14  Regression",
    "section": "\n14.4 The regression line",
    "text": "14.4 The regression line\nIf we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), our prediction \\(\\hat{Y}\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore:\n\\[\n\\left( \\frac{\\hat{Y}-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\n\\]\nWe can rewrite it like this:\n\\[\n\\hat{Y} = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y\n\\]\nIf there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use \\(x\\) at all for the prediction and simply predict the average \\(\\mu_Y\\). For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the son regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature. To add regression lines to plots, we will need the above formula in the form:\n\\[\n\\hat{Y} = b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x\n\\]\nHere we add the regression line to the original data:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\n\ngalton_heights |&gt; \n  ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) +\n  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x) \n\n\n\n\n\n\n\nThe regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). You can make same plot, but using standard units like this:\n\ngalton_heights |&gt; \n  ggplot(aes(scale(father), scale(son))) + \n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = r)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#regression-improves-precision",
    "href": "linear-models/regression.html#regression-improves-precision",
    "title": "14  Regression",
    "section": "\n14.5 Regression improves precision",
    "text": "14.5 Regression improves precision\nLet’s compare the two approaches to prediction that we have presented:\n\nRound fathers’ heights to closest inch, stratify, and then take the average.\nCompute the regression line and use it to predict.\n\nWe use a Monte Carlo simulation sampling \\(N=50\\) families:\n\nB &lt;- 1000\nN &lt;- 50\n\nset.seed(1983)\nconditional_avg &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  dat |&gt; filter(round(father) == 72) |&gt; \n    summarize(avg = mean(son)) |&gt; \n    pull(avg)\n  })\n\nregression_prediction &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  mu_x &lt;- mean(dat$father)\n  mu_y &lt;- mean(dat$son)\n  s_x &lt;- sd(dat$father)\n  s_y &lt;- sd(dat$son)\n  r &lt;- cor(dat$father, dat$son)\n  mu_y + r*(72 - mu_x)/s_x*s_y\n})\n\nAlthough the expected value of these two random variables is about the same:\n\nmean(conditional_avg, na.rm = TRUE)\n#&gt; [1] 70.5\nmean(regression_prediction)\n#&gt; [1] 70.5\n\nThe standard error for the regression prediction is substantially smaller:\n\nsd(conditional_avg, na.rm = TRUE)\n#&gt; [1] 0.964\nsd(regression_prediction)\n#&gt; [1] 0.452\n\nThe prediction obtained with the regression line is therefore much more stable than the prediction obtained using the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use na.rm=TRUE. The regression line is estimated using all the data.\nSo why not always use the regression line for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of the chapter.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#bivariate-normal-distribution",
    "href": "linear-models/regression.html#bivariate-normal-distribution",
    "title": "14  Regression",
    "section": "\n14.6 Bivariate normal distribution",
    "text": "14.6 Bivariate normal distribution\nCorrelation and the regression slope are widely used summary statistics, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases in which the correlation is not a useful summary. But there are many real-life examples.\nThe main way we motivate appropriate use of correlation as a summary, involves the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. As we saw in Section 14.2, they can be thin (high correlation) or circle-shaped (no correlation).\nA more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and the conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal, then the pair is approximately bivariate normal.\nWhen three or more variables have the property that each pair is bivariate normal, we say the variables follow a multivariate normal distribution or that they are jointly normal.\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:\n\ngalton_heights |&gt;\n  mutate(z_father = round((father - mean(father)) / sd(father))) |&gt;\n  filter(z_father %in% -2:2) |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = son)) +\n  facet_wrap( ~ z_father) \n\n\n\n\n\n\n\nNow we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\\[\n\\mathrm{E}(Y | X=x) = \\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\n\\]\nThis is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\). It is equivalent to the regression equation we showed earlier which can be written like this:\n\\[\n\\frac{\\mathrm{E}(Y \\mid X=x)  - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#variance-explained",
    "href": "linear-models/regression.html#variance-explained",
    "title": "14  Regression",
    "section": "\n14.7 Variance explained",
    "text": "14.7 Variance explained\nThe bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\\[\n\\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2}\n\\]\nTo see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72 inch father. This group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#there-are-two-regression-lines",
    "href": "linear-models/regression.html#there-are-two-regression-lines",
    "title": "14  Regression",
    "section": "\n14.8 There are two regression lines",
    "text": "14.8 There are two regression lines\nWe computed a regression line to predict the son’s height from father’s height. We used these calculations:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\nm_1 &lt;-  r * s_y / s_x\nb_1 &lt;- mu_y - m_1*mu_x\n\nwhich gives us the function \\(\\mathrm{E}(Y\\mid X=x) =\\) 37.3 + 0.46 \\(x\\).\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mathrm{E}(Y\\mid X=x) -\\) 37.3 \\(\\} /\\) 0.5.\nWe need to compute \\(\\mathrm{E}(X \\mid Y=y)\\). Since the data is approximately bivariate normal, the theory described earlier tells us that this conditional expectation will follow a line with slope and intercept:\n\nm_2 &lt;-  r * s_x / s_y\nb_2 &lt;- mu_x - m_2 * mu_y\n\nSo we get \\(\\mathrm{E}(X \\mid Y=y) =\\) 40.9 + 0.41y. Again, we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nHere is a plot showing the two regression lines, with blue for the predicting son heights with father heights, and red for predicting father heights with son heights:\n\ngalton_heights |&gt; \n  ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = b_1, slope = m_1, col = \"blue\") +\n  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = \"red\")",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#linear-models",
    "href": "linear-models/regression.html#linear-models",
    "title": "14  Regression",
    "section": "\n14.9 Linear models",
    "text": "14.9 Linear models\nWe are now ready to understand the title of this part of the book. Specifically, the connection between regression and linear models. We have described how, if data is bivariate normal, then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption, but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a linear model.\nWe note that linear here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, \\(3x - 4y + 5z\\) is a linear combination of \\(x\\), \\(y\\), and \\(z\\). We can also add a constant so \\(2 + 3x - 4y + 5z\\) is also a linear combination of \\(x\\), \\(y\\), and \\(z\\).\nWe previously described how if \\(X\\) and \\(Y\\) are bivariate normal, then if we look at only the pairs with \\(X=x\\), then \\(Y \\mid X=x\\) follows a normal distribution with expected value \\(\\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\\), which is a linear function of \\(x\\), and standard deviation \\(\\sigma_Y \\sqrt{1-\\rho^2}\\) that does not depend on \\(x\\). Note that if we write:\n\\[\nY = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nIf we assume \\(\\varepsilon\\) follows a normal distribution with expected value 0 and fixed standard deviation, then \\(Y\\) has the same properties as the regression setup gave us: it follows a normal distribution, the expected value is a linear function \\(x\\), and the standard deviation does not depend on \\(x\\).\n\n\n\n\n\n\nIn statistical textbooks, the \\(\\varepsilon\\)s are referred to as “errors,” which originally represented measurement errors in the initial applications of these models. These errors were associated with inaccuracies in measuring height, weight, or distance. However, the term “error” is now used more broadly, even when the \\(\\varepsilon\\)s do not necessarily signify an actual error. For instance, in the case of height, if someone is 2 inches taller than expected, based on their parents’ height, those 2 inches should not be considered an error. Despite its lack of descriptive accuracy, the term “error” is employed to elucidate the unexplained variability in the model, unrelated to other included terms.\n\n\n\nIf we were to specify a linear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N.\n\\]\nHere \\(x_i\\) is the father’s height, which is fixed (not random) due to the conditioning, and \\(Y_i\\) is the random son’s height that we want to predict. We can further assume that \\(\\varepsilon_i\\) are independent from each other and all have the same standard deviation.\nIn the above model, we know the \\(x_i\\), but to have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\). We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height \\(x\\). We show how to do this in the next section.\nAlthough this model is exactly the same one we derived earlier by assuming bivariate normal data, a somewhat nuanced difference is that, in the first approach, we assumed the data was bivariate normal and the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)s is not necessarily specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.\nOne reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\). Because not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.\nGiven how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable, as it is the predicted height of a son of a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:\n\\[\nY_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N\n\\]\nwith \\(\\bar{x} = 1/N \\sum_{i=1}^N x_i\\) the average of the \\(x\\). In this case, \\(\\beta_0\\) represents the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father.\nLater, specifically in Sections Chapter 19 and Chapter 16, we will see how the linear model representation permits us to use the same mathematical frameworks in other contexts and to achieve more complicated goals than predicting one variable from another.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#sec-lse",
    "href": "linear-models/regression.html#sec-lse",
    "title": "14  Regression",
    "section": "\n14.10 Least Squares Estimates",
    "text": "14.10 Least Squares Estimates\nFor linear models to be useful, we have to estimate the unknown \\(\\beta\\)s. The standard approach is to find the values that minimize the distance of the fitted model to the data. Specifically, we find the \\(\\beta\\)s that minimize the least squares (LS) equation show below. For Galton’s data, the LS equation looks like this:\n\\[\nRSS = \\sum_{i=1}^n \\left\\{  y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2\n\\]\nThe quantity we try to minimize is called the residual sum of squares (RSS).\nOnce we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them by placing a hat over the parameters. In our example we use \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nWe will demonstrate how we find these values using the previously defined galton_heights dataset. Let’s start by writing a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\).\n\nrss &lt;- function(beta0, beta1, data){\n  resid &lt;- galton_heights$son - (beta0 + beta1*galton_heights$father)\n  return(sum(resid^2))\n}\n\nSo for any pair of values, we get an RSS. Here is a plot of the RSS as a function of \\(\\beta_1\\), when we keep the \\(\\beta_0\\) fixed at 25.\n\nbeta1 = seq(0, 1, length = nrow(galton_heights))\nresults &lt;- data.frame(beta1 = beta1,\n                      rss = sapply(beta1, rss, beta0 = 25))\nresults |&gt; ggplot(aes(beta1, rss)) + geom_line() + \n  geom_line(aes(beta1, rss))\n\n\n\n\n\n\n\nWe can see a clear minimum for \\(\\beta_1\\) at around 0.65. However, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.\nTrial and error is not going to work in this case. We could search for a minimum within a fine grid of \\(\\beta_0\\) and \\(\\beta_1\\) values, but this is unnecessarily time-consuming since we can use calculus. Specifically, we take the partial derivatives, set them to 0, and solve for \\(\\beta_1\\) and \\(\\beta_2\\). Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will study these next. To learn the mathematics behind this, you can consult a book on linear models.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#the-lm-function",
    "href": "linear-models/regression.html#the-lm-function",
    "title": "14  Regression",
    "section": "\n14.11 The lm function",
    "text": "14.11 The lm function\nIn R, we can obtain the least squares estimates using the lm function. To fit the model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nwith \\(Y_i\\) being the son’s height and \\(x_i\\) being the father’s height, we can use this code to obtain the least squares estimates.\n\nfit &lt;- lm(son ~ father, data = galton_heights)\nfit$coefficients\n#&gt; (Intercept)      father \n#&gt;      37.288       0.461\n\nThe most common way we use lm is by using the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~). The intercept is added automatically to the model that will be fit.\nThe object fit includes more information about the fit. We can use the function summary to extract more of this information:\n\nsummary(fit)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = son ~ father, data = galton_heights)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -9.354 -1.566 -0.008  1.726  9.415 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  37.2876     4.9862    7.48  3.4e-12 ***\n#&gt; father        0.4614     0.0721    6.40  1.4e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.45 on 177 degrees of freedom\n#&gt; Multiple R-squared:  0.188,  Adjusted R-squared:  0.183 \n#&gt; F-statistic: 40.9 on 1 and 177 DF,  p-value: 1.36e-09\n\nTo understand some of the information included in this summary, we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables.\nIn Chapter 19, after describing a more complex case study, we gain further insights into the application of regression in R.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#lse-are-random-variables",
    "href": "linear-models/regression.html#lse-are-random-variables",
    "title": "14  Regression",
    "section": "\n14.12 LSE are random variables",
    "text": "14.12 LSE are random variables\nThe LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\). This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size \\(N=50\\), and compute the regression slope coefficient for each one:\n\nB &lt;- 1000\nN &lt;- 50\nlse &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    lm(son ~ father, data = _) |&gt; \n    coef()\n})\nlse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) \n\nWe can see the variability of the estimates by plotting their distributions:\n\n#&gt; \n#&gt; Attaching package: 'gridExtra'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     combine\n\n\n\n\n\n\n\nThe reason these look normal is because the central limit theorem applies here as well: for large enough \\(N\\), the least squares estimates will be approximately normal with expected value \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\nThe standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function. The function summary shows us the standard error estimates:\n\nsample_n(galton_heights, N, replace = TRUE) |&gt; \n  lm(son ~ father, data = _) |&gt; \n  summary() |&gt; \n  coef()\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    19.28     11.656    1.65 1.05e-01\n#&gt; father          0.72      0.169    4.25 9.79e-05\n\nYou can see that the standard errors estimates reported above are close to the standard errors from the simulation:\n\nlse |&gt; summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))\n#&gt;   se_0  se_1\n#&gt; 1 8.84 0.128\n\nThe summary function also reports t-statistics (t value) and p-values (Pr(&gt;|t|)). The t-statistic is not actually based on the central limit theorem, but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\hat{\\mathrm{SE}}(\\hat{\\beta}_0 )\\) and \\(\\hat{\\beta}_1 / \\hat{\\mathrm{SE}}(\\hat{\\beta}_1 )\\), follow a t-distribution with \\(N-p\\) degrees of freedom, with \\(p\\) the number of parameters in our model. In our example \\(p=2\\), and the two p-values are obtained from testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\), respectively.\nRemember that, as we described in Section Section 9.2.3, for large enough \\(N\\), the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about broom, an add-on package that makes this easy.\nAlthough we do not show examples in this book, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#predicted-values-are-random-variables",
    "href": "linear-models/regression.html#predicted-values-are-random-variables",
    "title": "14  Regression",
    "section": "\n14.13 Predicted values are random variables",
    "text": "14.13 Predicted values are random variables\nOnce we fit our model, we can obtain a prediction of \\(Y\\) by plugging the estimates into the regression model. For example, if the father’s height is \\(x\\), then our prediction \\(\\hat{Y}\\) for the son’s height will be:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\nWhen we plot \\(\\hat{Y}\\) versus \\(x\\), we see the regression line.\nKeep in mind that the prediction \\(\\hat{Y}\\) is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the ggplot2 layer geom_smooth(method = \"lm\") that we previously used plots \\(\\hat{Y}\\) and surrounds it by confidence intervals:\n\ngalton_heights |&gt; ggplot(aes(son, father)) +\n  geom_point() +\n  geom_smooth(formula = 'y ~ x',method = \"lm\")\n\n\n\n\n\n\n\nThe R function predict takes an lm object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:\n\nfit &lt;- galton_heights |&gt; lm(son ~ father, data = _) \n\ny_hat &lt;- predict(fit, se.fit = TRUE)\n\nnames(y_hat)\n#&gt; [1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#sec-diagnostic-plots",
    "href": "linear-models/regression.html#sec-diagnostic-plots",
    "title": "14  Regression",
    "section": "\n14.14 Diagnostic plots",
    "text": "14.14 Diagnostic plots\nWhen the linear model is assumed, rather than derived, all interpretations depend on the usefulness of the model. The lm function will fit the model and return summaries even when the model is wrong and not useful.\nVisually inspecting residuals, defined as the difference between observed values and predicted values:\n\\[\nr = Y - \\hat{Y} = Y - \\left(\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right),\n\\] and summaries of the residuals, is a powerful way to diagnose if the model is useful. Note that the residuals can be thought of estimates of the errors since:\n\\[\n\\varepsilon = Y - \\left(\\beta_0 + \\beta_1 x_i \\right).\n\\] In fact residuals are often denoted as \\(\\hat{\\varepsilon}\\). This motivates several diagnostic plots. Because we observe \\(r\\), but don’t observe \\(\\varepsilon\\), we based the plots on the residuals.\n\nBecause the errors are assumed not to depend on the expected value of \\(Y\\), a plot of \\(r\\) versus the fitted values \\(\\hat{Y}\\) should show no relationship.\nIn cases in which we assume the errors follow a normal distribution, a qqplot of standardized \\(r\\) should fall on a line when plotted against theoretical quantiles.\nBecause we assume the standard deviation of the errors is constant, if we plot the absolute value of the residuals, it should appear constant.\n\nWe prefer plots rather than summaries based on, for example, correlation because, as noted in Section 14.2.2, correlation is not always the best summary of association. The function plot applied to an lm object automatically plots these.\n\n\n\n\n\n\n\n\n\nplot(fit, which = 1:3)\n\nThis function can produce six different plots, and the argument which let’s you specify which you want to see. You can learn more by reading the plot.lm help file. However, some of the plots are based on more advanced concepts beyond the scope of this book. To learn more, we recommend an advanced book on regression analysis.\nIn Sections Chapter 19 and Chapter 16, we introduce data analysis challenges in which we may decide to not include certain variables in the model. In these cases, an important diagnostic test is to checks if the residuals are related to variables not included in the model.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#the-regression-fallacy",
    "href": "linear-models/regression.html#the-regression-fallacy",
    "title": "14  Regression",
    "section": "\n14.15 The regression fallacy",
    "text": "14.15 The regression fallacy\nWikipedia defines the sophomore slump as:\n\nA sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).\n\nIn Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The sophomore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this Fox Sports article2 asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”\nDoes the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for a widely used measure of success, the batting average, we see that this observation holds true for the top performing ROYs:\n\n\n\n\nnameFirst\nnameLast\nrookie_year\nrookie\nsophomore\n\n\n\nWillie\nMcCovey\n1959\n0.354\n0.238\n\n\nIchiro\nSuzuki\n2001\n0.350\n0.321\n\n\nAl\nBumbry\n1973\n0.337\n0.233\n\n\nFred\nLynn\n1975\n0.331\n0.314\n\n\nAlbert\nPujols\n2001\n0.329\n0.314\n\n\n\n\n\nIn fact, the proportion of players that have a lower batting average during their sophomore year is 0.7053571.\nSo is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all the players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).\nThe same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\nMiguel\nCabrera\n0.348\n0.313\n\n\nHanley\nRamirez\n0.345\n0.283\n\n\nMichael\nCuddyer\n0.331\n0.332\n\n\nScooter\nGennett\n0.324\n0.289\n\n\nJoe\nMauer\n0.324\n0.277\n\n\n\n\n\nBut these are not rookies! Also, look at what happens to the worst performers of 2013:\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\nDanny\nEspinosa\n0.158\n0.219\n\n\nDan\nUggla\n0.179\n0.149\n\n\nJeff\nMathis\n0.181\n0.200\n\n\nB. J.\nUpton\n0.184\n0.208\n\n\nAdam\nRosales\n0.190\n0.262\n\n\n\n\n\nTheir batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as a sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:\n\n\n\n\n\n\n\n\nThe correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with:\n\\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\]\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of \\(X\\), so it is expected that \\(Y\\) will regress to the mean.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#exercises",
    "href": "linear-models/regression.html#exercises",
    "title": "14  Regression",
    "section": "\n14.16 Exercises",
    "text": "14.16 Exercises\n1. Load the GaltonFamilies data from the HistData package. The children in each family are listed by gender and then by height. Create a dataset called galton_heights by picking a male and female at random.\n2. Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n3. Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/regression.html#footnotes",
    "href": "linear-models/regression.html#footnotes",
    "title": "14  Regression",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Francis_Galton↩︎\nhttps://web.archive.org/web20160815063904/http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/measurement-error-models.html",
    "href": "linear-models/measurement-error-models.html",
    "title": "\n15  Measurement error models\n",
    "section": "",
    "text": "15.1 Example: modeling a falling object\nAnother major application of linear models occurs in measurement error models. In these situations, non-random covariates, such as time, are frequently encountered, with randomness often arising from measurement errors rather than from sampling or inherent natural variability.\nTo understand these models, imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we currently know and adding some measurement error. The dslabs function rfalling_object generates these simulations:\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(dslabs)\nfalling_object &lt;- rfalling_object()\nThe assistants hand the data to Galileo, and this is what he sees:\nfalling_object |&gt; \n  ggplot(aes(time, observed_distance)) + \n  geom_point() +\n  ylab(\"Distance in meters\") + \n  xlab(\"Time in seconds\")\nGalileo does not know the exact equation, but by looking at the plot above, he deduces that the position should follow a parabola, which we can write like this:\n\\[\nf(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n\\]\nThe data does not fall exactly on a parabola. Galileo knows this is due to measurement error. His helpers make mistakes when measuring the distance. To account for this, he models the data with:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i, i=1,\\dots,n\n\\]\nwith \\(Y_i\\) representing distance in meters, \\(x_i\\) representing time in seconds, and \\(\\varepsilon_i\\) accounting for measurement error. The measurement error is assumed to be random, independent from each other, and having the same distribution for each \\(i\\). We also assume that there is no bias, which means the expected value \\(\\mathrm{E}[\\varepsilon] = 0\\).\nNote that this is a linear model because it is a linear combination of known quantities (\\(x\\) and \\(x^2\\) are known) and unknown parameters (the \\(\\beta\\)s are unknown parameters to Galileo). Unlike our previous examples, here \\(x\\) is a fixed quantity; we are not conditioning.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measurement error models</span>"
    ]
  },
  {
    "objectID": "linear-models/measurement-error-models.html#example-modeling-a-falling-object",
    "href": "linear-models/measurement-error-models.html#example-modeling-a-falling-object",
    "title": "\n15  Measurement error models\n",
    "section": "",
    "text": "Small discrepancies between a model’s predictions and observations are often attributed to measurement error, as in our example. In many cases, this is a useful and practical approximation. However, such differences can also reveal limitations in the model itself. Galileo’s experiments on gravity showed slight deviations from his predicted uniform acceleration, largely due to air resistance rather than flawed measurements. Similarly, Newton’s laws of gravity accurately described planetary motion, but small discrepancies in Mercury’s orbit, once considered observational errors, ultimately led to Einstein’s general theory of relativity. While assuming measurement error is often reasonable, it is crucial to recognize when discrepancies signal model limitations. The diagnostic plots discussed in Section 14.14 can help assess such limitations.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measurement error models</span>"
    ]
  },
  {
    "objectID": "linear-models/measurement-error-models.html#estimating-parameters-with-least-squares",
    "href": "linear-models/measurement-error-models.html#estimating-parameters-with-least-squares",
    "title": "\n15  Measurement error models\n",
    "section": "\n15.2 Estimating parameters with least squares",
    "text": "15.2 Estimating parameters with least squares\nTo pose a new physical theory and start making predictions about other falling objects, Galileo needs actual numbers, rather than unknown parameters. Using LSE seems like a reasonable approach. How do we find the LSE?\nLSE calculations do not require the errors to be approximately normal. The lm function will find the \\(\\beta\\)s that will minimize the residual sum of squares:\n\nfit &lt;- falling_object |&gt; \n  mutate(time_sq = time^2) |&gt; \n  lm(observed_distance~time+time_sq, data = _)\nsummary(fit)$coefficients\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)   56.347      0.472 119.303 1.80e-18\n#&gt; time          -0.548      0.675  -0.812 4.34e-01\n#&gt; time_sq       -4.791      0.200 -23.930 7.74e-11\n\nLet’s check if the estimated parabola fits the data. The broom function augment allows us to do this easily:\n\naugment(fit) |&gt; ggplot() +\n  geom_point(aes(time, observed_distance)) + \n  geom_line(aes(time, .fitted), col = \"blue\")\n\n\n\n\n\n\n\nThanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is:\n\\[\nd(t) = h_0 + v_0 t -  0.5 \\times 9.8 \\, t^2\n\\]\nwith \\(h_0\\) and \\(v_0\\) the starting height and velocity, respectively. The data we simulated above followed this equation, adding measurement error to simulate n observations for dropping the ball \\((v_0=0)\\) from the tower of Pisa \\((h_0=55.86)\\).\nThese are consistent with the parameter estimates:\n\ntidy(fit, conf.int = TRUE)\n#&gt; # A tibble: 3 × 7\n#&gt;   term        estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)   56.3       0.472   119.    1.80e-18    55.3     57.4  \n#&gt; 2 time          -0.548     0.675    -0.812 4.34e- 1    -2.03     0.938\n#&gt; 3 time_sq       -4.79      0.200   -23.9   7.74e-11    -5.23    -4.35\n\nThe Tower of Pisa height is within the confidence interval for \\(\\beta_0\\), the initial velocity 0 is in the confidence interval for \\(\\beta_1\\) (note the p-value is larger than 0.05), and the acceleration constant is in a confidence interval for \\(-2 \\times \\beta_2\\).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measurement error models</span>"
    ]
  },
  {
    "objectID": "linear-models/measurement-error-models.html#exercises",
    "href": "linear-models/measurement-error-models.html#exercises",
    "title": "\n15  Measurement error models\n",
    "section": "\n15.3 Exercises",
    "text": "15.3 Exercises\n1. The co2 dataset is a time series object of 468 CO2 observations, monthly from 1959 to 1997. Plot CO2 levels for the first 12 months and notice it seems to follow a sine wave with a frequency of 1 cycle per year. This means that a measurement error model that might work is\n\\[\ny_i = \\mu + A \\sin(2\\pi \\,t_i / 12 + \\phi) + \\varepsilon_i\n\\] with \\(t_i\\) the month number for observation \\(i\\). Is this a linear model for the parameters \\(mu\\), \\(A\\) and \\(\\phi\\)?\n2. Using trigonometry, we can show that we can rewrite this model as:\n\\[\ny_i = \\beta_0 + \\beta_1 \\sin(2\\pi t_i/12) + \\beta_2 \\cos(2\\pi t_i/12) + \\varepsilon_i\n\\]\nIs this a linear model?\n3. Find least square estimates for the \\(\\beta\\)s using lm. Show a plot of \\(y_i\\) versus \\(t_i\\) with a curve on the same plot showing \\(\\hat{Y}_i\\) versus \\(t_i\\).\n4. Now fit a measurement error model to the entire co2 dataset that includes a trend term that is a parabola as well as the sine wave model.\n5. Run diagnostic plots for the fitted model and describe the results.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Measurement error models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html",
    "href": "linear-models/treatment-effect-models.html",
    "title": "16  Treatment effect models",
    "section": "",
    "text": "16.1 Comparing group means\nUp to now, all our linear models have been applied to two or more continuous random variables. We assume the random variables are multivariate normal and use this to motivate a linear model. This approach covers many real-life examples of linear regression. However, linear models have many other applications. One of the most popular is to quantify treatment effects in randomized and controlled experiments. One of the first applications was in agriculture, where different plots of lands were treated with different combinations of fertilizers to try to determine if they were effective. In fact, the use of \\(Y\\) for the outcome in statistics is due to the mathematical theory being developed for crop yield as the outcome.\nSince then, the same ideas have been applied in other areas, such as randomized trials developed to determine if drugs cure or prevent diseases or if policies have an effect on social or educational outcomes. In the latter example, we think of the policy intervention as a treatment and follow the same mathematical procedure. The analyses used in A/B testing, widely used today by internet companies, are based on treatment effects models.\nMoreover, these models have been applied in observational studies where analysts attempt to use linear models to estimate effects of interest while accounting for potential confounders. For example, to estimate the effect of a diet high in fruits and vegetables on blood pressure, we would have to adjust for factors such as age, sex, and smoking status.\nIn this chapter, we consider an experiment designed to test for the effects of a high-fat diet on mouse physiology. Mice were randomly selected and divided into two groups: one group receiving a high-fat diet, considered the treatment, while the other group served as the control and received the usual chow diet. The data is included in the dslabs package:\nA boxplot shows that the high fat diet mice are, on average, heavier.\nHowever, given that we divided the mice randomly, is it possible that the observed difference is simply due to chance? Here, we can compute the sample average and standard deviation of each group and perform statistical inference on the difference of these means, similar to our approach for election forecasting in Chapters Chapter 9 and Chapter 12.\nThe sample averages for the two groups, high-fat and chow diets, are different:\nlibrary(tidyverse)\nmice_weights |&gt; group_by(diet) |&gt; summarize(average = mean(body_weight))\n#&gt; # A tibble: 2 × 2\n#&gt;   diet  average\n#&gt;   &lt;fct&gt;   &lt;dbl&gt;\n#&gt; 1 chow     31.5\n#&gt; 2 hf       36.7\nHowever, this is a random sample of mice, and the assignment to the diet group is also done randomly. So is this difference due to chance? We will use hypothesis testing, first described in Chapter 12, to answer this question.\nLet \\(\\mu_1\\) and \\(\\sigma_1\\) represent the weight average and standard deviation, respectively, that we would observe if the entire population of mice were on the high-fat diet. Define \\(\\mu_0\\) and \\(\\sigma_0\\) similarly, but for the chow diet. Define \\(N_1\\) and \\(N_0\\) as the sample sizes, and \\(\\bar{X}_1\\) and \\(\\bar{X}_0\\) the sample averages, for the for the high-fat and chow diets, respectively.\nSince the data comes from a random sample, the central limit theorem tells us that, if the sample is large enough, the difference in averages \\(\\bar{X}_1 - \\bar{X}_0\\) follows a normal distribution, with expected value \\(\\mu_1-\\mu_0\\) and standard error \\(\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}}\\).\nIf we define the null hypothesis as the high-fat diet having no effect, or \\(\\mu_1 - \\mu_0 = 0\\), this implies that\n\\[\n\\frac{\\bar{X}_1 - \\bar{X}_0}{\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}}}\n\\]\nhas expected value 0 and standard error 1 and therefore approximately follows a standard normal distribution.\nNote that we can’t compute this quantity in practice because the \\(\\sigma_1\\) and \\(\\sigma_0\\) are unknown. However, if we estimate them with the sample standard deviations, denote them \\(s_1\\) and \\(s_0\\) for the high-fat and chow diets, respectively, the central limit theorem still holds and tells us that\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_0}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_0^2}{N_0}}}\n\\]\nfollows a standard normal distribution when the null hypothesis is true. This implies that we can easily compute the probability of observing a value as large as the one we obtained:\nstats &lt;- mice_weights |&gt; \n  group_by(diet) |&gt; \n  summarize(xbar = mean(body_weight), s = sd(body_weight), n = n()) \nt_stat &lt;- with(stats, (xbar[2] - xbar[1])/sqrt(s[2]^2/n[2] + s[1]^2/n[1]))\nt_stat\n#&gt; [1] 9.34\nHere \\(t\\) is well over 3, so we don’t really need to compute the p-value 1-pnorm(t_stat) as we know it will be very small.\nNote that when \\(N\\) is not large enough, then the CLT does not apply. However, if the outcome data, in this case weight, follows a normal distribution, then \\(t\\) follows a t-distribution with \\(N_1+N_2-2\\) degrees of freedom. So the calculation of the p-value is the same except that we use pt instead of pnorm. Specifically, we use 1-pt(t_stat, with(stats, n[2]+n[1]-2)).\nDifferences in means are commonly examined in the scientific studies. As a result this t-statistic is one of the most widely reported summaries. When used to determine if an observed difference is statistically significant, we refer to the procedure as “performing a t test”.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Treatment effect models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#comparing-group-means",
    "href": "linear-models/treatment-effect-models.html#comparing-group-means",
    "title": "16  Treatment effect models",
    "section": "",
    "text": "In the computation above, we calculated the probability of observing a \\(t\\) value as large as the one obtained. However, when our interest includes deviations in both directions—for example, either an increase or a decrease in weight—we must consider the probability of obtaining a value of \\(t\\) as extreme as the one observed, regardless of sign. In that case, we use the absolute value of \\(t\\) and double the one-sided probability: 2*(1 - pnorm(abs(t-test))) or 2*(1-pt(abs(t_stat), with(stats, n[2]+n[1]-2))).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Treatment effect models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#one-factor-design",
    "href": "linear-models/treatment-effect-models.html#one-factor-design",
    "title": "16  Treatment effect models",
    "section": "\n16.2 One factor design",
    "text": "16.2 One factor design\nAlthough the t-test is useful for cases in which we compare two treatments, it is common to have other variables affect our outcomes. Linear models permit hypothesis testing in these more general situations. We start the description of the use of linear models for estimating treatment effects by demonstrating how they can be used to perform t-tests.\nIf we assume that the weight distributions for both chow and high-fat diets are normally distributed, we can write the following linear model to represent the data:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nwith \\(X_i\\) 1, if the \\(i\\)-th mice was fed the high-fat diet, and 0 otherwise, and the errors \\(\\varepsilon_i\\) independent and normally distributed with expected value 0 and standard deviation \\(\\sigma\\). Note that this mathematical formula looks exactly like the model we wrote out for the father-son heights. However, the fact that \\(x_i\\) is now 0 or 1 rather than a continuous variable, allows us to use it in this different context. In particular, notice that now \\(\\beta_0\\) represents the population average weight of the mice on the chow diet and \\(\\beta_0 + \\beta_1\\) represents the population average for the weight of the mice on the high-fat diet.\nA nice feature of this model is that \\(\\beta_1\\) represents the treatment effect of receiving the high-fat diet. The null hypothesis that the high-fat diet has no effect can be quantified as \\(\\beta_1 = 0\\). To perform hypothesis testing on the effect of the high fat diet we can estimate \\(\\beta_1\\) and compute the probability of an estimate being as large as the observed when the null hypothesis is true. So how do we estimate \\(\\beta_1\\) and compute this probability?\nA powerful characteristic of linear models is that we can estimate the \\(\\beta\\)s and their standard errors with the same LSE machinery:\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)\n\nBecause diet is a factor with two entries, the lm function knows to fit the linear model above with an \\(x_i\\), an indicator variable. The summary function shows us the resulting estimate, standard error, and p-value:\n\ncoefficients(summary(fit))\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept)    31.54      0.386   81.74 0.00e+00\n#&gt; diethf          5.14      0.548    9.36 8.02e-20\n\nUsing broom, we can write:\n\nlibrary(broom)\ntidy(fit, conf.int = TRUE) |&gt; filter(term == \"diethf\")\n#&gt; # A tibble: 1 × 7\n#&gt;   term   estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 diethf     5.14     0.548      9.36 8.02e-20     4.06      6.21\n\nThe statistic computed here is the estimate divided by its standard error: \\(\\hat{\\beta}_1 / \\hat{\\mathrm{SE}}(\\hat{\\beta}_1)\\). In the case of the simple one-factor model, we can show that this statistic is almost equivalent to the t-statistics computed in the previous section:\n\nc(coefficients(summary(fit))[2,3], t_stat)\n#&gt; [1] 9.36 9.34\n\nIntuitively, it makes sense, as both \\(\\hat{\\beta_1}\\) and the numerator of the t-test are estimates of the treatment effect.\nThe one minor difference is that the linear model does not assume a different standard deviation for each population. Instead, both populations share \\(\\mbox{SD}[\\varepsilon]\\) as a standard deviation. Note that, although we don’t demonstrate it with R here, we can redefine the linear model to have different standard errors for each group.\n\n\n\n\n\n\nIn the linear model description provided here, we assumed \\(\\varepsilon\\) follows a normal distribution. This assumption permits us to show that the statistics formed by dividing estimates by their estimated standard errors follow t-distribution, which in turn allows us to estimate p-values or confidence intervals. However, note that we do not need this assumption to compute the expected value and standard error of the least squared estimates. Furthermore, if the number of observations is large enough, then the central limit theorem applies and we can obtain p-values and confidence intervals even without the normal distribution assumption for the errors.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Treatment effect models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#two-factor-designs",
    "href": "linear-models/treatment-effect-models.html#two-factor-designs",
    "title": "16  Treatment effect models",
    "section": "\n16.3 Two factor designs",
    "text": "16.3 Two factor designs\nNote that this experiment included male and female mice, and male mice are known to be heavier. This explains why the residuals depend on the sex variable:\n\nboxplot(fit$residuals ~ mice_weights$sex)\n\n\n\n\n\n\n\nThis misspecification can have real implications; for instance, if more male mice received the high-fat diet, then this could explain the increase. Conversely, if fewer received it, we might underestimate the diet effect. Sex could be a confounder, indicating that our model can certainly be improved.\nFrom examining the data:\n\nmice_weights |&gt; ggplot(aes(diet, log2(body_weight), fill = sex)) + geom_boxplot()\n\n\n\n\n\n\n\nwe see that the diet effect is observed for both sexes and that males are heavier than females. Although not nearly as obvious, it also appears the diet effect is stronger in males.\nA linear model that permits a different expected value for the following four groups, 1) female on chow diet, 2) females on high-fat diet, 3) male on chow diet, and 4) males on high-fat diet, can be written like this:\n\\[\nY_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}  + \\beta_3 x_{i,3}  + \\beta_4 x_{i,4}  + \\varepsilon_i\n\\]\nwith \\(x_{i,1},\\dots,x_{i,4}\\) indicator variables for each of the four groups. Note that with this representation we allow the diet effect to be different for males and females.\nHowever, with this representation, none of the \\(\\beta\\)s represent the effect of interest: the diet effect. A powerful feature of linear models is that we can rewrite the model so that the expected value for each group remains the same, but the parameters represent the effects we are interested in. So, for example, in the representation\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1}  + \\beta_2 x_{i,2}  + \\beta_3 x_{i,1} x_{i,2}  + \\varepsilon_i\n\\]\nwith \\(x_{i,1}\\) an indicator that is 1 if individual \\(i\\) is on the high-fat diet \\(x_{i,2}\\) an indicator that is 1 if you are male, the \\(\\beta_1\\) is interpreted as the diet effect for females, \\(\\beta_2\\) as the average difference between males and females, and \\(\\beta_3\\) the difference in the diet effect between males and females. In statistics, \\(\\beta_3\\) is referred to as an interaction effect. The \\(\\beta_0\\) is considered the baseline value, which represents the population average weight of females on the chow diet.\nStatistical textbooks describe several other ways in which the model can be rewritten to obtain other types of interpretations. For example, we might want \\(\\beta_2\\) to represent the overall diet effect (the average between female and male effect) rather than the diet effect on females. This is achieved by defining what contrasts we are interested in.\nIn R, we can specify the linear model above using the following:\n\nfit &lt;- lm(body_weight ~ diet*sex, data = mice_weights)\n\nHere, the * denotes factor crossing, not multiplication: diet*sex is shorthand for diet+sex+diet:sex, to calculate diet; sex; and diet combined with sex.\n\ntidy(fit, conf.int = TRUE) |&gt; filter(!str_detect(term, \"Intercept\"))\n#&gt; # A tibble: 3 × 7\n#&gt;   term        estimate std.error statistic  p.value conf.low conf.high\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 diethf          3.88     0.624      6.22 8.02e-10    2.66       5.10\n#&gt; 2 sexM            7.53     0.627     12.0  1.27e-30    6.30       8.76\n#&gt; 3 diethf:sexM     2.66     0.891      2.99 2.91e- 3    0.912      4.41\n\nNote that the male effect is larger that the diet effect, and the diet effect is statistically significant for both sexes, with diet affecting males more by between 1 and 4.5 grams.\nA common approach applied when more than one factor is thought to affect the measurement is to simply include an additive effect for each factor, like this:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1}  + \\beta_2 x_{i,2}   + \\varepsilon_i\n\\]\nIn this model, the \\(\\beta_1\\) is a general diet effect that applies regardless of sex. In R, we use the following code, employing a + instead of *:\n\nfit &lt;- lm(body_weight ~ diet + sex, data = mice_weights)\n\nNote that this model does not account for the difference in diet effect between males and females. Diagnostic plots would reveal this deficiency by showing that the residuals are biased: they are, on average, negative for females on the diet and positive for males on the diet, rather than being centered around 0.\n\nplot(fit, which = 1)\n\n\n\n\n\n\n\nScientific studies, particularly within epidemiology and social sciences, frequently omit interaction terms from models due to the high number of variables. Adding interactions necessitates numerous parameters, which in extreme cases may prevent the model from fitting. However, this approach assumes that the interaction terms are zero, and if incorrect, it can skew the interpretation of the results. Conversely, when this assumption is valid, models excluding interactions are simpler to interpret, as parameters are typically viewed as the extent to which the outcome increases with the assigned treatment.\n\n\n\n\n\n\nLinear models are highly flexible and applicable in many contexts. For example, we can include many more factors than just 2. We have only just scratched the surface of how linear models can be used to estimate treatment effects. We highly recommend learning more about this by exploring linear model textbooks and R manuals that cover the use of functions such as lm, contrasts, and model.matrix.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Treatment effect models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#contrasts",
    "href": "linear-models/treatment-effect-models.html#contrasts",
    "title": "16  Treatment effect models",
    "section": "\n16.4 Contrasts",
    "text": "16.4 Contrasts\nIn the examples we have examined, each treatment had only two groups: diet had chow/high-fat, and sex had female/male. However, variables of interest often have more than one level. For example, we might have tested a third diet on the mice. In statistics textbooks, these variables are referred to as a factor, and the groups in each factor are called its levels.\nWhen a factor is included in the formula, the default behavior for lm is to define the intercept term as the expected value for the first level, and the other coefficient are to represent the difference, or contrast, between the other levels and first. We can see when we estimate the sex effect with lm like this:\n\nfit &lt;- lm(body_weight ~ sex, data = mice_weights)\ncoefficients(fit)\n#&gt; (Intercept)        sexM \n#&gt;       29.76        8.82\n\nTo recover the expected mean for males, we can simply add the two coefficients:\n\nsum(fit$coefficients[1:2])\n#&gt; [1] 38.6\n\nThe package emmeans simplifies the calculation and also calculates standard errors:\n\nlibrary(emmeans)\nemmeans(fit, ~sex)\n#&gt;  sex emmean    SE  df lower.CL upper.CL\n#&gt;  F     29.8 0.339 778     29.1     30.4\n#&gt;  M     38.6 0.346 778     37.9     39.3\n#&gt; \n#&gt; Confidence level used: 0.95\n\nNow, what if we really didn’t want to define a reference level? What if we wanted a parameter to represent the difference from each group to the overall mean? Can we write a model like this:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n\\] with \\(x_{i,1} = 1\\), if observation \\(i\\) is female and 0 otherwise, and \\(x_{i,2}=1\\), if observation \\(i\\) is male and 0 otherwise?\nUnfortunately, this representation has a problem. Note that the mean for females and males are represented by \\(\\beta_0 + \\beta_1\\) and \\(\\beta_0 + \\beta_2\\), respectively. This is a problem because the expected value for each group is just one number, say \\(\\mu_f\\) and \\(\\mu_m\\), and there is an infinite number of ways \\(\\beta_0 + \\beta_1 = \\mu_f\\) and \\(\\beta_0 +\\beta_2 = \\mu_m\\) (three unknowns with two equations). This implies that we can’t obtain a unique least squares estimate. In statistics, we say the model, or parameters, are unidentifiable. The default behavior in R solves this problem by requiring \\(\\beta_1 = 0\\), forcing \\(\\beta_0 = \\mu_m\\), which permits us to solve the system of equations.\nKeep in mind that this is not the only constraint that permits estimation of the parameters. Any linear constraint will do as it adds a third equation to our system. A widely used constraint is to require \\(\\beta_1 + \\beta_2 = 0\\). To achieve this in R, we can use the argument contrasts in the following way:\n\nfit &lt;- lm(body_weight ~ sex, data = mice_weights, \n          contrasts = list(sex = contr.sum))\ncoefficients(fit)\n#&gt; (Intercept)        sex1 \n#&gt;       34.17       -4.41\n\nWe see that the intercept is now larger, reflecting the overall mean rather than just the mean for females. The other coefficient, \\(\\beta_1\\), represents the contrast between females and the overall mean in our model. The coefficient for males is not shown because it is redundant: \\(\\beta_1= -\\beta_2\\).\nIf we want to see all the estimates, the emmeans package also makes the calculations for us:\n\ncontrast(emmeans(fit, ~sex))\n#&gt;  contrast estimate    SE  df t.ratio p.value\n#&gt;  F effect    -4.41 0.242 778 -18.200  &lt;.0001\n#&gt;  M effect     4.41 0.242 778  18.200  &lt;.0001\n#&gt; \n#&gt; P value adjustment: fdr method for 2 tests\n\nThe use of this alternative constraint is more practical when a factor has more than one level, and choosing a baseline becomes less convenient. Furthermore, we might be more interested in the variance of the coefficients rather than the contrasts between groups and the reference level.\nAs an example, consider that the mice in our dataset are actually from several generations:\n\ntable(mice_weights$gen)\n#&gt; \n#&gt;   4   7   8   9  11 \n#&gt;  97 195 193  97 198\n\nTo estimate the variability due to the different generations, a convenient model is:\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^J \\beta_j x_{i,j} + \\varepsilon_i\n\\]\nwith \\(x_{i,j}\\) indicator variables: \\(x_{i,j}=1\\) if mouse \\(i\\) is in level \\(j\\) and 0 otherwise, \\(J\\) representing the number of levels, in our example 5 generations, and the level effects constrained with\n\\[\n\\frac{1}{J} \\sum_{j=1}^J \\beta_j = 0 \\implies \\sum_{j=1}^J \\beta_j = 0.\n\\]\nThis constraint makes the model identifiable and also allows us to quantify the variability due to generations with:\n\\[\n\\sigma^2_{\\text{gen}} \\equiv \\frac{1}{J}\\sum_{j=1}^J \\beta_j^2\n\\]\nWe can see the estimated coefficients using the following:\n\nfit &lt;- lm(body_weight ~ gen,  data = mice_weights, \n          contrasts = list(gen = contr.sum))\ncontrast(emmeans(fit, ~gen)) \n#&gt;  contrast     estimate    SE  df t.ratio p.value\n#&gt;  gen4 effect    -0.122 0.705 775  -0.174  0.8620\n#&gt;  gen7 effect    -0.812 0.542 775  -1.497  0.3370\n#&gt;  gen8 effect    -0.113 0.544 775  -0.207  0.8620\n#&gt;  gen9 effect     0.149 0.705 775   0.212  0.8620\n#&gt;  gen11 effect    0.897 0.540 775   1.663  0.3370\n#&gt; \n#&gt; P value adjustment: fdr method for 5 tests\n\nIn the next section, we briefly describe a technique useful to study the variability associated with this factor.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Treatment effect models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#sec-anova",
    "href": "linear-models/treatment-effect-models.html#sec-anova",
    "title": "16  Treatment effect models",
    "section": "\n16.5 Analysis of variance (ANOVA)",
    "text": "16.5 Analysis of variance (ANOVA)\nWhen a factor has more than one level, it is common to want to determine if there is significant variability across the levels rather than specific difference between any given pair of levels. Analysis of variances (ANOVA) provides tools to do this.\nANOVA provides an estimate of \\(\\sigma^2_{\\text{gen}}\\) and a statistical test for the null hypothesis that the factor contributes no variability: \\(\\sigma^2_{\\text{gen}} =0\\).\nOnce a linear model is fit using one or more factors, the aov function can be used to perform ANOVA. Specifically, the estimate of the factor variability is computed along with a statistic that can be used for hypothesis testing:\n\nsummary(aov(fit))\n#&gt;              Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; gen           4    294    73.5    1.13   0.34\n#&gt; Residuals   775  50479    65.1\n\nKeep in mind that if given a model formula, aov will fit the model:\n\nsummary(aov(body_weight ~ gen, data = mice_weights))\n\nWe do not need to specify the constraint because ANOVA needs to constrain the sum to be 0 for the results to be interpretable.\nThis analysis indicates that generation is not statistically significant.\n\n\n\n\n\n\nWe do not include many details, for example, on how the summary statistics and p-values shown by aov are defined and motivated. There are several books dedicated to the analysis of variance, and textbooks on linear models often include chapters on this topic. Those interested in learning more about these topics can consult one of these textbooks.\n\n\n\n\n16.5.1 Multiple factors\nANOVA was developed to analyze agricultural data, which typically included several factors such as fertilizers, blocks of lands, and plant breeds.\nNote that we can perform ANOVA with multiple factors:\n\nsummary(aov(body_weight ~ sex + diet + gen,  data = mice_weights))\n#&gt;              Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; sex           1  15165   15165  389.80 &lt;2e-16 ***\n#&gt; diet          1   5238    5238  134.64 &lt;2e-16 ***\n#&gt; gen           4    295      74    1.89   0.11    \n#&gt; Residuals   773  30074      39                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis analysis suggests that sex is the biggest source of variability, which is consistent with previously made exploratory plots.\n\n\n\n\n\n\nOne of the key aspects of ANOVA (Analysis of Variance) is its ability to decompose the total variance in the data, represented by \\(\\sum_{i=1}^n Y_i^2\\), into individual contributions attributable to each factor in the study. However, for the mathematical underpinnings of ANOVA to be valid, the experimental design must be balanced. This means that for every level of any given factor, there must be an equal representation of the levels of all other factors. In our study involving mice, the design is unbalanced, requiring a cautious approach in the interpretation of the ANOVA results.\n\n\n\n\n16.5.2 Array representation\nWhen the model includes more than one factor, writing down linear models can become cumbersome. For example, in our two factor model, we would have to include indicator variables for both factors:\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^J \\beta_j x_{i,j} + \\sum_{k=1}^K \\beta_{J+k} x_{i,J+k} + \\varepsilon_i \\mbox{ with }\\sum_{j=1}^J \\beta_j=0 \\mbox{ and } \\sum_{k=1}^K \\beta_{J+k} = 0,\n\\]\nthe \\(x_{i,1},\\dots,x_{i,J}\\) indicator functions for the \\(J\\) levels in the first factor and \\(x_{i,J+1},\\dots,x_{i,J+K}\\) indicator functions for the \\(K\\) levels in the second factor.\nAn alternative approach widely used in ANOVA to avoid indicator variables, is to save the data in an array, using different Greek letters to denote factors and indices to denote levels:\n\\[\nY_{i,j,k} = \\mu + \\alpha_j + \\beta_k + \\varepsilon_{i,j,k}\n\\]\nwith \\(\\mu\\) the overall mean, \\(\\alpha_j\\) the effect of level \\(j\\) in the first factor, and \\(\\beta_k\\) the effect of level \\(k\\) in the second factor. The constraint can now be written as:\n\\[\n\\sum_{j=1}^J \\alpha_j = 0 \\text{ and } \\sum_{k=1}^K \\beta_k = 0\n\\]\nThis notation lends itself to estimating the effects by computing means across dimensions of the array.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Treatment effect models</span>"
    ]
  },
  {
    "objectID": "linear-models/treatment-effect-models.html#exercises",
    "href": "linear-models/treatment-effect-models.html#exercises",
    "title": "16  Treatment effect models",
    "section": "\n16.6 Exercises",
    "text": "16.6 Exercises\n1. Once you fit a model, the estimate of the standard error \\(\\sigma\\) can be obtained as follows:\n\nfit &lt;- lm(body_weight ~ diet, data = mice_weights)\nsummary(fit)$sigma\n\nCompute the estimate of \\(\\sigma\\) using both the model that includes only diet and a model that accounts for sex. Are the estimates the same? If not, why not?\n2. One of the assumption of the linear model fit by lm is that the standard deviation of the errors \\(\\varepsilon_i\\) is equal for all \\(i\\). This implies that it does not depend on the expected value. Group the mice by their weight like this:\n\nbreaks &lt;- with(mice_weights, seq(min(body_weight), max(body_weight), 1))\ndat &lt;- mutate(mice_weights, group = cut(body_weight, breaks, include_lowest = TRUE))\n\nCompute the average and standard deviation of body_weight for groups with more than 10 observations and use data exploration to verify if this assumption holds.\n3. The dataset also includes a variable indicating which litter the mice came from. Create a boxplot showing weights by litter. Use faceting to make separate plots for each diet and sex combination.\n4. Use a linear model to test for a litter effect, taking into account sex and diet. Use ANOVA to compare the variability explained by litter with that of other factors.\n5. The mice_weights data includes two other outcomes: bone density and percent fat. Create a boxplot illustrating bone density by sex and diet. Compare what the visualizations reveal about the diet effect by sex.\n6. Fit a linear model and conduct a separate test for the diet effect on bone density for each sex. Note that the diet effect is statistically significant for females but not for males. Then fit the model to the entire dataset that includes diet, sex and their interaction. Notice that the diet effect is significant, yet the interaction effect is not. Explain how this can happen. Hint: To fit a model to the entire dataset with a separate effect for males and females, you can use the formula ~ sex + diet:sex\n7. In Chapter 9, we talked about pollster bias and used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election:\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(pollster %in% c(\"Rasmussen Reports/Pulse Opinion Research\",\n                         \"The Times-Picayune/Lucid\") &\n           enddate &gt;= \"2016-10-15\" &\n           state == \"U.S.\") |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) \n\nWe want to answer the question: is there a pollster bias? Make a plot showing the spreads for each pollster.\n8. The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.\nThe urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(\\mu\\).\nTo answer the question “is there an urn model?” we will model the observed data \\(Y_{i,j}\\) in the following way:\n\\[\nY_{i,j} = \\mu + b_i + \\varepsilon_{i,j}\n\\]\nwith \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\), and \\(\\varepsilon_ij\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\), and standard deviation \\(\\sigma_i\\) regardless of \\(j\\).\nWhich of the following best represents our question?\n\nIs \\(\\varepsilon_{i,j}\\) = 0?\nHow close are the \\(Y_{i,j}\\) to \\(\\mu\\)?\nIs \\(b_1 \\neq b_2\\)?\nAre \\(b_1 = 0\\) and \\(b_2 = 0\\) ?\n\n9. On the right side of this model, only \\(\\varepsilon_{i,j}\\) is a random variable; the other two are constants. What is the expected value of \\(Y_{1,j}\\)?\n10. Suppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first poll, \\(Y_{1,1},\\dots,Y_{1,N_1}\\), where \\(N_1\\) is the number of polls conducted by the first pollster:\n\npolls |&gt; \n  filter(pollster==\"Rasmussen Reports/Pulse Opinion Research\") |&gt; \n  summarize(N_1 = n())\n\nWhat is the expected values \\(\\bar{Y}_1\\)?\n11. What is the standard error of \\(\\bar{Y}_1\\) ?\n12. Suppose we define \\(\\bar{Y}_2\\) as the average of poll results from the second poll, \\(Y_{2,1},\\dots,Y_{2,N_2}\\), where \\(N_2\\) is the number of polls conducted by the second pollster. What is the expected value \\(\\bar{Y}_2\\)?\n13. What is the standard error of \\(\\bar{Y}_2\\) ?\n14. Using what we learned by answering the questions above, what is the expected value of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)?\n15. Using what we learned by answering the questions above, what is the standard error of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)?\n16. The answer to the question above depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), which we don’t know. We learned that we can estimate these with the sample standard deviation. Write code that computes these two estimates.\n17. What does the CLT tell us about the distribution of \\(\\bar{Y}_2 - \\bar{Y}_1\\)?\n\nNothing because this is not the average of a sample.\nBecause the \\(Y_{ij}\\) are approximately normal, so are the averages.\nNote that \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages, so if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normally distributed variables is also normally distributed.\nThe data are not 0 or 1, so CLT does not apply.\n\n18. We have constructed a random variable that has an expected value of \\(b_2 - b_1\\), representing the difference in pollster bias. If our model holds, then this random variable has an approximately normal distribution, and we know its standard error. The standard error depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), but we can plug the sample standard deviations we computed above. We began by asking: is \\(b_2 - b_1\\) different from 0? Using all the information we have gathered above, construct a 95% confidence interval for the difference \\(b_2 - b_1\\).\n19. The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. Random variability does not seem to explain it. We can compute a p-value to relay the fact that chance does not explain it. What is the p-value?\n20. The statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error:\n\\[\n\\frac{\\bar{Y}_2 - \\bar{Y}_1}{\\sqrt{s_2^2/N_2 + s_1^2/N_1}}\n\\]\nis the t-statistic. Now notice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls.\nFor this exercise, create a new table:\n\npolls &lt;- polls_us_election_2016 |&gt; \n  filter(enddate &gt;= \"2016-10-15\" &\n           state == \"U.S.\") |&gt;\n  group_by(pollster) |&gt;\n  filter(n() &gt;= 5) |&gt; \n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |&gt;\n  ungroup()\n\nCompute the average and standard deviation for each pollster and examine the variability across the averages. Compare this to the variability within the pollsters, summarized by the standard deviation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Treatment effect models</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html",
    "href": "linear-models/association-tests.html",
    "title": "17  Association tests",
    "section": "",
    "text": "17.1 Case study: Funding success rates\nThe statistical models studied up to now are appropriate for continuous outcomes. We have not yet discussed inference for binary, categorical, and ordinal data. To give a very specific example, we will consider a case study examining funding success rates in the Netherlands, categorized by gender.\nA 2014 PNAS paper1 analyzed success rates from funding agencies in the Netherlands and concluded that their\nThe main evidence supporting this conclusion is based on a comparison of the percentages. Table S1 in the paper includes the information we need. Here are the three columns showing the overall outcomes:\nlibrary(tidyverse)\nlibrary(dslabs)\nresearch_funding_rates |&gt;\n  select(discipline, applications_total, success_rates_total) |&gt; head()\n#&gt;           discipline applications_total success_rates_total\n#&gt; 1  Chemical sciences                122                26.2\n#&gt; 2  Physical sciences                174                20.1\n#&gt; 3            Physics                 76                26.3\n#&gt; 4         Humanities                396                16.4\n#&gt; 5 Technical sciences                251                17.1\n#&gt; 6  Interdisciplinary                183                15.8\nWe also have these values for each gender:\nnames(research_funding_rates)\n#&gt;  [1] \"discipline\"          \"applications_total\"  \"applications_men\"   \n#&gt;  [4] \"applications_women\"  \"awards_total\"        \"awards_men\"         \n#&gt;  [7] \"awards_women\"        \"success_rates_total\" \"success_rates_men\"  \n#&gt; [10] \"success_rates_women\"\nWe can compute the totals that were successful, and the totals that were not, as follows:\ntotals &lt;- research_funding_rates |&gt; \n  select(-discipline) |&gt; \n  summarize_all(sum) |&gt;\n  summarize(yes_men = awards_men, \n            no_men = applications_men - awards_men, \n            yes_women = awards_women, \n            no_women = applications_women - awards_women)\nWe see that a larger percent of men than women received awards:\ntotals |&gt; summarize(percent_men = yes_men/(yes_men + no_men),\n                    percent_women = yes_women/(yes_women + no_women))\n#&gt;   percent_men percent_women\n#&gt; 1       0.177         0.149\nBut could this be due just to random variability? Here we learn how to perform inference for this type of data.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#case-study-funding-success-rates",
    "href": "linear-models/association-tests.html#case-study-funding-success-rates",
    "title": "17  Association tests",
    "section": "",
    "text": "results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#lady-tasting-tea",
    "href": "linear-models/association-tests.html#lady-tasting-tea",
    "title": "17  Association tests",
    "section": "\n17.2 Lady Tasting Tea",
    "text": "17.2 Lady Tasting Tea\nR.A. Fisher2 was one of the first to formalize hypothesis testing. The “Lady Tasting Tea” is one of the most famous examples.\nThe story is as follows: an acquaintance of Fisher claimed that she could tell if milk was added before or after tea was poured. Fisher was skeptical and, consequently, designed an experiment to test this claim. He gave her four pairs of cups of tea: one with milk poured first, the other after. The order was randomized. The null hypothesis here is that she is guessing. Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent.\nAs an example, suppose she identified 3 out of 4 correctly. Do we believe she has a special ability? The basic question we ask is: if the tester is actually guessing, what are the chances that she gets 3 or more correct? Just as we have done before, we can compute a probability under the null hypothesis that she is guessing for all 4. Under this null hypothesis, we can think of this particular example as picking 4 balls out of an urn with 4 blue (correct answer) and 4 red (incorrect answer) balls. Remember, she knows that there are four before tea and four after.\nUnder the null hypothesis that she is simply guessing, each ball has the same chance of being picked. We can then use combinations to determine each probability. The probability of picking 3 is \\(\\binom{4}{3} \\binom{4}{1} / \\binom{8}{4} = 16/70\\). The probability of picking all 4 correct is \\(\\binom{4}{4} \\binom{4}{0} / \\binom{8}{4}= 1/70\\). Thus, the chance of observing a 3 or something more extreme, under the null hypothesis, is \\(\\approx 0.24\\). This is the p-value. The procedure that produced this p-value is called Fisher’s exact test and it uses the hypergeometric distribution.\n\n\n\n\n\n\nAccording to accounts, the lady did succeed under Fisher’s test. Fisher’s point wasn’t to prove her right or wrong, but to show how to design an experiment and a statistical test to evaluate such claims.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#two-by-two-tables",
    "href": "linear-models/association-tests.html#two-by-two-tables",
    "title": "17  Association tests",
    "section": "\n17.3 Two-by-two tables",
    "text": "17.3 Two-by-two tables\nThe data from such experiments is usually summarized by a table like this:\n\ntab &lt;- matrix(c(3,1,1,3),2,2)\nrownames(tab) &lt;- c(\"Poured Before\", \"Poured After\")\ncolnames(tab) &lt;- c(\"Guessed before\", \"Guessed after\")\ntab\n#&gt;               Guessed before Guessed after\n#&gt; Poured Before              3             1\n#&gt; Poured After               1             3\n\nA two-by-two table summarizes the relationship between two binary variables by displaying the observed counts for each of the four possible combinations.\nThe function fisher.test can be used to carry out the inference described above. For example:\n\nfisher.test(tab, alternative = \"greater\")$p.value\n#&gt; [1] 0.243\n\nIn the next chapter, we will construct a two-by-two table for the funding success data. However, that analysis will require a different test than the hypergeometric approach.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#chi-square-test",
    "href": "linear-models/association-tests.html#chi-square-test",
    "title": "17  Association tests",
    "section": "\n17.4 Chi-square Test",
    "text": "17.4 Chi-square Test\nNotice that, in a sense, our funding rates example is similar to the Lady Tasting Tea. However, in the Lady Tasting Tea example, the number of blue and red beads is experimentally fixed and the number of answers given for each category is also fixed. This is because Fisher ensured there were four cups with milk poured before tea and four cups with milk poured after, and the lady knew this. Therefore, the answers would also have to include four before and four afters. In this case, the sum of the rows and the sum of the columns are fixed. This defines constraints on the possible ways we can fill the two by two table and also allows us to use the hypergeometric distribution. In general, this is not the case. Nonetheless, there is another approach, the Chi-squared test, which is described below.\nImagine we have four totals of applicants (290, 1,345, 177, 1,011), some are men and some are women, and some get funded while others do not. We saw that the success rates for men and women respectively were:\n\ntotals |&gt; summarize(percent_men = yes_men/(yes_men + no_men),\n                    percent_women = yes_women/(yes_women + no_women))\n#&gt;   percent_men percent_women\n#&gt; 1       0.177         0.149\n\nWould we see this again if we randomly assign funding at the overall rate:\n\nrate &lt;- with(totals, (yes_men + yes_women))/sum(totals)\nrate\n#&gt; [1] 0.165\n\nThe Chi-square test answers this question. The first step is to create the observed two-by-two data table:\n\no &lt;- with(totals, data.frame(men = c(no_men, yes_men),\n                             women = c(no_women, yes_women),\n                             row.names = c(\"no\", \"yes\")))\n\nThe general idea of the Chi-square test is to compare the observed two-by-two table to what you expect to see, which would be:\n\ne &lt;- with(totals, data.frame(men = (no_men + yes_men) * c(1 - rate, rate),\n                             women = (no_women + yes_women) * c(1 - rate, rate),\n                             row.names = c(\"no\", \"yes\")))\n                       \n\nWe can see that more men than expected and fewer women than expected received funding.\n\ncbind(o, e)\n#&gt;      men women  men women\n#&gt; no  1345  1011 1365   991\n#&gt; yes  290   177  270   197\n\nHowever, under the null hypothesis these observations are random variables. The Chi-square statistic quantifies how much the observed tables deviates from the expected by:\n\nTaking the difference between each observed and expected cell value.\nSquaring this difference.\nDividing each squared difference by the expected value.\nSumming all these values together to get the final statistic.\n\n\nsum((o - e)^2/e)\n#&gt; [1] 4.01\n\nThe Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function chisq.test takes a two-by-two table and returns the results from the test:\n\nchisq_test &lt;- chisq.test(o, correct = FALSE)\n\nWe see that the p-value is 0.045:\n\nchisq_test$p.value\n#&gt; [1] 0.0451\n\n\n\n\n\n\n\nBy default, the chisq.test function applies a continuity correction achieved by subtracting 0.5 in the following way:\n\nsum((abs(o - e) - 0.5)^2/e)\n#&gt; [1] 3.81\n\nNote that it matches the default behavior:\n\nchisq.test(o)$statistic\n#&gt; X-squared \n#&gt;      3.81",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#sec-glm",
    "href": "linear-models/association-tests.html#sec-glm",
    "title": "17  Association tests",
    "section": "\n17.5 Generalized linear models",
    "text": "17.5 Generalized linear models\nWe presented a way to perform hypothesis testing for determining if there is association between two binary outcomes. But we have not yet described how to quantify effects. Can we estimate the effect of being a woman in funding success in the Netherlands? Note that if our outcomes are binary, then the linear models presented in Chapter 16 are not appropriate because the \\(\\beta\\)s and \\(\\varepsilon\\) are continuous. However, an adaptation of these methods, that is widely used in, for example, medical studies, gives us a way to estimate effects along with their standard errors.\nThe idea is to model a transformation of the expected value of the outcomes with a linear model. The transformation is selected so that any continuous value is possible. The mathematical equation for a model with one variable looks like this:\n\\[\ng\\{\\mathrm{E}(Y_i)\\} = \\beta_0 + \\beta_1 x_i\n\\]\nTo finish describing the model, we impose a distribution on \\(Y\\), such as binomial or Poisson. These are referred to as generalized linear models.\nWe illustrate this with the funding rates example. We define \\(Y_i\\) to be 1 if person \\(i\\) received funding and 0 otherwise, and \\(x_i\\) to be 1 for person \\(i\\) is a woman and 0 if a man. For this data, the expected value of \\(Y_i\\) is the probability of funding for person \\(i\\) \\(\\mathrm{Pr}(Y_i=1)\\). We assume the outcomes \\(Y_i\\) are binomial, with \\(N=1\\) and probability \\(p_i\\). For binomial data, the most widely used transformation is the logit function \\(g(p) = \\log \\{p / (1-p)\\}\\), which maps numbers between 0 and 1 to any continuous number. The model looks like this:\n\\[\n\\log \\frac{\\mathrm{Pr}(Y_i=1)}{1-\\mathrm{Pr}(Y_i=1)} = \\beta_0 +  \\beta_1 x_i\n\\]\n\n17.5.1 The log odds ratio\nTo understand how \\(\\beta_1\\) can be used to quantify the effect of being a woman on success rates, first note that \\(\\mathrm{Pr}(Y_i=1)/\\left(1-\\mathrm{Pr}(Y_i=1)\\right) = \\mathrm{Pr}(Y_i=1)/\\mathrm{Pr}(Y_i=0)\\) is the odds of person \\(i\\) getting funding: the ratio of the probability of success and probability of failure. This implies that \\(e^{\\beta_0}\\) is the odds for men and \\(e^{\\beta_0}e^{\\beta_1}\\) is the odds for women, which implies \\(e^{\\beta_1}\\) is the odds for women divided by the odds for men. This quantity is called the odds ratio. To see this, note that if use \\(p_1\\) and \\(p_0\\) to denote the probability of success for women and men, respectively, then \\(e^{\\beta_1}\\) can be rewritten as:\n\\[\ne^{\\beta_1} = \\frac{p_1}{1-p_1} \\, / \\, \\frac{p_0}{1-p_0}\n\\]\nThe parameter \\(\\beta_1\\) therefore quantifies the log odds ratio.\nNow how do we estimate these parameters? Although the details are not described in this book, least squares is no longer the optimal way and instead we use an approach called maximum likelihood estimation (MLE).\nMore advanced mathematical derivations show that a version of the central limit theorem applies, and the estimates obtained this way are approximately normal when the number of observations is large. The theory also provides a way to calculate standard errors for the estimates of the \\(\\beta\\)s.\n\n17.5.2 Fitting the model\nTo obtain the maximum likelihood estimates using R, we can use the glm function with the family argument set to binomial. This defaults to using the logit transformation. Note that we do not have the individual level data, but because our model assumes the probability of success is the same for all women and all men, then the number of success can be modeled as binomial with \\(N_1\\) trials and probability \\(p_1\\) for women and binomial with \\(N_0\\) trials and probability \\(p_0\\) for men, where \\(N_1\\) and \\(N_0\\) are the total number of women and men. In this case, the glm function is used as follows:\n\nsuccess &lt;- with(totals, c(yes_men, yes_women))\nfailure &lt;- with(totals, c(no_men, no_women))\ngender &lt;- factor(c(\"men\", \"women\"))\nfit &lt;- glm(cbind(success, failure) ~ gender, family = \"binomial\") \ncoefficients(summary(fit))\n#&gt;             Estimate Std. Error z value  Pr(&gt;|z|)\n#&gt; (Intercept)   -1.534     0.0647   -23.7 3.83e-124\n#&gt; genderwomen   -0.208     0.1041    -2.0  4.54e-02\n\nThe estimate of the odds ratio is exp(fit$coef[2]) = 0.811982, interpreted as the odds being lowered by 20% for women compared to men. But is this due to chance? We already noted that the p-value is about 0.05, but the GLM approach also permits us to compute confidence intervals using the confint function. To show the interval for the more interpretable odds ratio, we simply exponentiate:\n\nexp(confint(fit, 2))\n#&gt;  2.5 % 97.5 % \n#&gt;  0.661  0.995\n\n\n\n\n\n\n\nWe have used a simple version of GLMs in which the only variable is binary. However, the method can be expanded to incorporate multiple variables, including continuous ones. In these contexts, the log odds ratio interpretation becomes more complex. Also, note that we have shown just one version of GLM appropriate for binomial data using a logit transformation. This version is often referred to as logistic regression. Nevertheless, GLM can be used with other transformation and distributions. You can learn more by consulting a GLM textbook.\n\n\n\n\n17.5.3 Simple standard error approximation for two-by-two table odds ratio\nUsing glm, we can obtain estimates, standard errors, and confidence intervals for a wide range of models. To do this, we use a rather complex algorithm. In the case of two-by-two tables, we can obtain a standard error for the log odds ratio using a simple approximation.\nIf two-by-two tables have the following entries:\n\n\n\nMen\nWomen\n\n\n\nAwarded\na\nb\n\n\nNot Awarded\nc\nd\n\n\n\nIn this case, the log odds ratio is simply\n\\[\n\\frac{a/c}{b/d} = \\frac{ad}{bc},\n\\]\nand the standard error for the log odds ratio is:\n\\[\n\\sqrt{1/a + 1/b + 1/c + 1/d}\n\\]\nThis implies that a 95% confidence interval for the log odds ratio can be formed by:\n\\[\n\\log\\left(\\frac{ad}{bc}\\right) \\pm 1.96 \\sqrt{1/a + 1/b + 1/c + 1/d}\n\\]\nWe can easily compute these quantities in R using:\n\ntwo_by_two &lt;- with(totals, data.frame(men = c(no_men, yes_men),\n                                      women = c(no_women, yes_women)))\nor &lt;- two_by_two[1,1]*two_by_two[2,2]/(two_by_two[2,1]*two_by_two[1,2])\nse &lt;- sqrt(sum(1/two_by_two))\n\nTo form a confidence interval for the odds ration we simply exponentiate the confidence interval for the log odds ratio:\n\nexp(log(or) + c(-1,1) * qnorm(0.975) * se)\n#&gt; [1] 0.662 0.996\n\nWe can confirm that we obtain the same estimate as when using glm, stored in fit:\n\nc(log(or), tidy(fit)$estimate[2])\n#&gt; [1] -0.208 -0.208\nc(se, tidy(fit)$std.error[2])\n#&gt; [1] 0.104 0.104\n\n\n\n\n\n\n\nKeep in mind that the p-values obtained with chisq.test and glm are slightly different. This is because these are both based on different approximation approaches.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#large-samples-small-p-values",
    "href": "linear-models/association-tests.html#large-samples-small-p-values",
    "title": "17  Association tests",
    "section": "\n17.6 Large samples, small p-values",
    "text": "17.6 Large samples, small p-values\nAs mentioned earlier, reporting only p-values is not always a useful way to report the results of data analysis. In scientific journals, some studies seem to overemphasize p-values. Some of these studies have large sample sizes and report impressively small p-values. Yet by looking closely at the results, we realize that the odds ratios are quite modest: barely bigger than 1. In this case, the difference may not be practically significant or scientifically significant.\nNote that the relationship between odds ratio and p-value is not one-to-one; it depends on the sample size. Therefore, a very small p-value does not necessarily mean a very large odds ratio. Observe what happens to the p-value if we multiply our two-by-two table by 10, which does not change the odds ratio:\n\ntwo_by_two_x_10 &lt;- two_by_two |&gt; mutate(men = men*10, women = women*10) \nchisq.test(two_by_two_x_10)$p.value\n#&gt; [1] 2.63e-10\n\n\n\n\n\n\n\nAlso, note that the log odds ratio is not defined if any of the cells of the two-by-two table is 0. This is because if \\(a\\), \\(b\\), \\(c\\), or \\(d\\) are 0, the \\(\\log(\\frac{ad}{bc})\\) is either the log of 0 or has a 0 in the denominator. For this situation, it is common practice to avoid 0s by adding 0.5 to each cell. This is referred to as the Haldane–Anscombe correction and has been shown, both in practice and theory, to work well.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#recommended-reading",
    "href": "linear-models/association-tests.html#recommended-reading",
    "title": "17  Association tests",
    "section": "\n17.7 Recommended reading",
    "text": "17.7 Recommended reading\nThe following resources provide deeper coverage of association tests and generalized linear models:\n\nAgresti, A. (2013). Categorical Data Analysis (3rd ed.).\nAuthoritative resource on association tests and categorical data.\nDobson, A. J., & Barnett, A. G. (2018). An Introduction to Generalized Linear Models (4th ed.).\nA clear, accessible introduction to GLMs.\nMcCullagh, P., & Nelder, J. A. (1989). Generalized Linear Models (2nd ed.).\nThe classic and definitive text on GLMs; more theoretical but essential for deep understanding.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#exercises",
    "href": "linear-models/association-tests.html#exercises",
    "title": "17  Association tests",
    "section": "\n17.8 Exercises",
    "text": "17.8 Exercises\n1. A famous athlete boasts an impressive career, winning 70% of her 500 career matches. Nevertheless, this athlete is criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.\n2. Why did we use the Chi-square test instead of Fisher’s exact test in the previous exercise?\n\nIt actually does not matter, since they give the exact same p-value.\nFisher’s exact and the Chi-square are different names for the same test.\nBecause the sum of the rows and columns of the two-by-two table are not fixed so the hypergeometric distribution is not an appropriate assumption for the null hypothesis. For this reason, Fisher’s exact test is rarely applicable with observational data.\nBecause the Chi-square test runs faster.\n\n3. Compute the odds ratio of “losing under pressure” along with a confidence interval.\n4. Notice that the p-value is larger than 0.05, but the 95% confidence interval does not include 1. What explains this?\n\nWe made a mistake in our code.\nThese are based on t-statistics so the connection between p-value and confidence intervals does not apply.\nDifferent approximations are used for the p-value and the confidence interval calculation. If we had a larger sample size, the match would be better.\nWe should use the Fisher exact test to get confidence intervals.\n\n5. Multiply the two-by-two table by 2 and see if the p-value and confidence interval are a better match.\n6. FIX Use the research_funding_rates data to estimate the log odds ratio and standard errors comparing women to men for each discipline. Compute a confidence interval and report all the disciplines for which one gender appears to be favored over the other.\n7. Divide the log odds ratio estimates by their respective standard errors and generate a qqplot comparing these to a standard normal. Do any of the disciplines deviate from what is expected by chance?\n8. During the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. Todd Vaziri hypothesized that “Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him).” We will test this hypothesis using association tests. The dslabs object sentiment_counts provides a table with the counts for several sentiments from each source (Android or iPhone):\n\nlibrary(tidyverse)\nlibrary(dslabs)\nsentiment_counts\n\nCompute an odds ratio comparing Android to iPhone for each sentiment and add it to the table.\n9. Compute a 95% confidence interval for each odds ratio.\n10. Generate a plot showing the estimated odds ratios along with their confidence intervals.\n11. FIX Test the null hypothesis that there is no difference between tweets from Android and iPhone and report the sentiments with p-values less than 0.05 and more likely to come from Android.\n12. For each sentiment, find the words assigned to that sentiment, keep words that appear at least 25 times, compute the odd ratio for each, and show a barplot for those with odds ratio larger than 2 or smaller than 1/2.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-tests.html#footnotes",
    "href": "linear-models/association-tests.html#footnotes",
    "title": "17  Association tests",
    "section": "",
    "text": "http://www.pnas.org/content/112/40/12349.abstract↩︎\nhttps://en.wikipedia.org/wiki/Ronald_Fisher↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Association tests</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html",
    "href": "linear-models/association-not-causation.html",
    "title": "18  Association is not causation",
    "section": "",
    "text": "18.1 Spurious correlation\nAssociation is not causation is perhaps the most important life lesson one can learn in a statistics class. Correlation is not causation is another way to say this. We have described tools useful for quantifying associations between variables. However, we must be careful not to over-interpret these associations.\nThere are many reasons why two observed variables might be correlated without a direct causal relationship. Below, we outline four common situations that can lead to misinterpreting an observed association.\nThe following comical example underscores the concept that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.\nDoes this mean that margarine consumption causes divorces? Or do divorces cause people to eat more margarine? Of course not. This is just an example of what we call a spurious correlation.\nYou can see many more absurd examples on the Spurious Correlations website1.\nThe cases presented on the website are all instances of what is generally called data dredging, data fishing, or data snooping. It’s basically a form of what in the US they call cherry picking. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.\nA Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:\nlibrary(data.table)\n#&gt; \n#&gt; Attaching package: 'data.table'\n#&gt; The following objects are masked from 'package:lubridate':\n#&gt; \n#&gt;     hour, isoweek, mday, minute, month, quarter, second, wday,\n#&gt;     week, yday, year\n#&gt; The following objects are masked from 'package:dplyr':\n#&gt; \n#&gt;     between, first, last\n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     transpose\nlibrary(ggplot2)\nN &lt;- 25\ng &lt;- 1000000\nsim_data &lt;- data.table(group = rep(1:g, each = N), x = rnorm(N*g), y = rnorm(N*g))\nNote we created groups and for each we generated a pair of independent vectors, \\(X\\) and \\(Y\\), with 25 observations each. Because we constructed the simulation, we know that \\(X\\) and \\(Y\\) are not correlated.\nNext, we compute the correlation between X and Y for each group and look for the max:\nres &lt;- sim_data[, .(r = cor(x, y)), by = group][order(-r)]\nmax(res$r)\n#&gt; [1] 0.784\nWe see a maximum correlation of 0.7840046. If you just plot the data from the group achieving this correlation, it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated:\nsim_data[group == res[which.max(r), group]] |&gt;\n  ggplot(aes(x, y)) +\n  geom_point() + \n  geom_smooth(formula = 'y ~ x', method = \"lm\")\nRemember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:\nres |&gt; ggplot(aes(x = r)) + geom_histogram(binwidth = 0.1, color = \"black\")\nIt’s simply a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.2041705, the largest one will be close to 1.\nIf we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:\nlibrary(broom)\nsim_data[group == res[which.max(r), group], broom::tidy(lm(y ~ x))][2,]\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic    p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 x        0.904     0.149      6.06 0.00000354\nThis practice, known as p-hacking, is widely discussed because it may undermine the reliability of scientific findings. Since journals often favor statistically significant results over null findings, researchers have an incentive to highlight significance. In fields such as epidemiology and the social sciences, for instance, an analyst might explore associations between an outcome and many exposures, but only report the one with a small p-value. Similarly, they might try several model specifications to adjust for confounding and select the one that gives the strongest result. In experimental settings, a study could be repeated multiple times, with only the “successful” run reported. Such practices are not always intentional misconduct; they often stem from limited statistical understanding or wishful thinking. More advanced statistics courses cover methods for adjusting analyses to account for these multiple comparisons.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Association is not causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#outliers",
    "href": "linear-models/association-not-causation.html#outliers",
    "title": "18  Association is not causation",
    "section": "\n18.2 Outliers",
    "text": "18.2 Outliers\nSuppose we measure two independent outcomes, \\(X\\) and \\(Y\\), and standardize each set of measurements. Now imagine we make a mistake and forget to standardize just one value, say, entry 23. We can illustrate this situation by simulating such data as follows:\n\nset.seed(1985)\nx &lt;- rnorm(100, 100, 1)\ny &lt;- rnorm(100, 84, 1)\nx[-23] &lt;- scale(x[-23])\ny[-23] &lt;- scale(y[-23])\n\nThe data look like this:\n\nplot(x, y)\n\n\n\n\n\n\n\nNot surprisingly, the correlation is very high:\n\ncor(x,y)\n#&gt; [1] 0.988\n\nBut this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:\n\ncor(x[-23], y[-23])\n#&gt; [1] -0.0442\n\nThere is an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:\n\nplot(rank(x), rank(y))\n\n\n\n\n\n\n\nThe outlier is no longer associated with a very large value, and the correlation decreases significantly:\n\ncor(rank(x), rank(y))\n#&gt; [1] 0.00251\n\nSpearman correlation can also be calculated like this:\n\ncor(x, y, method = \"spearman\")\n#&gt; [1] 0.00251\n\nIn the Recommended reading section, we include references on estimation techniques that are robust to outliers and applicable to a wide range of situations.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Association is not causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#reversing-cause-and-effect",
    "href": "linear-models/association-not-causation.html#reversing-cause-and-effect",
    "title": "18  Association is not causation",
    "section": "\n18.3 Reversing cause and effect",
    "text": "18.3 Reversing cause and effect\nAnother way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.\nA form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated2. Consider this quote from the article:\n\n\nWhen we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\n\n\nA very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.\nWe can illustrate cause-effect reversal by fitting the regression\n\\[\nX = \\beta_0 + \\beta_1 y + \\varepsilon\n\\]\nto the father–son height data, where \\(X\\) and \\(y\\) represents father and son heights, respectively. Interpreting \\(\\beta_1\\) causally here would be backwards, as it would imply that sons’ heights determine fathers’ heights. Using the previously defined galton_heights dataset, we do in fact obtain a statistically significant slope, demonstrating that statistical significance does not imply causation or the correct direction of effect.\n\nlm(father ~ son, data = galton_heights) |&gt; tidy() |&gt; filter(term == \"son\")\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic       p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 son      0.407    0.0636      6.40 0.00000000136\n\nThe model fits the data very well. However, if we look only at its mathematical formulation, it could easily be misinterpreted as implying that a son’s height causes his father’s height. From our knowledge of genetics and biology, we know the direction of influence is the opposite. The statistical model itself is not at fault, and the estimates and p-values are calculated correctly. What is misleading here is the interpretation, not the model or computation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Association is not causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#confounders",
    "href": "linear-models/association-not-causation.html#confounders",
    "title": "18  Association is not causation",
    "section": "\n18.4 Confounders",
    "text": "18.4 Confounders\nConfounders are perhaps the most common reason that leads to associations begin misinterpreted.\nIf \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if it, or some factor associated with it, is a cause of both \\(X\\) and \\(Y\\), thereby creating a spurious association between them unless we properly account for \\(Z\\).\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.\n\n18.4.1 Example: UC Berkeley admissions\nAdmission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and compute a statistical test, which clearly rejects the hypothesis that gender and admission are independent:\n\ntwo_by_two &lt;- admissions |&gt; group_by(gender) |&gt; \n  summarize(total_admitted = round(sum(admitted / 100 * applicants)), \n            not_admitted = sum(applicants) - sum(total_admitted)) |&gt;\n  select(-gender) \nchisq.test(two_by_two)$p.value\n#&gt; [1] 1.06e-21\n\nBut closer inspection shows a paradoxical result. Here are the percent admissions by major:\n\nadmissions |&gt; select(major, gender, admitted) |&gt;\n  pivot_wider(names_from = \"gender\", values_from = \"admitted\") |&gt;\n  mutate(women_minus_men = women - men)\n#&gt; # A tibble: 6 × 4\n#&gt;   major   men women women_minus_men\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1 A        62    82              20\n#&gt; 2 B        63    68               5\n#&gt; 3 C        37    34              -3\n#&gt; 4 D        33    35               2\n#&gt; 5 E        28    24              -4\n#&gt; # ℹ 1 more row\n\nFour out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.\nThe paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.\nDiscovering confounders, and understanding how they can lead to misleading conclusions, often requires exploratory data analysis and critical thinking. Looking at the table above, we notice substantial variability in admission rates across majors. Could this be influencing the overall results? To investigate, we add a selectivity column, the overall admission rate within each major, to the admissions table.\n\nselectivity &lt;- admissions |&gt;\n  group_by(major) |&gt;\n  summarize(selectivity = sum(admitted*applicants/100)/sum(applicants))\n\nNext, we examine how the number of applicants relates to major selectivity. If selectivity is a confounder, patterns may differ by gender across majors with varying selectivity.\n\nleft_join(admissions, selectivity, by = \"major\") |&gt;\n  ggplot(aes(selectivity, applicants, label = major)) +\n  geom_text() +\n  facet_wrap(~gender) \n\n\n\n\n\n\n\nWe can see the key issue right away: women are submitting far fewer applications to the less selective majors (A and B). So department selectivity is a confounder that influenced both gender (different application patterns) and admission outcome.\n\n18.4.2 Stratifying\nThe first hint that something was off in the interpretation of the overall admission rates came from stratifying by major. Stratifying by a known or potential confounder is a powerful technique for examining the relationship between two other variables. In exploratory data analysis, stratification can help us detect how a confounder might be distorting the analysis and can offer ideas for how to adjust for it.\nHere is an example in which we plot admissions stratified by major and show that women tend to apply to the more selective majors:\n\nadmissions |&gt; ggplot(aes(major, admitted, col = gender, size = applicants)) + \n  geom_point()\n\n\n\n\n\n\n\nWe see that, major by major, there is not much difference in admission rates between genders. However, the large number of men applying to Major B, which admits over 60% of applicants, causes the confusion in the overall comparison. This plot suggests that a more sophisticated analysis adjusting for major is needed. We will learn how to perform such an analysis in the next chapter, in Chapter 19.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Association is not causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#simpsons-paradox",
    "href": "linear-models/association-not-causation.html#simpsons-paradox",
    "title": "18  Association is not causation",
    "section": "\n18.5 Simpson’s paradox",
    "text": "18.5 Simpson’s paradox\nThe case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication to specific strata. As an illustrative example, suppose you have three random variables \\(X\\), \\(Y\\), and \\(Z\\), and we observe realizations of these. Here is a plot of simulated observations for \\(X\\) and \\(Y\\) along with the sample correlation:\n\n\n\n\n\n\n\n\nYou can see that \\(X\\) and \\(Y\\) are negatively correlated. However, once we stratify by \\(Z\\) (shown in different colors below), another pattern emerges:\n\n\n\n\n\n\n\n\nIt is really \\(Z\\) that is negatively correlated with \\(X\\). If we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated, as seen in the plot above.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Association is not causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#recommended-reading",
    "href": "linear-models/association-not-causation.html#recommended-reading",
    "title": "18  Association is not causation",
    "section": "\n18.6 Recommended reading",
    "text": "18.6 Recommended reading\nThe following resources are recommended for students who want to explore more about spurious correlation, robust estimation, and confounding:\n\nTyler Vigen. Spurious Correlations.\nA lighthearted but instructive look at how unrelated variables can appear strongly correlated. Helps develop critical thinking about association versus causation.Website\nHuber, P. J., & Ronchetti, E. M. (2009). Robust Statistics (2nd ed.).\nA comprehensive text on methods that remain reliable when data deviate from model assumptions. Covers theory and practical techniques for robust estimation.\nGreenland, S., Pearl, J., & Robins, J. M. (1999). Causal Diagrams for Epidemiologic Research. Epidemiology, 10(1), 37–48.\nA foundational paper explaining confounding and causal thinking using directed acyclic graphs (DAGs).\nRothman, K. J., Greenland, S., & Lash, T. L. (2021). Modern Epidemiology (4th ed.).\nA thorough reference on confounding, bias, and study design, often used in epidemiology and public health.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Association is not causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#exercises",
    "href": "linear-models/association-not-causation.html#exercises",
    "title": "18  Association is not causation",
    "section": "\n18.7 Exercises",
    "text": "18.7 Exercises\nFor the next set of exercises, we examine the data from a 2014 PNAS paper3 that analyzed success rates from funding agencies in the Netherlands and concluded:\n\nOur results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language used in instructional and evaluation materials.\n\nA response4 was published a few months later titled No evidence that gender contributes to personal research funding success in The Netherlands: A reaction to Van der Lee and Ellemers which concluded:\n\nHowever, the overall gender effect borders on statistical significance, despite the large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines (i.e., with low application success rates for both men and women), then an analysis across all disciplines could incorrectly show “evidence” of gender inequality.\n\nWho is correct here, the original paper or the response? Below, you will examine the data and come to your own conclusion.\n1. The primary evidence for the conclusion of the original paper relies on a comparison of the percentages. Table S1 in the paper includes the information we need:\n\nlibrary(dslabs)\nresearch_funding_rates\n\nConstruct the two-by-two table used for the conclusion about differences in awards by gender.\n2. Compute the difference in percentage from the two-by-two table.\n3. In the previous exercise, we noticed that the success rate is lower for women. But is it significant? Compute a p-value using a Chi-square test.\n4. We see that the p-value is about 0.05. So there appears to be some evidence of an association. But can we infer causation here? Is gender bias causing this observed difference? The response to the original paper claims that what we see here is similar to the UC Berkeley admissions example. Specifically, they state that this “could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines, then an analysis across all disciplines could incorrectly show ‘evidence’ of gender inequality.” To settle this dispute, create a dataset with number of applications, awards, and success rate for each gender. Re-order the disciplines by their overall success rate. Hint: use the reorder function to re-order the disciplines in a first step, then use pivot_longer, separate, and pivot_wider to create the desired table.\n5. To check if this is a case of Simpson’s paradox, plot the success rates versus disciplines, which have been ordered by overall success, with colors to denote the genders and size to denote the number of applications.\n6. We definitely do not see the same level of confounding as in the UC Berkeley example. It is hard to say that there is a clear confounder here. However, we do see that, based on the observed rates, some fields favor men and others favor women. We also see that the two fields with the largest difference favoring men are also the fields with the most applications. But, unlike the UC Berkeley example, women are not more likely to apply for the harder subjects. Is it possible some of the selection committees are biased and others are not?\nTo answer this question we start by checking if any of the differences seen above are statistically significant. Remember that even when there is no bias, we will see differences due to random variability in the review process as well as random variability across candidates. Perform a Chi-square test for each discipline. Hint: define a function that receives the total of a two-by-two table and returns a data frame with the p-value. Use the 0.5 correction. Then use the summarize function.\n7. In the medical sciences, there appears to be a statistically significant difference, but could this be a spurious correlation? We performed 9 tests. Reporting only the one case with a p-value less than 0.05 might be considered an example of cherry picking. Repeat the exercise above, but instead of a p-value, compute a log odds ratio divided by their standard error. Then use qq-plot to see how much these log odds ratios deviate from the normal distribution we would expect: a standard normal distribution.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Association is not causation</span>"
    ]
  },
  {
    "objectID": "linear-models/association-not-causation.html#footnotes",
    "href": "linear-models/association-not-causation.html#footnotes",
    "title": "18  Association is not causation",
    "section": "",
    "text": "http://tylervigen.com/spurious-correlations↩︎\nhttps://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated↩︎\nhttp://www.pnas.org/content/112/40/12349.abstract↩︎\nhttp://www.pnas.org/content/112/51/E7036.extract↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Association is not causation</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html",
    "href": "linear-models/multivariable-regression.html",
    "title": "19  Multivariable Regression",
    "section": "",
    "text": "19.1 Case study: Moneyball\nSince Galton’s original development, regression has become one of the most widely used tools in data analysis. One reason is that an adaptation of the original regression approach, based on linear models, permits us to find relationships between two variables taking into account the effects of other variables that affect both. This has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology.\nWhen we are unable to randomly assign each individual to a treatment or control groups, confounding becomes particularly prevalent. For instance, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in New York City. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Consequently, a naive regression model may lead to an overestimate of the negative health effects of fast food. So, how do we account for confounding in practice? In this chapter, we learn how multivariable regression can help with such situations and can be used to describe how one or more variables affect an outcome variable. We illustrate with a real-world example in which data was used to help pick under appreciated players to improve a resource-limited sports team.\nMoneyball: The Art of Winning an Unfair Game by Michael Lewis focuses on the Oakland Athletics (A’s) baseball team and its general manager, Billy Beane, the person tasked with building the team.\nTraditionally, baseball teams had used scouts to help them decide what players to hire. These scouts evaluate players by observing them perform, tending to favor athletic players with observable physical abilities. For this reason, scouts generally agree on who the best players are and, as a result, these players are often in high demand. This in turn drives up their salaries.\nFrom 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to hire the best players and, during that time, were one of the best teams. However, in 1995, the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball: in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s payroll of $39,679,746. The A’s could no longer afford the most sought-after players. As a result, Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to relying exclusively on scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a significant role in this approach. In this section, we will illustrate how data can be used to support this approach.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#case-study-moneyball",
    "href": "linear-models/multivariable-regression.html#case-study-moneyball",
    "title": "19  Multivariable Regression",
    "section": "",
    "text": "19.1.1 Baseball data\nStatistics have been recorded in baseball since its beginnings. The dataset we will be using, included in the Lahman library, goes back to the 19th century. For example, a summary statistic we will describe soon, the batting average (AVG), has been used for decades to summarize a player’s success. Other statistics1, such as home runs (HR) and runs batted in (RBI) are reported for each player in the game summaries included in the sports section of news outlets, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily chosen without much thought as to whether they actually predicted anything or were related to helping a team win.\nThis changed with Bill James2. In the late 1970s, he started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to determine which outcomes best predicted if a team would win sabermetrics3. Yet until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Today, sabermetrics is no longer confined to baseball; its principles have spread to other sports, where it is now known as sports analytics.\nIn this chapter, we will conduct a data analysis to evaluate whether one of Billy Beane’s key strategies—hiring players who “get on base”, holds up statistically. We will break down what this concept means, and through the lens of multivariable regression, demonstrate that it was not only statistically valid but also an economically effective approach. To understand the analysis, we will need to learn a bit about how baseball works.\n\n19.1.2 Baseball basics\nTo understand how regression helps us find undervalued players, we don’t need to delve into all the details of the game of baseball, which has over 100 rules. Here, we distill the basic knowledge necessary for effectively addressing the data science challenge.\nThe goal of baseball is simple: score more runs (points) than the other team. A player scores a run by starting at a spot called home plate, passing three bases in order (first, second, and third), and returning to home plate. The game begins with the opposing team’s pitcher throwing the ball toward the batter, who stands at home plate and tries to hit the ball. If the batter hits the ball far enough to run around all three bases and back to home plate in one play, this is called a home run4. If the batter doesn’t hit a home run, theystop at one of the bases. From there, they wait for their teammates to hit the ball so they can move to the next base. If the pitcher throws poorly, the batter gets to walk to first base as a penalty for the pitcher, referred to as base on balls (BB). A player on a base can try to run to the next base without waiting for a teammate’s hit. This is called stealing a base5 (SB).\nBatters can also fail to reach base, resulting in an out. Another way to make an out is a failed attempt at stealing a base. Each team continues batting until they accumulate three outs. Once this happens, the other team takes their turn to bat. Each team gets nine turns, called innings, to score runs.\nEach time a batter attempts to reach base, with the goal of eventually scoring a run, it is referred to as a plate appearance (PA). There are, with rare exceptions, five ways a plate appearance can be successful:\n\n\nSingle – The batter reaches first base.\n\nDouble – The batter reaches second base.\n\nTriple – The batter reaches third base.\n\nHome Run (HR) – The batter circles all bases and returns to home plate.\n\nBase on Balls (BB) – The pitcher throws poorly, and the batter is allowed to go to first base as a penalty for the pitcher.\n\nThe first four outcomes (Single, Double, Triple, and HR) are all considered hits, while a BB is not. This distinction is important for understanding the data and analysis that follow.\n\n19.1.3 No awards for base on balls\nHistorically, the batting average has been considered the most important offensive statistic. To define this average, we divide the total number of hits (H) by the total number of at bats (AB) defined as the number of times you either get a hit or make an out; BB are excluded. Today, this success rate ranges from 20% to 38%.\n\n\n\n\n\n\n\n\nOne of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. Instead of batting average, James proposed the use of the on-base percentage (OBP), which he defined as (H+BB)/PA, or simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that accumulates many more BB than the average player might go unrecognized if the batter does not excel in batting average.\nBut is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. We can use data to try and make a case that BB do indeed help produce runs and should be valued.\n\n19.1.4 Usign baseball data\nTo illustrate how data can be used to answer questiosn in Baseball we start with a simple example. We will compare BB to stolen bases (SB). In contrast to BB, total SB were considered important and an award6 given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data to determine if it’s better to pay for players with high BB or high SB?\nOne of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. Although we keep track of the number of runs scored by a player, remember that if player X bats right before someone who hits many HR, batter X will score many runs. Note these runs don’t necessarily happen if we hire player X, but not his HR hitting teammate.\nHowever, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? Let’s examine some data! We start by creating a data frame with statistics from 1962 (the first year all teams played 162 games, like today, instead of 154) to 2001 (the year before the team featured in Money Ball was constructed). We convert the data to a per game rate, because a small proportion of seasons had less games than usual due to strikes, and some teams played extra games due to tie breakers. We also define a singles column for later use.\n\nlibrary(tidyverse)\nlibrary(Lahman)\ndat &lt;- Teams |&gt; filter(yearID %in% 1962:2001) |&gt;\n  mutate(singles = H - X2B - X3B - HR) |&gt;\n  select(teamID, yearID, G, R, SB, singles, BB, HR) |&gt;\n  mutate(across(-c(teamID, yearID), ~ ./G)) |&gt; select(-G)\n\nNow let’s start with a obvious question: do teams that hit more HR score more runs?\n\ndat |&gt; ggplot(aes(HR, R)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nThe plot shows a strong association: teams with more HR tend to score more runs. Now let’s examine the relationship between stolen bases and runs:\n\ndat |&gt; ggplot(aes(SB, R)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nHere the relationship is not as clear.\nFinally, let’s examine the relationship between BB and runs:\n\ndat |&gt; ggplot(aes(BB, R)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nHere again we see a clear association. But does this mean that increasing a team’s BB causes an increase in runs? As we learned in Chapter 18, association is not causation. In fact, it looks like BB and HR are also associated:\n\ndat |&gt; ggplot(aes(HR, BB)) + geom_point(alpha = 0.5)\n\n\n\n\n\n\n\nWe know that HR cause runs because when a player hits a HR, they are guaranteed at least one run. Could it be that HR also cause BB and this makes it appear as if BB cause runs? Are BB cofounded with HR? Linear regression can help us parse out the information and quantify the associations. This, in turn, will aid us in determining what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BB, but keep the HR fixed?\n\n19.1.5 Regression applied to baseball statistics\nCan we use regression with these data? First, notice that the HR and runs data, shown above, appear to be bivariate normal. Specifically, qqplots confirm that the normal approximation for runs for each HR strata is useful here:\n\ndat |&gt; mutate(hr_strata = round(scale(HR))) |&gt;\n  filter(hr_strata %in% -2:3) |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = R)) +\n  facet_wrap(~hr_strata) \n\n\n\n\n\n\n\nSo we are ready to use linear regression to predict the number of runs a team will score, if we know how many HR the team hits using regression:\n\nhr_fit  &lt;- lm(R ~ HR, data = dat)\nsummary(hr_fit)$coef[2,]\n#&gt;   Estimate Std. Error    t value   Pr(&gt;|t|) \n#&gt;   1.86e+00   4.97e-02   3.74e+01  8.90e-193\n\nThe regression line can be plotted using the geom_smooth function:\n\ndat |&gt; ggplot(aes(HR, R)) + geom_point(alpha = 0.5) + geom_smooth(method = \"lm\")\n\nThe slope is 1.86. This tells us that teams that hit 1 more HR per game than the average team, score 1.86 more runs per game than the average team. Given that the most common final score is a difference of one run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because the As were working on a budget, they needed to find some other way to increase wins. In the next section we examine this more carefully.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#confounding",
    "href": "linear-models/multivariable-regression.html#confounding",
    "title": "19  Multivariable Regression",
    "section": "\n19.2 Confounding",
    "text": "19.2 Confounding\nPreviously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from BB, we a get slope of:\n\nbb_slope &lt;- lm(R ~ BB, data = dat)$coef[2]\nbb_slope \n#&gt;    BB \n#&gt; 0.743\n\nDoes this mean that if we go and hire low salary players with many BB, and who increase the number of walks per game by 2, our team will score 1.5 more runs per game? Association is not causation: although the data shows strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game, this does not mean that BB are the cause.\nNote that, if we compute the regression line slope for singles, we get:\n\nlm(R ~ singles, data = dat)$coef[2]\n#&gt; singles \n#&gt;   0.452\n\nwhich is a lower value than what we obtain for BB. Remember that a single gets you to first base just like a BB. Baseball fans will point out that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason is because of confounding.\nHere we show the correlation between HR, BB, and singles:\n\ndat |&gt; select(singles, BB, HR) |&gt; cor()\n#&gt;         singles      BB     HR\n#&gt; singles  1.0000 -0.0495 -0.171\n#&gt; BB      -0.0495  1.0000  0.406\n#&gt; HR      -0.1714  0.4064  1.000\n\nHR and BB are highly correlated! Experts will point out that pitchers are intimidated by players that excel at hitting HR and this leads to the bad performance that awards batters BB. As a result, HR hitters tend to have more BB, and a team with many HR will also have more BB. Although it may appear that BB cause runs, it is actually the HR that cause most of these runs. BB are confounded with HR. Nonetheless, could it be that BB still help? To find out, we somehow have to adjust for the HR effect. Multivariable regression can help with this.\nA first approach is to keep HR fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest tenth. We filter out the strata with few points, to avoid highly variable estimates, and then make a scatterplot for each strata:\n\ndat |&gt; mutate(hr_strata = round(HR, 1)) |&gt; \n  filter(hr_strata &gt;= 0.4 & hr_strata &lt;= 1.2) |&gt;\n  ggplot(aes(BB, R)) +  \n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = \"y~x\", method = \"lm\") +\n  facet_wrap(~hr_strata) \n\n\n\n\n\n\n\nOnce we stratify by HR, these slopes are substantially reduced:\n\ndat |&gt; mutate(hr_strata = round(HR, 1)) |&gt; \n  filter(hr_strata &gt;= 0.5 & hr_strata &lt;= 1.2) |&gt;  \n  group_by(hr_strata) |&gt;\n  summarize(coef = lm(R ~ BB)$coef[2])\n#&gt; # A tibble: 8 × 2\n#&gt;   hr_strata  coef\n#&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1       0.5 0.566\n#&gt; 2       0.6 0.405\n#&gt; 3       0.7 0.284\n#&gt; 4       0.8 0.370\n#&gt; 5       0.9 0.266\n#&gt; # ℹ 3 more rows\n\nRemember that the regression slope for predicting runs with BB was 0.7.\nThe slopes are reduced, but they are not 0, which indicates that BB are helpful for producing runs, just not as much as suggested by the single variable analysis. In fact, the values above are closer to the slope we obtained from singles, 0.5, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.\nAlthough our understanding of the application tells us that HR cause BB, but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BB. In this case, the slopes do not change much from the original:\n\ndat |&gt; mutate(bb_strata = round(BB, 1)) |&gt; \n  filter(bb_strata &gt;= 2.5 & bb_strata &lt;= 3.2) |&gt;  \n  group_by(bb_strata) |&gt;\n  summarize(coef = lm(R ~ HR)$coef[2])\n#&gt; # A tibble: 8 × 2\n#&gt;   bb_strata  coef\n#&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1       2.5  1.98\n#&gt; 2       2.6  1.07\n#&gt; 3       2.7  1.61\n#&gt; 4       2.8  1.50\n#&gt; 5       2.9  1.57\n#&gt; # ℹ 3 more rows\n\nThey are reduced slightly from 1.86, which is consistent with the fact that BB do in fact cause some runs.\nRegardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#sec-regression-in-r",
    "href": "linear-models/multivariable-regression.html#sec-regression-in-r",
    "title": "19  Multivariable Regression",
    "section": "\n19.3 Multivariable regression",
    "text": "19.3 Multivariable regression\nIt is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:\n\\[\n\\mathrm{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2\n\\]\nwith the slopes for \\(x_1\\) changing for different values of \\(x_2\\) and the other way around. But is there an easier approach?\nIf we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that \\(\\beta_1(x_2)\\) and \\(\\beta_2(x_1)\\) are constants. This, in turn, implies that the expectation of runs conditioned on HR and BB can be written as follows:\n\\[\n\\mathrm{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nThis model suggests that, if the number of HR is fixed at \\(x_2\\), we observe a linear relationship between runs and BB with an intercept of \\(\\beta_0 + \\beta_2 x_2\\). Our exploratory data analysis suggested that this is the case. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by \\(\\beta_1\\). In this analysis, referred to as multivariable regression, you will often hear people say that the BB slope \\(\\beta_1\\) is adjusted for the HR effect.\nBecause the data is approximately normal and conditional distributions were also normal, we are justified in using a linear model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i\n\\]\nwith \\(Y_i\\) runs per game for team \\(i\\), \\(x_{i,1}\\) BB per game, \\(x_{i,2}\\) the HR per game, and \\(\\varepsilon_i\\) assumed to be independent and identically distributed.\nTo use lm here, we need to let the function know we have two predictor variables. We use the + symbol as follows:\n\nfit &lt;- lm(R ~ BB + HR, data = dat)\nsummary(fit)$coef[2:3,]\n#&gt;    Estimate Std. Error t value  Pr(&gt;|t|)\n#&gt; BB    0.391     0.0273    14.3  1.55e-42\n#&gt; HR    1.570     0.0495    31.7 2.39e-153\n\nWhen we fit the model with only one variable, the estimated slopes were 0.39 and 0.39 for BB and HR, respectively. Note that when fitting the multivariable model both go down, with the BB effect decreasing much more.\n\n\n\n\n\n\nYou are ready to do exercises 1-12, if you want to practice before continuing.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#baseball-analytics",
    "href": "linear-models/multivariable-regression.html#baseball-analytics",
    "title": "19  Multivariable Regression",
    "section": "\n19.4 Baseball analytics",
    "text": "19.4 Baseball analytics\nIn the previous section, we used multivariable regression to confirm that walks (BB) are a significant predictor of runs scored. In this section, we will take a data-driven approach to develop a metric for ranking the offensive production of baseball players. Specifically, we will construct a regression model to predict the number of runs a player contributes based on their offensive statistics. By incorporating salary information into this analysis, we can identify players in 2002 who were projected to generate runs but were under-compensated. Importantly, this analysis uses data that excludes the 2002 season to mimic the challenge of building a team for the upcoming season.\nAt the individual player level, distinguishing between runs scored and runs produced is crucial for accurately assessing offensive contributions. While runs scored are directly recorded, they don’t fully capture the extent of a player’s impact. For instance, if player X hits a single and later scores on a teammate’s home run, the run is attributed to player X, but the teammate’s effort was essential in making it happen. This overlap highlights the shared nature of offensive production, complicating individual performance analysis.\nTo address this, we will first fit the model at the team level, where such individual-level nuances average out and do not affect overall results. Once the model is validated for teams, we will apply it to estimate contributions at the player level.\n\n\n\n\n\n\nTeams are divided into two leagues, American and National. Since they had slightly different rules during the period in question, we will fit the model only to the American League (AL), where the Oakland A’s played.\n\n\n\nSince teams accumulate significantly more plate appearances than individual players, we will model per plate appearance (PA) rates. This allows us to generalize the model fit to teams and apply it to players. Additionally, since triples are relatively rare, we will combine them with doubles into a single category, extra bases (XB), to simplify the model. Stolen bases will be excluded because prior analyses suggest they do not reliably correlate with increased scoring. As a result, the predictors in the model will include: base on balls (BB), singles, extra bases (XB) and home runs (HR). The data preparation step for this model is shown below:\n\ndat &lt;- Teams |&gt; \n  filter(yearID %in% 1962:2002 & lgID == \"AL\") |&gt;\n  mutate(XB = X2B + X3B, singles = H - XB - HR, PA = AB + BB) |&gt;\n  select(yearID, teamID, R, BB, singles, XB, HR, PA) |&gt;\n  mutate(across(-c(yearID, teamID, PA), ~ ./PA)) \n\nTo build the model, we make a reasonable assumption that our outcome variable (runs per plate appearance) and the four predictor variables (BB, singles, extra bases, and home runs) are jointly normal. This implies that for any one predictor, the relationship with the outcome is linear when the other predictors are held constant. Under this assumption, the linear regression model can be expressed as:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\varepsilon_i\n\\]\nwhere \\(Y_i\\) represents runs produced per plate appearance, and \\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}\\) represent BB, singles, extra bases, and HR per plate appearance, respectively.\nWe can fit the model to data before the 2002 season, since we are building a team for 2002 with the information we had before the season started:\n\nfit &lt;- dat |&gt; filter(yearID &lt; 2002) |&gt; lm(R ~ BB + singles + XB + HR, data = _)\n\nWe can use the fitted model to predict run production for each team:\n\nR_hat &lt;- dat |&gt; filter(yearID == 2002) |&gt; predict(fit, newdata = _)\n\nNote that this model, fit to 1962 to 2001 data, predicts runs for 2002 very well:\n\n\n\n\n\n\n\n\nWe also observe that Oakland, despite being one of the lowest-spending teams, managed to perform as an above-average offensive team. Additionally, note that the fitted model assigns similar weight to BB and singles:\n\nfit$coefficients[-1]\n#&gt;      BB singles      XB      HR \n#&gt;   0.454   0.586   0.914   1.452\n\nNow let’s apply the model to players. The Batting data frame includes player specific statistics. We prepare the data so that we save the same per-PA rates the model was fit to. Because players abilities change through time we only use data close to 2002 but to improve the precision of our estimates we use data from the three years before 2002, rather than just 2001. We also remove players with less than 100 PA to avoid imprecise summaries:\n\nplayers &lt;- Batting |&gt; \n  filter(yearID %in% 1999:2001) |&gt; \n  group_by(playerID) |&gt;\n  summarize(XB = sum(X2B + X3B), PA = sum(AB + BB), HR = sum(HR), H = sum(H), \n            singles = H - XB - HR, BB = sum(BB), AVG = sum(H)/sum(AB)) |&gt;\n  filter(PA &gt;= 100) |&gt;\n  mutate(across(-c(playerID, PA, AVG), ~ ./PA)) \n\nNow that statistics are in per-PA rates, we can use the model fitted to team-level data to predict how many runs per plate appearce each player will produce:\n\nplayers$R_hat &lt;- predict(fit, players)\n\nNext we use the Salary, People,Apperances data frames to add information we need for the rest of the analysis.\nWe start by adding the salary each player garnered in 2002 and remove players that did not play that year:\n\nplayers &lt;-  Salaries |&gt; \n  filter(yearID == 2002) |&gt; \n  select(playerID, salary) |&gt;\n  right_join(players, by = \"playerID\") |&gt;\n  filter(!is.na(salary))\n\nWe then add their first and last names for context. Additionally, because Major League Baseball players cannot negotiate their contracts or choose the team they play for until they have accumulated six years of playing time and become free agents, we include their debut year as we need to take this into consideration in our analysis.\n\nplayers &lt;- People |&gt; \n  select(playerID, nameFirst, nameLast, debut) |&gt;\n  mutate(debut = year(as.Date(debut))) |&gt;\n  right_join(players, by = \"playerID\")\n\nFinally, we remove pitchers since we are only interested in batters.\n\nplayers &lt;- Appearances |&gt; filter(yearID == 2002) |&gt; \n  group_by(playerID) |&gt;\n  summarize(G_p = sum(G_p)) |&gt;\n  right_join(players, by = \"playerID\") |&gt;\n  filter(G_p == 0) |&gt; select(-G_p)\n\nIf you followed baseball during the era in question you will recognize the top run producers and not be surprised that garnered high salaries:\n\nplayers |&gt; select(nameFirst, nameLast, R_hat, salary) |&gt; \n  arrange(desc(R_hat)) |&gt; head()\n#&gt; # A tibble: 6 × 4\n#&gt;   nameFirst nameLast R_hat   salary\n#&gt;   &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1 Barry     Bonds    0.239 15000000\n#&gt; 2 Todd      Helton   0.218  5000000\n#&gt; 3 Manny     Ramirez  0.216 15462727\n#&gt; 4 Jason     Giambi   0.215 10428571\n#&gt; 5 Larry     Walker   0.213 12666667\n#&gt; # ℹ 1 more row\n\nNote that because each team has, on average, 37.5 plate appearances a game, a rate of 0.24 runs per plate appearance translates to 9 runs a game, almost double the league average!\nIf we plot predicted runs produced versus salary we can see that there is substantial variation across players in predicted runs produced and that, not surprisingly, players that produce runs garner higher salaries.\n\n\n\n\n\n\n\n\nThe plot also highlights four players. Before the 2002 season Oakland lost their best run producer, Jason Giambi, because they could not compete with the New York Yankees offer of over 10 million salary. Johnny Damon, another above average run producers (the dashed line shows the average) also left as Oakland was not willing to pay the 7.25 million salary the Boston Red Sox offered him. Oakland had to make up for this run production, but with a very limited budget. To do this, their first addition was Scott Hatterberg, whose salary was only $900,000, one of the lowest in the league, yet was predicted to produce more than average runs. Note that among the players in our data frame, Hatterberg while having a below league average AVG, ranked near the top 10% in BB per plate appearance. Similarly, David Justice had a near league average AVG, but ranked near the top 5% in terms of BB per plate appearance. These savings permitted the As to upgrade one of their current players, Frank Menechino, for Ray Durham.\n\nfilter(players, playerID %in% c(\"damonjo01\", \"giambja01\", \"menecfr01\")) |&gt; \n  bind_rows(filter(players, playerID %in% c(\"justida01\", \"durhara01\", \"hattesc01\"))) |&gt; \n  select(nameFirst, nameLast, AVG, BB,  R_hat) \n#&gt; # A tibble: 6 × 5\n#&gt;   nameFirst nameLast    AVG     BB R_hat\n#&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Johnny    Damon     0.296 0.0930 0.141\n#&gt; 2 Jason     Giambi    0.330 0.188  0.215\n#&gt; 3 Frank     Menechino 0.245 0.137  0.126\n#&gt; 4 Ray       Durham    0.281 0.103  0.141\n#&gt; 5 Scott     Hatteberg 0.257 0.131  0.128\n#&gt; # ℹ 1 more row\n\nDespite losing two star players, the A’s predicted runs per game (R_hat) dropped only slightly, from 0.482 to 0.431, and the savings allowed them to acquire pitcher Billy Koch, who helped reduce opponents’ scoring.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#exercises",
    "href": "linear-models/multivariable-regression.html#exercises",
    "title": "19  Multivariable Regression",
    "section": "\n19.5 Exercises",
    "text": "19.5 Exercises\nWe have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing their stability across the years. Since we have to pick players based on their previous performances, we prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BB.\n1. Before we begin, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2002 table, keeping only players with more than 100 plate appearances:\n\nlibrary(Lahman)\ndat &lt;- Batting |&gt; filter(yearID == 2002) |&gt;\n  mutate(pa = AB + BB, \n         singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) |&gt;\n  filter(pa &gt;= 100) |&gt;\n  select(playerID, singles, bb)\n\nNow, compute a similar table called avg, but with rates computed over 1999-2001.\n2. You can use the inner_join function to combine the 2002 data and averages in the same table:\n\ndat &lt;- inner_join(dat, avg, by = \"playerID\")\n\nCompute the correlation between 2002 and the previous seasons for singles and BB.\n3. Note that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate.\n4. Now fit a linear model for each metric and use the confint function to compare the estimates.\n5. In a previous section, we computed the correlation between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons. We noticed that the highest correlation is between fathers and sons and the lowest is between mothers and sons. We can compute these correlations using:\n\nlibrary(HistData)\nset.seed(1)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  group_by(family, gender) |&gt;\n  sample_n(1) |&gt;\n  ungroup()\n\ncors &lt;- galton_heights |&gt; \n  pivot_longer(father:mother, names_to = \"parent\", values_to = \"parentHeight\") |&gt;\n  mutate(child = ifelse(gender == \"female\", \"daughter\", \"son\")) |&gt;\n  unite(pair, c(\"parent\", \"child\")) |&gt; \n  group_by(pair) |&gt;\n  summarize(cor = cor(parentHeight, childHeight))\n\nAre these differences statistically significant? To answer this, we will compute the slopes of the regression lines along with their standard errors. Start by using lm and the broom package to compute the slopes LSE and the standard errors.\n6. Repeat the exercise above, but compute a confidence interval as well.\n7. Plot the confidence intervals and notice that they overlap, which implies that the data is consistent with the inheritance of height being independent of sex.\n8. Because we are selecting children at random, we can actually do something like a permutation test here. Repeat the computation of correlations 100 times taking a different sample each time. Hint: Use similar code to what we used with simulations.\n9. Fit a linear regression model to obtain the effects of BB and HR on Runs (at the team level) in 1971. Use the tidy function in the broom package to obtain the results in a data frame.\n10. Now let’s repeat the above for each year since 1962 and make a plot. Use summarize and the broom package to fit this model for every year since 1962.\n11. Use the results of the previous exercise to plot the estimated effects of BB on runs.\n12. Advanced. Write a function that takes R, HR, and BB as arguments and fits two linear models: R ~ BB and R~BB+HR. Then use the summary function to obtain the BB for both models for each year since 1962. Then plot these against each other as a function of time.\n13. Since the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:\n\\[\n\\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{\\mbox{Singles} + 2 \\mbox{Doubles} + 3 \\mbox{Triples} + 4\\mbox{HR}}{\\mbox{AB}}\n\\]\nThey called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we demonstrate how this metric closely aligns with regression results.\nCompute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.\n14. For every year since 1962, compute the correlation between runs per game and OPS. Then plot these correlations as a function of year.\n15. Keep in mind that we can rewrite OPS as a weighted average of BB, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: The weight for BB relative to singles will be a function of AB and PA.\n16. Consider that the weight for BB, \\(\\frac{\\mbox{AB}}{\\mbox{PA}}\\), will change from team to team. To assess its variability, compute and plot this quantity for each team for each year since 1962. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.\n17. So now we know that the formula for OPS is proportional to \\(0.91 \\times \\mbox{BB} + \\mbox{singles} + 2 \\times \\mbox{doubles} + 3 \\times \\mbox{triples} + 4 \\times \\mbox{HR}\\). Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1962, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.\n18. We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1962, compute the OPS, the predicted runs with the regression model, and compute the correlation between the two, as well as the correlation with runs per game.\n19. We see that using the regression approach predicts runs slightly better than OPS, but not that much. However, note that we have been computing OPS and predicting runs for teams when these measures are used to evaluate players. Let’s show that OPS is quite similar to what one obtains with regression at the player level. For the 1962 season and onward, compute the OPS and the predicted runs from our model for each player, and plot them. Use the PA per game correction we used in the previous chapter:\n20. Which players have shown the largest difference between their rank by predicted runs and OPS?",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "linear-models/multivariable-regression.html#footnotes",
    "href": "linear-models/multivariable-regression.html#footnotes",
    "title": "19  Multivariable Regression",
    "section": "",
    "text": "https://www.mlb.com/stats/↩︎\nhttps://en.wikipedia.org/wiki/Bill_James↩︎\nhttps://en.wikipedia.org/wiki/Sabermetrics↩︎\nhttps://www.youtube.com/watch?v=JSE5kfxkzfk↩︎\nhttps://www.youtube.com/watch?v=JSE5kfxkzfk↩︎\nhttp://www.baseball-almanac.com/awards/lou_brock_award.shtml↩︎",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Multivariable Regression</span>"
    ]
  },
  {
    "objectID": "highdim/intro-highdim.html",
    "href": "highdim/intro-highdim.html",
    "title": "High dimensional data",
    "section": "",
    "text": "High-dimensional datasets are increasingly common in modern data analysis, especially in fields like genomics, image processing, natural language processing, and recommender systems. There is a variety of computational techniques and statistical concepts that are useful for analyzing datasets in which each observation is associated with a large number of numerical variables. In this part of the book, we introduce ideas that are useful in the analysis of these high-dimensional datasets. Specifically, we provide brief introductions to linear algebra, dimension reduction, matrix factorization, and regularization. As motivating examples, we use handwritten digit recognition and movie recommendation systems, both of which involve high-dimensional datasets with hundreds or thousands of variables per observation. We start this part of the book by demonstrating how to work with matrices in R.\nOne specific task we use to motivate linear algebra is measuring the similarity between two handwritten digits. Because each digit is represented by \\(28 \\times 28 = 784\\) pixel values, we cannot simply subtract two vectors as we would in a one-dimensional setting. Instead, we treat each observation as a point in a high-dimensional space and use a mathematical definition of distance to quantify similarity. Many machine learning techniques introduced later in the book rely on this geometric interpretation.\nWe also use this high-dimensional concept of distance to motivate dimension reduction, a set of techniques that summarize high-dimensional data in lower-dimensional representations that are easier to visualize and analyze, while preserving the essential information. Distance between observations provides a concrete example: we aim to reduce the number of variables while preserving the pairwise distances between observations as much as possible. This leads naturally to matrix factorization methods, which arise from the mathematical structure underlying these techniques.\nFinally, we introduce the concept of regularization, which is useful when analyzing high-dimensional data. In many applications, the large number of variables increases the risk of overfitting or cherry-picking results that appear significant by chance. Regularization provides a mathematically principled way to constrain models, improve generalization, and avoid misleading conclusions.\nTogether, these topics lay the groundwork for understanding and implementing many of the machine learning techniques we cover in the next part of the book.",
    "crumbs": [
      "High dimensional data"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html",
    "href": "highdim/matrices-in-R.html",
    "title": "\n20  Matrices in R\n",
    "section": "",
    "text": "20.1 Notation\nWhen the number of variables associated with each observation is large and they can all be represented as numbers, it is often more convenient to store the data in a matrix and perform the analysis using linear algebra operations, rather than storing the data in a data frame and using tidyverse or data.table functions. Matrix operations form the computational foundation for many applications of linear algebra in data analysis.\nIn fact, many of the most widely used machine learning algorithms—including linear regression, principal component analysis, neural networks, and deep learning—are built around linear algebra concepts, and their implementations rely heavily on efficient matrix operations. Becoming fluent in creating, manipulating, and interpreting matrices in R will make it easier to understand and implement these methods in practice.\nAlthough we introduce the mathematical framework of linear algebra in the next chapter, the examples we use there rely on being able to work with matrices in R. Therefore, before diving into the mathematical concepts, we begin here with the tools needed to create and operate on matrices in R.\nIn Section 20.11, at the end of the chapter, we present motivating examples and specific practical tasks that commonly arise in machine learning workflows, such as centering and scaling variables, computing distances, and applying linear transformations. These tasks can be completed efficiently using matrix operations, and the skills you develop in the earlier sections of the chapter will help you solve them. If you would like to understand the motivation for learning the matrix operations introduced in the previous sections, you may find it helpful to read that section first.\nA matrix is a two-dimensional object defined by its number of rows and columns. In data analysis, it is common to organize data so that each row represents an observation and each column represents a variable measured on those observations. This structure allows us to perform computations across observations or variables using matrix operations, which are both efficient and conceptually aligned with the linear algebra techniques introduced in the next chapters.\nIn mathematical notation, matrices are usually represented with bold uppercase letters:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\nx_{1,1}&x_{1,2}&\\dots & x_{1,p}\\\\\nx_{2,1}&x_{2,2}&\\dots & x_{2,p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n,1}&x_{n,2}&\\dots&x_{n,p}\\\\\n\\end{bmatrix}\n\\]\nwith \\(x_{i,j}\\) representing the \\(j\\)-th variable for the \\(i\\)-th observation. The matrix is said to have dimensions \\(n \\times p\\), meaning it has \\(n\\) rows and \\(p\\) columns.\nWe denote vectors with lower case bold letters and represent them as one column matrices, often referred to as column vectors. R follows this convention when converting a vector to a matrix.\nHowever, column vectors should not be confused with the columns of the matrix. They have this name simply because they have one column.\nMathematical descriptions of machine learning often make reference to vectors representing the \\(p\\) variables:\n\\[\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1\\\\\\\nx_2\\\\\\\n\\vdots\\\\\\\nx_p\n\\end{bmatrix}\n\\]\nTo distinguish between variables associated with the observations \\(i=1,\\dots,n\\), we add an index:\n\\[\n\\mathbf{x}_i = \\begin{bmatrix}\nx_{i,1}\\\\\nx_{i,2}\\\\\n\\vdots\\\\\nx_{i,p}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-matrix-notation",
    "href": "highdim/matrices-in-R.html#sec-matrix-notation",
    "title": "\n20  Matrices in R\n",
    "section": "",
    "text": "In machine learning, variables are often called features, while in statistics, they are often called covariates. Regardless of the terminology, when working with matrices, these are typically represented by the columns of the matrix, each column corresponds to one variable measured across all observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBold lower case letters are also commonly used to represent matrix columns rather than rows. This can be confusing because \\(\\mathbf{x}_1\\) can represent either the first row or the first column of \\(\\mathbf{X}\\). One way to distinguish is to use notation similar to computer code: using the colon \\(:\\) to represent all. So \\(\\mathbf{X}_{1,:}\\) represents the first row and \\(\\mathbf{X}_{:,1}\\) is the first column. Another approach is to distinguish by the letter used to index, with \\(i\\) used for rows and \\(j\\) used for columns. So \\(\\mathbf{x}_i\\) is the \\(i\\)th row and \\(\\mathbf{x}_j\\) is the \\(j\\)th column. With this approach, it is important to clarify which dimension, row or column is being represented. Further confusion can arise because, as aforementioned, it is common to represent all vectors, including the rows of a matrix, as one-column matrices.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-mnist",
    "href": "highdim/matrices-in-R.html#sec-mnist",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.2 Case study: MNIST",
    "text": "20.2 Case study: MNIST\nThe first step in handling mail received in the post office is to sort letters by zip code:\n\nIn the Machine Learning part of this book, we will describe how we can build computer algorithms to read handwritten digits, which robots then use to sort the letters. To do this, we first need to collect data, which in this case is a high-dimensional dataset and best stored in a matrix.\nThe MNIST dataset was generated by digitizing thousands of handwritten digits, already read and annotated by humans1. Below are three images of written digits.\n\n\n\n\n\n\n\n\nThe images are converted into \\(28 \\times 28 = 784\\) pixels and, for each pixel, we obtain a grey scale intensity between 0 (white) and 255 (black). The following plot shows the individual variables for each image:\n\n\n\n\n\n\n\n\nFor each digitized image, indexed by \\(i\\), we are provided with 784 variables and a categorical outcome, or label, representing the digit among \\(0, 1, 2, 3, 4, 5, 6, 7 , 8,\\) and \\(9\\) that the image is representing. Let’s load the data using the dslabs package:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nmnist &lt;- read_mnist()\n\nIn these cases, the pixel intensities are saved in a matrix:\n\nclass(mnist$train$images)\n#&gt; [1] \"matrix\" \"array\"\n\nThe labels associated with each image are included in a vector:\n\ntable(mnist$train$labels)\n#&gt; \n#&gt;    0    1    2    3    4    5    6    7    8    9 \n#&gt; 5923 6742 5958 6131 5842 5421 5918 6265 5851 5949\n\nTo simplify the code below, we will rename these x and y respectively:\n\nx &lt;- mnist$train$images\ny &lt;- mnist$train$labels\n\nBefore we define and work through the tasks designed to teach matrix operations, we begin by reviewing some basic matrix functionality in R.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-creating-a-matrix",
    "href": "highdim/matrices-in-R.html#sec-creating-a-matrix",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.3 Creating a matrix",
    "text": "20.3 Creating a matrix\nScalars, vectors, and matrices are the basic building blocks of linear algebra. We have already encountered vectors in earlier chapters, and in R, scalars are represented as vectors of length 1. We now extend this understanding to matrices, starting with how to create them in R.\nWe can create a matrix using the matrix function. The first argument is a vector containing the elements that will fill up the matrix. The second and third arguments determine the number of row and columns, respectively. So a typical way to create a matrix is to first obtain a vector of numbers containing the elements of the matrix and feeding it to the matrix function. For example, to create a \\(100 \\times 2\\) matrix of normally distributed random variables, we write:\n\nmat &lt;- matrix(rnorm(100*2), 100, 2)\n\nNote that by default the matrix is filled in column by column:\n\nmatrix(1:15, 3, 5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    4    7   10   13\n#&gt; [2,]    2    5    8   11   14\n#&gt; [3,]    3    6    9   12   15\n\nTo fill the matrix row by row, we can use the byrow argument:\n\nmatrix(1:15, 3, 5, byrow = TRUE)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    2    3    4    5\n#&gt; [2,]    6    7    8    9   10\n#&gt; [3,]   11   12   13   14   15\n\nThe function as.vector converts a matrix back into a vector:\n\nas.vector(matrix(1:15, 3, 5))\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\n\n\n\n\n\nIf the product of columns and rows does not match the length of the vector provided in the first argument, matrix recycles values. If the length of the vector is a sub-multiple or multiple of the number of rows, this happens without warning:\n\nmatrix(1:3, 3, 5)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    1    1    1    1\n#&gt; [2,]    2    2    2    2    2\n#&gt; [3,]    3    3    3    3    3\n\n\n\n\nThe function as.matrix() attempts to coerce its input into a matrix:\n\ndf &lt;- data.frame(a = 1:2, b = 3:4, c = 5:6)\nas.matrix(df)\n#&gt;      a b c\n#&gt; [1,] 1 3 5\n#&gt; [2,] 2 4 6",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#dimensions-of-a-matrix",
    "href": "highdim/matrices-in-R.html#dimensions-of-a-matrix",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.4 Dimensions of a matrix",
    "text": "20.4 Dimensions of a matrix\nThe dimension of a matrix is an important characteristic needed to assure that certain linear algebra operations can be performed. The dimension is a two-number summary defined as the number of rows \\(\\times\\) the number of columns.\nThe nrow function tells us how many rows that matrix has. Here is the number of rows x previously defined to store the MNIST training data:\n\nnrow(x)\n#&gt; [1] 60000\n\nThe function ncol tells us how many columns:\n\nncol(x)\n#&gt; [1] 784\n\nWe learn that our dataset contains 60,000 observations (images) and 784 variables (pixels).\nThe dim function returns the rows and columns:\n\ndim(x)\n#&gt; [1] 60000   784\n\nNow we can confirm that R follows the convention of defining vectors of length \\(n\\) as \\(n\\times 1\\) matrices or column vectors:\n\nvec &lt;- 1:10\ndim(matrix(vec))\n#&gt; [1] 10  1",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-matrix-subsetting",
    "href": "highdim/matrices-in-R.html#sec-matrix-subsetting",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.5 Subsetting",
    "text": "20.5 Subsetting\nTo extract a specific entry from a matrix, for example the 300th row of the 100th column, we write:\n\nx[300, 100]\n\nWe can extract subsets of the matrices by using vectors of indexes. For example, we can extract the first 100 pixels from the first 300 observations like this:\n\nx[1:300, 1:100]\n\nTo extract an entire row or subset of rows, we leave the column dimension blank. So the following code returns all the pixels for the first 300 observations:\n\nx[1:300,]\n\nSimilarly, we can subset any number of columns by keeping the first dimension blank. Here is the code to extract the first 100 pixels:\n\nx[,1:100]\n\n\n\n\n\n\n\nIf we subset just one row or just one column, the resulting object is no longer a matrix. For example notice what happens here:\n\ndim(x[300,])\n#&gt; NULL\n\nTo avoid this, we can use the drop argument:\n\ndim(x[100,,drop = FALSE])\n#&gt; [1]   1 784",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-transpose",
    "href": "highdim/matrices-in-R.html#sec-transpose",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.6 The transpose",
    "text": "20.6 The transpose\nA common operation when working with matrices is the transpose. We use the transpose to understand several concepts described in the next several sections. This operation simply converts the rows of a matrix into columns. We use the symbols \\(\\top\\) or \\('\\) next to the bold upper case letter to denote the transpose:\n\\[\n\\text{if } \\,\n\\mathbf{X} =\n\\begin{bmatrix}\n  x_{1,1}&\\dots & x_{1,p} \\\\\n  x_{2,1}&\\dots & x_{2,p} \\\\\n  \\vdots & \\ddots & \\vdots & \\\\\n  x_{n,1}&\\dots & x_{n,p}\n  \\end{bmatrix} \\text{ then }\\,\n\\mathbf{X}^\\top =\n\\begin{bmatrix}\n  x_{1,1}&x_{2,1}&\\dots & x_{n,1} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  x_{1,p}&x_{2,p}&\\dots & x_{n,p}\n  \\end{bmatrix}\n\\]\nIn R we compute the transpose using the function t\n\ndim(x)\n#&gt; [1] 60000   784\ndim(t(x))\n#&gt; [1]   784 60000\n\nOne use of the transpose is that we can write the matrix \\(\\mathbf{X}\\) as rows of the column vectors representing the variables for each individual observation in the following way:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n\\mathbf{x}_1^\\top\\\\\n\\mathbf{x}_2^\\top\\\\\n\\vdots\\\\\n\\mathbf{x}_n^\\top\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-row-col-summaries",
    "href": "highdim/matrices-in-R.html#sec-row-col-summaries",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.7 Row and column summaries",
    "text": "20.7 Row and column summaries\nA common operation with matrices is to apply the same function to each row or to each column. For example, we may want to compute row averages and standard deviations. The apply function lets you do this. The first argument is the matrix, the second is the dimension, 1 for rows, 2 for columns, and the third is the function to be applied.\nSo, for example, to compute the averages and standard deviations of each row, we write:\n\navgs &lt;- apply(x, 1, mean)\nsds &lt;- apply(x, 1, sd)\n\nTo compute these for the columns, we simply change the 1 to a 2:\n\navgs &lt;- apply(x, 2, mean)\nsds &lt;- apply(x, 2, sd)\n\nBecause these operations are so common, special functions are available to perform them. So, for example, the functions rowMeans computes the average of each row:\n\navg &lt;- rowMeans(x)\n\nand the function rowSds from the matrixStats packages computes the standard deviations for each row:\n\nlibrary(matrixStats)\nsds &lt;- rowSds(x)\n\nThe functions colMeans and colSds provide the version for columns.\nFor other fast implementations of common operation take a look at what is availabe in the matrixStats package.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-conditional-filtering",
    "href": "highdim/matrices-in-R.html#sec-conditional-filtering",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.8 Conditional filtering",
    "text": "20.8 Conditional filtering\nOne of the advantages of matrices operations over tidyverse operations is that we can easily select columns based on summaries of the columns.\nNote that logical filters can be used to subset matrices in a similar way in which they can be used to subset vectors. Here is a simple example subsetting columns with logicals:\n\nmatrix(1:15, 3, 5)[,c(FALSE, TRUE, TRUE, FALSE, TRUE)]\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    4    7   13\n#&gt; [2,]    5    8   14\n#&gt; [3,]    6    9   15\n\nThis implies that we can select rows with conditional expression. In the following example we remove all observations containing at least one NA:\n\nx[apply(!is.na(x), 1, all),]\n\nThis being a common operation, we have a matrixStats function to do it faster:\n\nx[!rowAnyNAs(x),]",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-indexing-matrices",
    "href": "highdim/matrices-in-R.html#sec-indexing-matrices",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.9 Indexing with matrices",
    "text": "20.9 Indexing with matrices\nAn operation that facilitates efficient coding is that we can change entries of a matrix based on conditionals applied to that same matrix. Here is a simple example:\n\nmat &lt;- matrix(1:15, 3, 5)\nmat[mat &gt; 6 & mat &lt; 12] &lt;- 0\nmat\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    4    0    0   13\n#&gt; [2,]    2    5    0    0   14\n#&gt; [3,]    3    6    0   12   15\n\nA useful application of this approach is that we can change all the NA entries of a matrix to something else:\n\nx[is.na(x)] &lt;- 0",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-vectorization-for-matrices",
    "href": "highdim/matrices-in-R.html#sec-vectorization-for-matrices",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.10 Vectorization for matrices",
    "text": "20.10 Vectorization for matrices\nIn R, if we subtract a vector from a matrix, the first element of the vector is subtracted from the first row, the second element from the second row, and so on. Using mathematical notation, we would write it as follows:\n\\[\n\\begin{bmatrix}\n  X_{1,1}&\\dots & X_{1,p} \\\\\n  X_{2,1}&\\dots & X_{2,p} \\\\\n   & \\vdots & \\\\\n  X_{n,1}&\\dots & X_{n,p}\n  \\end{bmatrix}\n-\n\\begin{bmatrix}\na_1\\\\\\\na_2\\\\\\\n\\vdots\\\\\\\na_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  X_{1,1}-a_1&\\dots & X_{1,p} -a_1\\\\\n  X_{2,1}-a_2&\\dots & X_{2,p} -a_2\\\\\n   & \\vdots & \\\\\n  X_{n,1}-a_n&\\dots & X_{n,p} -a_n\n  \\end{bmatrix}\n\\]\nThe same holds true for other arithmetic operations.\nThe function sweep facilitates this type of operation. It works similarly to apply. It takes each entry of a vector and applies an arithmetic operation to the corresponding row. Subtraction is the default arithmetic operation. So, for example, to center each row around the average, we can use:\n\nsweep(x, 1, rowMeans(x))\n\nIn R, if you add, subtract, multiple or divide two matrices, the operation is done elementwise. For example, if two matrices are stored in x and y, then:\n\nx*y\n\ndoes not result in matrix multiplication. Instead, the entry in row \\(i\\) and column \\(j\\) of this product is the product of the entry in row \\(i\\) and column \\(j\\) of x and y, respectively.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#sec-motivating-tasks",
    "href": "highdim/matrices-in-R.html#sec-motivating-tasks",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.11 Motivating tasks",
    "text": "20.11 Motivating tasks\nTo motivate the use of matrices in R, we will pose six tasks that illustrate how matrix operations can help with data exploration of the handwritten digits dataset. Each task highlights a basic matrix operation that is commonly used in R. By seeing how these operations can be implemented with fast and simple code, you will gain hands-on experience with the kinds of computations that arise frequently in high-dimensional data analysis. The primary goal of these tasks is to help you learn how matrix operations work in practice.\nVisualize the original image\nThe pixel intensities are provided as rows in a matrix. Using what we learned in Section 20.3, we can convert each row into a \\(28 \\times 28\\) matrix that we can visualize as an image. As an example, we will use the third observation. From the label, we know this is a:\n\ny[3]\n#&gt; [1] 4\n\nThe third row of the matrix x[3,] contains the 784 pixel intensities. If we assume these were entered in order, we can convert them back to a \\(28 \\times 28\\) matrix using:\n\ngrid &lt;- matrix(x[3,], 28, 28)\n\nTo visualize the data, we can use image in the following way:\n\nimage(1:28, 1:28, grid)\n\nHowever, because the y-axis in image goes bottom to top and x stores pixels top to bottom the code above shows shows a flipped image. To flip it back we can use:\n\nimage(1:28, 1:28, grid[, 28:1])\n\n\n\n\n\n\n\n\n\nDo some digits require more ink to write than others?\nLet’s study the distribution of the total pixel darkness and how it varies by digits.\nWe can use what we learned in Section Section 20.7}] to computed this average for each pixel and the display the values for each digit using a boxplot:\n\navg &lt;- rowMeans(x)\nboxplot(avg ~ y)\n\n\n\n\n\n\n\nFrom this plot we see that, not surprisingly, 1s use less ink than other digits.\nAre some pixels uninformative?\nLet’s study the variation of each pixel across digits and remove columns associated with pixels that don’t change much and thus can’t provide much information for classification.\nWe can what we learned in Section 20.8 to efficiently remove columns associated with pixels that don’t change much and thus do not inform digit classification.\nWe will also use what we learned in Section 20.7 to quantify the variation of each pixel with its standard deviation across all entries. Since each column represents a pixel, we use the colSds function from the matrixStats package:\n\nsds &lt;- colSds(x)\n\nA quick look at the distribution of these values shows that some pixels have very low entry to entry variability:\n\n\n\n\n\n\n\n\n\nhist(sds, breaks = 30, main = \"SDs\")\n\nThis makes sense since we don’t write in some parts of the box. Here is the variance plotted by location after using what we learned in Section 20.3 to creat a \\(28 \\times 28\\) matrix:\n\nimage(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])\n\n\n\n\n\n\n\n\n\nWe see that there is little variation in the corners.\nWe could remove features that have no variation since these can’t help us predict.\nSo if we wanted to remove uninformative predictors from our matrix, we could write this one line of code:\n\nnew_x &lt;- x[,colSds(x) &gt; 60]\ndim(new_x)\n#&gt; [1] 60000   322\n\nOnly the columns for which the standard deviation is above 60 are kept, which removes over half the predictors.\nCan we remove smudges?\nWe will first look at the distribution of all pixel values.\n\n\n\n\n\n\n\n\n\nhist(as.vector(x), breaks = 30, main = \"Pixel intensities\")\n\nThis shows a clear dichotomy which is explained as parts of the image with ink and parts without. If we think that values below, say, 50 are smudges, we can quickly make them zero using what we learned in Section 20.9\n\nnew_x &lt;- x\nnew_x[new_x &lt; 50] &lt;- 0\n\nBinarize the data\nThe histogram above seems to suggest that this data is mostly binary. A pixel either has ink or does not. Applying what we learned in Section 20.9, we can binarize the data using just matrix operations:\n\nbin_x &lt;- x\nbin_x[bin_x &lt; 255/2] &lt;- 0 \nbin_x[bin_x &gt; 255/2] &lt;- 1\n\nWe can also convert to a matrix of logicals and then coerce to numbers using what we learned in Section 20.10:\n\nbin_X &lt;- (x &gt; 255/2)*1\n\nStandardize the digits\nFinally, we will scale each column to have the same average and standard deviation.\nUsing what we learned in Section 20.10 implies that we can scale each row of a matrix as follows:\n\n(x - rowMeans(x))/rowSds(x)\n\nYet this approach does not work for columns. For columns, we can use the sweep function we learned in [sec-vectorization-for-matrices]:\n\nx_mean_0 &lt;- sweep(x, 2, colMeans(x))\n\nTo divide by the standard deviation, we change the default arithmetic operation to division as follows:\n\nx_standardized &lt;- sweep(x_mean_0, 2, colSds(x), FUN = \"/\")",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#exercises",
    "href": "highdim/matrices-in-R.html#exercises",
    "title": "\n20  Matrices in R\n",
    "section": "\n20.12 Exercises",
    "text": "20.12 Exercises\n1. Create a 100 by 10 matrix of randomly generated normal numbers. Put the result in x.\n2. Apply the three R functions that give you the dimension of x, the number of rows of x, and the number of columns of x, respectively.\n3. Add the scalar 1 to row 1, the scalar 2 to row 2, and so on, to the matrix x.\n4. Add the scalar 1 to column 1, the scalar 2 to column 2, and so on, to the matrix x. Hint: Use sweep with FUN = \"+\".\n5. Compute the average of each row of x.\n6. Compute the average of each column of x.\n7. For each digit in the MNIST training data, compute the proportion of pixels that are in a grey area, defined as values between 50 and 205. Make a boxplot by digit class. Hint: Use logical operators and rowMeans.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/matrices-in-R.html#footnotes",
    "href": "highdim/matrices-in-R.html#footnotes",
    "title": "\n20  Matrices in R\n",
    "section": "",
    "text": "http://yann.lecun.com/exdb/mnist/↩︎",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Matrices in R</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html",
    "href": "highdim/linear-algebra.html",
    "title": "21  Applied Linear Algebra",
    "section": "",
    "text": "21.1 The identity matrix\nLinear algebra is the main mathematical technique used to describe and motivate statistical methods and machine learning approaches. In this chapter, we introduce some of the mathematical concepts needed to understand these techniques. We use these concepts and techniques throughout the remainder of the book. ## Matrix multiplication\nA commonly used operation in data analysis is matrix multiplication. Here, we define and motivate the operation.\nLinear algebra originated from mathematicians developing systematic ways to solve systems of linear equations. For example:\n\\[\n\\begin{aligned}\nx +  3 y  - 2 z  &= 5\\\\\n3x + 5y + 6z &= 7\\\\\n2x + 4y + 3z &= 8\n\\end{aligned}\n\\]\nMathematicians figured out that by representing these linear systems of equations using matrices and vectors, predefined algorithms could be designed to solve any system of linear equations. A basic linear algebra class will teach some of these algorithms, such as Gaussian elimination, the Gauss-Jordan elimination, and the LU and QR decompositions. These methods are usually covered in detail in university level linear algebra courses.\nTo explain matrix multiplication, define two matrices: \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}&a_{m2}&\\dots&a_{mn}\n\\end{pmatrix}, \\,\n\\mathbf{B} = \\begin{pmatrix}\nb_{11}&b_{12}&\\dots&b_{1p}\\\\\nb_{21}&b_{22}&\\dots&b_{2p}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nb_{n1}&b_{n2}&\\dots&b_{np}\n\\end{pmatrix}\n\\]\nand define the product of matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) as the matrix \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) that has entries \\(c_{ij}\\) equal to the sum of the component-wise product of the \\(i\\)th row of \\(\\mathbf{A}\\) with the \\(j\\)th column of \\(\\mathbf{B}\\). Using R code, we can define \\(\\mathbf{C}= \\mathbf{A}\\mathbf{B}\\) as follows:\nBecause this operation is so common, R includes a mathematical operator %*% for matrix multiplication:\nUsing mathematical notation \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) looks like this:\n\\[\n\\begin{pmatrix}\na_{11}b_{11} + \\dots + a_{1n}b_{n1}&\na_{11}b_{12} + \\dots + a_{1n}b_{n2}&\n\\dots&\na_{11}b_{1p} + \\dots + a_{1n}b_{np}\\\\\na_{21}b_{11} + \\dots + a_{2n}b_{n1}&\na_{21}b_{12} + \\dots + a_{2n}b_{n2}&\n\\dots&\na_{21}b_{1p} + \\dots + a_{2n}b_{np}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{m1}b_{11} + \\dots +a_{mn}b_{n1}&\na_{m1}b_{12} + \\dots + a_{mn}b_{n2}&\n\\dots&\na_{m1}b_{1p} + \\dots + a_{mn}b_{np}\\\\\n\\end{pmatrix}\n\\]\nNote this definition implies that the multiplication \\(\\mathbf{A}\\mathbf{B}\\) is only possible when the number of rows of \\(\\mathbf{A}\\) matches the number of columns of \\(\\mathbf{B}\\).\nSo how does this definition of matrix multiplication help solve systems of equations? First, any system of equations with unknowns \\(x_1, \\dots x_n\\)\n\\[\n\\begin{aligned}\na_{11} x_1 + a_{12} x_2 \\dots + a_{1n}x_n &= b_1\\\\\na_{21} x_1 + a_{22} x_2 \\dots + a_{2n}x_n &= b_2\\\\\n\\vdots\\\\\na_{n1} x_1 + a_{n2} x_2 \\dots + a_{nn}x_n &= b_n\\\\\n\\end{aligned}\n\\]\ncan now be represented as matrix multiplication by defining the following matrices:\n\\[\n\\mathbf{A} =\\begin{pmatrix}\na_{11}&a_{12}&\\dots&a_{1n}\\\\\na_{21}&a_{22}&\\dots&a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\na_{n1}&a_{n2}&\\dots&a_{nn}\n\\end{pmatrix}\n,\\,\n\\mathbf{b} =\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\n\\vdots\\\\\nb_n\n\\end{pmatrix}\n,\\, \\mbox{ and }\n\\mathbf{x} =\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{pmatrix}\n\\]\nand rewriting the equation simply as:\n\\[\n\\mathbf{A}\\mathbf{x} =  \\mathbf{b}\n\\]\nThe linear algebra algorithms listed above, such as Gaussian elimination, provide a way to compute the inverse matrix \\(A^{-1}\\) that solves the equation for \\(\\mathbf{x}\\):\n\\[\n\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} =   \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\n\\]\nTo solve the first equation we wrote out in R, we can use the function solve:\nThe identity matrix, represented with a bold \\(\\mathbf{I}\\), is like the number 1, but for matrices: if you multiply a matrix by the identity matrix, you get back the matrix.\n\\[\n\\mathbf{I}\\mathbf{X} = \\mathbf{X}\n\\]\nIf you define \\(\\mathbf{I}\\) as matrix with the same number of rows and columns (referred to as square matrix) with 0s everywhere except the diagonal,\n\\[\n\\mathbf{I}=\\begin{pmatrix}\n1&0&\\dots&0\\\\\n0&1&\\dots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\dots&1\n\\end{pmatrix},\n\\]\nyou will obtain the desired property.\nNote that the definition of an inverse matrix implies that:\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{1}\n\\]\nBecause the default for the second argument in solve is an identity matrix, if we simply type solve(A), we obtain the inverse \\(\\mathbf{A}^{-1}\\). This means we can also obtain a solution to our system of equations with:\nsolve(A) %*% b",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html#distance",
    "href": "highdim/linear-algebra.html#distance",
    "title": "21  Applied Linear Algebra",
    "section": "\n21.2 Distance",
    "text": "21.2 Distance\nMany of the analyses we perform with high-dimensional data relate directly or indirectly to distance. For example, most machine learning techniques rely on being able to define distances between observations, using features or predictors. Clustering algorithms, for example, search for observations that are similar. But what does this mean mathematically?\nTo define distance, we introduce another linear algebra concept: the norm. Recall that a point in two dimensions can be represented in polar coordinates as:\n\n\n\n\n\n\n\n\nwith \\(\\theta = \\arctan{\\frac{x2}{x1}}\\) and \\(r = \\sqrt{x_1^2 + x_2^2}\\). If we think of the point as two dimensional column vector \\(\\mathbf{x} = (x_1, x_2)^\\top\\), \\(r\\) defines the norm of \\(\\mathbf{x}\\). The norm can be thought of as the size of the two-dimensional vector disregarding the direction: if we change the angle, the vector changes but the size does not. The point of defining the norm is that we can extrapolate the concept of size to higher dimensions. Specifically, we write the norm for any vector \\(\\mathbf{x}\\) as:\n\\[\n\\|\\mathbf{x}\\| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_p^2}\n\\]\nNote that we can use the linear algebra concepts we have learned to define the norm like this:\n\\[\n\\|\\mathbf{x}\\|^2 = \\mathbf{x}^\\top\\mathbf{x}\n\\]\nTo define distance, suppose we have two two-dimensional points: \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\). We can define how similar they are by simply using euclidean distance:\n\n\n\n\n\n\n\n\nWe know that the distance is equal to the length of the hypotenuse:\n\\[\n\\sqrt{(x_{11} - x_{12})^2 + (x_{21} - x_{22})^2}\n\\]\nThe reason we introduced the norm is because this distance is the size of the vector between the two points and this can be extrapolated to any dimension. The distance between two points, regardless of the dimensions, is defined as the norm of the difference\n\\[\n\\| \\mathbf{x}_1 - \\mathbf{x}_2\\|.\n\\]\nIf we use the digit data, the distance between the first and second observation will compute distance using all 784 features:\n\\[\n\\| \\mathbf{x}_1 - \\mathbf{x}_2 \\| = \\sqrt{ \\sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2 }\n\\]\nTo demonstrate, let’s pick the features for three digits:\n\nx_1 &lt;- x[6,]\nx_2 &lt;- x[17,]\nx_3 &lt;- x[16,]\n\nWe can compute the distances between each pair using the definitions we just learned:\n\nc(sum((x_1 - x_2)^2), sum((x_1 - x_3)^2), sum((x_2 - x_3)^2)) |&gt; sqrt()\n#&gt; [1] 2320 2331 2519\n\nIn R, the function crossprod(x) is convenient for computing norms. It multiplies t(x) by x:\n\nc(crossprod(x_1 - x_2), crossprod(x_1 - x_3), crossprod(x_2 - x_3)) |&gt; sqrt()\n#&gt; [1] 2320 2331 2519\n\nNote crossprod takes a matrix as the first argument. As a result, the vectors used here are being coerced into single column matrices. Also, note that crossprod(x,y) multiplies t(x) by y.\nWe can see that the distance is smaller between the first two. This agrees with the fact that the first two are 2s and the third is a 7.\n\ny[c(6, 17, 16)]\n#&gt; [1] 2 2 7\n\nWe can also compute all the distances at once relatively quickly using the function dist, which takes a matrix as input and computes the distance between each row and produces an object of class dist:\n\nd &lt;- dist(x[c(6,17,16),])\nclass(d)\n#&gt; [1] \"dist\"\n\nThis is convenient becasue there are several machine learning related functions in R that take objects of class dist as input.\nWe can see the distance we calculated above like this:\n\nd\n#&gt;      1    2\n#&gt; 2 2320     \n#&gt; 3 2331 2519\n\nNote that the diagonal is omitted because all self-distances are zero, and the upper triangle is also excluded due to the symmetry of the distance matrix, which follows from the fact that distance is commutative.\nTo access the entries using row and column indices, we need to coerce it into a matrix.\nThe image function allows us to quickly see an image of distances between observations. As an example, we compute the distance between each of the first 300 observations and then make an image:\n\nd &lt;- dist(x[1:300,])\nimage(as.matrix(d))\n\nIf we order this distance by the labels, we can see yellowish squares near the diagonal. This is because observations from the same digits tend to be closer than to different digits:\n\nimage(as.matrix(d)[order(y[1:300]), order(y[1:300])])",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html#sec-predictor-space",
    "href": "highdim/linear-algebra.html#sec-predictor-space",
    "title": "21  Applied Linear Algebra",
    "section": "\n21.3 Spaces",
    "text": "21.3 Spaces\nPredictor space is a concept that is often used to describe machine learning algorithms. The term space refers to an advanced mathematical definition for which we provide a simplified explanation to help understand the term predictor space when used in the context of machine learning algorithms.\nWe can think of all predictors \\((x_{i,1}, \\dots, x_{i,p})^\\top\\) for all observations \\(i=1,\\dots,n\\) as \\(n\\) \\(p\\)-dimensional points. A space can be thought of as the collection of all possible points that should be considered for the data analysis in question. This includes points we could see, but have not been observed yet. In the case of the handwritten digits, we can think of the predictor space as any point \\((x_{1}, \\dots, x_{p})^\\top\\) as long as each entry \\(x_i, \\, i = 1, \\dots, p\\) is between 0 and 255.\nSome Machine Learning algorithms also define subspaces. A commonly defined subspace in machine learning are neighborhoods composed of points that are close to a predetermined center. We do this by selecting a center \\(\\mathbf{x}_0\\), a minimum distance \\(r\\), and defining the subspace as the collection of points \\(\\mathbf{x}\\) that satisfy:\n\\[\n\\| \\mathbf{x} - \\mathbf{x}_0 \\| \\leq r.\n\\]\nWe can think of this subspace as a multidimensional sphere since every point is the same distance away from the center.\nOther machine learning algorithms partition the predictor space into non-overlapping regions and then make different predictions for each region using the data in the region. We will learn about these in Section 30.4.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html#recommended-reading",
    "href": "highdim/linear-algebra.html#recommended-reading",
    "title": "21  Applied Linear Algebra",
    "section": "\n21.4 Recommended reading",
    "text": "21.4 Recommended reading\n\nHefferon, J. (2017). Linear Algebra.\nA free and accessible open-source textbook designed for beginners, with clear explanations, worked examples, and plenty of exercises. Ideal for self-learners or students seeking to build both intuition and practical skills.Available online\nStrang, G. (2019). Linear Algebra and Learning from Data.\nFocuses on the role of linear algebra in modern machine learning and data science. A practical companion to statistical and algorithmic methods.\nSeber, G. A. F. (2003). Linear Regression Analysis (2nd ed.).\nA concise and mathematically rigorous exploration of regression, grounded in matrix algebra. Offers expanded coverage of diagnostics, model fitting, selection, and prediction.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/linear-algebra.html#exercises",
    "href": "highdim/linear-algebra.html#exercises",
    "title": "21  Applied Linear Algebra",
    "section": "\n21.5 Exercises",
    "text": "21.5 Exercises\n1. Generate two matrices, A and B, containing randomly generated and normally distributed numbers. The dimensions of these two matrices should be \\(4 \\times 3\\) and \\(3 \\times 6\\), respectively. Confirm that C &lt;- A %*% B produces the same results as:\n\nm &lt;- nrow(A)\np &lt;- ncol(B)\nC &lt;- matrix(0, m, p)\nfor(i in 1:m){\n  for(j in 1:p){\n    C[i,j] &lt;- sum(A[i,] * B[,j])\n  }\n}\n\n2. Solve the following system of equations using R:\n\\[\n\\begin{aligned}\nx + y + z + w &= 10\\\\\n2x + 3y - z - w &= 5\\\\\n3x - y + 4z - 2w &= 15\\\\\n2x + 2y - 2z - 2w &= 20\\\\\n\\end{aligned}\n\\]\n3. Define x and y:\n\nmnist &lt;- read_mnist()\nx &lt;- mnist$train$images[1:300,] \ny &lt;- mnist$train$labels[1:300]\n\nand compute the distance matrix:\n\nd &lt;- dist(x)\nclass(d)\n\nGenerate a boxplot showing the distances for the second row of d stratified by digits. Do not include the distance to itself, which we know is 0. Can you predict what digit is represented by the second row of x?\n4. Use the apply function and matrix algebra to compute the distance between the fourth digit mnist$train$images[4,] and all other digits represented in mnist$train$images. Then generate a boxplot as in exercise 2 and predict what digit is the fourth row.\n5. Compute the distance between each feature and the feature representing the middle pixel (row 14 column 14). Create an image plot of where the distance is shown with color in the pixel position.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Applied Linear Algebra</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html",
    "href": "highdim/dimension-reduction.html",
    "title": "\n22  Dimension reduction\n",
    "section": "",
    "text": "22.1 Motivation: preserving distance\nHigh-dimensional data can make data analysis challenging, especially when it comes to visualization and pattern discovery. For example, in the MNIST dataset, each image is represented by 784 pixels. To explore the relationships between all pairs of pixels, we would need to examine over 300,000 scatterplots. This quickly becomes impractical.\nIn this chapter, we introduce a powerful set of techniques known collectively as dimension reduction. The core idea is to reduce the number of variables in a dataset while preserving important characteristics, such as the distances between observations. With fewer dimensions, visualization becomes more feasible, and patterns in the data become easier to detect.\nThe main technique we focus on is Principal Component Analysis (PCA), a widely used method for unsupervised learning and exploratory data analysis. PCA is based on a mathematical tool called the singular value decomposition (SVD), which has applications beyond dimension reduction as well.\nWe begin with a simple illustrative example to build intuition, then introduce the mathematical concepts needed to understand PCA. We conclude the chapter by applying PCA to two more complex, real-world datasets to demonstrate its practical value.\nFor illustrative purposes, we consider a simple example with twin heights. Some pairs are adults, the others are children. Here we simulate 100 two-dimensional points where each point a pair of twins. We use the mvrnorm function from the MASS package to simulate bivariate normal data.\nset.seed(1983)\nlibrary(MASS)\nn &lt;- 100\nrho &lt;- 0.9\nsigma &lt;- 3\ns &lt;- sigma^2*matrix(c(1, rho, rho, 1), 2, 2)\nx &lt;- rbind(mvrnorm(n/2, c(69, 69), s),\n           mvrnorm(n/2, c(60, 60), s))\nA scatterplot quickly reveals that the correlation is high and that there are two groups of twins, the adults (upper right points) and the children (lower left points):\nOur features are \\(n\\) two-dimensional points, the two heights. For illustrative purposes, we will pretend that visualizing two dimensions is too challenging and we want to explore the data through a histogram of a one-dimensional variable. We therefore want to reduce the dimensions from two to one, but still be able to understand important characteristics of the data, in particular that the observations cluster into two groups: adults and children.\nTo show that the ideas presented here are generally useful, we will standardize the data so that observations are in standard units rather than inches:\nlibrary(matrixStats)\nx &lt;- sweep(x, 2, colMeans(x))\nx &lt;- sweep(x, 2, colSds(x), \"/\")\nIn the scatterplot above, we also show the distance between observation 1 and 2 (blue), and observation 1 and 51 (red). Note that the blue line is shorter, which implies that 1 and 2 are closer.\nWe can compute these distances using dist:\nd &lt;- as.matrix(dist(x))\nc(d[1, 2], d[2, 51])\n#&gt; [1] 0.595 1.388\nThis distance is based on two dimensions and we need a distance approximation based on just one.\nIf we look back at the hieght scatterplot above and imagine drawing a line between any pair of points, the length of that line represents the distance between the two points. Many of these lines tend to align along the diagonal direction, suggesting that most of the variation in the data is not purely horizontal or vertical but spread diagonally across both axes.\nNow imagine we rotate the entire point cloud so that the variation that was previously along the diagonal is now aligned with the horizontal axis. In this rotated view, the largest differences between points would show up in the first (horizontal) dimension. This would make it easier to describe the data using just one variables, the one that captures the most meaningful variation.\nIn the next section, we introduce a mathematical approach that makes this intuition precise. It allows us to find a rotation of the data that preserves the distances between points while reorienting the axes to highlight the most important directions of variation. This is the foundation of an approach called Principal Component Analysis (PCA), the most widely used technique for dimension reduction.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#rotations",
    "href": "highdim/dimension-reduction.html#rotations",
    "title": "\n22  Dimension reduction\n",
    "section": "\n22.2 Rotations",
    "text": "22.2 Rotations\nAny two-dimensional point \\((x_1, x_2)^\\top\\) can be written as the base and height of a triangle with a hypotenuse going from \\((0,0)^\\top\\) to \\((x_1, x_2)^\\top\\):\n\\[\nx_1 = r \\cos\\phi, \\,\\, x_2 = r \\sin\\phi\n\\]\nwith \\(r\\) the length of the hypotenuse and \\(\\phi\\) the angle between the hypotenuse and the x-axis.\nTo rotate the point \\((x_1, x_2)^\\top\\) around a circle with center \\((0,0)^\\top\\) and radius \\(r\\) by an angle \\(\\theta\\) we simply change the angle in the previous equation to \\(\\phi + \\theta\\):\n\\[\nz_1 = r \\cos(\\phi+ \\theta), \\,\\,\nz_2 = r \\sin(\\phi + \\theta)\n\\]\n\n\n\n\n\n\n\n\nWe can use trigonometric identities to rewrite \\((z_1, z_2)\\) as follows:\n\\[\n\\begin{aligned}\nz_1 = r \\cos(\\phi + \\theta) = r \\cos \\phi \\cos\\theta -  r \\sin\\phi \\sin\\theta =  x_1 \\cos(\\theta) -  x_2 \\sin(\\theta)\\\\\nz_2 = r \\sin(\\phi + \\theta) =  r \\cos\\phi \\sin\\theta + r \\sin\\phi \\cos\\theta =  x_1 \\sin(\\theta) + x_2 \\cos(\\theta)\n\\end{aligned}\n\\]\nNow we can rotate each point in the dataset by simply applying the formula above to each pair \\((x_{i,1}, x_{i,2})^\\top\\).\nHere is what the twin standardized heights look like after rotating each point by \\(-45\\) degrees:\n\n\n\n\n\n\n\n\nNote that while the variability of \\(x_1\\) and \\(x_2\\) are similar, the variability of \\(z_1\\) is much larger than the variability of \\(z_2\\). Also, notice that the distances between points appear to be preserved. In the next sections, we show mathematically that this in fact the case.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#linear-transformations",
    "href": "highdim/dimension-reduction.html#linear-transformations",
    "title": "\n22  Dimension reduction\n",
    "section": "\n22.3 Linear transformations",
    "text": "22.3 Linear transformations\nAny time a matrix \\(\\mathbf{X}\\) is multiplied by another matrix \\(\\mathbf{A}\\), we refer to the product \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{A}\\) as a linear transformation of \\(\\mathbf{X}\\). Below, we show that the rotations described above are a linear transformation. To see this, note that for any row \\(i\\), the first entry was:\n\\[\nz_{i,1} = a_{1,1} x_{i,1} + a_{2,1} x_{i,2}\n\\]\nwith \\(a_{1,1} = \\cos\\theta\\) and \\(a_{2,1} = -\\sin\\theta\\).\nThe second entry was also a linear transformation:\n\\[z_{i,2} = a_{1,2} x_{i,1} + a_{2,2} x_{i,2}\\]\nwith \\(a_{1,2} = \\sin\\theta\\) and \\(a_{2,2} = \\cos\\theta\\).\nWe can write these equations using matrix notation:\n\\[\n\\begin{pmatrix}\nz_1\\\\z_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na_{1,1}&a_{1,2}\\\\\na_{2,1}&a_{2,2}\n\\end{pmatrix}^\\top\n\\begin{pmatrix}\nx_1\\\\x_2\n\\end{pmatrix}\n\\]\nAn advantage of using linear algebra is that we can write the transformation for the entire dataset by saving all observations in a \\(N \\times 2\\) matrix:\n\\[\n\\mathbf{X} \\equiv\n\\begin{bmatrix}\n\\mathbf{x_1}^\\top\\\\\n\\vdots\\\\\n\\mathbf{x_n}^\\top\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{1,1}&x_{1,2}\\\\\n\\vdots&\\vdots\\\\\nx_{n,1}&x_{n,2}\n\\end{bmatrix}\n\\]\nWe can then obtain the rotated values \\(\\mathbf{z}_i\\) for each row \\(i\\) by applying a linear transformation of \\(X\\):\n\\[\n\\mathbf{Z} = \\mathbf{X} \\mathbf{A}\n\\mbox{ with }\n\\mathbf{A} = \\,\n\\begin{pmatrix}\na_{1,1}&a_{1,2}\\\\\na_{2,1}&a_{2,2}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\cos \\theta&\\sin \\theta\\\\\n-\\sin \\theta&\\cos \\theta\n\\end{pmatrix}\n.\n\\]\nIf we define:\n\ntheta &lt;- 2*pi*-45/360 #convert to radians\nA &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n\nWe can write code implementing a rotation by any angle \\(\\theta\\) using linear algebra:\n\nrotate &lt;- function(x, theta){\n  theta &lt;- 2*pi*theta/360\n  A &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n  x %*% A\n}\n\nThe columns of \\(\\mathbf{A}\\) are referred to as directions because if we draw a vector from \\((0,0)\\) to \\((a_{1,j}, a_{2,j})\\), it points in the direction of the line that will become the \\(j\\)-th dimension.\nAnother advantage of linear algebra is that if we can find the inverse matrix of \\(\\mathbf{A}^\\top\\), we can convert \\(\\mathbf{Z}\\) back to \\(\\mathbf{X}\\), again using a linear transformation.\nIn this particular case, we can use trigonometry to show that:\n\\[\nx_{i,1} = b_{1,1} z_{i,1} + b_{2,1} z_{i,2}\\\\\nx_{i,2} = b_{1,2} z_{i,1} + b_{2,2} z_{i,2}\n\\]\nwith \\(b_{2,1} = \\cos\\theta\\), \\(b_{2,1} = \\sin\\theta\\), \\(b_{1,2} = -\\sin\\theta\\), and \\(b_{2,2} = \\cos\\theta\\).\nThis implies that:\n\\[\n\\mathbf{X} = \\mathbf{Z}\n\\begin{pmatrix}\n\\cos \\theta&-\\sin \\theta\\\\\n\\sin \\theta&\\cos \\theta\n\\end{pmatrix}.\n\\] Note that the transformation used above is actually \\(\\mathbf{A}^\\top\\) which implies that\n\\[\n\\mathbf{Z} \\mathbf{A}^\\top = \\mathbf{X} \\mathbf{A}\\mathbf{A}^\\top\\ = \\mathbf{X}\n\\]\nand therefore that \\(\\mathbf{A}^\\top\\) is the inverse of \\(\\mathbf{A}\\). This also implies that all the information in \\(\\mathbf{X}\\) is included in the rotation \\(\\mathbf{Z}\\), and it can be retrieved via a linear transformation. A consequence is that for any rotation the distances are preserved. Here is an example for a 30 degree rotation, although it works for any angle:\n\nall.equal(as.matrix(dist(rotate(x, 30))), as.matrix(dist(x)))\n#&gt; [1] TRUE\n\nThe next section explains why this happens.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#orthogonal-transformations",
    "href": "highdim/dimension-reduction.html#orthogonal-transformations",
    "title": "\n22  Dimension reduction\n",
    "section": "\n22.4 Orthogonal transformations",
    "text": "22.4 Orthogonal transformations\nRecall that the distance between two points, say rows \\(h\\) and \\(i\\) of the transformation \\(\\mathbf{Z}\\), can be written like this:\n\\[\n\\|\\mathbf{z}_h - \\mathbf{z}_i\\| = (\\mathbf{z}_h - \\mathbf{z}_i)^\\top(\\mathbf{z}_h - \\mathbf{z}_i)\n\\]\nwith \\(\\mathbf{z}_h\\) and \\(\\mathbf{z}_i\\) the \\(p \\times 1\\) column vectors stored in the \\(h\\)-th and \\(i\\)-th rows of \\(\\mathbf{X}\\), respectively.\n\n\n\n\n\n\nRemember that we represent the rows of a matrix as column vectors. This explains why we use \\(\\mathbf{A}\\) when showing the multiplication for the matrix \\(\\mathbf{Z}=\\mathbf{X}\\mathbf{A}\\), but transpose the operation when showing the transformation for just one observation: \\(\\mathbf{z}_i = \\mathbf{A}^\\top\\mathbf{x}_i\\)\n\n\n\nUsing linear algebra, we can rewrite the quantity above as:\n\\[\n\\|\\mathbf{z}_h - \\mathbf{z}_i\\| =\n\\|\\mathbf{A}^\\top \\mathbf{x}_h - \\mathbf{A}^\\top\\mathbf{x}_i\\|^2 =\n(\\mathbf{x}_h - \\mathbf{x}_i)^\\top \\mathbf{A} \\mathbf{A}^\\top (\\mathbf{x}_h - \\mathbf{x}_i)\n\\]\nNote that if \\(\\mathbf{A} \\mathbf{A} ^\\top= \\mathbf{I}\\), then the distance between the \\(h\\)th and \\(i\\)th rows is the same for the original and transformed data.\nWe refer to transformations with the property \\(\\mathbf{A} \\mathbf{A}^\\top = \\mathbf{I}\\) as orthogonal transformations. These are guaranteed to preserve the distance between any two points.\nWe previously demonstrated our rotation has this property. We can confirm using R:\n\nA %*% t(A)\n#&gt;          [,1]     [,2]\n#&gt; [1,] 1.00e+00 1.01e-17\n#&gt; [2,] 1.01e-17 1.00e+00\n\nNotice that \\(\\mathbf{A}\\) being orthogonal also guarantees that the total sum of squares (TSS) of \\(\\mathbf{X}\\), defined as \\(\\sum_{i=1}^n \\sum_{j=1}^p x_{i,j}^2\\) is equal to the total sum of squares of the rotation \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{A}^\\top\\). To illustrate, observe that if we denote the rows of \\(\\mathbf{Z}\\) as \\(\\mathbf{z}_1, \\dots, \\mathbf{z}_n\\), then the sum of squares can be written as:\n\\[\n\\sum_{1=1}^n \\|\\mathbf{z}_i\\|^2 = \\sum_{i=1}^n \\|\\mathbf{A}^\\top\\mathbf{x}_i\\|^2 = \\sum_{i=1}^n \\mathbf{x}_i^\\top \\mathbf{A}\\mathbf{A}^\\top  \\mathbf{x}_i = \\sum_{i=1}^n \\mathbf{x}_i^\\top\\mathbf{x}_i = \\sum_{i=1}^n\\|\\mathbf{x}_i\\|^2\n\\]\nWe can confirm using R:\n\ntheta &lt;- -45\nz &lt;- rotate(x, theta) # works for any theta\nsum(x^2)\n#&gt; [1] 198\nsum(z^2)\n#&gt; [1] 198\n\nThis can be interpreted as a consequence of the fact that an orthogonal transformation guarantees that all the information is preserved.\nHowever, although the total is preserved, the sum of squares for the individual columns changes. Here we compute the proportion of TSS attributed to each column, referred to as the variance explained or variance captured by each column, for \\(\\mathbf{X}\\):\n\ncolSums(x^2)/sum(x^2)\n#&gt; [1] 0.5 0.5\n\nand \\(\\mathbf{Z}\\):\n\ncolSums(z^2)/sum(z^2)\n#&gt; [1] 0.9848 0.0152\n\nIn the next section, we describe how this last mathematical result can be useful.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#sec-pca",
    "href": "highdim/dimension-reduction.html#sec-pca",
    "title": "\n22  Dimension reduction\n",
    "section": "\n22.5 Principal Component Analysis (PCA)",
    "text": "22.5 Principal Component Analysis (PCA)\nWe have established that orthogonal transformations preserve both the distances between observations and the total sum of squares (TSS). However, while the TSS remains unchanged, the way it is distributed across the columns can vary depending on the transformation.\nThe main idea behind Principal Component Analysis (PCA) is to find an orthogonal transformation that concentrates as much of the variance as possible into the first few columns. This allows us to reduce the dimensionality of the problem by focusing only on those columns that capture most of the variation in the data.\nIn our example, we aim to find a rotation that maximizes the variance explained in the first column. The following code performs a grid search across rotations from –90 to 0 degrees to identify such a transformation:\n\n\n\n\n\n\n\n\n\nangles &lt;- seq(0, -90)\nv &lt;- sapply(angles, function(angle) colSums(rotate(x, angle)^2))\nvariance_explained &lt;- v[1,]/sum(x^2)\nplot(angles, variance_explained, type = \"l\")\n\nWe find that a -45 degree rotation appears to achieve the maximum, with over 98% of the total variability explained by the first dimension. We denote this rotation matrix with \\(\\mathbf{V}\\):\n\ntheta &lt;- 2*pi*-45/360 #convert to radians\nV &lt;- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2, 2)\n\nWe can rotate the entire dataset using:\n\\[\n\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\n\\]\n\nz &lt;- x %*% V\n\nThe following animation further illustrates how different rotations affect the variability explained by the dimensions of the rotated data:\n\n\n\n\n\n\n\n\nThe first dimension of z is referred to as the first principal component (PC). Because almost all the variation is explained by this first PC, the distance between rows in x can be very well approximated by the distance calculated with just z[,1].\n\n\n\n\n\n\n\n\nWe also notice that the two groups, adults and children, can be clearly observed with the one number summary, better than with any of the two original dimensions.\n\n\n\n\n\n\n\n\n\nhist(x[,1], breaks = seq(-4,4,0.5))\nhist(x[,2], breaks = seq(-4,4,0.5))\nhist(z[,1], breaks = seq(-4,4,0.5))\n\nWe can visualize these to see how the first component summarizes the data. In the plot below, red represents high values and blue negative values:\n\n\n\n\n\n\n\n\nThis idea extends naturally to more than two dimensions. As in the two-dimensional case, we begin by finding a \\(p \\times 1\\) vector \\(\\mathbf{v}_1\\) with \\(\\|\\mathbf{v}_1\\| = 1\\) that maximizes \\(\\|\\mathbf{X} \\mathbf{v}_1\\|\\). The projection \\(\\mathbf{X} \\mathbf{v}_1\\) defines the first principal component (PC), with \\(\\mathbf{v}_1\\|\\) pointing in the direction of greatest variation in the data.\nTo find the second principal component, we remove the variation explained by the first principal component from \\(\\mathbf{X}\\). This gives the residual matrix:\n\\[\n\\mathbf{r} = \\mathbf{X} - \\mathbf{X} \\mathbf{v}_1 \\mathbf{v}_1^\\top\n\\]\nWe then find a vector \\(\\mathbf{v}_2\\) with \\(\\|\\mathbf{v}_2\\| = 1\\) that maximizes \\(\\|\\mathbf{r} \\mathbf{v}_2\\|\\). The projection \\(\\mathbf{X} \\mathbf{v}_2\\) is the second principal component.\nThis process continues: at each step, we subtract the variation explained by the previous components and find the next direction that captures the greatest remaining variance. Repeating this until all \\(p\\) directions have been found, we construct the full rotation matrix \\(\\mathbf{V}\\) and the corresponding principal component matrix \\(\\mathbf{Z}\\):\n\\[\n\\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{v}_1 & \\dots & \\mathbf{v}_p\n\\end{bmatrix}, \\quad\n\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\n\\]\nEach column of \\(\\mathbf{Z}\\) is a principal component, and the columns of \\(\\mathbf{V}\\) define the new rotated coordinate system.\nThe idea of distance preservation extends naturally to higher dimensions. For a matrix \\(\\mathbf{X}\\) with \\(p\\) columns, the transformation \\(\\mathbf{Z} = \\mathbf{X}\\mathbf{V}\\) preserves the pairwise distances between rows, while reorienting the data so that the variance explained by each column of \\(\\mathbf{Z}\\) is ordered from largest to smallest. If the variance of the columns \\(\\mathbf{Z}_j\\) for \\(j &gt; k\\) is very small, those dimensions contribute little to the overall variation in the data and, by extension, to the distances between observations.\nIn such cases, we can approximate the original distances between rows using only the first \\(k\\) columns of \\(\\mathbf{Z}\\). If \\(k \\ll p\\), this yields a much lower-dimensional representation of the data that still retains the essential structure. This is the key advantage of PCA: it allows us to summarize high-dimensional data efficiently, making visualization, computation, and modeling more tractable without losing the main patterns in the data.\n\n\n\n\n\n\nThe solution to the PCA maximization problem is not unique. For example, \\(\\|\\mathbf{X} \\mathbf{v}\\| = \\|-\\mathbf{X} \\mathbf{v}\\|\\), so both \\(\\mathbf{v}\\) and \\(-\\mathbf{v}\\) produce the same principal component direction. Similarly, if we flip the sign of any column in \\(\\mathbf{Z}\\) (the principal components), we can preserve the equality \\(\\mathbf{X} = \\mathbf{Z} \\mathbf{V}^\\top\\) as long as we also flip the sign of the corresponding column in \\(\\mathbf{V}\\) (the rotation matrix). This means we are free to change the sign of any column in \\(\\mathbf{Z}\\) and \\(\\mathbf{V}\\) without affecting the PCA result.\n\n\n\nIn R, we can find the principal components of any matrix with the function prcomp:\n\npca &lt;- prcomp(x, center = FALSE)\n\nKeep in mind that default behavior is to center the columns of x before computing the PCs, an operation we don’t need in our example because our matrix is scaled.\nThe object pca includes the rotated data \\(Z\\) in pca$x and the rotation \\(\\mathbf{V}\\) in pca$rotation.\nWe can see that columns of the pca$rotation are indeed the rotation obtained with -45 (remember the sign is arbitrary):\n\npca$rotation\n#&gt;         PC1    PC2\n#&gt; [1,] -0.707  0.707\n#&gt; [2,] -0.707 -0.707\n\nThe square root of the variation of each column is included in the pca$sdev component. This implies we can compute the variance explained by each PC using:\n\npca$sdev^2/sum(pca$sdev^2)\n#&gt; [1] 0.9848 0.0152\n\nThe function summary performs this calculation for us:\n\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                          PC1    PC2\n#&gt; Standard deviation     1.403 0.1745\n#&gt; Proportion of Variance 0.985 0.0152\n#&gt; Cumulative Proportion  0.985 1.0000\n\nWe also see that we can rotate x (\\(\\mathbf{X}\\)) and pca$x (\\(\\mathbf{Z}\\)) as explained with the mathematical formulas above:\n\nall.equal(pca$x, x %*% pca$rotation)\n#&gt; [1] TRUE\nall.equal(x, pca$x %*% t(pca$rotation))\n#&gt; [1] TRUE",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#examples",
    "href": "highdim/dimension-reduction.html#examples",
    "title": "\n22  Dimension reduction\n",
    "section": "\n22.6 Examples",
    "text": "22.6 Examples\n\n22.6.1 Iris example\nThe iris dataset is a widely used example in data analysis courses. It contains four botanical measurements—sepal length, sepal width, petal length, and petal width—for flowers from three different species:\n\nnames(iris)\n#&gt; [1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n#&gt; [5] \"Species\"\n\nIf you inspect the species column using iris$Species, you’ll see that the observations are grouped by species.\nWhen we visualize the pairwise distances between observations, a clear pattern emerges: the data appears to cluster into three distinct groups, with one species standing out as more clearly separated from the other two. This structure is consistent with the known species labels:\n\nx &lt;- iris[,1:4] |&gt; as.matrix()\nd &lt;- dist(x)\nimage(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, \"RdBu\")))\n\n\n\n\n\n\n\n\n\nOur features matrix has four dimensions, but three are very correlated:\n\ncor(x)\n#&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; Sepal.Length        1.000      -0.118        0.872       0.818\n#&gt; Sepal.Width        -0.118       1.000       -0.428      -0.366\n#&gt; Petal.Length        0.872      -0.428        1.000       0.963\n#&gt; Petal.Width         0.818      -0.366        0.963       1.000\n\nIf we apply PCA to the iris dataset, we expect that the first few principal components will capture most of the variation in the data. Because the original variables are highly correlated, PCA should allow us to approximate the distances between observations using just two dimensions, effectively compressing the data while preserving its structure.\nWe can use the summary function to examine how much variance is explained by each principal component:\n\npca &lt;- prcomp(x)\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                          PC1    PC2    PC3     PC4\n#&gt; Standard deviation     2.056 0.4926 0.2797 0.15439\n#&gt; Proportion of Variance 0.925 0.0531 0.0171 0.00521\n#&gt; Cumulative Proportion  0.925 0.9777 0.9948 1.00000\n\nThe first two dimensions account for almost 98% of the variability. Thus, we should be able to approximate the distance very well with two dimensions. We confirm this by computing the distance with just the first two dimensions and comparing to the original:\n\nd_approx &lt;- dist(pca$x[, 1:2])\nplot(d, d_approx); abline(0, 1, col = \"red\")\n\n\n\n\n\n\n\n\n\nA useful application of this result is that we can now visualize the distances between observations in a two-dimensional plot. In this plot, the distance between any pair of points approximates the actual distance between the corresponding observations in the original high-dimensional space.\n\ndata.frame(pca$x[,1:2], Species = iris$Species) |&gt;\n  ggplot(aes(PC1, PC2, fill = Species)) +\n  geom_point(cex = 3, pch = 21) +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\n\nWe color the observations by their labels and notice that, with these two dimensions, we achieve almost perfect separation.\nLooking more closely at the resulting PCs and rotations:\n\n\n\n\n\n\n\n\nwe learn that the first PC is obtained by taking a weighted average of sepal length, petal length, and petal width (red in first column), and subtracting a quantity proportional to sepal width (blue in first column). The second PC is a weighted average of petal length and petal width, minus a weighted average of sepal length and sepal width.\n\n22.6.2 MNIST example\nThe written digits example has 784 features. Is there any room for data reduction? We will use PCA to answer this.\nIf not already loaded, let’s begin by loading the data:\n\nlibrary(dslabs)\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\nBecause the pixels are so small, we expect pixels close to each other on the grid to be correlated, meaning that dimension reduction should be possible.\nLet’s compute the PCs. This will take a few seconds as it is a rather large matrix:\n\npca &lt;- prcomp(mnist$train$images)\n\nand examine the variance explained by each PC:\n\n\n\n\n\n\n\n\n\nplot(pca$sdev^2/sum(pca$sdev^2), xlab = \"PC\", ylab = \"Variance explained\")\n\nWe can see that the first few PCs already explain a large percent of the variability. Furthermore, simply by looking at the first two PCs we already see information about the labels. Here is a random sample of 500 digits:\n\n\n\n\n\n\n\n\nWe can also see the rotation values on the 28 \\(\\times\\) 28 grid to get an idea of how pixels are being weighted in the transformations that result in the PCs:\n\n\n\n\n\n\n\n\nWe can clearly see that first PC appears to be separating the 1s (red) from the 0s (blue). We can vaguely discern digits, or parts of digits, in the other three PCs as well. By looking at the PCs stratified by digits, we get further insights. For example, we see that the second PC separates 4s, 7s, and 9s from the rest:\n\n\n\n\n\n\n\n\nWe can also confirm that the lower variance PCs appear related to unimportant variability, mainly smudges in the corners:",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "highdim/dimension-reduction.html#exercises",
    "href": "highdim/dimension-reduction.html#exercises",
    "title": "\n22  Dimension reduction\n",
    "section": "\n22.7 Exercises",
    "text": "22.7 Exercises\n1. We want to explore the tissue_gene_expression predictors by plotting them.\n\ndim(tissue_gene_expression$x)\n\nWe hope to get an idea of which observations are close to each other, but the predictors are 500-dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type.\n2. The predictors for each observation are measured on the same measurement device (a gene expression microarray) after an experimental procedure. A different device and procedure is used for each observation. This may introduce biases that affect all predictors for each observation in the same way. To explore the effect of this potential bias, for each observation, compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation.\n3. We see an association with the first PC and the observation averages. Redo the PCA, but only after removing the center.\n4. For the first 10 PCs, make a boxplot showing the values for each tissue.\n5. Plot the percent variance explained by PC number. Hint: Use the summary function.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html",
    "href": "highdim/regularization.html",
    "title": "23  Regularization",
    "section": "",
    "text": "23.1 Case study: recommendation systems\nRecommendation systems, such as the one used by Amazon, operate by analyzing the ratings that customers give to various products. These ratings form a large dataset. The system uses this data to predict how likely a specific user is to favorably rate a particular product. For example, if the system predicts that a user is likely to give a high rating to a certain book or gadget, it will recommend that item to them. In essence, the system tries to guess which products a user will like based on the ratings provided by them and other customers for various items. This approach helps in personalizing recommendations to suit individual preferences.\nDuring its initial years of operation, Netflix used a 5-star recommendation system. One star suggested it was not a good movie, whereas five stars suggested it was an excellent movie. Here, we provide the basics of how these recommendations are made, motivated by some of the approaches taken by the winners of the Netflix challenges.\nIn October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009, the winners were announced1. You can read a summary of how the winning algorithm was put together here: http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/ and a more detailed explanation here: https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf. We will now show you some of the data analysis strategies used by the winning team.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#sec-recommendation-systems",
    "href": "highdim/regularization.html#sec-recommendation-systems",
    "title": "23  Regularization",
    "section": "",
    "text": "23.1.1 \nThe Netflix data is not publicly available, but the GroupLens research lab2 generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users. We make a small subset of this data available via the dslabs package:\n\nlibrary(data.table)\nlibrary(dslabs)\nmovielens |&gt; tibble:::tibble() |&gt; head(5)\n#&gt; # A tibble: 5 × 7\n#&gt;   movieId title                      year genres userId rating timestamp\n#&gt;     &lt;int&gt; &lt;chr&gt;                     &lt;int&gt; &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;\n#&gt; 1      31 Dangerous Minds            1995 Drama       1    2.5    1.26e9\n#&gt; 2    1029 Dumbo                      1941 Anima…      1    3      1.26e9\n#&gt; 3    1061 Sleepers                   1996 Thril…      1    3      1.26e9\n#&gt; 4    1129 Escape from New York       1981 Actio…      1    2      1.26e9\n#&gt; 5    1172 Cinema Paradiso (Nuovo c…  1989 Drama       1    4      1.26e9\n\nEach row represents a rating given by one user to one movie.\nBecause we are working with a relatively large dataset, we will convert the data frame to a data.table object to take advantage of more efficient data wrangling.\n\ndt &lt;- as.data.table(movielens)\n\nWe can see the number of unique users that provided ratings and how many unique movies were rated:\n\ndt[, .(n_users = uniqueN(userId), n_movies = uniqueN(movieId))]\n#&gt;    n_users n_movies\n#&gt;      &lt;int&gt;    &lt;int&gt;\n#&gt; 1:     671     9066\n\nIf we multiply those two numbers, we get a figure exceeding 5 million, yet our dataset contains only about 100,000 rows. This tells us that not every user rated every movie. In fact, the median number of movies rated per user is 71. We can think of the data as a very large matrix, with users as rows and movies as columns, filled with many missing entries. The goal of a movie recommendation system is to accurately predict those missing values.\nLet’s look at some of the general properties of the data to better understand the challenges.\nThe first thing we notice is that some movies receive far more ratings than others. The distribution below highlights this disparity, which isn’t surprising, blockbuster films tend to be seen and rated by millions, while niche or independent films attract far fewer viewers. A second observation is that users vary significantly in how active they are at rating movies:\n\n\n\n\n\n\n\n\nWe need to build an algorithm with the collected data that will then be applied outside our control when users look for movie recommendations. To test our idea, we will split the data into a training set, which we will use to develop our approach, and a test set in which we will compute the accuracy of our predictions.\nWe will do this only for users that have provided at least 100 ratings.\n\ndt &lt;- dt[, if (.N &gt;= 100) .SD, by = userId]\n\nFor each one of these users, we will split their ratings into 80% for training and 20% for testing.\n\nset.seed(2006)\nindexes &lt;- split(1:nrow(dt), dt$userId)\n\ntest_ind &lt;- sapply(indexes, function(i) sample(i, ceiling(length(i)*.2))) \ntest_ind &lt;- sort(unlist(test_ind))\n\nWe keep only the columns we use in this analysis and convert userId ad movieId to characters so we can use as indeces:\n\ndt[, `:=`(userId = as.character(userId), movieId = as.character(movieId))]\ncols &lt;- c(\"userId\", \"movieId\", \"title\", \"rating\")\ntest_set &lt;- dt[test_ind, ..cols] \ntrain_set &lt;- dt[-test_ind, ..cols]\n## Remove  movies in the test set that have no ratings in the training set\ntest_set &lt;- test_set[movieId %in% train_set$movieId]\n\nWe will use the array representation described in Section 16.5, for the training data. Specifically, we denote ranking for movie \\(j\\) by user \\(i\\) as \\(y_{i,j}\\).\nNote that two different movies can have the same title. For example, our dataset has three movies titled “King Kong”. Titles are therefore not unique and we can’t use them as IDs.\n\n\n\n\n\n\nAlthough it is convenient to use the notation \\(y_{i,j}\\), it is important to remember that we do not observe a value for every pair \\((i,j)\\). While the total number of users \\(i = 1, \\dots, I\\) and movies \\(j = 1, \\dots, J\\) defines a theoretical space of \\(I \\times J\\) possible ratings, the actual number of observed ratings,denoted by \\(N\\), is typically much smaller. In our example, \\(N\\) is far less than \\(I \\times J\\), reflecting the fact that most users rate only a small fraction of all available movies. We will use the mathematical notation \\(\\sum_{i,j}\\) to represent a summation over all \\(N\\) observed pairs.\n\n\n\n\n23.1.2 Loss function\nThe Netflix challenge decided on a winner based on the root mean squared error (RMSE) computed on the test set. Specifically, if \\(y_{i,j}\\) is the rating for movie \\(j\\) by user \\(i\\) in the test set and \\(\\hat{y}_{i,j}\\) is our prediction based on the training set, RMSE was defined as:\n\\[\n\\mbox{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i,j} (y_{i,j} - \\hat{y}_{i,j})^2}\n\\]\nwith \\(N\\) being the number of user/movie combinations for which we made predictions and the sum occurring over all these combinations.\nWe can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good. We define a function to compute this quantity for any set of residuals:\n\nrmse &lt;- function(r) sqrt(mean(r^2))\n\nIn this chapter and the next, we introduce two concepts, regularization and latent factor analysis, that were used by the winners of the Netflix challenge to obtain the winning RMSE.\n\n\n\n\n\n\nIn Chapter 29, we provide a formal discussion of the mean squared error.\n\n\n\n\n23.1.3 A first model\nLet’s start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. What number should this prediction be? We can use a model based approach to answer this. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look as follows:\n\\[\nY_{i,j} = \\mu + \\varepsilon_{i,j}\n\\]\nwith \\(\\varepsilon_{i,j}\\) independent errors sampled from the same distribution centered at 0 and \\(\\mu\\) the true rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of \\(\\mu\\) and, in this case, is the average of all ratings:\n\nmu &lt;- mean(train_set$rating)\n\nIf we predict all unknown ratings with \\(\\hat{\\mu}\\) we obtain an RMSE of\n\nrmse(test_set$rating - mu)\n#&gt; [1] 1.04\n\nMathematical theory tells us that if you plug in any other number, you get a higher RMSE. Here is an example:\n\nrmse(test_set$rating - 3)\n#&gt; [1] 1.16\n\nTo win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do better!\n\n23.1.4 User effects\nIf we visualize the average rating for each user\n\nhist(with(dt, tapply(rating, userId, mean)), nclass = 30)\n\n\n\n\n\n\n\nwe notice that there is substantial variability across users: some users are very cranky and others love most movies. To account for this, we can use a linear model with a treatment effect \\(\\alpha_i\\) for each user. The sum \\(\\mu + \\alpha_i\\) can be interpreted as the typical rating user \\(i\\) gives to movies. We can write the model as:\n\\[\nY_{i,j} = \\mu + \\alpha_i + \\varepsilon_{i,j}\n\\]\nStatistics textbooks refer to the \\(\\alpha\\)s as treatment effects. In the Netflix challenge papers, they refer to them as bias.\nNote that this model has 263 user effect parameters.\nWe can again use least squares to estimate the \\(\\alpha_i\\) in the following way:\n\nfit &lt;- lm(rating ~ userId, data = train_set)\n\nNote that because there are hundreds of \\(\\alpha_i\\), as each movie gets one, the lm() function will be slow here. In this case, we can show that the least squares estimate \\(\\hat{\\alpha}_i\\) is just the average of \\(y_{i,j} - \\hat{\\mu}\\) for each user \\(i\\). So we can compute them this way:\n\nmu &lt;- mean(train_set$rating)\na &lt;- with(train_set[, .(a = mean(rating - mu)), by = userId], \n          setNames(a, userId))\n\nNote that going forward, in the code, we use a to represent \\(\\alpha\\), b to represent \\(\\beta\\), and we drop the hat notation to represent estimates.\nLet’s see how much our prediction improves once we use \\(\\hat{y}_{i,j} = \\hat{\\mu} + \\hat{\\alpha}_i\\). Because we know ratings can’t be below 0.5 or above 5, we define the function clamp:\n\nclamp &lt;- function(x, min = 0.5, max = 5) pmax(pmin(x, max), min)\n\nto keep predictions in that range and then compute the RMSE:\n\nresid &lt;- with(test_set, rating - clamp(mu + a[userId]))\nrmse(resid)\n#&gt; [1] 0.958\n\nThe RMSE is reduced so we already see an improvement. But can we make it better?\n\n23.1.5 Movie effects\nWe know from experience that some movies are just generally rated higher than others. We can use a linear model with a treatment effect \\(\\beta_j\\) for each movie, which can be interpreted as movie effect or the difference between the average ranking for movie \\(j\\) and the overall average \\(\\mu\\):\n\\[\nY_{i,j} = \\mu + \\alpha_i + \\beta_j +\\varepsilon_{i,j}\n\\]\nWe can again use least squares to estimate the \\(b_i\\) in the following way:\n\nfit &lt;- lm(rating ~ userId + movieId, data = train_set)\n\nHowever, this code generates a very large matrix with all the indicator variables needed to represent all 263 users and 8133 movies and the code will take time to run. So instead, for illustrative purposes, we use an approximation used by the wining team: first computing the least square estimate \\(\\hat{\\mu}\\) and \\(\\hat{\\alpha}_i\\), and then estimating \\(\\hat{\\beta}_j\\) as the average of the residuals \\(y_{i,j} - \\hat{\\mu} - \\hat{\\alpha}_i\\):\n\nmu &lt;- mean(train_set$rating)\na &lt;- with(train_set[, .(a = mean(rating - mu)), by = userId], \n          setNames(a, userId))\nb &lt;- with(train_set[, .(b = mean(rating - mu - a[userId])), by = movieId], \n          setNames(b, movieId))\n\nWe can now construct predictors with this code and see that our RMSE improves.\n\nresid &lt;- with(test_set, rating - clamp(mu + a[userId] + b[movieId]))\nrmse(resid)\n#&gt; [1] 0.911\n\n\n23.1.6 Penalized least squares\nIf we look at the top movies based on our estimates of the movie effect \\(\\hat{\\beta}_j\\), we find that they are all obscure movies, with just one rating:\n\ntop_movies &lt;- names(b[b == max(b)])\ntrain_set[, .(n = .N), by = .(movieId, title)][movieId %in% top_movies]\n#&gt;    movieId                                         title     n\n#&gt;     &lt;char&gt;                                        &lt;char&gt; &lt;int&gt;\n#&gt; 1:    1450 Prisoner of the Mountains (Kavkazsky plennik)     1\n#&gt; 2:    1563                         Dream With the Fishes     1\n#&gt; 3:    1819                          Storefront Hitchcock     1\n#&gt; 4:    3892                            Anatomy (Anatomie)     1\n#&gt; 5:    4076                                     Two Ninas     1\n#&gt; 6:    4591                               Erik the Viking     1\n#&gt; 7:    4796                         Grass Is Greener, The     1\n#&gt; 8:    5427                                       Caveman     1\n\nDo we really think these are the top movies in our database?\nNote that only one of these top rated movies appears in our test set and it received the worst rating:\n\ntest_set[movieId %in% top_movies, .(movieId, title, rating)]\n#&gt;    movieId              title rating\n#&gt;     &lt;char&gt;             &lt;char&gt;  &lt;num&gt;\n#&gt; 1:    3892 Anatomy (Anatomie)      1\n\nLarge positive or negative estimates should be treated with caution when they are based on just a few ratings. These estimates are more likely to be noisy. And since large prediction errors can increase our Root Mean Squared Error (RMSE), it’s better to be conservative when we are uncertain.\nIn earlier chapters, we handled uncertainty by using standard errors and confidence intervals. But when making predictions, we don’t get to give a range, we must pick a single number. That’s where regularization comes in.\nRegularization is a technique that helps us make more stable predictions when dealing with small sample sizes. It does this by shrinking large estimates toward zero, especially when they are based on only a few data points. In this way, regularization has similarities to the Bayesian shrinkage ideas we discussed in Chapter 10.\nAs an example, suppose the overall average movie rating is \\(\\mu = 3\\). Now imagine:\n\nMovie 1 has 100 user ratings.\nMovies 2, 3, 4, and 5 each have only 1 user rating.\n\nIf we estimate movie effects using least squares, we subtract \\(\\mu\\) from each movie’s average rating. For Movie 1, this average is based on 100 users, so it’s a pretty reliable estimate. But for Movies 2-5, we are relying on a single rating, which is much less reliable. These one-off ratings might look accurate now, but there is a good chance they won’t generalize well to new users.\nIn fact, for Movies 2–5, it might be safer to ignore the single rating and simply assume they are average movies. This cautious approach helps prevent extreme predictions based on limited data. By shrinking these uncertain estimates toward the overall average, we make the results more regularm hence the term regularization. This strategy often leads to smaller errors when predicting new data, ultimately helping reduce the RMSE.\n\n23.1.7 A penalized approach\nThe most common way to regularize predictions is through penalized regression, which controls how much the movie effects \\(\\beta_j\\) are allowed to vary. Instead of minimizing only the sum of squared errors, we modify the objective function by adding a penalty term that discourages large values of \\(\\beta_j\\), especially when these estimates are based on limited data. Specifically, we redefine the quantity we minimize by adding a penalty to the residual sum of squares:\n\\[\n\\sum_{i,j} \\left(y_{i,j} - (\\mu + \\alpha_i + \\beta_j) \\right)^2 + \\lambda \\sum_{j} \\beta_j^2\n\\] The first term is just the sum of squares and the second is a penalty that gets larger when many \\(\\beta_j\\)s are large.\nIn this particular model, using calculus, we can actually show that, if we know \\(\\mu\\) and the \\(\\alpha_i\\)s, the values of \\(\\beta_j\\) that minimize this equation are:\n\\[\n\\hat{\\beta}_j(\\lambda) = \\frac{1}{n_j + \\lambda} \\sum_{i=1}^{n_i} \\left(y_{i,j} - \\mu - \\alpha_i\\right)\n\\]\nwhere \\(n_j\\) is the number of ratings made for movie \\(j\\).\nLet’s examine what happens when we set the penality term \\(\\lambda\\) to 4 and we plug our previously calcualted estimates \\(\\hat{\\mu}\\) and \\(\\hat{\\alpha}_i\\) to estimate obtain a regularized estimate \\(\\hat{\\beta}_j(\\lambda)\\).\n\nlambda &lt;- 4\nmu &lt;- mean(train_set$rating)\na &lt;- with(train_set[, .(a = mean(rating - mu)), by = userId], \n          setNames(a, userId))\ntmp &lt;- train_set[, .(b = sum(rating - mu - a[userId])/(.N + lambda)), by = movieId]\nb_reg &lt;- with(tmp, setNames(b, movieId))\n\nTo see how the estimates shrink, let’s make a plot of the regularized estimates versus the least squares estimates.\n\n\n\n\n\n\n\n\nWe see that some of the largest movie effects estimates come from movies with few ratings and that regularization shrinks these towards 0.\nNow, let’s look at the top 5 best movies based on the penalized estimates \\(\\hat{b}_i(\\lambda)\\):\n\ntop_movies &lt;- names(sort(-b_reg)[1:5])\ntrain_set[, .(n = .N), by = .(movieId, title)][movieId %in% top_movies]\n#&gt;    movieId                     title     n\n#&gt;     &lt;char&gt;                    &lt;char&gt; &lt;int&gt;\n#&gt; 1:     858            Godfather, The   107\n#&gt; 2:     318 Shawshank Redemption, The   138\n#&gt; 3:    1276            Cool Hand Luke    32\n#&gt; 4:    2318                 Happiness    18\n#&gt; 5:      50       Usual Suspects, The   109\n\nThese make more sense with some movies that are watched more and have more ratings in the training set.\nThis approach will have our desired effect: when our sample size \\(n_j\\) is very large, we obtain a stable estimate and the penalty \\(\\lambda\\) is effectively ignored since \\(n_j+\\lambda \\approx n_j\\). Yet when the \\(n_j\\) is small, then the estimate \\(\\hat{\\beta}_i(\\lambda)\\) is shrunken towards 0. The larger the \\(\\lambda\\), the more we shrink.\nThe RMSE also improves:\n\nresid &lt;- with(test_set, rating - clamp(mu + a[userId] + b_reg[movieId]))\nrmse(resid)\n#&gt; [1] 0.89",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#penalized-least-squares-1",
    "href": "highdim/regularization.html#penalized-least-squares-1",
    "title": "23  Regularization",
    "section": "\n23.2 Penalized least squares",
    "text": "23.2 Penalized least squares\nIn the previous section, we motivated penalizing the size of the movie effect estimates to reduce overfitting. This idea generalizes to a very popular approach for fitting linear models called penalized least squares. If you have a multivariate linear model\n\\[\nY_i = \\beta_0 + \\sum_{j=1}^p x_{i,j}\\beta_j + \\varepsilon_i, \\quad i = 1, \\dots, N\n\\] with a large number of parameters \\(p\\), fitting a least squares model can lead to overfitting. As discussed in the previous section, one way to address this issue is to penalize the size of the parameter estimates by using a penalized least squares approach:\n\\[\n\\frac{1}{N}\\sum_{i=1}^N \\left( y_i - \\left(\\beta_0 + \\sum_{j=1}^p x_{i,j}\\beta_j\\right)\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nThe first term is the normalized residual sum of squares, minimized by the lm function in R. We normalize, divide by \\(N\\), so that the interpretation of \\(\\lambda\\) does not depend on data size. The second term is a penalty that increases with the size of the coefficients. The mean level \\(\\beta_0\\) is not penalized.\n\n\n\n\n\n\nThe concept of penalization extends beyond least squares and can be applied to more general models, such as penalized likelihood.\n\n\n\nWe can use calculus and linear algebra to find the \\(\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_p)^\\top\\) that minimizes this expression. The MASS package includes the lm.ridge function to perform this computation. It works similarly to lm, but requires you to specify a \\(\\lambda\\):\n\nlibrary(MASS)\nfit &lt;- lm.ridge(y ~ x, lambda = 0.0001)\n\nThis approach is called ridge regression, which is just another name for least squares with an the penalty term \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\).\nOur movie rating model is also linear, with one parameter for each user and one for each movie. Regularization is especially important in this setting given the large number of parameters. We can compute regularized estimates using:\n\nlibrary(MASS)\nl &lt;- 4/nrow(train_set)\nfit &lt;- lm.ridge(rating ~ userId + movieId, lambda = l, data = train_set)\n\nSimilar to the reason we didn’t run lm, we don’t run this because it’s computationally expensive, userId and movieId are factors that result in over 8,000 indicator variables in the design matrix (see [Section 16.5]).\nIn the next section, we present an approximation that allows fast estimation of the model parameters.\n\n\n\n\n\n\nThe linear model variables \\(x_{i,1},\\dots,x_{i,p}\\) needed to build a movie recommendation system would include hundreds of indicator variables, one for each movie and one for each user. Although we don’t cover the mathematical or computational details in this book, it is important to note that much more efficient algorithms exist than those used by general-purpose functions like lm and lm.ridge. These functions are not optimized for scenarios where most of the variables in each row are zeros, as is the case here. Therefore, we do not recommend using them for problems of this scale and sparsity.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#selecting-penalty-terms",
    "href": "highdim/regularization.html#selecting-penalty-terms",
    "title": "23  Regularization",
    "section": "\n23.3 Selecting Penalty Terms",
    "text": "23.3 Selecting Penalty Terms\nHow do we select $$? In [Chapter 29], we describe formal methods, but here we simply compute RMSE for a range of $$ values. We implement a Alternative Least Square (ALS) type algorithm to estimate \\(\\alpha\\) and \\(\\beta\\). Namely, if because if the \\(\\alpha\\)s are known we have a closed mathematical solution for the penalized lease square etimate of \\(\\beta\\), and similar for the reverse case, alternating iteratively converages to the actual penalized squares estimates.\n\nn_iter &lt;- 10\nlambdas &lt;- seq(0, 0.0002, len = 100)\nN &lt;- nrow(train_set)\nmu &lt;- mean(train_set$rating)\nrmses &lt;- sapply(lambdas, function(lambda){\n  b &lt;- with(train_set[, .(b = 0), by = movieId], setNames(b, movieId))\n  for (iter in 1:n_iter) {\n    a &lt;- with(train_set[, .(a = mean(rating - mu - b[movieId])), by = userId], \n          setNames(a, userId))\n    tmp &lt;- train_set[, .(b = sum(rating - mu - a[userId])/(.N + N*lambda)), by = movieId]\n    b &lt;- with(tmp, setNames(b, movieId))\n  }\n  resid &lt;- with(test_set, rating - clamp(mu + a[userId] + b[movieId]))\n  rmse(resid)\n})\n\nWe then plot RMSE as a function of \\(\\lambda\\):\n\nplot(lambdas, rmses, type = \"l\")\n\n\n\n\n\n\n\nThe minimum RMSE is obtained for:\n\nlambdas[which.min(rmses)]\n#&gt; [1] 3.43e-05\n\nWe use this \\(\\lambda\\) to compute the final regularized estimates:\n\nn_iter &lt;- 10\nlambda &lt;- lambdas[which.min(rmses)]\nN &lt;- nrow(train_set)\nmu &lt;- mean(train_set$rating)\nb &lt;- with(train_set[, .(b = 0), by = movieId], setNames(b, movieId))\nfor (iter in 1:n_iter) {\n  a &lt;- with(train_set[, .(a = mean(rating - mu - b[movieId])), by = userId], \n            setNames(a, userId))\n  tmp &lt;- train_set[, .(b = sum(rating - mu - a[userId])/(.N + N*lambda)), by = movieId]\n  b &lt;- with(tmp, setNames(b, movieId))\n}\n\nAnd make final predictions:\n\nresid &lt;- with(test_set, rating - clamp(mu + a[userId] + b[movieId]))\nrmse(resid)\n#&gt; [1] 0.874\n\nWe can see that regularization improves our RMSE by comparing to the other three approaches, but now using the least square estimates rather than the approximations.\n\n\n\n\nmodel\nRMSE\n\n\n\nJust the mean\n1.043\n\n\nUser effect\n0.958\n\n\nUser + movie effect\n0.893\n\n\nRegularized\n0.874",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#recommended-reading",
    "href": "highdim/regularization.html#recommended-reading",
    "title": "23  Regularization",
    "section": "\n23.4 Recommended reading",
    "text": "23.4 Recommended reading\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning (2nd ed.).\nA highly accessible introduction to statistical learning, including clear explanations and examples of regularization techniques such as ridge regression and the lasso. Available online for free.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.).\nThe authoritative reference on regularization and other machine learning techniques. More theoretical and in-depth, suited for readers looking for a deeper mathematical understanding.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#exercises",
    "href": "highdim/regularization.html#exercises",
    "title": "23  Regularization",
    "section": "\n23.5 Exercises",
    "text": "23.5 Exercises\n1. For the movielens data, compute the number of ratings for each movie and then plot it against the year the movie was released. Use the square root transformation on the counts.\n2. We see that, on average, movies that were released after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.\nAmong movies that came out in 1993 or later, what are the 25 movies with the most ratings per year? Also, report their average rating.\n3. From the table constructed in the previous example, we see that the most rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post 1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.\n4. In the previous exercise, we see that the more a movie is rated, the higher the rating. Suppose you are doing a predictive analysis in which you need to fill in the missing ratings with some value. Which of the following strategies would you use?\n\nFill in the missing values with average rating of all movies.\nFill in the missing values with 0.\nFill in the value with a lower value than the average since lack of rating is associated with lower ratings. Try out different values and evaluate prediction in a test set.\nNone of the above.\n\n5. The movielens dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column date with the date. Hint: Use the as_datetime function in the lubridate package.\n6. Compute the average rating for each week and plot this average against day. Hint: Use the round_date function before you group_by.\n7. The plot shows some evidence of a time effect. If we define \\(d_{u,i}\\) as the day for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate:\n\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i} + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i}\\beta + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i}\\beta_i + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\).\n\n8. The movielens data also has a genres column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots.\n9. The plot shows strong evidence of a genre effect. If we define \\(g_{u,i}\\) as the genre for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate:\n\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i} + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + d_{u,i}\\beta + \\varepsilon_{u,i}\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + \\sum_{k=1}^K x_{u,i} \\beta_k + \\varepsilon_{u,i}\\), with \\(x^k_{u,i} = 1\\) if \\(g_{u,i}\\) is genre \\(k\\).\n\n\\(Y_{u,i} = \\mu + b_i + \\beta_j + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\).\n\nAn education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let’s simulate a dataset for 100 schools. First, let’s simulate the number of students in each school.\n\nset.seed(1986)\nn &lt;- round(2^rnorm(1000, 8, 1))\n\nNow let’s assign a true quality for each school completely independent from size. This is the parameter we want to estimate.\n\nmu &lt;- round(80 + 2 * rt(1000, 5))\nrange(mu)\nschools &lt;- data.frame(id = paste(\"PS\",1:100), \n                      size = n, \n                      quality = mu,\n                      rank = rank(-mu))\n\nWe can see that the top 10 schools are:\n\nschools |&gt; top_n(10, quality) |&gt; arrange(desc(quality))\n\nNow let’s have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points:\n\nscores &lt;- sapply(1:nrow(schools), function(i){\n  scores &lt;- rnorm(schools$size[i], schools$quality[i], 30)\n  scores\n})\nschools &lt;- schools |&gt; mutate(score = sapply(scores, mean))\n\n10. What are the top schools based on the average score? Show just the ID, size, and the average score.\n11. Compare the median school size to the median school size of the top 10 schools based on the score.\n12. According to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercise for the worst 10 schools.\n13. The same is true for the worst schools! They are small as well. Plot the average score versus school size to see what’s going on. Highlight the top 10 schools based on the true quality. Use the log scale transform for the size.\n14. We can see that the standard error of the score has larger variability when the school is smaller. This is a basic statistical reality we learned in the probability and inference sections. In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score.\nLet’s use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools:\n\noverall &lt;- mean(sapply(scores, mean))\n\nand then define, for each school, how it deviates from that average. Write code that estimates the score above average for each school, but dividing by \\(n + \\lambda\\) instead of \\(n\\), with \\(n\\) the school size and \\(\\lambda\\) a regularization parameter. Try \\(\\lambda = 3\\).\n15. Notice that this improves things a bit. The number of small schools that are not highly ranked is now 4. Is there a better \\(\\lambda\\)? Find the \\(\\lambda\\) that minimizes the RMSE = \\(1/100 \\sum_{i=1}^{100} (\\mbox{quality} - \\mbox{estimate})^2\\).\n16. Rank the schools based on the average obtained with the best \\(\\alpha\\). Note that no small school is incorrectly included.\n17. A common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don’t subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from exercise 14, but without removing the overall mean.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/regularization.html#footnotes",
    "href": "highdim/regularization.html#footnotes",
    "title": "23  Regularization",
    "section": "",
    "text": "http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/↩︎\nhttps://grouplens.org/↩︎",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html",
    "href": "highdim/latent-factor-models.html",
    "title": "24  Latent factor models",
    "section": "",
    "text": "24.1 Factor analysis\nIn the previous chapter, we described how the model:\n\\[\nY_{i,j} = \\mu + \\alpha_i + \\beta_j + \\varepsilon_{i,j}\n\\]\ncan be used to model movie ratings and help make useful predictions. This model accounts for user differences through \\(\\alpha_i\\) and movie effects through \\(\\beta_j\\). However, the model ignores an important source of information related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well.\nTo see an example of this, we compute residuals:\n\\[\nr_{i,j} = y_{i,j} - (\\hat{\\mu} + \\hat{\\alpha}_i + \\hat{\\beta}_j)\n\\]\nusing the mu, a and b computed in the previous chapter,\nand see how the residuals for three different movies correlate with The Godfather:\nWe see that the correlation varies from very strong positive to negative across the other three movies.\nIn this chapter, we introduce Latent Factor Models, an approach that captures these correlation patterns and improves prediction accuracy. We present the ideas in the context of movie recommendation systems, where latent factors represent hidden groups of users with similar preferences and movie characteristics. We highlight the connection between factor models and Principal Component Analysis (PCA), showing how both reduce dimensionality by uncovering structure in the data. Finally, we close the chapter by introducing a related technique, the Singular Value Decomposition (SVD), which provides a general mathematical framework underlying both PCA and latent factor methods.\nWe start with a simple simulated example. Specifically, we simulate ratings \\(Y_{i,j}\\) for six movies and 120 users. For simplicity we assume that these ratings have been adjusted for the movie and user effects described in Chapter 23. We stored the results for 120 user and 6 movie effects in the object y:\ndim(y)\n#&gt; [1] 120   6\nIf we examine the correlation between movies based on user ratings, we notice a pattern:\ncor(y)\n#&gt;                      Godfather Godfather 2 Goodfellas Scent of a Woman\n#&gt; Godfather                1.000       0.671      0.558           -0.527\n#&gt; Godfather 2              0.671       1.000      0.471           -0.450\n#&gt; Goodfellas               0.558       0.471      1.000           -0.888\n#&gt; Scent of a Woman        -0.527      -0.450     -0.888            1.000\n#&gt; You've Got Mail         -0.734      -0.649     -0.487            0.451\n#&gt; Sleepless in Seattle    -0.721      -0.739     -0.505            0.475\n#&gt;                      You've Got Mail Sleepless in Seattle\n#&gt; Godfather                     -0.734               -0.721\n#&gt; Godfather 2                   -0.649               -0.739\n#&gt; Goodfellas                    -0.487               -0.505\n#&gt; Scent of a Woman               0.451                0.475\n#&gt; You've Got Mail                1.000                0.756\n#&gt; Sleepless in Seattle           0.756                1.000\nIt appears that ratings are positively correlated within genres, for example, among mob movies or among romance movies, and negatively correlated across the two genres. In statistics, such patterns are often explained by factors: unobserved, or latent, variables that account for the correlations or associations among the observed variables. Under certain assumptions, which we will describe below, these latent factors can be estimated from the data and used to capture the underlying structure driving the correlations.\nOne approach is to use our knowledge of movies genres to define a factor that distinguishes between mob and romance movies:\nq &lt;- c(1, 1, 1, -1, -1, -1)\nWe code mob movies with -1 and romance movies with 1.\nTo quantify each user’s genre preference, we fit a linear model for every user \\(i\\):\np &lt;- apply(y, 1, function(y_i) lm(y_i ~ q - 1)$coef)\nHere, we use -1 in the formula because the residuals have mean 0 and no intercept is needed.\nThe resulting vector p represents, for each user, the difference in average ratings between mob and romance movies. A positive value indicates a preference for mob movies, a negative value indicates a preference for romance movies, and values near 0 suggest no strong preference. The histogram below shows users cluster into these three type:\nhist(p, breaks = seq(-2, 2, 0.1))\nTo see that we can approximate \\(Y_{i,j}\\) with \\(p_iq_j\\) we convert the vectors to matrices and use linear algebra:\np &lt;- matrix(p); q &lt;- matrix(q)\nplot(p %*% t(q), y)\nHowever, after removing this mob/romance effect, we still see structure in the correlation:\ncor(y - p %*% t(q))\n#&gt;                      Godfather Godfather 2 Goodfellas Scent of a Woman\n#&gt; Godfather                1.000       0.185     -0.545            0.557\n#&gt; Godfather 2              0.185       1.000     -0.618            0.594\n#&gt; Goodfellas              -0.545      -0.618      1.000           -0.671\n#&gt; Scent of a Woman         0.557       0.594     -0.671            1.000\n#&gt; You've Got Mail         -0.280      -0.186      0.619           -0.641\n#&gt; Sleepless in Seattle    -0.198      -0.364      0.650           -0.656\n#&gt;                      You've Got Mail Sleepless in Seattle\n#&gt; Godfather                     -0.280               -0.198\n#&gt; Godfather 2                   -0.186               -0.364\n#&gt; Goodfellas                     0.619                0.650\n#&gt; Scent of a Woman              -0.641               -0.656\n#&gt; You've Got Mail                1.000                0.353\n#&gt; Sleepless in Seattle           0.353                1.000\nThis structure seems to be driven by Al Pacino being in the movie or not. This implies we could add another factor in a second column:\nq &lt;- cbind(c(1, 1, 1, -1, -1, -1),\n           c(1, 1, -1, 1, -1, -1))\nWe can then, for each factor, obtain an estimates preference for each user:\np &lt;- t(apply(y, 1, function(y_i) lm(y_i ~ q-1)$coefficient))\nWe use the transpose t because apply binds results into columns and we want a row for each user.\nOur approximation based on two factors does an even better job of predicting how our residuals deviate from 0:\nplot(p %*% t(q), y)\nThis approximation with two factors can be written as:\n\\[\nY_{i,j} \\approx p_{i,1}q_{j,1} + p_{i,2}q_{j,2}, i = 1 \\dots, I \\mbox{ and } j = 1, \\dots, J\n\\] with \\(I\\) the number of users and \\(J\\) the number of movies.\nUsing matrix representation we can rewrite the above question like this:\n\\[\n\\mathbf{Y} \\approx \\mathbf{P}\\mathbf{Q}^\\top\n\\] with \\(\\mathbf{Y}\\) an \\(I\\times J\\) matrix with entries \\(Y_{i,j}\\), \\(\\mathbf{P}\\) a \\(I\\times K\\) matrix with entries \\(p_{i,k}\\), and \\(\\mathbf{Q}\\) a \\(J\\times K\\) matrix with entries \\(q_{j,k}\\).\nThis analysis provides insights into the process generating our data since \\(\\mathbf{P}\\) contains user-specific parameters and \\(\\mathbf{Q}\\) contains movie-specific parameters. The approach is often referred to as matrix factorization because the rating matrix has been factorized into two lower-dimensional, interpretable matrices.\nNote that the apporach also provides compression since the \\(120 \\times 6 = 720\\) observations can be well approximated by a matrix multiplication of a \\(120 \\times 2\\) matrix \\(\\mathbf{P}\\) and a \\(6 \\times 2\\) matrix \\(\\mathbf{Q}\\), a total of 252 parameters.\nIn our example with simulated data, we deduced the factors \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\) from the sample correlation and our knowledge of movies. These ended up working well. However, in general deducing factors is not this easy. Furthermore, factors that provide good approximation might be more complicated than containing just two values. For example, The Godfather III has a romantic subplot so we might not know what value to assign it in q.\nSo, can we estimate the factors? A challenge is that if \\(\\mathbf{P}\\) is unknown, our model is no longer linear: we can’t use lm to estimate both \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\). In the next sections, we describe how PCA can be used to estimate \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\).",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Latent factor models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#sec-factor-analysis",
    "href": "highdim/latent-factor-models.html#sec-factor-analysis",
    "title": "24  Latent factor models",
    "section": "",
    "text": "There is a faster way to make this computation using linear algebra. This is because the lm function is computing the least squares estimates by taking the derivative of the sum of squares, equaling it to 0, and noting the solution \\(\\hat{\\boldsymbol{\\beta}}_i\\) satisfies:\n\\[\n(\\mathbf{q}^\\top\\mathbf{q}) \\, \\hat{\\boldsymbol{\\beta}}_i  = \\mathbf{q}^\\top \\mathbf{y}_i\n\\]\nwith \\(\\mathbf{y}_i\\) column vector represeting the row of y passed to y_i in the apply function. Because \\(\\mathbf{q}\\) does not change for each user, rather than have lm recompute the equation for each user, we can perform the calculation on each row of y to get the \\(\\beta_i\\) for all users \\(i\\) like this:\n\np &lt;- t(qr.solve(crossprod(q)) %*% t(q) %*% t(y))",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Latent factor models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#connection-to-pca",
    "href": "highdim/latent-factor-models.html#connection-to-pca",
    "title": "24  Latent factor models",
    "section": "\n24.2 Connection to PCA",
    "text": "24.2 Connection to PCA\nNotice that in Section 22.5 we learned that if we perform PCA on the matrix \\(\\boldsymbol{\\varepsilon}\\), we obtain a transformation \\(\\mathbf{V}\\) that permits us to rewrite:\n\\[\n\\mathbf{Y} = \\mathbf{Z} \\mathbf{V}^\\top\n\\]\nwith \\(\\mathbf{Z}\\) the matrix of principal components.\nLet’s perform PCA and examine the results:\n\npca &lt;- prcomp(y, center = FALSE)\n\nFirst, notice that the first two PCs explain over 85% of the variability:\n\npca$sdev^2/sum(pca$sdev^2)\n#&gt; [1] 0.6939 0.1790 0.0402 0.0313 0.0303 0.0253\n\nNext, notice that the first column of \\(\\mathbf{V}\\):\n\npca$rotation[,1]\n#&gt;            Godfather          Godfather 2           Goodfellas \n#&gt;                0.306                0.261                0.581 \n#&gt;     Scent of a Woman      You've Got Mail Sleepless in Seattle \n#&gt;               -0.570               -0.294               -0.300\n\nis assigning positive values to the mob movies and negative values to the romance movies.\nThe second column:\n\npca$rotation[,2]\n#&gt;            Godfather          Godfather 2           Goodfellas \n#&gt;                0.354                0.377               -0.382 \n#&gt;     Scent of a Woman      You've Got Mail Sleepless in Seattle \n#&gt;                0.437               -0.448               -0.442\n\nis coding for Al Pacino movies.\nPCA is automatically discovering the structure we inferred using our knowledge of the movies. This is not a coincidence, there is a mathematical connection that explains why PCA aligns with these latent patterns. To see this assume that data \\(\\mathbf{Y}\\) follows the model:\n\\[\nY_{i,j} = \\sum_{k=1}^K p_{i,k}q_{j,k} + \\varepsilon_{i,j}, i=1,\\dots,I, \\, j = 1,\\dots J \\mbox{ or }\n\\mathbf{Y} =  \\mathbf{P}\\mathbf{Q} ^\\top + \\boldsymbol{\\varepsilon}\n\\] with the constraint\n\\[\n\\mathbf{Q}^\\top\\mathbf{Q} = \\mathbf{I}\n\\]\nTo understand why we need this constraint, notice that without it the model is not uniquely defined, it is not identifiable. For example, we can multiply any column of \\(\\mathbf{P}\\) by a constant \\(c &gt; 0\\) and divide the corresponding column of \\(\\mathbf{Q}\\) by the same constant, and the product \\(\\mathbf{P}\\mathbf{Q}^\\top\\) remains unchanged. This constraint removes the scaling ambiguity and ensures that the factorization has a well-defined form.\nThe first \\(K\\) columns of the principal components and the associated rotation matrix provide estimates of \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\), respectively. In other words, PCA can be viewed as a special case of latent factor modeling where the latent factors are chosen to be orthogonal and ordered by the variance they explain. This explains why PCA naturally recovers interpretable patterns, such as genre preferences or actor-specific effects, without us explicitly coding them into the model.\nAnother way to see this connection is through optimization. PCA can be formulated as the solution to a least squares problem: among all possible \\(K\\)-dimensional projections of the data, PCA finds the one that minimizes the reconstruction error, that is, the sum of squared differences between the original data \\(\\mathbf{Y}\\) and its approximation \\(\\mathbf{P}\\mathbf{Q}^\\top\\). In this sense, PCA provides the best rank-\\(K\\) approximation of \\(\\mathbf{Y}\\) in the least squares sense.\nThis dual interpretation—both as a factor model and as a least squares optimizer—highlights why PCA is such a powerful tool for uncovering hidden structure in high-dimensional data: it compresses the data efficiently while also providing factors that capture the dominant sources of variation.\n\n\n\n\n\n\nEven with the orthogonality constraint, the solution is not completely unique.\nThere remains a sign indeterminacy: we can flip the sign of any column in \\(\\mathbf{P}\\) and the corresponding column in \\(\\mathbf{Q}\\) (multiply one by \\(-1\\) and the other by \\(-1\\)) without changing the product \\(\\mathbf{P}\\mathbf{Q}^\\top\\).\nThus, the factorization is identifiable only up to these sign changes.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Latent factor models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#case-study-movie-recommendations",
    "href": "highdim/latent-factor-models.html#case-study-movie-recommendations",
    "title": "24  Latent factor models",
    "section": "\n24.3 Case study: movie recommendations",
    "text": "24.3 Case study: movie recommendations\nIf we examine the correlation structure of the movies we simulated earlier, we again observe clear patterns:\n\n#&gt;                         Goodfellas Godfather: Part II, The\n#&gt; Goodfellas                  1.0000                   0.486\n#&gt; Godfather: Part II, The     0.4856                   1.000\n#&gt; You've Got Mail            -0.2657                  -0.246\n#&gt; Scent of a Woman            0.0789                   0.235\n#&gt; Sleepless in Seattle       -0.4302                  -0.474\n#&gt; Godfather, The              0.4977                   0.837\n#&gt;                         You've Got Mail Scent of a Woman\n#&gt; Goodfellas                       -0.266           0.0789\n#&gt; Godfather: Part II, The          -0.246           0.2348\n#&gt; You've Got Mail                   1.000          -0.3571\n#&gt; Scent of a Woman                 -0.357           1.0000\n#&gt; Sleepless in Seattle              0.415          -0.5178\n#&gt; Godfather, The                   -0.348           0.5479\n#&gt;                         Sleepless in Seattle Godfather, The\n#&gt; Goodfellas                            -0.430          0.498\n#&gt; Godfather: Part II, The               -0.474          0.837\n#&gt; You've Got Mail                        0.415         -0.348\n#&gt; Scent of a Woman                      -0.518          0.548\n#&gt; Sleepless in Seattle                   1.000         -0.441\n#&gt; Godfather, The                        -0.441          1.000\n\nThis suggests that we can improve the predictions from Chapter 23 by leveraging these correlations. For instance, ratings for The Godfather should inform ratings for The Godfather II. But what other patterns might help?\nTo account for interaction such as these we use a latent factor model. Specifically, we extend the model from Chapter 23 to include factors that capture similarities between movies:\n\\[\nY_{i,j} = \\mu + \\alpha_i + \\beta_j + \\sum_{k=1}^K p_{i,k}q_{j,k} + \\varepsilon_{i,j}\n\\]\nRecall that, as explained in the previous chapter, we do not observe all \\((i,j)\\) combinations. Writing the data as an \\(I \\times J\\) matrix \\(\\mathbf{Y}\\), with \\(I\\) users and \\(J\\) movies, would produce many missing values.\nHowever, the sum \\(\\sum_{k=1}^K p_{i,k}q_{j,k}\\) can be expressed as the matrix product \\(\\mathbf{P}\\mathbf{Q}^\\top\\), where \\(\\mathbf{P}\\) is an \\(I \\times K\\) matrix and \\(\\mathbf{Q}\\) is a \\(J \\times K\\) matrix. Estimating all parameters fills in every cell of the \\(I \\times J\\) matrix, giving predictions for all \\((i,j)\\) pairs.\nGiven the large number of parameters and the sparsity of the data, especially for movies with few ratings, it is appropriate to use penalized least squares. We minimize:\n\\[\n\\frac{1}{N}\n\\sum_{i,j} \\left[Y_{i,j} - \\left(\\mu + \\alpha_i + \\beta_j + \\sum_{k=1}^K p_{i,k}q_{j,k}\\right)\\right]^2 +\n\\lambda_1 \\left(\n\\|\\boldsymbol{\\alpha}\\|^2 +\n\\|\\boldsymbol{\\beta}\\|^2\n\\right) +\n\\lambda_2 \\left(\n\\sum_{k=1}^K \\|\\mathbf{p}_k\\|^2+\n\\sum_{k=1}^K \\|\\mathbf{q}_k\\|^2\n\\right)\n\\]\nHere, \\(N\\) is the number of observed ratings, \\(I\\) the number of users, and \\(J\\) the number of movies. The vectors \\(\\boldsymbol{\\alpha}\\) and \\(\\boldsymbol{\\beta}\\) are the user and movie effects, \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_I)^\\top\\) and \\(\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_J)^\\top\\). The vectors \\(\\mathbf{p}_k = (p_{1,k}, \\dots, p_{I,k})^\\top\\) and \\(\\mathbf{q}_k = (q_{1,k}, \\dots, q_{J,k})^\\top\\) are the \\(k\\)-th latent factor components. Recall that \\(|\\boldsymbol{\\alpha}|^2\\) denotes the sum of squares, \\(\\sum_{i=1}^I \\alpha_i^2\\).\nWe use two penalties: \\(\\lambda_1\\) for the linear effects (\\(\\alpha\\)s and \\(\\beta\\)s), and \\(\\lambda_2\\) for the latent factors. This lets us regularize the two components differently, reflecting their distinct roles in the model.\nHow do we estimate the parameters in this model? As mentioned, the challenge comes from the fact that both the \\(p\\)s and \\(q\\)s are unknown and appear multiplied together, making the model nonlinear in its parameters. In earlier sections, we highlighted the connection between factor analysis and Principal Component Analysis (PCA), and showed that in settings with complete data, PCA can be used to estimate the latent factors of a factor analysis model.\nHowever, recommendation systems present a critical complication: the rating matrix is extremely sparse. Most users only rate a small subset of movies, so the data matrix has many missing entries. PCA and related approaches require a fully observed matrix, so they cannot be applied directly in this context. In addition, while PCA provides least squares estimates, here we want to use penalized least squares, which allows us to regularize the parameters and avoid overfitting when the data for some users or movies are limited.\nTo address these challenges, we turn to an iterative optimization method called Alternating Least Squares (ALS). The key idea is to alternately estimate \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\): fix one matrix and solve for the other using penalized least squares, then switch. This alternating approach also extends easily to penalized versions of the model, where user/movie effects and latent factors are regularized separately. For these reasons, ALS has become one of the standard approaches for fitting latent factor models in recommendation systems, where sparsity makes direct application of PCA infeasible.\nThe dslabs package provides the fit_recommender_model function, which implements this approach which we use here:\n\nfit &lt;- with(train_set, fit_recommender_model(rating, userId, movieId, min_ratings = 20, reltol = 1e-5))\n\nWe can now build a prediction for the test set based on the estimated parameters and note that it improves prediction:\n\nresid  &lt;- with(test_set, rating - \n                 clamp(fit$mu + fit$a[as.character(userId)] + fit$b[as.character(movieId)] + \n                 rowSums(fit$p[as.character(userId),] * fit$q[as.character(movieId),])))\nrmse(resid)\n#&gt; [1] 0.86\n\n\n\n\n\n\n\nNote that further gains are possible by optimizing the penalty terms and tuning the number of latent factors. The team from BellKor, who ultimately won the Netflix Prize, had another key insight that further improved prediction: they accounted for the information hidden in the missing ratings. In practice, users tend to avoid movies they expect to dislike, so the absence of a rating is not random but itself carries signal. Incorporating this additional structure into the model helped them achieve state-of-the-art performance.\n\n\n\n\n24.3.1 Visualizing factors\nExamining the estimated movie factors \\(\\mathbf{q}_k\\) reveals that they are interpretable.\nThe estimates for these factors are contained in fit$q. Because some movies do not have enough ratings to stably estimate latent factors, the fit_recommender_model function excludes these movies from the estimation. We therefore obtain the estimated movie factors \\(\\mathbf{q}_k\\) using:\n\nfactors &lt;- fit$q[fit$n_item &gt;= fit$min_ratings,] \n\nTo make interpretation easier, we replace the row names (which are movie IDs) with movie titles:\n\nrownames(factors) &lt;- movielens[match(rownames(factors), movieId)]$title\n\nPlotting the first two factors reveals several insights. As shown below, the movies highlighted in blue demonstrate that mob films cluster together, while romance films without Al Pacino also cluster together:\n\n\n\n\n\n\n\n\nLooking at the movies with the most extreme factor values confirms that the dimensions are interpretable. The first factor separates critically acclaimed films:\n\nnames(sort(factors[,1], decreasing = TRUE)[1:5])\n#&gt; [1] \"2001: A Space Odyssey\"                                               \n#&gt; [2] \"Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb\"\n#&gt; [3] \"Clockwork Orange, A\"                                                 \n#&gt; [4] \"Taxi Driver\"                                                         \n#&gt; [5] \"Silence of the Lambs, The\"\n\nfrom Hollywood blockbusters:\n\nnames(sort(factors[,1])[1:5])\n#&gt; [1] \"Armageddon\"                               \n#&gt; [2] \"Pearl Harbor\"                             \n#&gt; [3] \"Mission: Impossible II\"                   \n#&gt; [4] \"Con Air\"                                  \n#&gt; [5] \"Star Wars: Episode I - The Phantom Menace\"\n\nThe second factor contrasts cult or offbeat films:\n\nnames(sort(factors[,2], decreasing = TRUE)[1:5])\n#&gt; [1] \"Showgirls\"           \"Harold and Maude\"    \"Leaving Las Vegas\"  \n#&gt; [4] \"Bone Collector, The\" \"Scary Movie\"\n\nwith epics:\n\nnames(sort(factors[,2])[1:5])\n#&gt; [1] \"Lord of the Rings: The Two Towers, The\"            \n#&gt; [2] \"Lord of the Rings: The Return of the King, The\"    \n#&gt; [3] \"Lord of the Rings: The Fellowship of the Ring, The\"\n#&gt; [4] \"Dances with Wolves\"                                \n#&gt; [5] \"Matrix, The\"\n\nThese results demonstrate that the latent factors capture meaningful distinctions in film genres and styles, showing how matrix factorization can uncover interpretable structure from sparse rating data.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Latent factor models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#singular-value-decomposition",
    "href": "highdim/latent-factor-models.html#singular-value-decomposition",
    "title": "24  Latent factor models",
    "section": "\n24.4 Singular Value Decomposition",
    "text": "24.4 Singular Value Decomposition\nA related technique often used in latent factor analysis is the Singular Value Decomposition (SVD). It states any \\(N \\times p\\) matrix can be written:\n\\[\n\\mathbf{Y} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^\\top\n\\]\nwhere \\(\\mathbf{U}\\) is an orthogonal \\(N \\times p\\) matrix, \\(\\mathbf{V}\\) an orthogonal \\(p \\times p\\) matrix, and \\(\\mathbf{D}\\) diagonal with \\(d_{1,1} \\geq d_{2,2} \\geq \\dots \\geq d_{p,p}\\). SVD is connected to PCA because \\(\\mathbf{V}\\) provides the rotations for the principal components, while \\(\\mathbf{U}\\mathbf{D}\\) are the principal components themselves. Squaring the diagonal entries of \\(\\mathbf{D}\\) gives the sums of squares:\n\\[\n\\mathbf{U}^\\top \\mathbf{D} \\mathbf{U} = \\mathbf{D}^2\n\\]\nIn R, we can compute the SVD with svd and confirm its relationship to PCA:\n\nx &lt;- matrix(rnorm(1000), 100, 10)\npca &lt;- prcomp(x, center = FALSE)\ns &lt;- svd(x)\n\nall.equal(pca$rotation, s$v, check.attributes = FALSE)\n#&gt; [1] TRUE\nall.equal(pca$sdev^2, s$d^2/(nrow(x) - 1))\n#&gt; [1] TRUE\nall.equal(pca$x, s$u %*% diag(s$d), check.attributes = FALSE)\n#&gt; [1] TRUE\n\nAs an optimization, note that s$u %*% diag(s$d) can be written more efficiently as:\n\nsweep(s$u, 2, s$d, \"*\")",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Latent factor models</span>"
    ]
  },
  {
    "objectID": "highdim/latent-factor-models.html#exercises",
    "href": "highdim/latent-factor-models.html#exercises",
    "title": "24  Latent factor models",
    "section": "\n24.5 Exercises",
    "text": "24.5 Exercises\nIn this exercise set, we use the singular value decomposition (SVD) to estimate factors in an example related to the first application of factor analysis: finding factors related to student performance in school.\nWe construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage points each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:\n\nset.seed(1987)\nn &lt;- 100\nk &lt;- 8\nSigma &lt;- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) \nm &lt;- MASS::mvrnorm(n, rep(0, 3), Sigma)\nm &lt;- m[order(rowMeans(m), decreasing = TRUE),]\ny &lt;- m %x% matrix(rep(1, k), nrow = 1) +\n  matrix(rnorm(matrix(n * k * 3)), n, k * 3)\ncolnames(y) &lt;- c(paste(rep(\"Math\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Science\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Arts\",k), 1:k, sep=\"_\"))\n\nOur goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all simply random independent numbers. Are all students just about as good? Does being good in one subject imply one will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors, we can explain much of the variability in this \\(100 \\times 24\\) dataset.\nYou can visualize the 24 test scores for the 100 students by plotting an image:\n\nmy_image &lt;- function(x, zlim = range(x), ...){\n  colors = rev(RColorBrewer::brewer.pal(9, \"RdBu\"))\n  cols &lt;- 1:ncol(x)\n  rows &lt;- 1:nrow(x)\n  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = \"n\", yaxt = \"n\",\n        xlab=\"\", ylab=\"\",  col = colors, zlim = zlim, ...)\n  abline(h=rows + 0.5, v = cols + 0.5)\n  axis(side = 1, cols, colnames(x), las = 2)\n}\n\nmy_image(y)\n\n1. How would you describe the data based on this figure?\n\nThe test scores are all independent of each other.\nThe students that test well are at the top of the image and there seems to be three groupings by subject.\nThe students that are good at math are not good at science.\nThe students that are good at math are not good at humanities.\n\n2. You can examine the correlation between the test scores directly like this:\n\nmy_image(cor(y), zlim = c(-1,1))\nrange(cor(y))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWhich of the following best describes what you see?\n\nThe test scores are independent.\nMath and science are highly correlated, but the humanities are not.\nThere is high correlation between tests in the same subject, but no correlation across subjects.\nThere is a correlation among all tests, but higher if the tests are in science and math and even higher within each subject.\n\n3. Remember that orthogonality means that \\(U^{\\top}U\\) and \\(V^{\\top}V\\) are equal to the identity matrix. This implies that we can also rewrite the decomposition as:\n\\[ \\mathbf{Y V} = \\mathbf{U D} \\mbox{ or } \\mathbf{U}^{\\top}\\mathbf{Y} = \\mathbf{D V}^{\\top}\\]\nWe can think of \\(\\mathbf{YV}\\) and \\(\\mathbf{U}^{\\top}\\mathbf{V}\\) as two transformations of \\(\\mathbf{Y}\\) that preserve the total variability.\nUse the function svd to compute the SVD of y. This function will return \\(U\\), \\(V\\) and the diagonal entries of \\(D\\).\n\ns &lt;- svd(y)\nnames(s)\n\nYou can check that the SVD works by typing:\n\ny_svd &lt;- sweep(s$u, 2, s$d, FUN = \"*\") %*% t(s$v)\nmax(abs(y - y_svd))\n\nCompute the sum of squares of the columns of \\(Y\\) and store them in ss_y. Then compute the sum of squares of columns of the transformed \\(\\mathbf{YV}\\) and store them in ss_yv. Confirm that sum(ss_y) is equal to sum(ss_yv).\n4. We see that the total sum of squares is preserved. This is because \\(\\mathbf{V}\\) is orthogonal. Now to start understanding how \\(\\mathbf{YV}\\) is useful, plot ss_y against the column number and then do the same for ss_yv. What do you observe?\n5. We see that the variability of the columns of \\(\\mathbf{YV}\\) is decreasing. Furthermore, we see that, relative to the first three, the variability of the columns beyond the third is almost 0. Now notice that we didn’t have to compute ss_yv because we already have the answer. How? Remember that \\(\\mathbf{YV} = \\mathbf{UD}\\) and because \\(\\mathbf{U}\\) is orthogonal, we know that the sum of squares of the columns of \\(\\mathbf{UD}\\) are the diagonal entries of \\(\\mathbf{D}\\) squared. Confirm this by plotting the square root of ss_yv versus the diagonal entries of \\(\\mathbf{D}\\).\n6. From the above we know that the sum of squares of the columns of \\(\\mathbf{Y}\\) (the total sum of squares) add up to the sum of s$d^2, and that the transformation \\(\\mathbf{YV}\\) gives us columns with sums of squares equal to s$d^2. Now compute what percent of the total variability is explained by just the first three columns of \\(\\mathbf{YV}\\).\n7. We see that almost 99% of the variability is explained by the first three columns of \\(\\mathbf{YV}  = \\mathbf{UD}\\). So we get the sense that we should be able to explain much of the variability and structure we found while exploring the data with a few columns. Before we continue, let’s show a useful computational trick to avoid creating the matrix diag(s$d). To motivate this, we note that if we write \\(\\mathbf{U}\\) out in its columns \\([\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_p]\\), then \\(\\mathbf{UD}\\) is equal to:\n\\[\n\\mathbf{UD} = [\\mathbf{u}_1 d_{1,1}, \\mathbf{u}_2 d_{2,2}, \\dots, \\mathbf{u}_p d_{p,p}]\n\\]\nUse the sweep function to compute \\(UD\\) without constructing diag(s$d) and without using matrix multiplication.\n8. We know that \\(\\mathbf{u}_1 d_{1,1}\\), the first column of \\(\\mathbf{UD}\\), has the most variability of all the columns of \\(\\mathbf{UD}\\). Earlier we saw an image of \\(Y\\):\n\nmy_image(y)\n\nin which we can see that the student to student variability is quite large and that it appears that students that are good in one subject are good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student and plot it against \\(\\mathbf{u}_1 d_{1,1}\\), and describe what you find.\n9. We note that the signs in SVD are arbitrary because:\n\\[ \\mathbf{U D V}^{\\top} = (-\\mathbf{U}) D (-\\mathbf{V})^{\\top} \\]\nWith this in mind, we see that the first column of \\(\\mathbf{UD}\\) is almost identical to the average score for each student except for the sign.\nThis implies that multiplying \\(\\mathbf{Y}\\) by the first column of \\(\\mathbf{V}\\) must be performing a similar operation to taking the average. Make an image plot of \\(\\mathbf{V}\\) and describe the first column relative to others and how this relates to taking an average.\n10. We already saw that we can rewrite \\(UD\\) as:\n\\[\n\\mathbf{u}_1 d_{1,1} + \\mathbf{u}_2 d_{2,2} + \\dots + \\mathbf{u}_p d_{p,p}\n\\]\nwith \\(\\mathbf{u}_j\\) the j-th column of \\(\\mathbf{U}\\). This implies that we can rewrite the entire SVD as:\n\\[\\mathbf{Y} = \\mathbf{u}_1 d_{1,1} \\mathbf{v}_1 ^{\\top} + \\mathbf{u}_2 d_{2,2} \\mathbf{v}_2 ^{\\top} + \\dots + \\mathbf{u}_p d_{p,p} \\mathbf{v}_p ^{\\top}\\]\nwith \\(\\mathbf{V}_j\\) the jth column of \\(\\mathbf{V}\\). Plot \\(\\mathbf{u}_1\\), then plot \\(\\mathbf{v}_1^{\\top}\\) using the same range for the y-axis limits. Then make an image of \\(\\mathbf{u}_1 d_{1,1} \\mathbf{v}_1 ^{\\top}\\) and compare it to the image of \\(\\mathbf{Y}\\). Hint: Use the my_image function defined above and use the drop=FALSE argument to assure the subsets of matrices are matrices.\n11. We see that with just a vector of length 100, a scalar, and a vector of length 24, we actually come close to reconstructing the original \\(100 \\times 24\\) matrix. This is our first matrix factorization:\n\\[\n\\mathbf{Y} \\approx d_{1,1} \\mathbf{u}_1 \\mathbf{v}_1^{\\top}\n\\] We know it explains s$d[1]^2/sum(s$d^2) * 100 percent of the total variability. Our approximation only explains the observation that good students tend to be good in all subjects. But another aspect of the original data that our approximation does not explain was the higher similarity we observed within subjects. We can see this by computing the difference between our approximation and original data and then computing the correlations. You can see this by running this code:\n\nresid &lt;- y - with(s,(u[,1, drop=FALSE]*d[1]) %*% t(v[,1, drop=FALSE]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nNow that we have removed the overall student effect, the correlation plot reveals that we have not yet explained the within subject correlation nor the fact that math and science are closer to each other than to the arts. So let’s explore the second column of the SVD. Repeat the previous exercise but for the second column: Plot \\(\\mathbf{u}_2\\), then plot \\(\\mathbf{v}_2^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(\\mathbf{u}_2 d_{2,2} \\mathbf{v}_2 ^{\\top}\\) and compare it to the image of resid.\n12. The second column clearly relates to a student’s difference in ability in math/science versus the arts. We can see this most clearly from the plot of s$v[,2]. Adding the matrix we obtain with these two columns will help with our approximation:\n\\[\n\\mathbf{Y} \\approx d_{1,1} \\mathbf{u}_1 \\mathbf{v}_1^{\\top} + d_{2,2} \\mathbf{u}_2 \\mathbf{v}_2^{\\top}\n\\]\nWe know it will explain:\n\nsum(s$d[1:2]^2)/sum(s$d^2) * 100\n\npercent of the total variability. We can compute new residuals like this:\n\nresid &lt;- y - with(s,sweep(u[,1:2], 2, d[1:2], FUN=\"*\") %*% t(v[,1:2]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nand see that the structure that is left is driven by the differences between math and science. Confirm this by plotting \\(\\mathbf{u}_3\\), then plot \\(\\mathbf{v}_3^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(\\mathbf{u}_3 d_{3,3} \\mathbf{v}_3 ^{\\top}\\) and compare it to the image of resid.\n13. The third column clearly relates to a student’s difference in ability in math and science. We can see this most clearly from the plot of s$v[,3]. Adding the matrix we obtain with these two columns will help with our approximation:\n\\[\n\\mathbf{Y} \\approx d_{1,1} \\mathbf{u}_1 \\mathbf{v}_1^{\\top} + d_{2,2} \\mathbf{u}_2 \\mathbf{v}_2^{\\top} + d_{3,3} \\mathbf{u}_3 \\mathbf{v}_3^{\\top}\n\\]\nWe know it will explain:\n\nsum(s$d[1:3]^2)/sum(s$d^2) * 100\n\npercent of the total variability. We can compute new residuals like this:\n\nresid &lt;- y - with(s,sweep(u[,1:3], 2, d[1:3], FUN=\"*\") %*% t(v[,1:3]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWe no longer see structure in the residuals: they seem to be independent of each other. This implies that we can describe the data with the following model:\n\\[\n\\mathbf{Y} =  \\mathbf{u}_1 \\mathbf{v}_1^{\\top} + d_{2,2} \\mathbf{u}_2 \\mathbf{v}_2^{\\top} + d_{3,3} \\mathbf{u}_3 \\mathbf{v}_3^{\\top} + \\varepsilon\n\\]\nwith \\(\\varepsilon\\) a matrix of independent identically distributed errors. This model is useful because we summarize \\(100 \\times 24\\) observations with \\(3 \\times (100+24+1) = 375\\) numbers. Furthermore, the three components of the model have useful interpretations: 1) the overall ability of a student, 2) the difference in ability between the math/sciences and arts, and 3) the remaining differences between the three subjects. The sizes \\(d_{1,1}, d_{2,2}\\) and \\(d_{3,3}\\) tell us the variability explained by each component. Finally, note that the components \\(d_{j,j} \\mathbf{u}_j \\mathbf{v}_j^{\\top}\\) are equivalent to the jth principal component.\nFinish the exercise by plotting an image of \\(Y\\), an image of \\(d_{1,1} \\mathbf{u}_1 \\mathbf{v}_1^{\\top} + d_{2,2} \\mathbf{u}_2 \\mathbf{v}_2^{\\top} + d_{3,3} \\mathbf{u}_3 \\mathbf{v}_3^{\\top}\\) and an image of the residuals, all with the same zlim.",
    "crumbs": [
      "High dimensional data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Latent factor models</span>"
    ]
  },
  {
    "objectID": "ml/intro-ml.html",
    "href": "ml/intro-ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine learning has achieved remarkable successes in a variety of applications. These range from the postal service’s use of machine learning for reading handwritten zip codes to the development of voice recognition systems like Apple’s Siri. Other significant advances include spam and malware detection, housing price prediction algorithms, and the ongoing development of autonomous vehicles.\nThe field of Artificial Intelligence (AI) has been evolving for several decades. Traditional AI systems, including some chess-playing machines, often relied on decision-making based on preset rules and knowledge representation. However, with the advent of data availability, machine learning has gained prominence. It focuses on decision-making through algorithms trained with data. In recent years, the terms AI and Machine Learning have been used interchangeably in many contexts, though they have distinct meanings. AI broadly refers to systems or applications that exhibit intelligent behavior, encompassing both rule-based approaches and machine learning. Machine Learning specifically involves learning from data to make decisions or predictions.\nIn this part of the book, we will delve into the concepts, ideas, and methodologies of machine learning. We will also demonstrate their practical application, using the example of recognizing handwritten digits, a classic problem that exemplifies the power and utility of machine learning techniques.",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml/notation-and-terminology.html",
    "href": "ml/notation-and-terminology.html",
    "title": "\n25  Notation and terminology\n",
    "section": "",
    "text": "25.1 Terminology\nIn Section 20.2, we introduced the MNIST handwritten digits dataset. Here we describe how the task of automatically reading these digits can be framed as a machine learning challenge. In doing so, we introduce machine learning mathematical notation and terminology used throughout this part of the book.\nOriginally, mail sorting in the post office involved humans reading zip codes written on the envelopes. Today, thanks to machine learning algorithms, a computer can read zip codes and then a robot sorts the letters. We will learn how to build algorithms that can read a digitized handwritten digit.\nIn machine learning, data comes in the form of the outcome we want to predict and the features that we will use to predict the outcome. We build algorithms that take feature values as input and returns a prediction for the outcome when we don’t know the outcome. The machine learning approach is to train an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don’t know the outcome.\nPrediction problems can be divided into categorical and continuous outcomes. For categorical outcomes can be any one of \\(K\\) classes. The number of classes can vary greatly across applications. For example, in the digit reader data, \\(K=10\\) with the classes being the digits 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. In speech recognition, the outcomes are all possible words or phrases we are trying to detect. Spam detection has two outcomes: spam or not spam. In this book, we denote the \\(K\\) categories with indexes \\(k=1,\\dots,K\\). However, for binary data we will use \\(k=0,1\\) for mathematical conveniences that we demonstrate later.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Notation and terminology</span>"
    ]
  },
  {
    "objectID": "ml/notation-and-terminology.html#notation",
    "href": "ml/notation-and-terminology.html#notation",
    "title": "\n25  Notation and terminology\n",
    "section": "\n25.2 Notation",
    "text": "25.2 Notation\nWe will use \\(Y\\) to denote the outcome and \\(X_1, \\dots, X_p\\) to denote the features. These features are also sometimes referred to as predictors or covariates, and we will treat these terms as synonyms.\nThe first step in building a machine learning algorithm is to clearly identify what the outcomes and features are. In Section 20.2, we showed that each digitized image \\(i\\) is associated with a categorical outcome \\(Y_i\\) and a set of features \\(X_{i,1}, \\dots, X_{i,p}\\), with \\(p=784\\). For convenience, we often use boldface notation \\(\\mathbf{X}_i = (X_{i,1}, \\dots, X_{i,p})^\\top\\) to represent the vector of predictors, following the notation introduced in Section 20.1. When referring to an arbitrary set of features rather than a specific image, we drop the index \\(i\\) and simply use \\(Y\\) and \\(\\mathbf{X} = (X_1, \\dots, X_p)\\). We use uppercase to emphasize that these are random variables. Observed values are denoted in lowercase, such as \\(\\mathbf{X} = \\mathbf{x}\\). In practice, when writing code, we typically use lowercase.\nThe machine learning task is to build an algorithm that predicts the outcome given any combination of features. At first glance, this might seem impossible, but we will start with very simple examples and gradually build toward more complex cases. We begin with one predictor, then extend to two predictors, and eventually tackle real-world challenges involving thousands of predictors.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Notation and terminology</span>"
    ]
  },
  {
    "objectID": "ml/notation-and-terminology.html#the-machine-learning-challenge",
    "href": "ml/notation-and-terminology.html#the-machine-learning-challenge",
    "title": "\n25  Notation and terminology\n",
    "section": "\n25.3 The machine learning challenge",
    "text": "25.3 The machine learning challenge\nThe general setup is as follows. We have a series of features and an unknown outcome we want to predict:\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature p\n\n\n?\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(\\dots\\)\n\\(X_p\\)\n\n\n\n\nTo build a model that predicts outcomes from observed features \\(X_1=x_1, X_2=x_2, \\dots, X_p=x_p\\), we need a dataset where the outcomes are known:\n\n\n\n\noutcome\nfeature 1\nfeature 2\nfeature 3\n\\(\\dots\\)\nfeature 5\n\n\n\n\\(y_{1}\\)\n\\(x_{1,1}\\)\n\\(x_{1,2}\\)\n\\(x_{1,3}\\)\n\\(\\dots\\)\n\\(x_{1,p}\\)\n\n\n\\(y_{2}\\)\n\\(x_{2,1}\\)\n\\(x_{2,2}\\)\n\\(x_{2,3}\\)\n\\(\\dots\\)\n\\(x_{2,p}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\\(y_n\\)\n\\(x_{n,1}\\)\n\\(x_{n,2}\\)\n\\(x_{n,3}\\)\n\\(\\dots\\)\n\\(x_{n,p}\\)\n\n\n\n\n\nWhen the outcome is continuous, we refer to the task as prediction. The model returns a function \\(f\\) that produces a prediction \\(\\hat{y} = f(x_1, x_2, \\dots, x_p)\\) for any feature vector. We call \\(y\\) the actual outcome. Predictions \\(\\hat{y}\\) are rarely exact, so we measure accuracy by the error, defined as \\(y - \\hat{y}\\).\nWhen the outcome is categorical, the task is called classification. The model produces a decision rule that prescribes which of the \\(K\\) classes should be predicted. Typically, models output functions \\(f_k(x_1, \\dots, x_p)\\), one for each class \\(k\\). In the binary case, a common rule is: if \\(f_1(x_1, \\dots, x_p) &gt; C\\), predict class 1, otherwise predict class 2, with \\(C\\) a chosen cutoff. Here, predictions are either right or wrong.\nIt is worth noting that terminology varies across textbooks and courses. Sometimes prediction is used for both categorical and continuous outcomes. The term regression is also used for continuous outcomes, but here we avoid it to prevent confusion with linear regression. In most contexts, whether outcomes are categorical or continuous will be clear, so we will simply use prediction or classification as appropriate.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Notation and terminology</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html",
    "href": "ml/evaluation-metrics.html",
    "title": "26  Evaluation metrics",
    "section": "",
    "text": "26.1 Training and test sets\nBefore we start describing approaches to optimize the way we build algorithms, we first need to define what we mean when we say one approach is better than another. In this section, we focus on describing ways in which machine learning algorithms are evaluated. Specifically, we need to quantify what we mean by “better”.\nFor our first introduction to machine learning concepts, we will start with a boring and simple example: how to predict sex using height. As we explain how to build a prediction algorithm with this example, we will start to set down the first building block needed to understand machine learning. Soon enough, we will be undertaking more interesting challenges.\nWe introduce the caret package, which provides useful functions to facilitate machine learning in R, and we describe it in more detail in Section 32.1. For our first example, we use the height data provided by the dslabs package.\nWe start by defining the outcome and predictors:\nIn this case, we have only one predictor, height, and y is clearly a categorical outcome since observed values are either Male or Female. We know that we will not be able to predict \\(Y\\) very accurately based on \\(X\\) because male and female average heights are not that different relative to within group variability. But can we do better than guessing? To answer this question, we need a quantitative definition of “better”.\nUltimately, a machine learning algorithm is evaluated on how it performs in the real world with completely new datasets. However, when developing an algorithm, we usually have a dataset for which we know the outcomes, as we do with the heights: we know the sex of every student in our dataset. Therefore, to mimic the ultimate evaluation process, we typically split the data into two parts and act as if we don’t know the outcome for one of these. We stop pretending we don’t know the outcome to evaluate the algorithm, but only after we are done constructing it. We refer to the group for which we know the outcome, and that we use to develop the algorithm, as the training set. We refer to the group for which we pretend we don’t know the outcome as the test set.\nA standard way of generating the training and test sets is by randomly splitting the data. The caret package includes the function createDataPartition that helps us generate indexes for randomly splitting the data into training and test sets:\nset.seed(2007)\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\nThe argument times is used to define how many random samples of indexes to return, the argument p is used to define what proportion of the data is represented by the index, and the argument list is used to decide if we want the indexes returned as a list or not. We can use the result of the createDataPartition function call to define the training and test sets as follows:\ntest_set &lt;- heights[test_index, ]\ntrain_set &lt;- heights[-test_index, ]\nWe will now develop an algorithm using only the training set. Once we are done developing the algorithm, we will freeze it and evaluate it using the test set. The simplest way to evaluate the algorithm when the outcomes are categorical is by simply reporting the proportion of cases that were correctly predicted in the test set. This metric is usually referred to as overall accuracy.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#overall-accuracy",
    "href": "ml/evaluation-metrics.html#overall-accuracy",
    "title": "26  Evaluation metrics",
    "section": "\n26.2 Overall accuracy",
    "text": "26.2 Overall accuracy\nTo demonstrate the use of overall accuracy, we will build two competing algorithms and compare them.\nLet’s start by developing the simplest possible machine algorithm: guessing the outcome.\n\ny_hat &lt;- sample(c(\"Male\", \"Female\"), length(test_index), replace = TRUE)\n\nNote that we are completely ignoring the predictor and simply guessing the sex.\nIn machine learning applications, it is useful to use factors to represent the categorical outcomes because R functions developed for machine learning, such as those in the caret package, require or recommend that categorical outcomes be coded as factors. So convert y_hat to factors using the factor function:\n\ny_hat &lt;- factor(y_hat, levels = levels(test_set$sex))\n\nThe overall accuracy is simply defined as the overall proportion that is predicted correctly:\n\nmean(y_hat == test_set$sex)\n#&gt; [1] 0.516\n\nNot surprisingly, our accuracy is about 50%. We are guessing!\nCan we do better? Exploratory data analysis suggests we can because, on average, males are slightly taller than females:\n\nlibrary(tidyverse)\nheights |&gt; group_by(sex) |&gt; summarize(avg = mean(height), sd = sd(height))\n#&gt; # A tibble: 2 × 3\n#&gt;   sex      avg    sd\n#&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Female  64.9  3.76\n#&gt; 2 Male    69.3  3.61\n\nBut how do we make use of this insight? Let’s try another simple approach: predict Male if height is within two standard deviations from the average male.\n\ny_hat &lt;- factor(ifelse(x &gt; 62, \"Male\", \"Female\"), levels(test_set$sex))\n\nThe accuracy goes up from 0.50 to about 0.80:\n\nmean(y == y_hat)\n#&gt; [1] 0.793\n\nBut can we do even better? In the example above, we used a cutoff of 62, but we can examine the accuracy obtained for other cutoffs and then pick the value that provides the best results. But remember, it is important that we optimize the cutoff using only the training set: the test set is only for evaluation. Although for this simplistic example it is not much of a problem, later we will learn that evaluating an algorithm on the training set can lead to overfitting, which often results in over-optimistic assessments.\nHere we examine the accuracy of 10 different cutoffs and pick the one yielding the best result:\n\ncutoff &lt;- seq(61, 70)\naccuracy &lt;- sapply(cutoff, function(x){\n  y_hat &lt;- factor(ifelse(train_set$height &gt; x, \"Male\", \"Female\"), levels = levels(test_set$sex))\n  mean(y_hat == train_set$sex)\n})\n\nWe can make a plot showing the accuracy obtained on the training set for males and females:\n\n\n\n\n\n\n\n\nWe see that the maximum value is:\n\nmax(accuracy)\n#&gt; [1] 0.85\n\nwhich is much higher than 0.5. The cutoff resulting in this accuracy is:\n\nbest_cutoff &lt;- cutoff[which.max(accuracy)]\nbest_cutoff\n#&gt; [1] 64\n\nWe can now test this cutoff on our test set to make sure our accuracy is not overly optimistic:\n\ny_hat &lt;- factor(ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\"), levels = levels(test_set$sex))\ny_hat &lt;- factor(y_hat)\nmean(y_hat == test_set$sex)\n#&gt; [1] 0.804\n\nWe see that it is a bit lower than the accuracy observed for the training set, but it is still better than guessing. And by testing on a dataset that we did not train on, we know our result is not due to cherry-picking a good result.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#the-confusion-matrix",
    "href": "ml/evaluation-metrics.html#the-confusion-matrix",
    "title": "26  Evaluation metrics",
    "section": "\n26.3 The confusion matrix",
    "text": "26.3 The confusion matrix\nThe prediction rule we developed in the previous section predicts Male if the student is taller than 64 inches. Given that the average female is about 64 inches, this prediction rule seems wrong. What happened? If a student is the height of the average female, shouldn’t we predict Female?\nGenerally speaking, overall accuracy can be a deceptive measure. To see this, we will start by constructing what is referred to as the confusion matrix, which basically tabulates each combination of prediction and actual value. We can do this in R simply using table(predicted = y_hat, actual = test_set$sex), but the confusionMatrix function in the caret package computes the confusion matrix and much more:\n\ncm &lt;- confusionMatrix(data = y_hat, reference = test_set$sex)\ncm$table\n#&gt;           Reference\n#&gt; Prediction Female Male\n#&gt;     Female     48   32\n#&gt;     Male       71  374\n\nIf we study this table closely, it reveals a problem. If we compute the accuracy separately for each sex, we get (in the next section, we explain that, in this contxt, sensitivity and specificity a are equivalent to accuracy for females and males, respectively):\n\ncm$byClass[c(\"Sensitivity\", \"Specificity\")]\n#&gt; Sensitivity Specificity \n#&gt;       0.403       0.921\n\nWe notice an imbalance: too many females are predicted to be male. We are calling almost half of the females male! How can our overall accuracy be so high then? This is because the prevalence of males in this dataset is high. These heights were collected from three data sciences courses, two of which had higher male enrollment:\n\ncm$byClass[\"Prevalence\"]\n#&gt; Prevalence \n#&gt;      0.227\n\nSo when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men. This type of bias can actually be a big problem in practice. If your training data is biased in some way, you are likely to develop algorithms that are biased as well. The fact that we used a test set does not matter because it is also derived from the original biased dataset. This is one of the reasons we look at metrics other than overall accuracy when evaluating a machine learning algorithm.\nThere are several metrics that we can use to evaluate an algorithm in a way that prevalence does not cloud our assessment, and these can all be derived from the confusion matrix. A general improvement to using overall accuracy is to study sensitivity and specificity separately.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#sec-senistivity-and-specificity",
    "href": "ml/evaluation-metrics.html#sec-senistivity-and-specificity",
    "title": "26  Evaluation metrics",
    "section": "\n26.4 Sensitivity and specificity",
    "text": "26.4 Sensitivity and specificity\nTo define sensitivity and specificity, we need a binary outcome. When outcomes are categorical, we can still use these terms by focusing on one specific category of interest. For example, in a digit recognition task, we might ask: What is the specificity of correctly identifying the digit 2 as opposed to any other digit? Once we choose a specific category, we treat observations in that category as positive cases (\\(Y=1\\)) and all others as negative cases (\\(Y=0\\)). This binary framing allows us to compute sensitivity and specificity in the usual way.\nIn general, sensitivity is defined as the ability of an algorithm to predict a positive outcome when the actual outcome is positive: \\(\\hat{Y}=1\\) when \\(Y=1\\). Because an algorithm that calls everything positive (\\(\\hat{Y}=1\\) no matter what) has perfect sensitivity, this metric on its own is not enough to judge an algorithm. For this reason, we also examine specificity, which is generally defined as the ability of an algorithm to predict a negative \\(\\hat{Y}=0\\) when the actual outcome is a negative \\(Y=0\\). We can summarize in the following way:\n\nHigh sensitivity: \\(Y=1 \\implies \\hat{Y}=1\\)\n\nHigh specificity: \\(Y=0 \\implies \\hat{Y} = 0\\)\n\n\nAlthough the above is often considered the definition of specificity, another way to think of specificity is by the proportion of positive calls that are actually positive:\n\nHigh specificity: \\(\\hat{Y}=1 \\implies Y=1\\).\n\nTo provide precise definitions, we name the four entries of the confusion matrix:\n\n\n\n\n\nActually Positive\nActually Negative\n\n\n\nPredicted positive\nTrue positives (TP)\nFalse positives (FP)\n\n\nPredicted negative\nFalse negatives (FN)\nTrue negatives (TN)\n\n\n\n\n\nSensitivity is typically quantified by \\(TP/(TP+FN)\\), the proportion of actual positives (the first column = \\(TP+FN\\)) that are called positives (\\(TP\\)). This quantity is referred to as the true positive rate (TPR) or recall.\nSpecificity is defined as \\(TN/(TN+FP)\\) or the proportion of negatives (the second column = \\(FP+TN\\)) that are called negatives (\\(TN\\)). This quantity is also called the true negative rate (TNR). There is another way of quantifying specificity which is \\(TP/(TP+FP)\\) or the proportion of outcomes called positives (the first row or \\(TP+FP\\)) that are actually positives (\\(TP\\)). This quantity is referred to as positive predictive value (PPV) and also as precision. Note that, unlike TPR and TNR, precision depends on prevalence since higher prevalence implies you can get higher precision even when guessing.\nThe multiple names can be confusing, so we include a table to help us remember the terms. The table includes a column that shows the definition if we think of the proportions as probabilities.\n\n\n\n\n\n\n\n\n\nMeasure of\nName 1\nName 2\nDefinition\nProbability representation\n\n\n\nsensitivity\nTPR\nRecall\n\\(\\frac{\\mbox{TP}}{\\mbox{TP} + \\mbox{FN}}\\)\n\\(\\mathrm{Pr}(\\hat{Y}=1 \\mid Y=1)\\)\n\n\nspecificity\nTNR\n1-FPR\n\\(\\frac{\\mbox{TN}}{\\mbox{TN}+\\mbox{FP}}\\)\n\\(\\mathrm{Pr}(\\hat{Y}=0 \\mid Y=0)\\)\n\n\nspecificity\nPPV\nPrecision\n\\(\\frac{\\mbox{TP}}{\\mbox{TP}+\\mbox{FP}}\\)\n\\(\\mathrm{Pr}(Y=1 \\mid \\hat{Y}=1)\\)\n\n\n\nThe caret function confusionMatrix computes all these metrics for us once we define which category is the “positive” (\\(Y=1\\)). The function expects factors as input, and the first level is considered the positive outcome, though it can be redefined with the positive argument. In our example, Female is the first level because it comes before Male alphabetically. If you type this into R, you will see several metrics including accuracy, sensitivity, specificity, and PPV.\nYou can access these directly, for example, like this:\n\ncm$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.804\ncm$byClass[c(\"Sensitivity\",\"Specificity\", \"Prevalence\")]\n#&gt; Sensitivity Specificity  Prevalence \n#&gt;       0.403       0.921       0.227\n\nWe can see that the high overall accuracy is possible despite relatively low sensitivity. As we hinted at above, the reason this happens is because of the low prevalence (0.23): the proportion of females is low. Because prevalence is low, failing to predict actual females as females (low sensitivity) does not lower the overall accuracy as much as failing to predict actual males as males (low specificity). This is an example of why it is important to examine sensitivity and specificity and not just accuracy. Before applying this algorithm to general datasets, we need to ask ourselves if prevalence in the general population will be the same as in our training dataset.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "href": "ml/evaluation-metrics.html#balanced-accuracy-and-f_1-score",
    "title": "26  Evaluation metrics",
    "section": "\n26.5 Balanced accuracy and \\(F_1\\) score",
    "text": "26.5 Balanced accuracy and \\(F_1\\) score\nAlthough we usually recommend studying both specificity and sensitivity, it is often useful to have a one-number summary, for example, for optimization purposes. One metric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as balanced accuracy. Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average. In fact, the \\(F_1\\)-score, a widely used one-number summary, is the harmonic average of precision and recall:\n\\[\n\\frac{1}{\\frac{1}{2}\\left(\\frac{1}{\\mbox{recall}} +\n    \\frac{1}{\\mbox{precision}}\\right) }\n\\]\nBecause it is easier to write, you often see this harmonic average rewritten as:\n\\[\n2 \\times \\frac{\\mbox{precision} \\cdot \\mbox{recall}}\n{\\mbox{precision} + \\mbox{recall}}\n\\]\nwhen defining \\(F_1\\).\nDepending on the context, some types of errors are more costly than others. For instance, in the case of plane safety, it is much more important to maximize sensitivity over specificity: failing to predict a plane will malfunction before it crashes is a much more costly error than grounding a plane when, in fact, the plane is in perfect condition. In a capital murder criminal case, the opposite is true since a false positive can lead to executing an innocent person. The \\(F_1\\)-score can be adapted to weigh specificity and sensitivity differently. To do this, we define \\(\\beta\\) to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average:\n\\[\n\\frac{1}{\\frac{\\beta^2}{1+\\beta^2}\\frac{1}{\\mbox{recall}} +\n    \\frac{1}{1+\\beta^2}\\frac{1}{\\mbox{precision}} }\n\\]\nThe F_meas function in the caret package computes this summary with \\(\\beta\\) defaulting to 1.\nLet’s rebuild our prediction algorithm, but this time maximizing the F-score instead of overall accuracy:\n\ncutoff &lt;- seq(61, 70)\nF_1 &lt;- sapply(cutoff, function(x){\n  y_hat &lt;- ifelse(train_set$height &gt; x, \"Male\", \"Female\")\n  F_meas(data = factor(y_hat, levels(test_set$sex)), \n         reference = factor(train_set$sex))\n})\n\nAs before, we can plot these \\(F_1\\) measures versus the cutoffs:\n\n\n\n\n\n\n\n\nWe see that it is maximized at \\(F_1\\) value of:\n\nmax(F_1)\n#&gt; [1] 0.647\n\nThis maximum is achieved when we use the following cutoff:\n\nbest_cutoff &lt;- cutoff[which.max(F_1)]\nbest_cutoff\n#&gt; [1] 66\n\nA cutoff of 66 makes more sense than 64 because it falls closer to the midpoint between the average heights of males and females. Additionally, it provides a better balance between sensitivity and specificity in the resulting confusion matrix.\n\ny_hat &lt;- ifelse(test_set$height &gt; best_cutoff, \"Male\", \"Female\") |&gt; \n  factor(levels = levels(test_set$sex))\nsensitivity(data = y_hat, reference = test_set$sex)\n#&gt; [1] 0.63\nspecificity(data = y_hat, reference = test_set$sex)\n#&gt; [1] 0.833\n\nWe now see that we do much better than guessing, that both sensitivity and specificity are relatively high.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#prevalence-matters-in-practice",
    "href": "ml/evaluation-metrics.html#prevalence-matters-in-practice",
    "title": "26  Evaluation metrics",
    "section": "\n26.6 Prevalence matters in practice",
    "text": "26.6 Prevalence matters in practice\nA machine learning algorithm with very high TPR and TNR may not be useful in practice when prevalence is close to either 0 or 1. To see this, consider the case of a doctor that specializes in a rare disease and is interested in developing an algorithm for predicting who has the disease.\nThe doctor shares data with about 1/2 cases and 1/2 controls and some predictors. You then develop an algorithm with TPR=0.99 and TNR = 0.99. You are excited to explain to the doctor that this means that if a patient has the disease, the algorithm is very likely to predict correctly. The doctor is not impressed and explains that your TNR is too low for this algorithm to be used in practice. This is because this is a rare disease with a prevalence in the general population of 0.5%. The doctor reminds you of Bayes formula:\n\\[\n\\begin{aligned}\n&\\mathrm{Pr}(Y = 1\\mid \\hat{Y}=1) = \\mathrm{Pr}(\\hat{Y}=1 \\mid Y=1) \\frac{\\mathrm{Pr}(Y=1)}{\\mathrm{Pr}(\\hat{Y}=1)} \\implies\\\\\n&\\text{Precision} = \\text{TPR} \\times \\frac{\\text{Prevalence}}{\\text{TPR}\\times \\text{Prevalence} + \\text{FPR}\\times(1-\\text{Prevalence})} \\approx 0.33  \n\\end{aligned}\n\\]\nHere is plot of precision as a function of prevalence with TPR and TNR both equal to 95%:\n\n\n\n\n\n\n\n\nAlthough your algorithm has a precision of about 95% on the data you train on, with prevalence of 50%, if applied to the general population, the algorithm’s precision would be just 33%. The doctor can’t use an algorithm with 33% of people receiving a positive test actually not having the disease. Note that even if your algorithm had perfect sensitivity, the precision would still be around 33%. So you need to greatly decrease your FPR for the algorithm to be useful in practice.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#roc-and-precision-recall-curves",
    "href": "ml/evaluation-metrics.html#roc-and-precision-recall-curves",
    "title": "26  Evaluation metrics",
    "section": "\n26.7 ROC and precision-recall curves",
    "text": "26.7 ROC and precision-recall curves\nWhen comparing the two methods (guessing versus using a height cutoff), we looked at accuracy and \\(F_1\\). The second method clearly outperformed the first. However, while we considered several cutoffs for the second method, for the first we only considered one approach: guessing with equal probability. Be aware that guessing Male with higher probability would give us higher accuracy due to the bias in the sample. But, as described above, this would come at the cost of lower sensitivity. The curves we describe in this section will help us see this.\nRemember that for each of these parameters, we can get a different sensitivity and specificity. For this reason, a very common approach to evaluating methods is to compare them graphically by plotting both.\nA widely used plot that does this is the receiver operating characteristic (ROC) curve. If you are wondering about this name, consult the ROC Wikipedia page1.\nThe ROC curve plots sensitivity, represented as the TPR, versus 1 - specificity represented as the false positive rate (FPR). Here we compute the TPR and FPR needed for different probabilities of guessing male:\n\nprobs &lt;- seq(0, 1, length.out = 10)\nguessing &lt;- sapply(probs, function(p){\n  y_hat &lt;- \n    sample(c(\"Male\", \"Female\"), nrow(test_set), TRUE, c(p, 1 - p)) |&gt; \n    factor(levels = c(\"Female\", \"Male\"))\n  c(FPR = 1 - specificity(y_hat, test_set$sex),\n    TPR = sensitivity(y_hat, test_set$sex))\n})\n\nWe can use similar code to compute these values for our our second approach. By plotting both curves together, we are able to compare sensitivity for different values of specificity:\n\n\n\n\n\n\n\n\nWe see that we obtain higher sensitivity with the cutoff approach for all values of specificity, which implies it is in fact a better method than guessing. Keep in mind that ROC curves for guessing always fall on the identity line. Also, note that when making ROC curves, it is often nice to add the cutoff associated with each point.\nThe packages pROC and plotROC are useful for generating these plots.\nROC curves have one weakness and it is that neither of the measures plotted depends on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot. The idea is similar, but we instead plot precision against recall:\n\n\n\n\n\n\n\n\nFrom the plot on the left, we immediately see that the precision of guessing is not high. This is because the prevalence is low. From the plot on the right, we also see that if we change \\(Y=1\\) to mean Male instead of Female, the precision increases. Note that the ROC curve would remain the same.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#sec-mse",
    "href": "ml/evaluation-metrics.html#sec-mse",
    "title": "26  Evaluation metrics",
    "section": "\n26.8 Mean Squared Error",
    "text": "26.8 Mean Squared Error\nUp to now we have described evaluation metrics that apply exclusively to categorical data. Specifically, for binary outcomes, we have described how sensitivity, specificity, accuracy, and \\(F_1\\) can be used as quantification. However, these metrics are not useful for continuous outcomes.\nIn this section, we describe how the general approach to defining “best” in machine learning is to define a loss function, which can be applied to both categorical and continuous data.\nThe most commonly used loss function is the squared loss function. If \\(\\hat{y}\\) is our predictor and \\(y\\) is the observed outcome, the squared loss function is simply: \\((\\hat{y} - y)^2\\).\nBecause we often model \\(y\\) as the outcome of a random process, theoretically, it does not make sense to compare algorithms based on \\((\\hat{y} - y)^2\\) as the minimum can change from sample to sample. For this reason, we minimize mean squared error (MSE):\n\\[\n\\text{MSE} \\equiv \\mathrm{E}\\{(\\hat{Y} - Y)^2 \\}\n\\]\nConsider that if the outcomes are binary, the MSE is equivalent to one minus expected accuracy, since \\((\\hat{y} - y)^2\\) is 0 if the prediction was correct and 1 otherwise.\nDifferent algorithms will result in different predictions \\(\\hat{Y}\\), and therefore different MSE. In general, our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible.\nHowever, note that the mean squared error (MSE) is a theoretical quantity that depends on the unknown data-generating process. How do we estimate it in practice? Because we typically have a test set consisting of many independent observations \\(y_1, \\dots, y_N\\), a natural and widely used estimate of the MSE is based on the average of the squared errors over this test set:\n\\[\n\\hat{\\mbox{MSE}} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n\\]\nwith the \\(\\hat{y}_i\\) generated completely independently from the the \\(y_i\\).\nHowever, the estimate \\(\\hat{\\text{MSE}}\\) is a random variable. In fact, \\(\\text{MSE}\\) and \\(\\hat{\\text{MSE}}\\) are often referred to as the true error and apparent error, respectively. Due to the complexity of some machine learning, it is difficult to derive the statistical properties of how well the apparent error estimates the true error. In Chapter 29, we introduce cross-validation an approach to estimating the MSE.\nWe conclude this chapter by noting that squared loss is not the only possible choice of loss function. For instance, the mean absolute error (MAE) replaces squared errors \\((\\hat{Y}_i - Y_i)^2\\) with their absolute values \\(|\\hat{Y}_i - Y_i|\\). Other loss functions are also possible, depending on the context and goals of the analysis. In this book, however, we focus on squared loss, since it is by far the most widely used and provides important mathematical conveniences that simplify both theory and computation.\n\n\n\n\n\n\nIn practice, we often report the root mean squared error (RMSE), which is simply \\(\\sqrt{\\mbox{MSE}}\\), because it is in the same units as the outcomes.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#exercises",
    "href": "ml/evaluation-metrics.html#exercises",
    "title": "26  Evaluation metrics",
    "section": "\n26.9 Exercises",
    "text": "26.9 Exercises\nThe reported_height and height datasets were collected from three classes taught in the Departments of Computer Science and Biostatistics, as well as remotely through the Extension School. The Biostatistics class was taught in 2016 along with an online version offered by the Extension School. On 2016-01-25 at 8:15 AM, during one of the lectures, the instructors asked students to fill in the sex and height questionnaire that populated the reported_height dataset. The online students filled the survey during the next few days, after the lecture was posted online. We can use this insight to define a variable, call it type, to denote the type of student: inclass or online:\n\nlibrary(lubridate)\ndat &lt;- mutate(reported_heights, date_time = ymd_hms(time_stamp)) |&gt;\n  filter(date_time &gt;= make_date(2016, 01, 25) & \n           date_time &lt; make_date(2016, 02, 1)) |&gt;\n  mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & \n                         between(minute(date_time), 15, 30),\n                       \"inclass\", \"online\")) |&gt; select(sex, type)\nx &lt;- dat$type\ny &lt;- factor(dat$sex, c(\"Female\", \"Male\"))\n\n1. Show summary statistics that indicate that type is predictive of sex.\n2. Instead of using height to predict sex, use the type variable.\n3. Show the confusion matrix.\n4. Use the confusionMatrix function in the caret package to report accuracy.\n5. Now use the sensitivity and specificity functions to report specificity and sensitivity.\n6. What is the prevalence (% of females) in the dat dataset defined above?",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/evaluation-metrics.html#footnotes",
    "href": "ml/evaluation-metrics.html#footnotes",
    "title": "26  Evaluation metrics",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Evaluation metrics</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html",
    "href": "ml/conditionals.html",
    "title": "\n27  Conditional probabilities and expectations\n",
    "section": "",
    "text": "27.1 Conditional probabilities\nIn machine learning applications, we rarely can predict outcomes perfectly. For example, spam detectors often miss emails that are clearly spam, Siri often misunderstands the words we are saying, and sometimes your bank thinks your card was stolen when it was not. The most common reason for not being able to build perfect algorithms is that it is impossible. To see this, consider that most datasets will include groups of observations with the same exact observed values for all predictors, but with different outcomes.\nBecause our prediction rules are functions, equal inputs (the predictors) implies equal outputs (the predictions). Therefore, for a challenge in which the same predictors are associated with different outcomes across different individual observations, it is impossible to predict correctly for all these cases. We saw a simple example of this in the previous section: for any given height \\(x\\), you will have both males and females that are \\(x\\) inches tall.\nHowever, none of this means that we can’t build useful algorithms that are much better than guessing, and in some cases better than expert opinions. To achieve this in an optimal way, we make use of probabilistic representations of the problem based on the ideas presented in Section 14.3. Observations with the same observed values for the predictors may not all be the same, but we can assume that they all have the same probability of this class or that class. We will write this idea out mathematically for the case of categorical data.\nWe use the notation \\((X_1 = x_1,\\dots,X_p=x_p)\\) to represent the fact that we have observed values \\(x_1,\\dots,x_p\\) for covariates \\(X_1, \\dots, X_p\\). This does not imply that the outcome \\(Y\\) will take a specific value. Instead, it implies a specific probability. In particular, we denote the conditional probabilities for each class \\(k\\) with:\n\\[\n\\mathrm{Pr}(Y=k \\mid X_1 = x_1,\\dots,X_p=x_p), \\, \\mbox{for}\\,k=1,\\dots,K.\n\\]\nTo avoid writing out all the predictors, we will use bold letters like this: \\(\\mathbf{X} \\equiv (X_1,\\dots,X_p)^\\top\\) and \\(\\mathbf{x} \\equiv (x_1,\\dots,x_p)^\\top\\). We will also use the following notation for the conditional probability of being class \\(k\\):\n\\[\np_k(\\mathbf{x}) = \\mathrm{Pr}(Y=k \\mid \\mathbf{X}=\\mathbf{x}), \\, \\mbox{for}\\, k=1,\\dots,K.\n\\] Notice that the \\(p_k(\\mathbf{x})\\) have to add up to 1 for each \\(\\mathbf{x}\\), so once we know \\(K-1\\), we know all. When the outcome is binary, we only need to know 1, so we drop the \\(k\\) and use the notation \\(p(\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\).\nThese probabilities guide the construction of an algorithm that makes the best prediction: for any given \\(\\mathbf{x}\\), we will predict the class \\(k\\) with the largest probability among \\(p_1(\\mathbf{x}), p_2(\\mathbf{x}), \\dots p_K(\\mathbf{x})\\). In mathematical notation, we write it like this:\n\\[\\hat{y} = \\max_k p_k(\\mathbf{x})\\]\nIn machine learning, we refer to this as Bayes’ Rule. But this is a theoretical rule since, in practice, we don’t know \\(p_k(\\mathbf{x}), k=1,\\dots,K\\). In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. The better our probability estimates \\(\\hat{p}_k(\\mathbf{x})\\), the better our predictor.\nSo how well we predict depends on two things: 1) how close are the \\(\\max_k p_k(\\mathbf{x})\\) to 1 or 0 (perfect certainty) and 2) how close our estimates \\(\\hat{p}_k(\\mathbf{x})\\) are to \\(p_k(\\mathbf{x})\\). We can’t do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities.\nThe first restriction does imply that we have limits as to how well even the best possible algorithm can perform. You should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, with digit readers for example, in others, our success is restricted by the randomness of the process, such as medical diagnosis from biometric data.\nKeep in mind that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context. As discussed in Chapter 26, sensitivity and specificity may differ in importance. But even in these cases, having a good estimate of the \\(p_k(x), k=1,\\dots,K\\) will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. For instance, we can simply change the cutoffs used to predict one outcome or the other. In the plane example, we may ground the plane anytime the probability of malfunction is lower than 1 in a million as opposed to the default 1/2 used when error types are equally undesired.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Conditional probabilities and expectations</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html#conditional-probabilities",
    "href": "ml/conditionals.html#conditional-probabilities",
    "title": "\n27  Conditional probabilities and expectations\n",
    "section": "",
    "text": "Do not be confused by the fact that we use \\(p\\) for two different things: the conditional probability \\(p(\\mathbf{x})\\) and the number of predictors \\(p\\).",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Conditional probabilities and expectations</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html#conditional-expectations",
    "href": "ml/conditionals.html#conditional-expectations",
    "title": "\n27  Conditional probabilities and expectations\n",
    "section": "\n27.2 Conditional expectations",
    "text": "27.2 Conditional expectations\nFor binary data, you can think of the probability \\(\\mathrm{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x})\\) as the proportion of 1s in the stratum of the population for which \\(\\mathbf{X}=\\mathbf{x}\\). Many of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between conditional probabilities and conditional expectations.\nBecause the expectation is the average of values \\(y_1,\\dots,y_n\\) in the population, in the case in which the \\(y\\)s are 0 or 1, the expectation is equivalent to the probability of randomly picking a one since the average is simply the proportion of ones:\n\\[\n\\mathrm{E}(Y \\mid \\mathbf{X}=\\mathbf{x})=\\mathrm{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}).\n\\]\nAs a result, we often only use the expectation to denote both the conditional probability and conditional expectation.\nJust like with categorical outcomes, in most applications the same observed predictors do not guarantee the same continuous outcomes. Instead, we assume that the outcome follows the same conditional distribution. We will now explain why we use the conditional expectation to define our predictors.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Conditional probabilities and expectations</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html#conditional-expectations-minimize-squared-loss-function",
    "href": "ml/conditionals.html#conditional-expectations-minimize-squared-loss-function",
    "title": "\n27  Conditional probabilities and expectations\n",
    "section": "\n27.3 Conditional expectations minimize squared loss function",
    "text": "27.3 Conditional expectations minimize squared loss function\nWhy do we care about the conditional expectation in machine learning? This is because the expected value has an attractive mathematical property: it minimizes the MSE. Specifically, of all possible predictions \\(\\hat{Y}\\),\n\\[\n\\hat{Y} = \\mathrm{E}(Y \\mid \\mathbf{X}=\\mathbf{x}) \\, \\mbox{ minimizes } \\, \\mathrm{E}\\{ (\\hat{Y} - Y)^2  \\mid  \\mathbf{X}=\\mathbf{x} \\}\n\\]\nDue to this property, a succinct description of the main task of machine learning is that we use data to estimate:\n\\[\nf(\\mathbf{x}) \\equiv \\mathrm{E}( Y  \\mid  \\mathbf{X}=\\mathbf{x} )\n\\]\nfor any set of features \\(\\mathbf{x} = (x_1, \\dots, x_p)^\\top\\).\nThis is easier said than done, since this function can take any shape and \\(p\\) can be very large. Consider a case in which we only have one predictor \\(x\\). The expectation \\(\\mathrm{E}(Y  \\mid  X=x )\\) can be any function of \\(x\\): a line, a parabola, a sine wave, a step function, anything. It gets even more complicated when we consider instances with large \\(p\\), in which case \\(f(\\mathbf{x})\\) is a function of a multidimensional vector \\(\\mathbf{x}\\). For example, in our digit reader example \\(p = 784\\)!\nThe main way in which competing machine learning algorithms differ is in their approach to estimating this conditional expectation.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Conditional probabilities and expectations</span>"
    ]
  },
  {
    "objectID": "ml/conditionals.html#exercises",
    "href": "ml/conditionals.html#exercises",
    "title": "\n27  Conditional probabilities and expectations\n",
    "section": "\n27.4 Exercises",
    "text": "27.4 Exercises\n1. Compute conditional probabilities for being Male for the heights dataset. Round the heights to the closest inch. Plot the estimated conditional probability \\(P(x) = \\mathrm{Pr}(\\mbox{Male} | \\mbox{height}=x)\\) for each \\(x\\).\n2. In the plot we just made, we see high variability for low values of height. This is because we have few data points in these strata. This time use the quantile function for quantiles \\(0.1,0.2,\\dots,0.9\\) and the cut function to assure each group has the same number of points. Hint: For any numeric vector x, you can create groups based on quantiles as we demonstrate below.\n\ncut(x, quantile(x, seq(0, 1, 0.1)), include.lowest = TRUE)\n\n3. Generate data from a bivariate normal distribution using the MASS package like this:\n\nSigma &lt;- 9*matrix(c(1,0.5,0.5,1), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 10000, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nYou can make a quick plot of the data using plot(dat). Use an approach similar to the previous exercise to estimate the conditional expectations and make a plot.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Conditional probabilities and expectations</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html",
    "href": "ml/smoothing.html",
    "title": "\n28  Smoothing\n",
    "section": "",
    "text": "28.1 Example: Is it a 2 or a 7?\nBefore continuing with machine learning algorithms, we introduce the important concept of smoothing. Smoothing is a widely used technique across data analysis, also known as curve fitting or low-pass filtering. Its purpose is to uncover underlying trends in noisy data when the exact shape of the trend is unknown. The key idea is to assume that the trend itself is smooth, like a gently curving surface, while the noise consists of unpredictable, irregular fluctuations. By leveraging this smoothness assumption, smoothing methods help us separate the systematic pattern from the random wobbles.\nPart of what we explain in this section are the assumptions that permit us to extract the trend from the noise.\nTo motivate the need for smoothing and make the connection with machine learning, we will construct a simplified version of the MNIST dataset with just two classes for the outcome and two predictors. Specifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the proportion of dark pixels in the upper left quadrant (\\(X_1\\)) and the lower right quadrant (\\(X_2\\)). We also selected a random sample of 1,000 digits, 800 in the training set and 200 in the test set. Both sets are almost evenly distributed with 2s and 7s. We provide this dataset in the mnist_27 object in the dslabs package. For the training data, we have \\(n=800\\) observed outcomes \\(y_1,\\dots,y_n\\), with \\(Y\\) indicating whether the digit is 7 or 2, and \\(n=800\\) features \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\), with each feature a two-dimensional point \\(\\mathbf{x}_i = (x_{i,1}, x_{i,2})^\\top\\).\nTo illustrate how to interpret \\(X_1\\) and \\(X_2\\), we include four example images. On the left are the original images of the two digits with the largest and smallest values for \\(X_1\\) and on the right we have the images corresponding to the largest and smallest values of \\(X_2\\):\nHere is a plot of the observed \\(X_2\\) versus observed \\(X_1\\) with color determining if \\(y\\) is 1 (blue) or 0 (red):\nlibrary(caret)\nlibrary(dslabs)\nmnist_27$train |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point()\nWe can immediately see some patterns. For example, if \\(x_1\\) (the upper left panel) is very large, then the digit is probably a 7. Also, for smaller values of \\(x_1\\), the 2s appear to be in the mid range values of \\(x_2\\).\nWe can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging.\nWe haven’t really learned any algorithms yet, so let’s try building an algorithm using multivariable regression. The model is simply:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid \\mathbf{X}=\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) =\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nWe fit can fit this model using least squares and obtain an estimate \\(\\hat{p}(\\mathbf{x})\\) by using the least square estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\). We define a decision rule by predicting \\(\\hat{y}=1\\) if \\(\\hat{p}(\\mathbf{x})&gt;0.5\\) and 0 otherwise.\nWe get an accuracy of 0.775, well above 50%. Not bad for our first try. But can we do better?\nBecause we constructed the mnist_27 example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the true conditional distribution \\(p(\\mathbf{x})\\). Keep in mind that in practice we don’t have access to the true conditional distribution. We include it in this educational example because it permits the comparison of \\(\\hat{p}(\\mathbf{x})\\) to the true \\(p(\\mathbf{x})\\). This comparison teaches us the limitations of different algorithms.\nWe have stored the true \\(p(\\mathbf{x})\\) in the mnist_27 and can plot it as an image. We draw a curve that separates values of \\(\\mathbf{x}\\) for which \\(p(\\mathbf{x}) &gt; 0.5\\) and those for which \\(p(\\mathbf{x}) &lt; 0.5\\):\nTo start understanding the limitations of regression, first note that with regression \\(\\hat{p}(\\mathbf{x})\\), has to be a plane. As a result the boundary defined by the decision rule is given by: \\(\\hat{p}(\\mathbf{x}) = 0.5\\):\n\\[\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5 \\implies\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = 0.5  \\implies\nx_2 = (0.5-\\hat{\\beta}_0)/\\hat{\\beta}_2  -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\]\nThis implies that for the boundary, \\(x_2\\) is a linear function of \\(x_1\\), which suggests that our regression approach has no chance of capturing the non-linear nature of the true \\(p(\\mathbf{x})\\). Below is a visual representation of \\(\\hat{p}(\\mathbf{x})\\) which clearly shows how it fails to capture the shape of \\(p(\\mathbf{x})\\):\nWe need something more flexible: a method that permits estimates with shapes other than a plane. Smoothing techniques permit this flexibility. We will start by describing nearest neighbor and kernel approaches. To understand why we cover this topic, remember that the concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#signal-plus-noise-model",
    "href": "ml/smoothing.html#signal-plus-noise-model",
    "title": "\n28  Smoothing\n",
    "section": "\n28.2 Signal plus noise model",
    "text": "28.2 Signal plus noise model\nTo explain these concepts, we will focus first on a problem with just one predictor. Specifically, we try to estimate the time trend in the 2008 US popular vote poll margin (the difference between Obama and McCain). Later we will learn how to extend smoothing ideas to higher dimensions.\n\npolls_2008 |&gt; ggplot(aes(day, margin)) + geom_point()\n\n\n\n\n\n\n\nFor the purposes of the popular vote example, do not think of it as a forecasting problem. Instead, we are simply interested in learning the shape of the trend after the election is over.\nWe assume that for any given day \\(x\\), there is a true preference among the electorate \\(f(x)\\), but due to the uncertainty introduced by the polling, each data point comes with an error \\(\\varepsilon\\). A mathematical model for the observed poll margin is:\n\\[\nY_i = f(x_i) + \\varepsilon_i\n\\]\nTo think of this as a machine learning problem, consider that we want to predict \\(Y\\) given a day \\(x\\). If we knew the conditional expectation \\(f(x) = \\mathrm{E}(Y \\mid X=x)\\), we would use it. But since we don’t know this conditional expectation, we have to estimate it. Let’s use regression, since it is the only method we have learned up to now.\n\n\n\n\n\n\n\n\nThe fitted regression line does not appear to describe the trend very well. For example, on September 4 (day -62), the Republican Convention was held and the data suggest that it gave John McCain a boost in the polls. However, the regression line does not capture this potential trend. To see the lack of fit more clearly, we note that points above the fitted line (blue) and those below (red) are not evenly distributed across days. We therefore need an alternative, more flexible approach.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#bin-smoothing",
    "href": "ml/smoothing.html#bin-smoothing",
    "title": "\n28  Smoothing\n",
    "section": "\n28.3 Bin smoothing",
    "text": "28.3 Bin smoothing\nThe general idea of smoothing is to group data points into strata in which the value of \\(f(x)\\) can be assumed to be constant. We can make this assumption when we think \\(f(x)\\) changes slowly and, as a result, \\(f(x)\\) is almost constant in small windows of \\(x\\). An example of this idea for the poll_2008 data is to assume that public opinion remained approximately the same within a week’s time. With this assumption in place, we have several data points with the same expected value.\nIf we fix a day to be in the center of our week, call it \\(x_0\\), then for any other day \\(x\\) such that \\(|x - x_0| \\leq h\\), with \\(h = 3.5\\), we assume \\(f(x)\\) is a constant \\(f(x) = \\mu\\). This assumption implies that:\n\\[\nE[Y_i | X_i = x_i ] \\approx \\mu \\mbox{   if   }  |x_i - x_0| \\leq 3.5\n\\]\nIn smoothing, we call the size of the interval satisfying \\(|x_i - x_0| \\leq 3.5\\) the window size, bandwidth or span. Later we will learn how to optimize this parameter.\nThis assumption implies that a good estimate for \\(f(x_0)\\) is the average of the \\(y_i\\) values in the window. If we define \\(A_0\\) as the set of indexes \\(i\\) such that \\(|x_i - x_0| \\leq 3.5\\) and \\(N_0\\) as the number of indexes in \\(A_0\\), then our estimate is:\n\\[\n\\hat{f}(x_0) = \\frac{1}{N_0} \\sum_{i \\in A_0}  y_i\n\\]\nWe make this calculation with each value of \\(x\\) as the center. In the poll example, for each day, we would compute the average of the values within a week with that day in the center. Here are two examples: \\(x_0 = -125\\) and \\(x_0 = -55\\). The blue segment represents the resulting average.\n\n\n\n\n\n\n\n\nBy computing this mean for every point, we form an estimate of the underlying curve \\(f(x)\\). Below we show the procedure happening as we move from the -155 up to 0. At each value of \\(x_0\\), we keep the estimate \\(\\hat{f}(x_0)\\) and move on to the next point:\n\n\n\n\n\n\n\n\nThe final code and resulting estimate look like this:\n\nspan &lt;- 7 \nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"box\", bandwidth = span))\n\npolls_2008 |&gt; mutate(fit = fit$y) |&gt;\n  ggplot(aes(x = day)) +\n  geom_point(aes(y = margin), size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(y = fit), color = \"red\")",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#kernels",
    "href": "ml/smoothing.html#kernels",
    "title": "\n28  Smoothing\n",
    "section": "\n28.4 Kernels",
    "text": "28.4 Kernels\nThe bin smoother’s estimate can look quite wiggly. A main reason is that as the window slides, points abruptly enter or leave the bin, causing jumps in the average. We can reduce these discontinuities with a kernel smoother. A kernel smoother assigns a weight to each data point according to its distance from the target location \\(x_0\\), then forms a weighted average.\nFormally, let \\(K\\) be a nonnegative kernel function and let \\(h&gt;0\\) be a bandwidth or window width. Define weights\n\\[\nw_{x_0}(x_i) = K\\!\\left(\\frac{x_i - x_0}{h}\\right),\n\\]\nand estimate the trend at \\(x_0\\) with\n\\[\n\\hat{f}(x_0) \\;=\\; \\frac{\\sum_{i=1}^N w_{x_0}(x_i)\\,y_i}{\\sum_{i=1}^N w_{x_0}(x_i)}.\n\\]\nThe bin smoother is a special case with the boxcar (or uniform) kernel \\(K(u) = 1\\) if \\(|u| \\leq 1\\) and 0 otherwise, which corresponds to assigning weight 1 inside the window and 0 outside. This is why, in the code above, we used kernel = \"box\" with ksmooth. To attenuate the wiggles caused by abrupt point entry and exit, we can use a smooth kernel that gives more weight to points near \\(x_0\\) and rapidly decays for distant points. The option kernel = \"normal\" in ksmooth does exactly this by using the standard normal density for \\(K\\).\nBelow we visualize the box and normal kernels for \\(x_0 = -125\\) and \\(h = 3.5\\), showing how the boxcar kernel weighs all in-bin points equally, while the normal kernel downweights points near the edges.\n\n\n\n\n\n\n\n\nThe final code and resulting plot for the normal kernel look like this:\n\nfit &lt;- with(polls_2008, ksmooth(day, margin, kernel = \"normal\", bandwidth = 7))\n\npolls_2008 |&gt; mutate(smooth = fit$y) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") + \n  geom_line(aes(day, smooth), color = \"red\")\n\n\n\n\n\n\n\nNotice that this version looks smoother.\nThere are several functions in R that implement bin smoothers. One example is ksmooth, shown above. In practice, however, we typically prefer methods that use slightly more complex models than fitting a constant. The final result above, for example, is still somewhat wiggly in parts we don’t expect it to be (between -125 and -75, for example). Methods such as loess, which we explain next, improve on this.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#local-weighted-regression-loess",
    "href": "ml/smoothing.html#local-weighted-regression-loess",
    "title": "\n28  Smoothing\n",
    "section": "\n28.5 Local weighted regression (loess)",
    "text": "28.5 Local weighted regression (loess)\nA limitation of the bin smoother approach just described is that we need small windows for the approximately constant assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates \\(\\hat{f}(x)\\). Here we describe how local weighted regression (loess) permits us to consider larger window sizes. To do this, we will use a mathematical result, referred to as Taylor’s theorem, which tells us that if you look closely enough at any smooth function \\(f(x)\\), it will look like a line. To see why this makes sense, consider the curved edges gardeners make using straight-edged spades:\n\n(“Downing Street garden path edge”1 by Flickr user Number 102. CC-BY 2.0 license3.)\nInstead of assuming the function is approximately constant in a window, we assume the function is locally linear. We can consider larger window sizes with the linear assumption than with a constant. Instead of the one-week window, we consider a larger one in which the trend is approximately linear. We start with a three-week window and later consider and evaluate other options:\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) \\mbox{   if   }  |x_i - x_0| \\leq 21\n\\]\nFor every point \\(x_0\\), loess defines a window and fits a line within that window. Here is an example showing the fits for \\(x_0=-125\\) and \\(x_0 = -55\\):\n\n\n\n\n\n\n\n\nThe fitted value at \\(x_0\\) becomes our estimate \\(\\hat{f}(x_0)\\). Below we show the procedure happening as we move from the -155 up to 0:\n\n\n\n\n\n\n\n\nThe final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters:\n\ntotal_days &lt;- diff(range(polls_2008$day))\nspan &lt;- 21/total_days\nfit &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008)\npolls_2008 |&gt; mutate(smooth = fit$fitted) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") +\n  geom_line(aes(day, smooth), color = \"red\")\n\n\n\n\n\n\n\nDifferent spans give us different estimates. We can see how different window sizes lead to different estimates:\n\n\n\n\n\n\n\n\nHere are the final estimates:\n\n\n\n\n\n\n\n\nThere are three other differences between loess and the typical bin smoother.\n1. Rather than keeping the bin size the same, loess keeps the number of points used in the local fit the same. This number is controlled via the span argument, which expects a proportion. For example, if N is the number of data points and span=0.5, then for a given \\(x\\), loess will use the 0.5*N closest points to \\(x\\) for the fit.\n2. When fitting a line locally, loess uses a weighted approach. Basically, instead of using least squares, we minimize a weighted version:\n\\[\n\\sum_{i=1}^N w_0(x_i) \\left[y_i - \\left\\{\\beta_0 + \\beta_1 (x_i-x_0)\\right\\}\\right]^2\n\\]\nHowever, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight:\n\\[\nK(u)= \\left( 1  - |u|^3\\right)^3 \\mbox{ if } |u| \\leq 1 \\mbox{ and } K(u) = 0 \\mbox{ if } |u| &gt; 1\n\\]\nTo define the weights, we denote \\(2h\\) as the window size and define \\(w_0(x_i)\\) as above: \\(w_0(x_i) = K\\left(\\frac{x_i - x_0}{h}\\right)\\).\nThis kernel differs from the Gaussian kernel in that more points get values closer to the max:\n\n\n\n\n\n\n\n\n3. loess has the option of fitting the local model robustly. An iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration. To use this option, we use the argument family=\"symmetric\".\n\n28.5.1 Fitting parabolas\nTaylor’s theorem also tells us that if you look at any mathematical function closely enough, it looks like a parabola. The theorem also states that you don’t have to look as closely when approximating with parabolas as you do when approximating with lines. This means we can make our windows even larger and fit parabolas instead of lines.\n\\[\nE[Y_i | X_i = x_i ] = \\beta_0 + \\beta_1 (x_i-x_0) + \\beta_2 (x_i-x_0)^2 \\mbox{   if   }  |x_i - x_0| \\leq h\n\\]\nYou may have noticed that when we showed the code for using loess, we set degree = 1. This tells loess to fit polynomials of degree 1, a fancy name for lines. If you read the help page for loess, you will see that the argument degree defaults to 2. By default, loess fits parabolas not lines. Here is a comparison of the fitting lines (red dashed) and fitting parabolas (orange solid):\n\ntotal_days &lt;- diff(range(polls_2008$day))\nspan &lt;- 28/total_days\nfit_1 &lt;- loess(margin ~ day, degree = 1, span = span, data = polls_2008)\nfit_2 &lt;- loess(margin ~ day, span = span, data = polls_2008)\n\npolls_2008 |&gt; mutate(smooth_1 = fit_1$fitted, smooth_2 = fit_2$fitted) |&gt;\n  ggplot(aes(day, margin)) +\n  geom_point(size = 3, alpha = .5, color = \"grey\") +\n  geom_line(aes(day, smooth_1), color = \"red\", lty = 2) +\n  geom_line(aes(day, smooth_2), color = \"orange\", lty = 1) \n\n\n\n\n\n\n\nThe degree = 2 gives us more wiggly results. In general, we actually prefer degree = 1 as it is less prone to this kind of noise.\n\n28.5.2 Beware of default smoothing parameters\nThe geom_smooth function in the ggplot2 package supports a variety of smoothing methods. By default, it uses loess or a related method called Generalized Additive Model, the latter if any window of data exceeds 1000 observations. We can request that loess is used using the method function:\n\npolls_2008 |&gt; ggplot(aes(day, margin)) +\n  geom_point() + \n  geom_smooth(method = loess, formula = y ~ x)\n\n\n\n\n\n\n\nBut be careful with default parameters as they are rarely optimal. However, you can conveniently change them:\n\npolls_2008 |&gt; ggplot(aes(day, margin)) +\n  geom_point() + \n  geom_smooth(method = loess, formulat = y ~ x, method.args = list(span = 0.15, degree = 1))",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#sec-smoothing-ml-connection",
    "href": "ml/smoothing.html#sec-smoothing-ml-connection",
    "title": "\n28  Smoothing\n",
    "section": "\n28.6 Connecting smoothing to machine learning",
    "text": "28.6 Connecting smoothing to machine learning\nTo see how smoothing relates to machine learning with a concrete example, consider again our Section 28.1 example. If we define the outcome \\(Y = 1\\) for digits that are seven and \\(Y=0\\) for digits that are 2, then we are interested in estimating the conditional probability:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2).\n\\]\nwith \\(x_1\\) and \\(x_2\\) the two predictors defined in Section 28.1. In this example, the 0s and 1s we observe are “noisy” because for some regions the probabilities \\(p(\\mathbf{x})\\) are not that close to 0 or 1. We therefore need to estimate \\(p(\\mathbf{x})\\). Smoothing is an alternative to accomplishing this. In Section 28.1, we saw that linear regression was not flexible enough to capture the non-linear nature of \\(p(\\mathbf{x})\\), thus smoothing approaches provide an improvement. In Section 29.1, we describe a popular machine learning algorithm, k-nearest neighbors, which is based on the concept of smoothing.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#exercises",
    "href": "ml/smoothing.html#exercises",
    "title": "\n28  Smoothing\n",
    "section": "\n28.7 Exercises",
    "text": "28.7 Exercises\n1. The dslabs package provides the following dataset with mortality counts for Puerto Rico for 2015-2018.\n\nlibrary(dslabs)\nhead(pr_death_counts)\n\nRemove data from before May 2018, then use the loess function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long.\n2. Plot the smooth estimates against day of the year, all on the same plot but with different colors.\n3. Suppose we want to predict 2s and 7s in our mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power. In fact, if we fit a regular logistic regression, the coefficient for x_2 is not significant!\n\nlibrary(dslabs)\nglm(y ~ x_2, family = \"binomial\", data = mnist_27) \n\nPlotting a scatterplot here is not useful since y is binary:\n\nwith(mnist_27$train, plot(x_2, y)\n\nFit a loess line to the data above and plot the results. Notice that there is predictive power, except the conditional probability is not linear.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/smoothing.html#footnotes",
    "href": "ml/smoothing.html#footnotes",
    "title": "\n28  Smoothing\n",
    "section": "",
    "text": "https://www.flickr.com/photos/49707497@N06/7361631644↩︎\nhttps://www.flickr.com/photos/number10gov/↩︎\nhttps://creativecommons.org/licenses/by/2.0/↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Smoothing</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html",
    "href": "ml/resampling-methods.html",
    "title": "29  Resampling methods",
    "section": "",
    "text": "29.1 Motivation with k-nearest neighbors\nIn this chapter, we introduce resampling, one of the most important ideas in machine learning. Here we focus on the conceptual and mathematical aspects. We will describe how to implement resampling methods in practice with the caret package later in Section 32.1. To motivate the concept, we will use the two predictor digits data presented in Section 28.1 and introduce k-nearest neighbors (kNN), to demonstrate the ideas.\nWe are interested in estimating the conditional probability function:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y = 1 \\mid X_1 = x_1 , X_2 = x_2).\n\\]\nas defined in Section 28.6.\nWith k-nearest neighbors (kNN) we estimate \\(p(\\mathbf{x})\\) in a similar way to bin smoothing. First, we define the distance between all observations based on the features. Then, for any point \\(\\mathbf{x}_0\\), we estimate \\(p(\\mathbf{x})\\) by identifying the \\(k\\) nearest points to \\(\\mathbf{x}_0\\) and afterwards taking an average of the \\(y\\)s associated with these points. We refer to the set of points used to compute the average as the neighborhood.\nDue to the connection we described earlier between conditional expectations and conditional probabilities, this gives us \\(\\hat{p}(\\mathbf{x}_0)\\), just like the bin smoother gave us an estimate of a trend. As with bin smoothers, we can control the flexibility of our estimate through the \\(k\\) parameter: larger \\(k\\)s result in smoother estimates, while smaller \\(k\\)s result in more flexible and wiggly estimates.\nTo implement the algorithm, we can use the knn3 function from the caret package. Looking at the help file for this package, we see that we can call it in one of two ways. We will use the first way in which we specify a formula and a data frame. The data frame contains all the data to be used. The formula has the form outcome ~ predictor_1 + predictor_2 + predictor_3 and so on. Therefore, we type y ~ x_1 + x_2. If we are going to use variables in the data frame, we can use the . like this y ~ .. We also need to pick \\(k\\), which is set to k = 5 by default. The final call looks like this:\nlibrary(dslabs)\nlibrary(caret)\nknn_fit &lt;- knn3(y ~ ., data = mnist_27$train, k = 5)\nIn this case, since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance.\nThe predict function for knn3 produces a probability for each class. We can keep the probability of being a 7 as the estimate \\(\\hat{p}(\\mathbf{x})\\) using type = \"prob\". Here we obtain the actual prediction using type = \"class\":\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.815\nWe see that kNN, with the default parameter, already beats regression. To see why this is the case, we plot \\(\\hat{p}(\\mathbf{x})\\) and compare it to the true conditional probability \\(p(\\mathbf{x})\\):\nWe see that kNN adapts better to the non-linear shape of \\(p(\\mathbf{x})\\). However, our estimate has some islands of blue in the red area, which intuitively does not make much sense. We notice that we have higher accuracy in the train set compared to the test set:\ny_hat_knn &lt;- predict(knn_fit, mnist_27$train, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$train$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.856\n\ny_hat_knn &lt;- predict(knn_fit, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.815\nThis is due to what we call over-training.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Resampling methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#over-training",
    "href": "ml/resampling-methods.html#over-training",
    "title": "29  Resampling methods",
    "section": "\n29.2 Over-training",
    "text": "29.2 Over-training\nWith kNN, over-training is at its worst when we set \\(k = 1\\). With \\(k = 1\\), the estimate for each \\(\\mathbf{x}\\) in the training set is obtained with just the \\(y\\) corresponding to that point. In this case, if the \\(x_1\\) and \\(x_2\\) are unique, we will obtain perfect accuracy in the training set because each point is used to predict itself (if the predictors are not unique and have different outcomes for at least one set of predictors, then it is impossible to predict perfectly).\nHere we fit a kNN model with \\(k = 1\\) and confirm we get near perfect accuracy in the training set:\n\nknn_fit_1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 1)\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$train, type = \"class\")\nconfusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[[\"Accuracy\"]]\n#&gt; [1] 0.994\n\nBut in the test set, accuracy is actually worse than what we obtained with regression:\n\ny_hat_knn_1 &lt;- predict(knn_fit_1, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.81\n\nWe can see the over-fitting problem by plotting the decision rule boundaries produced by \\(\\hat{p}(\\mathbf{x})\\):\n\n\n\n\n\n\n\n\nThe estimate \\(\\hat{p}(\\mathbf{x})\\) follows the training data too closely (left). You can see that, in the training set, boundaries have been drawn to perfectly surround a single red point in a sea of blue. Because most points \\(\\mathbf{x}\\) are unique, the prediction is either 1 or 0 and the prediction for that point is the associated label. However, once we introduce the test set (right), we see that many of these small islands now have the opposite color and we end up making several incorrect predictions.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Resampling methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#over-smoothing",
    "href": "ml/resampling-methods.html#over-smoothing",
    "title": "29  Resampling methods",
    "section": "\n29.3 Over-smoothing",
    "text": "29.3 Over-smoothing\nAlthough not as badly as with \\(k=1\\), we saw that with \\(k = 5\\) we also over-trained. Hence, we should consider a larger \\(k\\). Let’s try, as an example, a much larger number: \\(k = 401\\).\n\nknn_fit_401 &lt;- knn3(y ~ ., data = mnist_27$train, k = 401)\ny_hat_knn_401 &lt;- predict(knn_fit_401, mnist_27$test, type = \"class\")\nconfusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;     0.76\n\nThe estimate turns out to be similar to the one obtained with regression:\n\n\n\n\n\n\n\n\nIn this case, \\(k\\) is so large that it does not permit enough flexibility. We call this over-smoothing.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Resampling methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#tuning-parameter",
    "href": "ml/resampling-methods.html#tuning-parameter",
    "title": "29  Resampling methods",
    "section": "\n29.4 Tuning parameter",
    "text": "29.4 Tuning parameter\nIt is very common for machine learning algorithms to require that we set one or more values before fitting the model. A simple example is the choice of \\(k\\) in k-Nearest Neighbors (kNN). In Chapter 30, we will see additional examples. These values are referred to as tuning parameters, and an important part of applying machine learning in practice is choosing them, often called tuning the model.\nSo how do we pick tuning parameters? For instance, how do we decide on the best \\(k\\) in kNN? In principle, we want the value of \\(k\\) that maximizes accuracy or, equivalently, minimizes the expected MSE as defined in Section 26.8. The challenge is that we do not know the true expected error. The goal of resampling methods is to estimate this error for any given algorithm and set of tuning parameters, such as \\(k\\).\nTo see why we need resampling, let’s repeat what we did earlier: compare training and test set accuracy, but now for a range of \\(k\\) values. We can then plot the accuracy estimates for each choice of \\(k\\):\n\n\n\n\n\n\n\n\nFirst, note that the estimate obtained from the training set is generally more optimistic, higher accuracy, than the estimate from the test set, with the gap being larger for smaller values of \\(k\\). This is a classic symptom of overfitting.\nSo should we simply pick the \\(k\\) that maximizes accuracy and report this value? There are two key problems with this approach:\n\nThe accuracy-versus-\\(k\\) plot is quite jagged. We do not expect small changes in \\(k\\) to cause large swings in accuracy or MSE. The jaggedness arises because the accuracy is estimated from a finite sample, making it a random variable.\nAlthough we used the test set to estimate accuracy for each \\(k\\), we also used the same test set to select the best \\(k\\). As a result, the reported minimum test set error is overly optimistic and will not generalize to new, unseen data.\n\nResampling methods provide a principled solution to both problems by reducing variability and ensuring that test data are not used twice—once for evaluation and again for tuning.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Resampling methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#mathematical-description-of-resampling-methods",
    "href": "ml/resampling-methods.html#mathematical-description-of-resampling-methods",
    "title": "29  Resampling methods",
    "section": "\n29.5 Mathematical description of resampling methods",
    "text": "29.5 Mathematical description of resampling methods\nIn the previous section, we introduced kNN as an example to motivate the topic of this chapter. In this particular case, there is just one parameter, \\(k\\), that affects the performance of the algorithm. However, in general, machine algorithms may have multiple parameters so we use the notation \\(\\lambda\\) to represent any set of parameters needed to define a machine learning algorithm. We also introduce notation to distinguish the predictions you get with each set of parameters with \\(\\hat{y}(\\lambda)\\) and the MSE for this choice with \\(\\text{MSE}(\\lambda)\\). Our goal is to find the \\(\\lambda\\) that minimizes \\(\\text{MSE}(\\lambda)\\). Resampling methods help us estimate \\(\\text{MSE}(\\lambda)\\).\nAn intuitive first attempt is the apparent error defined in Section 26.8 and used in the previous section:\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{N}\\sum_{i = 1}^N \\left\\{\\hat{y}_i(\\lambda) - y_i\\right\\}^2\n\\]\nAs noted in the previous section, this estimate is a random error, based on just one test set, with enough variability to affect the choice of the best \\(\\lambda\\) substantially.\nNow imagine a world in which we could obtain data repeatedly, say from new random samples. We could take a very large number \\(B\\) of new samples, split them into training and test sets, and define:\n\\[\n\\frac{1}{B} \\sum_{b=1}^B \\frac{1}{N}\\sum_{i=1}^N \\left\\{\\hat{y}_i^b(\\lambda) - y_i^b\\right\\}^2\n\\]\nwith \\(y_i^b\\) the \\(i\\)th observation in sample \\(b\\) and \\(\\hat{y}_{i}^b(\\lambda)\\) the prediction obtained with the algorithm defined by parameter \\(\\lambda\\) and trained independently of \\(y_i^b\\). The law of large numbers tells us that as \\(B\\) becomes larger, this quantity gets closer and closer to \\(MSE(\\lambda)\\). This is of course a theoretical consideration as we rarely get access to more than one dataset for algorithm development, but the concept inspires the idea behind resampling methods.\nThe general idea is to generate a series of different random samples from the data at hand. There are several approaches to doing this, but all randomly generate several smaller datasets that are not used for training, and instead are used to estimate MSE. Next, we describe cross validation, one of the most widely used resampling methods.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Resampling methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#cross-validation",
    "href": "ml/resampling-methods.html#cross-validation",
    "title": "29  Resampling methods",
    "section": "\n29.6 Cross validation",
    "text": "29.6 Cross validation\nOverall, we are provided a dataset (blue) and we need to build an algorithm, using this dataset, that will eventually be used in completely independent datasets (yellow) that we might not even see.\n\n\n\n\n\n\n\n\nSo to imitate this situation, we start by carving out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a training set (blue) and a test set (red). We will train the entirety of our algorithm, including the choice of parameter \\(\\lambda\\), exclusively on the training set and use the test set only for evaluation purposes.\nWe usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the MSE without fitting an impractical number of models. Typical choices are to use 10%-20% of the data for testing.\n\n\n\n\n\n\n\n\nLet’s reiterate that it is indispensable that we not use the test set at all: not for filtering out rows, not for selecting features, not for anything!\nBut then how do we optimize \\(\\lambda\\)? In cross validation, we achieve this by splitting the training set into two: the training and validation sets.\n\n\n\n\n\n\n\n\nWe will do this many times assuring that the estimates of MSE obtained in each dataset are independent from each other. There are several proposed methods for doing this. Here we describe one of these approaches, K-fold cross validation, in detail to provide the general idea used in all approaches.\n\n29.6.1 K-fold cross validation\nAs a reminder, we are going to imitate the concept used when introducing this version of the MSE:\n\\[\n\\mbox{MSE}(\\lambda) \\approx\\frac{1}{B} \\sum_{b = 1}^B \\frac{1}{N}\\sum_{i = 1}^N \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2\n\\]\nWe want to generate a dataset that can be thought of as independent random sample, and do this \\(B\\) times. The K in K-fold cross validation, represents the number of time \\(B\\). In the illustrations, we are showing an example that uses \\(B = 5\\).\nWe will eventually end up with \\(B\\) samples, but let’s start by describing how to construct the first: we simply pick \\(M = N/B\\) observations at random (we round if \\(M\\) is not a round number) and think of these as a random sample \\(y_1^b, \\dots, y_M^b\\), with \\(b = 1\\). We call this the validation set.\nNow we can fit the model in the training set, then compute the apparent error on the independent set:\n\\[\n\\hat{\\mbox{MSE}}_b(\\lambda) = \\frac{1}{M}\\sum_{i = 1}^M \\left(\\hat{y}_i^b(\\lambda) - y_i^b\\right)^2\n\\]\nAs a reminder, this is just one sample and will therefore return a noisy estimate of the true error. In K-fold cross validation, we randomly split the observations into \\(B\\) non-overlapping sets:\n\n\n\n\n\n\n\n\nNow we repeat the calculation above for each of these sets \\(b = 1,\\dots,B\\) and obtain \\(\\hat{\\mbox{MSE}}_1(\\lambda),\\dots, \\hat{\\mbox{MSE}}_B(\\lambda)\\). Then, for our final estimate, we compute the average:\n\\[\n\\hat{\\mbox{MSE}}(\\lambda) = \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\mbox{MSE}}_b(\\lambda)\n\\]\nand obtain an estimate of our loss. A final step would be to select the \\(\\lambda\\) that minimizes the MSE.\n\n29.6.2 How many folds?\nNow how do we pick the cross validation fold? Large values of \\(B\\) are preferable because the training data better imitates the original dataset. However, larger values of \\(B\\) will have much slower computation time: for example, 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason, the choices of \\(B = 5\\) and \\(B = 10\\) are popular.\nOne way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead, we would just pick \\(B\\) sets of some size at random.\n\n29.6.3 Estimate MSE of our optimized algorithm\nWe have described how to use cross validation to optimize parameters. However, we now have to take into account the fact that the optimization occurred on the training data and we therefore need an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on:\n\n\n\n\n\n\n\n\nWe can actually do cross validation again:\n\n\n\n\n\n\n\n\nand obtain a final estimate of our expected loss. However, note that last cross validation iteration means that our entire compute time gets multiplied by \\(K\\). You will soon learn that fitting each algorithm takes time because we are performing many complex computations. As a result, we are always looking for ways to reduce this time. For the final evaluation, we often just use the one test set.\nOnce we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the optimized parameters.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Resampling methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#boostrap-resampling",
    "href": "ml/resampling-methods.html#boostrap-resampling",
    "title": "29  Resampling methods",
    "section": "\n29.7 Boostrap resampling",
    "text": "29.7 Boostrap resampling\nTypically, cross-validation involves partitioning the original dataset into a training set to train the model and a testing set to evaluate it. With the bootstrap approach, based on the ideas described in Chapter 13, you can create multiple different training datasets via bootstrapping. This method is sometimes called bootstrap aggregating or bagging.\nIn bootstrap resampling, we create a large number of bootstrap samples from the original training dataset. Each bootstrap sample is created by randomly selecting observations with replacement, usually the same size as the original training dataset. For each bootstrap sample, we fit the model and compute the MSE estimate on the observations not selected in the random sampling, referred to as the out-of-bag observations. These out-of-bag observations serve a similar role to a validation set in standard cross-validation.\nWe then average the MSEs obtained in the out-of-bag observations from each bootstrap sample to estimate the model’s performance.\nThis approach is actually the default approach in the caret package. We describe how to implement resampling methods with the caret package in the next chapter.\n\n29.7.1 Comparison of MSE estimates\nIn Section 29.1, we computed an estimate of MSE based just on the provided test set (shown in red in the plot below). Here we show how the cross-validation techniques described above help reduce variability. The green curve below shows the results of applying K-fold cross validation with 10 folds, leaving out 10% of the data for validation. We can see that the variance is reduced substantially. The blue curve is the result of using 100 bootstrap samples to estimate MSE. The variability is reduced even further, but at the cost of a 10 fold increase in computation time.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Resampling methods</span>"
    ]
  },
  {
    "objectID": "ml/resampling-methods.html#exercises",
    "href": "ml/resampling-methods.html#exercises",
    "title": "29  Resampling methods",
    "section": "\n29.8 Exercises",
    "text": "29.8 Exercises\nGenerate a set of random predictors and outcomes like this:\n\nset.seed(1996)\nn &lt;- 1000\np &lt;- 10000\nx &lt;- matrix(rnorm(n * p), n, p)\ncolnames(x) &lt;- paste(\"x\", 1:ncol(x), sep = \"_\")\ny &lt;- rbinom(n, 1, 0.5) |&gt; factor()\n\nx_subset &lt;- x[ ,sample(p, 100)]\n\n1. Because x and y are completely independent, you should not be able to predict y using x with accuracy larger than 0.5. Confirm this by running cross validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model. Hint: use the caret train function. The results component of the output of train shows you the accuracy. Ignore the warnings.\n2. Now instead of a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the \\(y = 1\\) group to those in the \\(y = 0\\) group, for each predictor, using a t-test. You can perform this step as follows:\n\ndevtools::install_bioc(\"genefilter\")\ninstall.packages(\"genefilter\")\nlibrary(genefilter)\ntt &lt;- colttests(x, y)\n\nCreate a vector of the p-values and call it pvals.\n3. Create an index ind with the column numbers of the predictors that were “statistically significantly” associated with y. Use a p-value cutoff of 0.01 to define “statistically significant”. How many predictors survive this cutoff?\n4. Re-run the cross validation but after redefining x_subset to be the subset of x defined by the columns showing “statistically significant” association with y. What is the accuracy now?\n5. Re-run the cross validation again, but this time using kNN. Try out the following grid of tuning parameters: k = seq(101, 301, 25). Make a plot of the resulting accuracy.\n6. In exercises 3 and 4, we see that despite the fact that x and y are completely independent, we were able to predict y with accuracy higher than 70%. We must be doing something wrong then. What is it?\n\nThe function train estimates accuracy on the same data it uses to train the algorithm.\nWe are over-fitting the model by including 100 predictors.\nWe used the entire dataset to select the columns used in the model. This step needs to be included as part of the algorithm. The cross validation was done after this selection.\nThe high accuracy is just due to random variability.\n\n7. Advanced. Re-do the cross validation but this time include the selection step in the cross validation. The accuracy should now be close to 50%.\n8. Load the tissue_gene_expression dataset. Use the train function to predict tissue from gene expression. Use kNN. What k works best?\n9. The createResample function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the mnist_27 dataset like this:\n\nset.seed(1995)\nindexes &lt;- createResample(mnist_27$train$y, 10)\n\nHow many times do 3, 4, and 7 appear in the first re-sampled index?\n10. We see that some numbers appear more than once and others appear no times. This must be so for each dataset to be independent. Repeat the exercise for all the re-sampled indexes.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Resampling methods</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html",
    "href": "ml/algorithms.html",
    "title": "30  Examples of algorithms",
    "section": "",
    "text": "30.1 Logistic regression\nThere are hundreds of machine learning algorithms. Here we provide a few examples spanning rather different approaches. Throughout the chapter, we will be using the two predictor digits data introduced in Section 28.1 to demonstrate how the algorithms work. We focus on the concepts and ideas behind the algorithms using illustrative datasets from the dslabs package.\nLater, in Chapter 32, we show an efficient way to implement these ideas using the caret package.\nIn Section 28.1, we used linear regression to predict classes by fitting the model:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) =\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\] using least squares after assigning numeric values of 0 and 1 to the outcomes \\(y\\), and applied regression as if the data were continuous. An obvious problem with this approach is that \\(\\hat{p}(\\mathbf{x})\\) can be negative and larger than 1:\nfit_lm &lt;- lm(y ~ x_1 + x_2, data = mutate(mnist_27$train,y = ifelse(y == 7, 1, 0)))\nrange(fit_lm$fitted)\n#&gt; [1] -0.22  1.92\nTo avoid this, we can apply the approach described in Section 17.5 that is more appropriate for binary data. We write the model like this:\n\\[\n\\log \\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nWe can then find the maximum likelihood estimates (MLE) of the model parameters and predict using the estimate \\(p(\\mathbf{x})\\) to obtain an accuracy of 0.775. We see that logistic regression performs similarly to regression. This is not surprising given that the estimate of \\(\\hat{p}(\\mathbf{x})\\) looks similar as well:\nJust like regression, the decision rule is a line, a fact that can be corroborated mathematically. Defining \\(g(x) = \\log \\{x/(1-x)\\}\\), we have:\n\\[\ng^{-1}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2) = 0.5 \\implies\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 = g(0.5) = 0 \\implies\nx_2 = -\\hat{\\beta}_0/\\hat{\\beta}_2 -\\hat{\\beta}_1/\\hat{\\beta}_2 x_1\n\\]\nThus, much like with regression, \\(x_2\\) is a linear function of \\(x_1\\). This implies that our logistic regression approach has no chance of capturing the non-linear nature of the true \\(p(\\mathbf{x})\\). We now describe some techniques that estimate the conditional probability in a more flexible way.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Examples of algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#logistic-regression",
    "href": "ml/algorithms.html#logistic-regression",
    "title": "30  Examples of algorithms",
    "section": "",
    "text": "You are ready to do exercises 1 - 11.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Examples of algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#k-nearest-neighbors",
    "href": "ml/algorithms.html#k-nearest-neighbors",
    "title": "30  Examples of algorithms",
    "section": "\n30.2 k-nearest neighbors",
    "text": "30.2 k-nearest neighbors\nWe introduced the kNN algorithm in Section 29.1. In Section 29.7.1, we noted that \\(k=31\\) provided the highest accuracy in the test set. Using \\(k=31\\), we obtain an accuracy 0.825, an improvement over regression. A plot of the estimated conditional probability shows that the kNN estimate is flexible enough and does indeed capture the shape of the true conditional probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou are ready to do exercises 12 - 13.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Examples of algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#generative-models",
    "href": "ml/algorithms.html#generative-models",
    "title": "30  Examples of algorithms",
    "section": "\n30.3 Generative models",
    "text": "30.3 Generative models\nWe have described how, when using squared loss, the conditional expectation provides the best approach to developing a decision rule. In a binary case, the smallest true error we can achieve is determined by Bayes’ rule, which is a decision rule based on the true conditional probability:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y = 1 \\mid \\mathbf{X}=\\mathbf{x})\n\\]\nWe have described several approaches to estimating \\(p(\\mathbf{x})\\). In all these approaches, we estimate the conditional probability directly and do not consider the distribution of the predictors. In machine learning, these are referred to as discriminative approaches.\nHowever, Bayes’ theorem tells us that knowing the distribution of the predictors \\(\\mathbf{X}\\) may be useful. Methods that model the joint distribution of \\(Y\\) and \\(\\mathbf{X}\\) are referred to as generative models (we model how the entire data, \\(\\mathbf{X}\\) and \\(Y\\), are generated). We start by describing the most general generative model, Naive Bayes, and then proceed to describe two specific cases, quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA).\n\n30.3.1 Naive Bayes\nRecall that Bayes rule tells us that we can rewrite \\(p(\\mathbf{x})\\) as follows:\n\\[\np(\\mathbf{x}) = \\mathrm{Pr}(Y = 1|\\mathbf{X}=\\mathbf{x}) = \\frac{f_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\mathrm{Pr}(Y = 1)}\n{ f_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\mathrm{Pr}(Y = 0)  + f_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\mathrm{Pr}(Y = 1) }\n\\]\nwith \\(f_{\\mathbf{X}|Y = 1}\\) and \\(f_{\\mathbf{X}|Y = 0}\\) representing the distribution functions of the predictor \\(\\mathbf{X}\\) for the two classes \\(Y = 1\\) and \\(Y = 0\\). The formula implies that if we can estimate these conditional distributions, we can develop a powerful decision rule. However, this is a big if.\nAs we go forward, we will encounter examples in which \\(\\mathbf{X}\\) has many dimensions and we do not have much information about the distribution. In these cases, Naive Bayes will be practically impossible to implement. However, there are instances in which we have a small number of predictors (not much more than 2) and many categories in which generative models can be quite powerful. We describe two specific examples and use our previously described case studies to illustrate them.\nLet’s start with a very simple and uninteresting, yet illustrative, case: the example related to predicting sex from height.\n\nset.seed(1995)\ny &lt;- heights$height\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntrain_set &lt;- heights |&gt; slice(-test_index)\ntest_set &lt;- heights |&gt; slice(test_index)\n\nIn this case, the Naive Bayes approach is particularly appropriate because we know that the normal distribution is a good approximation for the conditional distributions of height given sex for both classes \\(Y = 1\\) (female) and \\(Y = 0\\) (male). This implies that we can approximate the conditional distributions \\(f_{X|Y = 1}\\) and \\(f_{X|Y = 0}\\) by simply estimating averages and standard deviations from the data:\n\nparams &lt;- train_set |&gt; group_by(sex) |&gt; \n  summarize(avg = mean(height), sd = sd(height))\nparams\n#&gt; # A tibble: 2 × 3\n#&gt;   sex      avg    sd\n#&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Female  64.8  4.14\n#&gt; 2 Male    69.2  3.57\n\nThe prevalence, which we will denote with \\(\\pi = \\mathrm{Pr}(Y = 1)\\), can be estimated from the data with:\n\npi &lt;- train_set |&gt; summarize(pi = mean(sex == \"Female\")) |&gt; pull(pi)\npi\n#&gt; [1] 0.212\n\nNow we can use our estimates of average and standard deviation to get an actual rule:\n\nx &lt;- test_set$height\nf0 &lt;- dnorm(x, params$avg[2], params$sd[2])\nf1 &lt;- dnorm(x, params$avg[1], params$sd[1])\np_hat_bayes &lt;- f1*pi / (f1*pi + f0*(1 - pi))\n\nOur Naive Bayes estimate \\(\\hat{p}(x)\\) looks a lot like a logistic regression estimate:\n\n\n\n\n\n\n\n\nIn fact, we can show that the Naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as the Elements of Statistical Learning1. We can see that they are similar empirically by comparing the two resulting curves.\n\n30.3.2 Controlling prevalence\nOne useful feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence. Using our sample, we estimated \\(f_{X|Y = 1}\\), \\(f_{X|Y = 0}\\) and \\(\\pi\\). If we use hats to denote the estimates, we can write \\(\\hat{p}(x)\\) as:\n\\[\n\\hat{p}(x)= \\frac{\\hat{f}_{X|Y = 1}(x) \\hat{\\pi}}\n{ \\hat{f}_{X|Y = 0}(x)(1-\\hat{\\pi}) + \\hat{f}_{X|Y = 1}(x)\\hat{\\pi} }\n\\]\nAs we discussed earlier, our sample has a much lower prevalence, 0.21, than the general population. So if we use the rule \\(\\hat{p}(x) &gt; 0.5\\) to predict females, our accuracy will be affected due to the low sensitivity:\n\ny_hat_bayes &lt;- ifelse(p_hat_bayes &gt; 0.5, \"Female\", \"Male\")\nsensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))\n#&gt; [1] 0.213\n\nAgain, this is because the algorithm gives more weight to specificity to account for the low prevalence:\n\nspecificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))\n#&gt; [1] 0.967\n\nThis is mainly due to the fact that \\(\\hat{\\pi}\\) is substantially less than 0.5, so we tend to predict Male more often. It makes sense for a machine learning algorithm to do this in our sample because we do have a higher percentage of males. But if we were to extrapolate this to a general population, our overall accuracy would be affected by the low sensitivity.\nThe Naive Bayes approach gives us a direct way to correct this since we can simply force \\(\\hat{\\pi}\\) to be whatever value we want it to be. So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule, we could simply change \\(\\hat{\\pi}\\) to 0.5 like this:\n\np_hat_bayes_unbiased &lt;- f1 * 0.5 / (f1 * 0.5 + f0 * (1 - 0.5)) \ny_hat_bayes_unbiased &lt;- ifelse(p_hat_bayes_unbiased &gt; 0.5, \"Female\", \"Male\")\n\nNote the difference in sensitivity with a better balance:\n\nsensitivity(factor(y_hat_bayes_unbiased), factor(test_set$sex))\n#&gt; [1] 0.693\nspecificity(factor(y_hat_bayes_unbiased), factor(test_set$sex))\n#&gt; [1] 0.832\n\nThe new rule also gives us a very intuitive cutoff between 66-67, which is about the middle of the female and male average heights:\n\nplot(x, p_hat_bayes_unbiased)\nabline(h = 0.5, lty = 2) \nabline(v = 67, lty = 2)\n\n\n\n\n\n\n\n\n30.3.3 Quadratic discriminant analysis\nQuadratic Discriminant Analysis (QDA) is a version of Naive Bayes in which we assume that the distributions \\(p_{\\mathbf{X}|Y = 1}(x)\\) and \\(p_{\\mathbf{X}|Y = 0}(\\mathbf{x})\\) are multivariate normal. The simple example we described in the previous section is actually QDA. Let’s now look at a slightly more complicated case: the 2 or 7 example.\nIn this example, we have two predictors so we assume each one is bivariate normal. This implies that we need to estimate two averages, two standard deviations, and a correlation for each case \\(Y = 1\\) and \\(Y = 0\\). Once we have these, we can approximate the distributions \\(f_{X_1,X_2|Y = 1}\\) and \\(f_{X_1, X_2|Y = 0}\\). We can easily estimate parameters from the data:\n\nparams &lt;- mnist_27$train |&gt; \n  group_by(y) |&gt; \n  summarize(avg_1 = mean(x_1), avg_2 = mean(x_2), \n            sd_1= sd(x_1), sd_2 = sd(x_2), \n            r = cor(x_1, x_2))\nparams\n#&gt; # A tibble: 2 × 6\n#&gt;   y     avg_1 avg_2   sd_1   sd_2     r\n#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2     0.136 0.287 0.0670 0.0600 0.415\n#&gt; 2 7     0.238 0.290 0.0745 0.104  0.468\n\nWith these estimates in place, all we need are the prevalence \\(\\pi\\) to compute:\n\\[\n\\hat{p}(\\mathbf{x})= \\frac{\\hat{f}_{\\mathbf{X}|Y = 1}(\\mathbf{x}) \\hat{\\pi}}\n{ \\hat{f}_{\\mathbf{X}|Y = 0}(x)(1-\\hat{\\pi}) + \\hat{f}_{\\mathbf{X}|Y = 1}(\\mathbf{x})\\hat{\\pi} }\n\\]\nNote that the densities \\(f\\) are bivariate normal distributions. Here we provide a visual way of showing the approach. We plot the data and use contour plots to give an idea of what the two estimated normal densities look like (we show the curve representing a region that includes 95% of the points):\n\n\n\n\n\n\n\n\nWe can fit QDA using the qda function the MASS package:\n\ntrain_qda &lt;- MASS::qda(y ~ ., data = mnist_27$train)\ny_hat &lt;- predict(train_qda, mnist_27$test)$class\n\nWe see that we obtain relatively good accuracy:\n\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"] \n#&gt; Accuracy \n#&gt;    0.815\n\nThe conditional probability looks relatively good, although it does not fit as well as the kernel smoothers:\n\n\n\n\n\n\n\n\nOne reason QDA does not work as well as the kernel methods is because the assumption of normality does not quite hold. Although for the 2s it seems reasonable, for the 7s it does seem to be off. Notice the slight curvature in the points for the 7s:\n\nmnist_27$train |&gt; mutate(y = factor(y)) |&gt; \n  ggplot(aes(x_1, x_2, fill = y, color = y)) + \n  geom_point(show.legend = FALSE) + \n  stat_ellipse(type = \"norm\") +\n  facet_wrap(~y)\n\n\n\n\n\n\n\nQDA can work well here, but it becomes harder to use as the number of predictors increases. Here we have 2 predictors and had to compute 4 means, 4 SDs, and 2 correlations. Notice that if we have 10 predictors, we have 45 correlations for each class. In general, the formula is \\(K\\times p(p-1)/2\\), which gets big fast. Once the number of parameters approaches the size of our data, the method becomes impractical due to overfitting.\n\n30.3.4 Linear discriminant analysis\nA relatively simple solution to QDA’s problem of having too many parameters is to assume that the correlation structure is the same for all classes, which reduces the number of parameters we need to estimate. In this case, the the distributions looks like this:\n\n\n\n\n\n\n\n\nWe can fit LDA using the MASS lda function:\n\ntrain_lda &lt;- MASS::lda(y ~ ., data = mnist_27$train)\ny_hat &lt;- predict(train_lda, mnist_27$test)$class\n\nNow the size of the ellipses as well as the angles are the same. This is because they are assumed to have the same standard deviations and correlations. Although this added constraint lowers the number of parameters, the rigidity lowers our accuracy to:\n\nconfusionMatrix(y_hat, mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.775\n\nWhen we force this assumption, we can show mathematically that the boundary is a line, just as with logistic regression. For this reason, we call the method linear discriminant analysis (LDA). Similarly, for QDA, we can show that the boundary must be a quadratic function.\n\n\n\n\n\n\n\n\nIn the case of LDA, the lack of flexibility does not permit us to capture the non-linearity in the true conditional probability function.\n\n30.3.5 Connection to distance\nThe normal density is:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left\\{ - \\frac{(x-\\mu)^2}{\\sigma^2}\\right\\}\n\\]\nIf we remove the constant \\(1/(\\sqrt{2\\pi} \\sigma)\\) and then take the log, we get:\n\\[\n- \\frac{(x-\\mu)^2}{\\sigma^2}\n\\]\nwhich is the negative of a distance squared scaled by the standard deviation. For higher dimensions, the same is true except the scaling is more complex and involves correlations.\n\n\n\n\n\n\nYou are now ready to do exercises 14-17.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Examples of algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#sec-trees",
    "href": "ml/algorithms.html#sec-trees",
    "title": "30  Examples of algorithms",
    "section": "\n30.4 Classification and regression trees (CART)",
    "text": "30.4 Classification and regression trees (CART)\n\n30.4.1 The curse of dimensionality\nWe described how methods such as LDA and QDA are not meant to be used with many predictors \\(p\\) because the number of parameters that we need to estimate becomes too large. For example, with the digits example \\(p = 784\\), we would have over 600,000 parameters with LDA, and we would multiply that by the number of classes for QDA. Kernel methods, such as kNN or local regression, do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the curse of dimensionality. The dimension here refers to the fact that when we have \\(p\\) predictors, the distance between two observations is computed in \\(p\\)-dimensional space.\nA useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility, and to be flexible we need to keep the neighborhoods small.\nTo see how this becomes an issue for higher dimensions, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1:\n\n\n\n\n\n\n\n\nNow for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to \\(\\sqrt{.10} \\approx .316\\):\n\n\n\n\n\n\n\n\nUsing the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is \\(\\sqrt[3]{.10} \\approx 0.464\\). In general, to include 10% of the data in a case with \\(p\\) dimensions, we need an interval with each side of size \\(\\sqrt[p]{.10}\\) of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.\n\n\n\n\n\n\n\n\nBy the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.\nHere we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on regression and decision trees and their extension to random forests.\n\n30.4.2 CART motivation\nTo motivate this section, we will use a new dataset that includes the breakdown of the composition of olive oil into 8 fatty acids:\n\nnames(olive)\n#&gt;  [1] \"region\"      \"area\"        \"palmitic\"    \"palmitoleic\"\n#&gt;  [5] \"stearic\"     \"oleic\"       \"linoleic\"    \"linolenic\"  \n#&gt;  [9] \"arachidic\"   \"eicosenoic\"\n\nFor illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.\n\ntable(olive$region)\n#&gt; \n#&gt; Northern Italy       Sardinia Southern Italy \n#&gt;            151             98            323\n\nWe remove the area column because we won’t use it as a predictor.\n\nolive &lt;- select(olive, -area)\n\nUsing kNN, we can achieve a test set accuracy of 0.9712321. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia.\n\n\n\n\n\n\n\n\nThis implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic.\n\n\n\n\n\n\n\n\nIn Section 21.3, we defined predictor spaces, which in this case consists of eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye, we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category.\nThis in turn can be used to define an algorithm with perfect accuracy. Specifically, we define the following decision rule: if eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linoleic is larger than \\(10.535\\), predict Sardinia, and if lower, predict Northern Italy. We can draw this decision tree as follows:\n\n\n\n\n\n\n\n\nDecision trees like this are often used in practice. For example, to decide on a person’s risk of poor outcome after having a heart attack, doctors use the following:\n\n\n\n\n(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-1842.)\nA tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as nodes. Regression and decision trees operate by predicting an outcome variable \\(y\\) by partitioning the predictors.\n\n30.4.3 Regression trees\nWhen using trees, and the outcome is continuous, we call the approach a regression tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation \\(f(x) = \\mathrm{E}(Y | X = x)\\) with \\(Y\\) the poll margin and \\(x\\) the day.\n\n\n\n\n\n\n\n\nThe general idea here is to build a decision tree and, at the end of each node, obtain a predictor \\(\\hat{y}\\). A mathematical way to describe this is: we are partitioning the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\), and then for any predictor \\(x\\) that falls within region \\(R_j\\), estimate \\(f(x)\\) with the average of the training observations \\(y_i\\) for which the associated predictor \\(x_i\\) is also in \\(R_j\\).\nBut how do we decide on the partition \\(R_1, R_2, \\ldots, R_J\\) and how do we choose \\(J\\)? Here is where the algorithm gets a bit complicated.\nRegression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example, this space is the interval [-155, 1]. But after the first step, we will have two partitions. After the second step, we will split one of these partitions into two and will have three partitions, then four, and so on. Later we describe how we pick the partition to further partition, and when to stop.\nFor each existing partition, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions, which we will call \\(R_1(j,s)\\) and \\(R_2(j,s)\\), that split our observations in the current partition by asking if \\(x_j\\) is bigger than \\(s\\):\n\\[\nR_1(j,s) = \\{\\mathbf{x} \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\}\n\\]\nIn our current example, we only have one predictor, so we will always choose \\(j = 1\\), but in general this will not be the case. Now after we define the new partitions \\(R_1\\) and \\(R_2\\), and we decide to stop the partitioning, we compute predictors by taking the average of all the observations \\(y\\) for which the associated \\(\\mathbf{x}\\) is in \\(R_1\\) and \\(R_2\\). We refer to these two as \\(\\hat{y}_{R_1}\\) and \\(\\hat{y}_{R_2}\\) respectively.\nBut how do we pick \\(j\\) and \\(s\\)? Basically we find the pair that minimizes the residual sum of squares (RSS):\n\\[\n\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 +\n\\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\nThis is then applied recursively to the new regions \\(R_1\\) and \\(R_2\\). We describe how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.\nLet’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the rpart function in the rpart package.\n\nlibrary(rpart)\nfit &lt;- rpart(margin ~ ., data = polls_2008)\n\nIn this case, there is only one predictor. Thus we do not have to decide which predictor \\(j\\) to split by, we simply have to decide what value \\(s\\) we use to split. We can visually see where the splits were made:\n\nplot(fit, margin = 0.1)\ntext(fit, cex = 0.75)\n\n\n\n\n\n\n\n\n\nThe first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5, respectively, and so on. We end up with 8 partitions. The final estimate \\(\\hat{f}(x)\\) looks like this:\n\npolls_2008 |&gt; \n  mutate(y_hat = predict(fit)) |&gt; \n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_step(aes(day, y_hat), col = \"red\")\n\n\n\n\n\n\n\nNote that the algorithm stopped partitioning at 8 nodes. This decision is based on a measure called the complexity parameter (cp). Every time the data is split into two new partitions, the training set RSS decreases because the model gains flexibility to adapt to the data. If splitting continued until every observation was isolated in its own partition, the RSS would eventually reach 0—since the average of a single value is the value itself. To prevent such overfitting, the algorithm requires that each new split reduce the RSS by at least a factor of cp. Larger values of cp therefore stop the algorithm earlier, leading to fewer nodes.\nHowever, cp is not the only factor that controls partitioning. Another important parameter is the minimum number of observations required in a node before it can be split, set by the minsplit argument in rpart (default: 20). In addition, rpart allows users to specify the minimum number of observations allowed in a terminal node (leaf), controlled by the minbucket argument. By default, minbucket = round(minsplit / 3).\nAs expected, if we set cp = 0 and minsplit = 2, then our prediction is as flexible as possible and our predictor is our original data:\n\n\n\n\n\n\n\n\nIntuitively we know that this is not a good approach as it will generally result in over-training. These cp, minsplit, and minbucket parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restricts flexibility.\nSo how do we pick these parameters? We can use cross validation, just like with any tuning parameter. Here is the resulting tree when we use cross validation to choose cp:\n\n\n\n\n\n\n\n\nNote that if we already have a tree and want to apply a higher cp value, we can use the prune function. We call this pruning a tree because we are snipping off partitions that do not meet a cp criterion. Here is an example where we create a tree that used a cp = 0 and then we prune it back:\n\nfit &lt;- rpart(margin ~ ., data = polls_2008, control = rpart.control(cp = 0))\npruned_fit &lt;- prune(fit, cp = 0.01)\n\n\n30.4.4 Classification (decision) trees\nClassification trees, or decision trees, are used in prediction problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome.\nThe first difference is that we form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition (as we can’t take the average of categories).\nThe second is that we can no longer use RSS to choose the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the Gini Index and Entropy.\nIn a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The Gini Index is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define \\(\\hat{p}_{j,k}\\) as the proportion of observations in partition \\(j\\) that are of class \\(k\\). The Gini Index is defined as:\n\\[\n\\mbox{Gini}(j) = \\sum_{k = 1}^K \\hat{p}_{j,k}(1-\\hat{p}_{j,k})\n\\]\nIf you study the formula carefully, you will see that it is in fact 0 in the perfect scenario described above.\nEntropy is a related quantity, defined as:\n\\[\n\\mbox{entropy}(j) = -\\sum_{k = 1}^K \\hat{p}_{j,k}\\log(\\hat{p}_{j,k}), \\mbox{ with } 0 \\times \\log(0) \\mbox{ defined as }0\n\\]\nIf we use a classification tree on the 2 or 7 example, we achieve an accuracy of 0.81 which is better than regression, but is not as good as what we achieved with kernel methods.\nThe plot of the estimated conditional probability shows us the limitations of classification trees:\n\n\n\n\n\n\n\n\nNote that with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity.\nClassification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are also easy to visualize (_if smal_Tl enough). Finally, they can model human decision processes and don’t require use of dummy predictors for categorical variables. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random forests, explained next, improve on several of these shortcomings.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Examples of algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#random-forests",
    "href": "ml/algorithms.html#random-forests",
    "title": "30  Examples of algorithms",
    "section": "\n30.5 Random forests",
    "text": "30.5 Random forests\nRandom forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees: a forest of trees constructed with randomness. It has two features that help accomplish this.\nThe first step is bootstrap aggregation or bagging. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest. The specific steps are as follows.\n1. Build \\(B\\) decision trees using the training set. We refer to the fitted models as \\(T_1, T_2, \\dots, T_B\\).\n2. For every observation in the test set, form a prediction \\(\\hat{y}_j\\) using tree \\(T_j\\).\n3. For continuous outcomes, form a final prediction with the average \\(\\hat{y} = \\frac{1}{B} \\sum_{j = 1}^B \\hat{y}_j\\). For categorical data classification, predict \\(\\hat{y}\\) with majority vote (most frequent class among \\(\\hat{y}_1, \\dots, \\hat{y}_B\\)).\nSo how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let \\(N\\) be the number of observations in the training set. To create \\(T_j, \\, j = 1,\\ldots,B\\) from the training set we do the following:\n1. Create a bootstrap training set by sampling \\(N\\) observations from the training set with replacement. This is the first way to induce randomness.\n2. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.\nTo illustrate how the first steps can result in smoother estimates, we will fit a random forest to the 2008 polls data. We will use the randomForest function in the randomForest package:\n\nlibrary(randomForest)\nfit &lt;- randomForest(margin ~ ., data = polls_2008) \n\nNote that if we apply the function plot to the resulting object, we see how the error rate of our algorithm changes as we add trees:\n\nplot(fit)\n\n\n\n\n\n\n\n\n\nIn this case, the accuracy improves as we add more trees until we have used about 300 trees after which accuracy stabilizes.\nThe resulting estimate for this random forest, obtained with\n\ny_hat &lt;-  predict(fit, newdata = polls_2008)\n\nis shown with the red curve below:\n\n\n\n\n\n\n\n\nNotice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step functions can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure, you see each of the bootstrap samples for several values of \\(b\\) and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.\n\n\n\n\n\n\n\n\n\nlibrary(randomForest)\ntrain_rf &lt;- randomForest(y ~ ., data = mnist_27$train)\n\nThe accuracy for the random forest fit for our 2 or 7 example is 0.825. Here is what the conditional probabilities look like:\n\n\n\n\n\n\n\n\nVisualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum, the smoother the final estimate will be. If we use a node size of 31, the number of neighbors we used with kNN, we get an accuracy of 0.845. The selected model improves accuracy and provides a smoother estimate:\n\n\n\n\n\n\n\n\nRandom forest performs better than trees in all the examples we have considered. However, a disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine variable importance. To define variable importance, we count how often a predictor is used in the individual trees. You can learn more about variable importance in an advanced machine learning book3. The caret package includes the function varImp that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Examples of algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#exercises",
    "href": "ml/algorithms.html#exercises",
    "title": "30  Examples of algorithms",
    "section": "\n30.6 Exercises",
    "text": "30.6 Exercises\n1. Create a dataset using the following code:\n\nn &lt;- 100\nSigma &lt;- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nUse the caret package to partition into a test and training set of equal size. Train a linear model and report the RMSE. Repeat this exercise 100 times and make a histogram of the RMSEs and report the average and standard deviation. Hint: adapt the code shown earlier like this:\n\nlibrary(caret)\ny &lt;- dat$y\ntest_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\ntrain_set &lt;- dat |&gt; slice(-test_index)\ntest_set &lt;- dat |&gt; slice(test_index)\nfit &lt;- lm(y ~ x, data = train_set)\ny_hat &lt;- fit$coef[1] + fit$coef[2]*test_set$x\nsqrt(mean((y_hat - test_set$y)^2))\n\nand put it inside a call to replicate.\n2. Now we will repeat the above but using larger datasets. Repeat exercise 1 but for datasets with n &lt;- c(100, 500, 1000, 5000, 10000). Save the average and standard deviation of RMSE from the 100 repetitions. Hint: use the sapply or map functions.\n3. Describe what you observe with the RMSE as the size of the dataset becomes larger.\n\nOn average, the RMSE does not change much as n gets larger, while the variability of RMSE does decrease.\nBecause of the law of large numbers, the RMSE decreases: more data, more precise estimates.\n\nn = 10000 is not sufficiently large. To see a decrease in RMSE, we need to make it larger.\nThe RMSE is not a random variable.\n\n4. Now repeat exercise 1, but this time make the correlation between x and y larger by changing Sigma like this:\n\nn &lt;- 100\nSigma &lt;- 9*matrix(c(1, 0.95, 0.95, 1), 2, 2)\ndat &lt;- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"x\", \"y\"))\n\nRepeat the exercise and note what happens to the RMSE now.\n5. Which of the following best explains why the RMSE in exercise 4 is so much lower than exercise 1:\n\nIt is just luck. If we do it again, it will be larger.\nThe Central Limit Theorem tells us the RMSE is normal.\nWhen we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y. This correlation has a much bigger effect on RMSE than n. Large n simply provide us more precise estimates of the linear model coefficients.\nThese are both examples of regression, so the RMSE has to be the same.\n\n6. Create a dataset using the following code:\n\nn &lt;- 1000\nSigma &lt;- matrix(c(1, 3/4, 3/4, 3/4, 1, 0, 3/4, 0, 1), 3, 3)\ndat &lt;- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"y\", \"x_1\", \"x_2\"))\n\nNote that y is correlated with both x_1 and x_2, but the two predictors are independent of each other.\n\ncor(dat)\n\nUse the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2. Train a linear model and report the RMSE.\n7. Repeat exercise 6 but now create an example in which x_1 and x_2 are highly correlated:\n\nn &lt;- 1000\nSigma &lt;- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)\ndat &lt;- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |&gt;\n  data.frame() |&gt; setNames(c(\"y\", \"x_1\", \"x_2\"))\n\nUse the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2 Train a linear model and report the RMSE.\n8. Compare the results in 6 and 7 and choose the statement you agree with:\n\nAdding extra predictors can improve RMSE substantially, but not when they are highly correlated with another predictor.\nAdding extra predictors improves predictions equally in both exercises.\nAdding extra predictors results in over fitting.\nUnless we include all predictors, we have no predicting power.\n\n9. Define the following dataset:\n\nmake_data &lt;- function(n = 1000, p = 0.5, \n                      mu_0 = 0, mu_1 = 2, \n                      sigma_0 = 1,  sigma_1 = 1){\n  y &lt;- rbinom(n, 1, p)\n  f_0 &lt;- rnorm(n, mu_0, sigma_0)\n  f_1 &lt;- rnorm(n, mu_1, sigma_1)\n  x &lt;- ifelse(y == 1, f_1, f_0)\n  test_index &lt;- createDataPartition(y, times = 1, p = 0.5, list = FALSE)\n  list(train = data.frame(x = x, y = as.factor(y)) |&gt; \n         slice(-test_index),\n       test = data.frame(x = x, y = as.factor(y)) |&gt; \n         slice(test_index))\n}\n\nNote that we have defined a variable x that is predictive of a binary outcome y.\n\ndat$train |&gt; ggplot(aes(x, color = y)) + geom_density()\n\nCompare the accuracy of linear regression and logistic regression.\n10. Repeat the simulation from exercise 9 100 times and compare the average accuracy for each method and notice they give practically the same answer.\n11. Generate 25 different datasets changing the difference between the two class: delta &lt;- seq(0, 3, len = 25). Plot accuracy versus delta.\n12. We can see what the data looks like if we add 1s to our 2 or 7 examples using this code:\n\nlibrary(dslabs)\nmnist_127$train |&gt; ggplot(aes(x_1, x_2, color = y)) + geom_point()\n\nFit QDA using the qda function in the MASS package then create a confusion matrix for predictions on the test. Which of the following best describes the confusion matrix:\n\nIt is a two-by-two table.\nBecause we have three classes, it is a two-by-three table.\nBecause we have three classes, it is a three-by-three table.\nConfusion matrices only make sense when the outcomes are binary.\n\n13. The byClass component returned by the confusionMatrix object provides sensitivity and specificity for each class. Because these terms only make sense when data is binary, each row represents sensitivity and specificity when a particular class is 1 (positives) and the other two are considered 0s (negatives). Based on the values returned by confusionMatrix, which of the following is the most common mistake:\n\nCalling 1s either a 2 or 7.\nCalling 2s either a 1 or 7.\nCalling 7s either a 1 or 2.\nAll mistakes are equally common.\n\n\nCreate a grid of x_1 and x_2 using:\n\n\nGS &lt;- 150\nnew_x &lt;- with(mnist_127$train,\n  expand.grid(x_1 = seq(min(x_1), max(x_1), len = GS),\n              x_2 = seq(min(x_2), max(x_2), len = GS)))\n\nthen visualize the decision rule by coloring the regions of the Cartesian plan to represent the label that would be called in that region.\n14. Repeat exercise 13 but for LDA. Which of the following explains why LDA has worse accuracy:\n\nLDA separates the space with lines making it too rigid.\nLDA divides the space into two and there are three classes.\nLDA is very similar to QDA the difference is due to chance.\nLDA can’t be used with more than one class.\n\n15. Now repeat exercise 13 for kNN with \\(k=31\\) and compute and compare the overall accuracy for all three methods.\n16. To understand how a simple method like kNN can outperform a model that explicitly tries to emulate Bayes’ rule, explore the conditional distributions of x_1 and x_2 to see if the normal approximation holds. Generative models can be very powerful, but only when we are able to successfully approximate the joint distribution of predictors conditioned on each class.\n17. Earlier we used logistic regression to predict sex from height. Use kNN to do the same. Use the code described in this chapter to select the \\(F_1\\) measure and plot it against \\(k\\). Compare to the \\(F_1\\) of about 0.6 we obtained with regression.\n18. Create a simple dataset where the outcome grows 0.75 units on average for every increase in a predictor:\n\nn &lt;- 1000\nsigma &lt;- 0.25\nx &lt;- rnorm(n, 0, 1)\ny &lt;- 0.75 * x + rnorm(n, 0, sigma)\ndat &lt;- data.frame(x = x, y = y)\n\nUse rpart to fit a regression tree and save the result to fit.\n19. Plot the final tree so that you can see where the partitions occurred.\n20. Make a scatterplot of y versus x along with the predicted values based on the fit.\n21. Now model with a random forest instead of a regression tree using randomForest from the randomForest package, and remake the scatterplot with the prediction line.\n22. Use the function plot to see if the random forest has converged or if we need more trees.\n23. It seems that the default values for the random forest result in an estimate that is too flexible (not smooth). Re-run the random forest but this time with nodesize set at 50 and maxnodes set at 25. Remake the plot.\n24. This dslabs dataset includes the tissue_gene_expression with a matrix x:\n\nlibrary(dslabs)\ndim(tissue_gene_expression$x)\n\nwith the gene expression measured on 500 genes for 189 biological samples representing seven different tissues. The tissue type is stored in tissue_gene_expression$y.\n\ntable(tissue_gene_expression$y)\n\nFit a random forest using the randomForest function in the package randomForest. Then use the varImp function to see which are the top 10 most predictive genes. Make a histogram of the reported importance to get an idea of the distribution of the importance values.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Examples of algorithms</span>"
    ]
  },
  {
    "objectID": "ml/algorithms.html#footnotes",
    "href": "ml/algorithms.html#footnotes",
    "title": "30  Examples of algorithms",
    "section": "",
    "text": "https://web.stanford.edu/~hastie/Papers/ESLII.pdf↩︎\nhttps://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid = 1759289&mirid = 1&type = 2↩︎\nhttps://web.stanford.edu/~hastie/Papers/ESLII.pdf↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Examples of algorithms</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html",
    "href": "ml/clustering.html",
    "title": "31  Clustering",
    "section": "",
    "text": "31.1 Hierarchical clustering\nThe algorithms we have described so far are examples of a general approach referred to as supervised machine learning. The term supervised comes from the fact that the outcomes in the training set are used to supervise the construction of a prediction algorithm.\nIn contrast, there is another branch of machine learning known as unsupervised learning. In this setting, the outcomes are not provided, and the goal is instead to uncover structure in the data, often in the form of groups. These methods are also referred to as clustering algorithms because they group observations into clusters based on similarities in their predictors.\nIn the two examples we have considered so far, clustering would not be very effective. For example, given only heights, we would not expect an algorithm to cleanly discover two groups (male and female) because the overlap is substantial. Similarly, in the digit classification example, the distributions of twos and sevens overlap enough to make clustering challenging.\nHowever, there are many applications in which unsupervised learning is a powerful tool, particularly for exploratory data analysis. For example, clustering can reveal hidden subgroups in biological, social, or market data that may not be obvious at first glance.\nA first step in any clustering algorithm is defining a distance (or similarity) between observations or groups of observations. Once a distance is defined, the next step is to decide how to join observations into clusters. There are many algorithms for doing this. In this chapter, we introduce two common approaches: hierarchical clustering and k-means clustering.\nA first step in any clustering algorithm is defining a distance between observations or groups of observations. Then we need to decide how to join observations into clusters. There are many algorithms for doing this. Here we introduce two as examples: hierarchical and k-means.\nWe startin by constructing a simple example based on movie ratings. Here we quickly construct a matrix y that has ratings for the 50 movies with the most ratings among users with at least 100 ratings:\nNotice that if we compute distances directly from the original ratings, movies with generally high ratings will tend to be close to each other, as will movies with generally low ratings. However, this is not what we are actually interested in. Instead, we want to capture how ratings correlate across users for the 263 different users. To achieve this, we center the ratings for each movie by removing the movie effect:\nAs described in Chapter 23, many of the entries are missing because not every user rates every movie, so we use the argument na.rm = TRUE.\nWith the centered data, we can now cluster movies based on their rating patterns. The first step is to compute distances between each pair of movies using the dist function:\nThe dist function automatically accounts for missing values and standardizes the distance measurements, ensuring that the computed distance between two movies does not depend on the number of ratings available.\nWith the distance between each pair of movies computed, we need an algorithm to define groups, based on these distances. Hierarchical clustering starts by defining each observation as a separate group, then the two closest groups are joined into new groups. We then continue joining the closest groups into new groups iteratively until there is just one group including all the observations. The hclust function implements this algorithm and takes a distance as input.\nh &lt;- hclust(d)\nWe can see the resulting groups using a dendrogram. The function plot applied to an hclust object creates a dendrogram:\nplot(h, cex = 0.65, main = \"\", xlab = \"\")\nThis graph gives us an approximation between the distance between any two movies. To find this distance, we find the first location, from top to bottom, where these movies split into two different groups. The height of this location is the distance between these two groups. So, for example, the distance between the three Star Wars movies is 8 or less, while the distance between Raiders of the Lost of Ark and Silence of the Lambs is about 17.\nTo generate actual groups, we can do one of two things: 1) decide on a minimum distance needed for observations to be in the same group or 2) decide on the number of groups you want and then find the minimum distance that achieves this. The function cutree can be applied to the output of hclust to perform either of these two operations and generate groups.\ngroups &lt;- cutree(h, k = 10)\nNote that the clustering provides some insights into types of movies. Group 2 appears to be critically aclaimed movies:\nnames(groups)[groups == 2]\n#&gt; [1] \"American Beauty\"      \"Fargo\"                \"Godfather, The\"      \n#&gt; [4] \"Pulp Fiction\"         \"Silence of the La...\" \"Usual Suspects, The\"\nAnd Group 9 appears to be nerd movies:\nnames(groups)[groups == 9]\n#&gt; [1] \"Lord of the Rings...\" \"Lord of the Rings...\" \"Lord of the Rings...\"\n#&gt; [4] \"Star Wars IV - A ...\" \"Star Wars V - The...\" \"Star Wars VI - Re...\"\nWe can change the size of the group by either making k larger or h smaller.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html#heatmaps",
    "href": "ml/clustering.html#heatmaps",
    "title": "31  Clustering",
    "section": "\n31.2 Heatmaps",
    "text": "31.2 Heatmaps\nA powerful visualization tool for discovering clusters or patterns in your data is the heatmap. The idea is simple: plot an image of your data matrix with colors used as the visual cue and both the columns and rows ordered according to the results of a clustering algorithm. We will demonstrate this with the tissue_gene_expression dataset in dslabs.\nWe start by scaling the columns of the gene expression matrix because we care about relative differences in gene expression. After scaling, we compute perform clustering on both the observations and the predictors:\n\nlibrary(dslabs)\ny &lt;- sweep(tissue_gene_expression$x, 2, colMeans(tissue_gene_expression$x))\nh_1 &lt;- hclust(dist(y))\nh_2 &lt;- hclust(dist(t(y)))\n\nNote that we have performed two clusterings: we clustered samples (h_1) and genes (h_2).\nNow we can use the results of this clustering to order the rows and columns:\n\nimage(y[h_1$order, h_2$order])\n\nThe heatmap function that does all this for us:\n\nheatmap(y, col = RColorBrewer::brewer.pal(11, \"Spectral\"))\n\nNote we do not show the results of the heatmap function because there are too many features for the plot to be useful. We will therefore filter some columns and remake the plots.\nIf only a few features are different between clusters, including all the features can add enough noise that making cluster detection challenging. A simple approach to avoid this is to assume low variability features are not informative and include only high variance features. For example, in the movie example, users with low variance in their ratings are not really distinguishing movies: all the movies seem about the same to them.\nHere is an example code showing how we can include only the features with high variance in a heatmap:\n\nlibrary(matrixStats)\nsds &lt;- colSds(y, na.rm = TRUE)\no &lt;- order(sds, decreasing = TRUE)[1:25]\nheatmap(y[,o], col = RColorBrewer::brewer.pal(11, \"Spectral\"))\n\n\n\n\n\n\n\nNote there are several other heatmap functions in R. A popular example is the heatmap.2 in the gplots package.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html#k-means",
    "href": "ml/clustering.html#k-means",
    "title": "31  Clustering",
    "section": "\n31.3 k-means",
    "text": "31.3 k-means\nWe have so far described just one clustering algorithm: hierarchical clustering. However, there are many others that take quite different approaches. To demonstrate another, we now introduce the k-means clustering algorithm.\nThe k-means algorithm requires us to predefine $k$, the number of clusters we want to create. Once $k$ is set, the algorithm proceeds iteratively:\n\n\nInitialization: Define \\(k\\) centers (chosen at random).\n\nAssignment step: Each observation is assigned to the cluster with the closest center.\n\nUpdate step: The centers are redefined by taking the mean of the observations in each cluster. These new centers are called centroids.\n\nIteration: Steps 2 and 3 are repeated until the centers stabilize (converge).\n\nWe can run k-means to see if it can discover the different tissues:\n\nset.seed(2001)\nk &lt;- kmeans(y, centers = 7)\n\nThe cluster assignments are stored in the cluster component:\n\nk$cluster[1:5]\n#&gt; cerebellum_1 cerebellum_2 cerebellum_3 cerebellum_4 cerebellum_5 \n#&gt;            7            7            7            7            7\n\nBecause the initial centers are chosen at random (hence the use of set.seed), the resulting clusters can vary. To improve stability, we can repeat the process multiple times with different random starting points and keep the best result. The number of random starts is set with the nstart argument:\n\nk &lt;- kmeans(y, centers = 7, nstart = 100)\n\nWe can evaluate how well the clusters match the known tissue labels:\n\ntable(tissue_gene_expression$y, k$cluster)\n#&gt;              \n#&gt;                1  2  3  4  5  6  7\n#&gt;   cerebellum  33  0  0  0  0  5  0\n#&gt;   colon        0 34  0  0  0  0  0\n#&gt;   endometrium  0  0  0 15  0  0  0\n#&gt;   hippocampus  0  0  0  0  0 31  0\n#&gt;   kidney       0  1  0  0 38  0  0\n#&gt;   liver        0  0 26  0  0  0  0\n#&gt;   placenta     0  0  0  0  0  0  6\n\nThe results show that k-means did a reasonably good job at recovering the tissue groups. For example, clusters 1, 3, 4, 5, and 7 correspond to cerebellum, liver, endometrium, kidney, and placenta, respectively. All hippocampus samples are grouped in cluster 6, though five cerebellum samples are incorrectly included there. Cluster 2 captures almost all colon samples, with just one kidney misclassified.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html#recommended-reading",
    "href": "ml/clustering.html#recommended-reading",
    "title": "31  Clustering",
    "section": "\n31.4 Recommended Reading",
    "text": "31.4 Recommended Reading\n\nKaufman, L. & Rousseeuw, P. J. (2005).Finding Groups in Data: An Introduction to Cluster Analysis.\nWiley.\n— A foundational text on clustering, introducing key methods such as partitioning, hierarchical clustering, and density-based approaches, with practical insights and examples.\nMaechler, M., Rousseeuw, P., Struyf, A., Hubert, M., & Hornik, K. (2025).cluster: Cluster Analysis Basics and Extensions.\nR package version 2.1.8. Available at: https://cran.r-project.org/package=cluster\n— The main R package for clustering, providing implementations of many of the methods described in Kaufman & Rousseeuw (2005), along with practical extensions and tools for applied work.\nHastie, T., Tibshirani, R., & Friedman, J. (2009).The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.).\nSpringer.\n— A comprehensive reference covering supervised and unsupervised learning, including clustering, dimension reduction, and th",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/clustering.html#exercises",
    "href": "ml/clustering.html#exercises",
    "title": "31  Clustering",
    "section": "\n31.5 Exercises",
    "text": "31.5 Exercises\n1. Load the tissue_gene_expression dataset. Remove the row means and compute the distance between each observation. Store the result in d.\n2. Make a hierarchical clustering plot and add the tissue types as labels.\n3. Run a k-means clustering on the data with \\(K=7\\). Make a table comparing the identified clusters to the actual tissue types. Run the algorithm several times to see how the answer changes.\n4. Make a heatmap of the 50 most variable genes. Make sure the observations show up in the columns, that the predictors are centered, and add a color bar to show the different tissue types. Hint: use the ColSideColors argument to assign colors. Also, use col = RColorBrewer::brewer.pal(11, \"RdBu\") for a better use of colors.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html",
    "href": "ml/ml-in-practice.html",
    "title": "32  Machine learning in practice",
    "section": "",
    "text": "32.1 The caret package\nNow that we have learned several methods and explored them with simple examples, we will try them out on a real example: the MNIST digits.\nWe can load this data using the following dslabs package function:\nThe dataset includes two components, a training set and a test set:\nEach of these components includes a matrix with features in the columns:\nand vector with the classes as integers:\nBecause we want this example to run on a small laptop and in less than one hour, we will consider a subset of the dataset. We will sample 10,000 random rows from the training set and 1,000 random rows from the test set:\nWhen fitting models to large datasets, we recommend using matrices instead of data frames, as matrix operations tend to be faster. In the caret package, predictor matrices must have column names to track features accurately during prediction on the test set. If the matrices lack column names, you can assign names based on their position:\nWe have already learned about several machine learning algorithms. Many of these algorithms are implemented in R. However, they are distributed via different packages, developed by different authors, and often use different syntax. The caret package tries to consolidate these differences and provide consistency. It currently includes over 200 different methods which are summarized in the caret package manual1. Keep in mind that caret does not include the packages needed to run each possible algorithm. To apply a machine learning method through caret you still need to install the library that implement the method. The required packages for each method are described in the package manual.\nThe caret package also provides a function that performs cross validation for us. Here we provide some examples showing how we use this helpful package. We will first use the 2 or 7 example to illustrate and, in later sections, we use the package to run algorithms on the larger MNIST dataset.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#sec-caret",
    "href": "ml/ml-in-practice.html#sec-caret",
    "title": "32  Machine learning in practice",
    "section": "",
    "text": "32.1.1 The train function\nThe R functions that fit machine algorithms are all slightly different. Functions such as lm, glm, qda, lda, knn3, rpart and randomForest use different syntax, have different argument names and produce objects of different types.\nThe caret train function lets us train different algorithms using similar syntax. So, for example, we can type the following to train three different models:\n\nlibrary(caret)\n#&gt; Loading required package: lattice\ntrain_glm &lt;- train(y ~ ., method = \"glm\", data = mnist_27$train)\ntrain_qda &lt;- train(y ~ ., method = \"qda\", data = mnist_27$train)\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)\n\nAs we explain in more detail in Section 32.1.3, the train function selects parameters for you using a resampling method to estimate the MSE, with bootstrap as the default.\n\n32.1.2 The predict function\nThe predict function is very useful for machine learning applications. This function takes an object from a fitting function and a data frame with features \\(\\mathbf{x}\\) for which to predict, and returns predictions for these features.\nHere is an example with logistic regression:\n\nfit &lt;- glm(y ~ ., data = mnist_27$train, family = \"binomial\")\np_hat &lt;- predict(fit, newdata = mnist_27$test)\n\nIn this case, the function is simply computing\n\\[\n\\hat{p}(\\mathbf{x}) = g^{-1}\\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 \\right) \\text{ with } g(p) = \\log\\frac{p}{1-p} \\implies g^{-1}(\\mu) = \\frac{1}{1-e^{-\\mu}}\n\\]\nfor the x_1 and x_2 in the test set mnist_27$test. With these estimates in place, we can make our predictions and compute our accuracy:\n\ny_hat &lt;- factor(ifelse(p_hat &gt; 0.5, 7, 2))\n\nHowever, note that predict does not always return objects of the same type; it depends on what type of object it is applied to. To learn about the specifics, you need to look at the help file specific for the type of fit object that is being used.\npredict is actually a special type of function in R called a generic function. Generic functions call other functions depending on what kind of object it receives. So if predict receives an object coming out of the lm function, it will call predict.lm. If it receives an object coming out of glm, it calls predict.glm. If the fit is from knn3, it calls predict.knn3, and so on. These functions are similar but not exactly. You can learn more about the differences by reading the help files:\n\n?predict.glm\n?predict.qda\n?predict.knn3\n\nThere are many other versions of predict and many machine learning algorithms define their own predict function.\nAs with train, the caret packages unifies the use of predict with the function predict.train. This function takes the output of train and produces prediction of categories or estimates of \\(p(\\mathbf{x})\\).\nThe code looks the same for all methods:\n\ny_hat_glm &lt;- predict(train_glm, mnist_27$test, type = \"raw\")\ny_hat_qda &lt;- predict(train_qda, mnist_27$test, type = \"raw\")\ny_hat_knn &lt;- predict(train_knn, mnist_27$test, type = \"raw\")\n\nThis permits us to quickly compare the algorithms. For example, we can compare the accuracy like this:\n\nfits &lt;- list(glm = y_hat_glm, qda = y_hat_qda, knn = y_hat_knn)\nsapply(fits, function(fit) confusionMatrix(fit, mnist_27$test$y)$overall[[\"Accuracy\"]])\n#&gt;   glm   qda   knn \n#&gt; 0.775 0.815 0.835\n\n\n32.1.3 Resampling\nWhen an algorithm includes a tuning parameter, train automatically uses a resampling method to estimate MSE and decide among a few default candidate values. To find out what parameter or parameters are optimized, you can read the caret manual2 or study the output of:\n\nmodelLookup(\"knn\")\n\nTo obtain all the details of how caret implements kNN you can use:\n\ngetModelInfo(\"knn\")\n\nIf we run it with default values:\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", data = mnist_27$train)\n\nyou can quickly see the results of the cross validation using the ggplot function. The argument highlight highlights the max:\n\nggplot(train_knn, highlight = TRUE)\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the caret package.\n#&gt;   Please report the issue at &lt;https://github.com/topepo/caret/issues&gt;.\n\n\n\n\n\n\n\nBy default, the resampling is performed by taking 25 bootstrap samples, each comprised of 25% of the observations.\nFor the kNN method, the default is to try \\(k=5,7,9\\). We change this using the tuneGrid argument. The grid of values must be supplied by a data frame with the parameter names as specified in the modelLookup output.\nHere we present an example where we try out 38 values between 1 and 75. To do this with caret, we need to define a column named k, so we use this: data.frame(k = seq(1, 75, 2)). Note that when running this code, we are fitting 38 versions of kNN to 25 bootstrapped samples. Since we are fitting \\(38 \\times 25 = 950\\) kNN models, running this code will take several seconds.\n\ntrain_knn &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(1, 75, 2)))\nggplot(train_knn, highlight = TRUE)\n\n\n\n\n\n\n\n\nBecause resampling methods are random procedures, the same code can result in different results. To assure reproducible results you should set the seed, as we did at the start of this chapter.\n\nTo access the parameter that maximized the accuracy, you can use this:\n\ntrain_knn$bestTune\n#&gt;     k\n#&gt; 37 73\n\nand the best performing model like this:\n\ntrain_knn$finalModel\n#&gt; 73-nearest neighbor model\n#&gt; Training set outcome distribution:\n#&gt; \n#&gt;   2   7 \n#&gt; 401 399\n\nThe function predict will use this best performing model. Here is the accuracy of the best model when applied to the test set, which we have not yet used because the cross validation was done on the training set:\n\nconfusionMatrix(predict(train_knn, mnist_27$test, type = \"raw\"),\n                mnist_27$test$y)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.825\n\nBootstrapping is not always the best approach to resampling (see Section 32.4, for an example). If we want to change our resampling method, we can use the trainControl function. For example, the code below runs 10-fold cross validation. This means we have 10 samples using 90% of the observations to train in each sample. We accomplish this using the following code:\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10, p = .9)\ntrain_knn_cv &lt;- train(y ~ ., method = \"knn\", \n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(1, 71, 2)),\n                   trControl = control)\n\n\n\n\n\n\n\nThe results component of the train output includes several summary statistics related to the variability of the cross validation estimates:\n\nnames(train_knn$results)\n#&gt; [1] \"k\"          \"Accuracy\"   \"Kappa\"      \"AccuracySD\" \"KappaSD\"\n\nYou can learn many more details about the caret package, from the manual3.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#preprocessing",
    "href": "ml/ml-in-practice.html#preprocessing",
    "title": "32  Machine learning in practice",
    "section": "\n32.2 Preprocessing",
    "text": "32.2 Preprocessing\nWe often transform predictors before running the machine algorithm. We also remove predictors that are clearly not useful. We call these steps preprocessing.\nExamples of preprocessing include standardizing the predictors, taking the log transform of some predictors, removing predictors that are highly correlated with others, and removing predictors with very few non-unique values or close to zero variation.\nFor example, we can run the nearZeroVar function from the caret package to see that several features do not vary much from observation to observation. We can see that there is a large number of features with close to 0 variability:\n\nlibrary(matrixStats)\nsds &lt;- colSds(x)\nhist(sds, breaks = 256)\n\n\n\n\n\n\n\nThis is expected because there are parts of the image that rarely contain writing (dark pixels).\nThe caret packages includes a function that recommends features to be removed due to near zero variance:\n\nnzv &lt;- nearZeroVar(x)\n\nWe can see the columns recommended for removal are the near the edges:\n\nimage(matrix(1:784 %in% nzv, 28, 28))\n\n\nrafalib::mypar()\nimage(matrix(1:784 %in% nzv, 28, 28))\n\n\n\n\n\n\n\nSo we end up removing\n\nlength(nzv)\n#&gt; [1] 532\n\npredictors.\nThe caret package features the preProcess function, which allows users to establish a predefined set of preprocessing operations based on a training set. This function is designed to apply these operations to new datasets without recalculating anything on the test set, ensuring that all preprocessing steps are consistent and derived solely from the training data.\nBelow is an example demonstrating how to remove predictors with near-zero variance and then center the remaining predictors:\n\npp &lt;- preProcess(x, method = c(\"nzv\", \"center\"))\ncentered_subsetted_x_test &lt;- predict(pp, newdata = x_test)\ndim(centered_subsetted_x_test)\n#&gt; [1] 1000  252\n\nAdditionally, the train function in caret includes a preProcess argument that allows users to specify which preprocessing steps to apply automatically during model training. We will explore this functionality further in the context of a k-Nearest Neighbors model in [@knn-in-practice].",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#parallelization",
    "href": "ml/ml-in-practice.html#parallelization",
    "title": "32  Machine learning in practice",
    "section": "\n32.3 Parallelization",
    "text": "32.3 Parallelization\nDuring cross-validation or bootstrapping, the process of fitting models to different samples or using varying parameters can be performed independently. Imagine you are fitting 100 models; if you had access to 100 computers, you could theoretically speed up the process by a factor of 100 by fitting each model on a separate computer and then aggregating the results. In reality, most modern computers, including many personal computers, are equipped with multiple processors that allow for such parallel execution. This method, known as parallelization, leverages these multiple processors to conduct several computational tasks simultaneously, significantly accelerating the model training process. By distributing the workload across different processors, parallelization makes it feasible to manage large datasets and complex modeling procedures efficiently.\nThe caret package is set up to run in parallel but you have to let R know that you are want to parallelize your work. To do this we can use the doParallel package:\n\nlibrary(doParallel)\n#&gt; Loading required package: foreach\n#&gt; Loading required package: iterators\n#&gt; Loading required package: parallel\nnc &lt;- detectCores() - 1   # it is convention to leave 1 core for the OS\ncl &lt;- makeCluster(nc)\nregisterDoParallel(cl)\n\nIf you do use parallelization, make sure to let R know you are done with the following lines of code:\n\nstopCluster(cl)\nstopImplicitCluster()\n\n\n\n\n\n\n\nWhen parallelizing tasks across multiple processors, it’s important to consider the risk of running out of memory. Each processor might require a copy of the data or substantial portions of it, which can multiply overall memory demands. This is especially challenging if the data or models are large.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#sec-knn-in-practice",
    "href": "ml/ml-in-practice.html#sec-knn-in-practice",
    "title": "32  Machine learning in practice",
    "section": "\n32.4 k-nearest neighbors",
    "text": "32.4 k-nearest neighbors\n\n\n\n\n\n\nBefore starting this section, note that the first two calls to the train function in the code below can take several hours to run. This is a common challenge when training machine learning algorithms since we have to run the algorithm for each cross validation split and each set of tuning parameters being considered. In the next section, we will provide some suggestions on how to predict the duration of the process and ways to reduce.\n\n\n\nAs we will see soon, the optimal \\(k\\) for the MNIST data is between 1 and 7. For small values of \\(k\\), bootstrapping can be problematic for estimating MSE for a k-Nearest Neighbors (kNN). This is because bootstrapping involves sampling with replacement from the original dataset which implies the closest neighbor will often appear twice but considered two independent observations by kNN. This is a unrealistic scenario that can distort the estimate MSE when, for example, \\(k=3\\). We therefore use cross-validation to estimate our MSE.\nThe first step is to optimize for \\(k\\).\n\ntrain_knn &lt;- train(x, y, method = \"knn\", \n                   preProcess = \"nzv\",\n                   trControl = trainControl(\"cv\", number = 20, p = 0.95),\n                   tuneGrid = data.frame(k = seq(1, 7, 2)))\n\nOnce we optimize our algorithm, the predict function defaults to using the best performing algorithm fit with the entire training data:\n\ny_hat_knn &lt;- predict(train_knn, x_test, type = \"raw\")\n\nWe achieve relatively high accuracy:\n\nconfusionMatrix(y_hat_knn, factor(y_test))$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.953\n\n\n32.4.1 Dimension reduction with PCA\nAn alternative to removing low variance columns directly is to use dimension reduction on the feature matrix before applying the algorithms.\n\npca &lt;- prcomp(x)\n\nWe can actually explain, say, 75% of the variability in the predictors with a small number of dimensions:\n\np &lt;- which(cumsum(pca$sdev^2)/sum(pca$sdev^2) &gt;= 0.75)[1]\n\nWe can then re-run our algorithm on these 33 features:\n\nfit_knn_pca &lt;- knn3(pca$x[,1:p], y, k = train_knn$bestTune)\n\nWhen predicting, it is important that we not use the test set when finding the PCs nor any summary of the data, as this could result in overtraining. We therefore compute the averages needed for centering and the rotation on the training set:\n\nnewdata &lt;-  sweep(x_test, 2, colMeans(x)) %*% pca$rotation[,1:p]\ny_hat_knn_pca &lt;- predict(fit_knn_pca, newdata, type = \"class\")\n\nWe obtain similar accuracy, while using only 33 dimensions:\n\nconfusionMatrix(y_hat_knn_pca, factor(y_test))$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.959\n\nIn this example, we used the \\(k\\) optimized for the raw data, not the principal components. Note that to obtain an unbiased MSE estimate we have to recompute the PCA for each cross-validation sample and apply to the validation set. Because the train function includes PCA as one of the available preprocessing operations we can achieve this with this modification of the code above:\n\ntrain_knn_pca &lt;- train(x, y, method = \"knn\", \n                       preProcess = c(\"nzv\", \"pca\"),\n                       trControl = trainControl(\"cv\", number = 20, p = 0.95,\n                                                preProcOptions = list(pcaComp = p)),\n                       tuneGrid = data.frame(k = seq(1, 7, 2)))\ny_hat_knn_pca &lt;- predict(train_knn_pca, x_test, type = \"raw\")\nconfusionMatrix(y_hat_knn_pca, factor(y_test))$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.962\n\nA limitation of this approach is that we don’t get to optimize the number of PCs used in the analysis. To do this we need to write our own method. The caret manual4 describe how to do this.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#random-forest",
    "href": "ml/ml-in-practice.html#random-forest",
    "title": "32  Machine learning in practice",
    "section": "\n32.5 Random Forest",
    "text": "32.5 Random Forest\nWith the random forest algorithm several parameters can be optimized, but the main one is mtry, the number of predictors that are randomly selected for each tree. This is also the only tuning parameter that the caret function train permits when using the default implementation from the randomForest package.\n\nlibrary(randomForest)\ntrain_rf &lt;- train(x, y, method = \"rf\", \n                  preProcess = \"nzv\",\n                  tuneGrid = data.frame(mtry = seq(5, 15)))\ny_hat_rf &lt;- predict(train_rf, x_test, type = \"raw\")\n\nNow that we have optimized our algorithm, we are ready to fit our final model:\n\ny_hat_rf &lt;- predict(train_rf, x_test, type = \"raw\")\n\nAs with kNN, we also achieve high accuracy:\n\nconfusionMatrix(y_hat_rf, y_test)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.951\n\nBy optimizing some of the other algorithm parameters, we can achieve even higher accuracy.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#testing-and-improving-computation-time",
    "href": "ml/ml-in-practice.html#testing-and-improving-computation-time",
    "title": "32  Machine learning in practice",
    "section": "\n32.6 Testing and improving computation time",
    "text": "32.6 Testing and improving computation time\nThe default method for estimating accuracy used by the train function is to test prediction on 25 bootstrap samples. This can result in long compute times. For example, if we are considering several values, say 10, of the tuning parameters, we will fit the algorithm 250 times. We can use the system.time function to estimate how long it takes to run the algorithm once:\n\nnzv &lt;- nearZeroVar(x)\nsystem.time({fit_rf &lt;- randomForest(x[, -nzv], y,  mtry = 9)})\n#&gt;    user  system elapsed \n#&gt;    60.7     0.3    61.0\n\nand use this to estimate the total time for the 250 iterations. In this case it will be several hours.\nOne way to reduce run time is to use k-fold cross validation with a smaller number of test sets. A popular choice is leaving out 5 test sets with 20% of the data. To use this we set the trControl argument in train to trainControl(method = \"cv\", number = 5, p = .8).\nFor random forest, we can also speed up the training step by running less trees per fit. After running the algorithm once, we can use the plot function to see how the error rate changes as the number of trees grows.\nHere we can see that error rate stabilizes after about 200 trees:\n\nplot(fit_rf)\n\n\n\n\n\n\n\nWe can use this finding to speed up the cross validation procedure. Specifically, because the default is 500, by adding the argument ntree = 200 to the call to train above, the procedure will finish 2.5 times faster.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#variable-importance",
    "href": "ml/ml-in-practice.html#variable-importance",
    "title": "32  Machine learning in practice",
    "section": "\n32.7 Variable importance",
    "text": "32.7 Variable importance\nThe following function computes the importance of each feature:\n\nimp &lt;- importance(fit_rf)\n\nWe can see which features are being used most by plotting an image:\n\nmat &lt;- rep(0, ncol(x))\nmat[-nzv] &lt;- imp\nimage(matrix(mat, 28, 28))",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#diagnostics",
    "href": "ml/ml-in-practice.html#diagnostics",
    "title": "32  Machine learning in practice",
    "section": "\n32.8 Diagnostics",
    "text": "32.8 Diagnostics\nAn important part of data analysis is visualizing results to determine why we are failing. How we do this depends on the application. Below we show the images of digits for which we made an incorrect prediction. Here are some errors for the random forest:\n\n\n\n\n\n\n\n\nBy examining errors like this, we often find specific weaknesses to algorithms or parameter choices and can try to correct them.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#ensembles",
    "href": "ml/ml-in-practice.html#ensembles",
    "title": "32  Machine learning in practice",
    "section": "\n32.9 Ensembles",
    "text": "32.9 Ensembles\nThe idea of an ensemble is similar to the idea of combining data from different pollsters to obtain a better estimate of the true support for each candidate.\nIn machine learning, one can usually greatly improve the final results by combining the results of different algorithms.\nHere is a simple example where we compute new class probabilities by taking the average of random forest and kNN. We can see that the accuracy improves:\n\np_rf &lt;- predict(fit_rf, x_test[,-nzv], type = \"prob\")  \np_rf &lt;- p_rf / rowSums(p_rf)\np_knn_pca  &lt;- predict(train_knn_pca, x_test, type = \"prob\")\np &lt;- (p_rf + p_knn_pca)/2\ny_pred &lt;- factor(apply(p, 1, which.max) - 1)\nconfusionMatrix(y_pred, y_test)$overall[\"Accuracy\"]\n#&gt; Accuracy \n#&gt;    0.968\n\nWe have just built an ensemble with just two algorithms. By combing more similarly performing, but uncorrelated, algorithms we can improve accuracy further.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#exercises",
    "href": "ml/ml-in-practice.html#exercises",
    "title": "32  Machine learning in practice",
    "section": "\n32.10 Exercises",
    "text": "32.10 Exercises\n1. In the exercises in Chapter 30 we saw that changing maxnodes or nodesize in the randomForest function improved our estimate. Let’s use the train function to help us pick these values. From the caret manual we see that we can’t tune the maxnodes parameter or the nodesize argument with randomForest, so we will use the Rborist package and tune the minNode argument. Use the train function to try values minNode &lt;- seq(5, 250, 25). See which value minimizes the estimated RMSE.\n2. This dslabs tissue_gene_expression dataset includes a matrix x:\n\nlibrary(dslabs)\ndim(tissue_gene_expression$x)\n\nwith the gene expression measured on 500 genes for 189 biological samples representing seven different tissues. The tissue type is stored in y:\n\ntable(tissue_gene_expression$y)\n\nSplit the data in training and test sets, then use kNN to predict tissue type and see what accuracy you obtain. Try it for \\(k = 1, 3, \\dots, 11\\).\n3. We are going to apply LDA and QDA to the tissue_gene_expression dataset. We will start with simple examples based on this dataset and then develop a realistic example.\nCreate a dataset with just the classes cerebellum and hippocampus (two parts of the brain) and a predictor matrix with 10 randomly selected columns. Estimate the accuracy of LDA.\n\nset.seed(1993)\ntissues &lt;- c(\"cerebellum\", \"hippocampus\")\nind &lt;- which(tissue_gene_expression$y %in% tissues)\ny &lt;- droplevels(tissue_gene_expression$y[ind])\nx &lt;- tissue_gene_expression$x[ind, ]\nx &lt;- x[, sample(ncol(x), 10)]\n\n4. In this case, LDA fits two 10-dimensional normal distributions. Look at the fitted model by looking at the finalModel component of the result of train. Notice there is a component called means that includes the estimate means of both distributions. Plot the mean vectors against each other and determine which predictors (genes) appear to be driving the algorithm.\n5. Repeat exercises 3 with QDA. Does it have a higher accuracy than LDA?\n6. Are the same predictors (genes) driving the algorithm? Make a plot as in exercise 4.\n7. One thing we see in the previous plot is that the value of predictors correlate in both groups: some predictors are low in both groups while others are high in both groups. The mean value of each predictor, colMeans(x), is not informative or useful for prediction and often, for interpretation purposes, it is useful to center or scale each column. This can be achieved with the preProcessing argument in train. Re-run LDA with preProcessing = \"scale\". Note that accuracy does not change but see how it is easier to identify the predictors that differ more between groups in the plot made in exercise 4.\n8. In the previous exercises, we saw that both approaches worked well. Plot the predictor values for the two genes with the largest differences between the two groups in a scatterplot to see how they appear to follow a bivariate distribution as assumed by the LDA and QDA approaches. Color the points by the outcome.\n9. Now we are going to increase the complexity of the challenge slightly: we will consider all the tissue types.\n\nset.seed(1993)\ny &lt;- tissue_gene_expression$y\nx &lt;- tissue_gene_expression$x\nx &lt;- x[, sample(ncol(x), 10)]\n\nWhat accuracy do you get with LDA?\n10. We see that the results are slightly worse. Use the confusionMatrix function to learn what type of errors we are making.\n11. Plot an image of the centers of the seven 10-dimensional normal distributions.\n12. Make a scatterplot along with the prediction from the best fitted model.\n13. Use the rpart function to fit a classification tree to the tissue_gene_expression dataset. Use the train function to estimate the accuracy. Try out cp values of seq(0, 0.05, 0.01). Plot the accuracy to report the results of the best model.\n14. Study the confusion matrix for the best fitting classification tree. What do you observe happening for placenta?\n15. Notice that placentas are called endometrium more often than placenta. Note also that the number of placentas is just six, and that, by default, rpart requires 20 observations before splitting a node. Thus it is not possible with these parameters to have a node in which placentas are the majority. Rerun the above analysis, but this time permit rpart to split any node by using the argument control = rpart.control(minsplit = 0). Does the accuracy increase? Look at the confusion matrix again.\n16. Plot the tree from the best fitting model obtained in exercise 11.\n17. We can see that with just six genes, we are able to predict the tissue type. Now let’s see if we can do even better with a random forest. Use the train function and the rf method to train a random forest. Try out values of mtry ranging from, at least, seq(50, 200, 25). What mtry value maximizes accuracy? To permit small nodesize to grow as we did with the classification trees, use the following argument: nodesize = 1. This will take several seconds to run. If you want to test it out, try using smaller values with ntree. Set the seed to 1990.\n18. Use the function varImp on the output of train and save it to an object called imp.\n19. The rpart model we ran above produced a tree that used just six predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was fit_rpart, we can extract the names like this:\n\nind &lt;- !(fit_rpart$finalModel$frame$var == \"&lt;leaf&gt;\")\ntree_terms &lt;- \n  fit_rpart$finalModel$frame$var[ind] |&gt;\n  unique() |&gt;\n  as.character()\ntree_terms\n\nWhat is the variable importance in the random forest call for these predictors? Where do they rank?\n20. Extract the top 50 predictors based on importance, take a subset of x with just these predictors and apply the function heatmap to see how these genes behave across the tissues. We will introduce the heatmap function in Chapter 31.\n21. Previously, we compared the conditional probability \\(p(\\mathbf{x})\\) given two predictors \\(\\mathbf{x} = (x_1, x_2)^\\top\\) to the fit \\(\\hat{p}(\\mathbf{x})\\) obtained with a machine learning algorithm by making image plots. The following code can be used to make these images and include a curve at the values of \\(x_1\\) and \\(x_2\\) for which the function is \\(0.5\\):\n\nplot_cond_prob &lt;- function(x_1, x_2, p){\n  data.frame(x_1 = x_1, x_2 = x_2, p = p) |&gt;\n    ggplot(aes(x_1, x_2)) +\n    geom_raster(aes(fill = p), show.legend = FALSE) +\n    stat_contour(aes(z = p), breaks = 0.5, color = \"black\") +\n    scale_fill_gradientn(colors = c(\"#F8766D\", \"white\", \"#00BFC4\"))\n}\n\nWe can see the true conditional probability for the 2 or 7 example like this:\n\nwith(mnist_27$true_p, plot_cond_prob(x_1, x_2, p))\n\nFit a kNN model and make this plot for the estimated conditional probability. Hint: Use the argument newdata = mnist_27$train to obtain predictions for a grid points.\n22. Notice that, in the plot made in exercise 1, the boundary is somewhat wiggly. This is because kNN, like the basic bin smoother, does not use a kernel. To improve this we could try loess. By reading through the available models part of the caret manual, we see that we can use the gamLoess method. We need to install the gam package, if we have not done so already. We see that we have two parameters to optimize:\n\nmodelLookup(\"gamLoess\")\n#&gt;      model parameter  label forReg forClass probModel\n#&gt; 1 gamLoess      span   Span   TRUE     TRUE      TRUE\n#&gt; 2 gamLoess    degree Degree   TRUE     TRUE      TRUE\n\nUse cross-validation to pick a span between 0.15 and 0.75. Keep degree = 1. What span does cross-validation select?\n23. Show an image plot of the estimate \\(\\hat{p}(x,y)\\) resulting from the model fit in exercise 22. How does the accuracy compare to that of kNN? Comment on the difference between the estimate obtained with kNN.\n24. Use the mnist_27 training set to build a model with several of the models available from the caret package. For example, you can try these:\n\nmodels &lt;- c(\"glm\", \"lda\",  \"naive_bayes\",  \"svmLinear\", \"gamboost\",  \n            \"gamLoess\", \"qda\", \"knn\", \"kknn\", \"loclda\", \"gam\", \"rf\", \n            \"ranger\",\"wsrf\", \"Rborist\", \"avNNet\", \"mlp\", \"monmlp\", \"gbm\", \n            \"adaboost\", \"svmRadial\", \"svmRadialCost\", \"svmRadialSigma\")\n\nWe have not explained many of these, but apply them anyway using train with all the default parameters. Keep the results in a list. You might need to install some packages. Keep in mind that you will likely get some warnings.\n25. Now that you have all the trained models in a list, use sapply or map to create a matrix of predictions for the test set. You should end up with a matrix with length(mnist_27$test$y) rows and length(models) columns.\n26. Compute accuracy for each model on the test set.\n27. Build an ensemble prediction by majority vote and compute its accuracy.\n28. Earlier we computed the accuracy of each method on the training set and noticed they varied. Which individual methods do better than the ensemble?\n29. It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from cross validation with the training data. Obtain these estimates and save them in an object.\n30. Now let’s only consider the methods with an estimated accuracy of 0.8 when constructing the ensemble. What is the accuracy now?\n31. Note that if two machine algorithms methods predict the same outcome, ensembling them will not change the prediction. For each pair of algorithms compare the percent of observations for which they make the same prediction. Use this to define a function and then use the heatmap function to visualize the results. Hint: use the method = \"binary\" argument in the dist function.\n32. Note that each method can also produce an estimated conditional probability. Instead of majority vote, we can take the average of these estimated conditional probabilities. For most methods, we can the use the type = \"prob\" in the train function. Note that some of the methods require you to use the argument trControl=trainControl(classProbs=TRUE) when calling train. Also, these methods do not work if classes have numbers as names. Hint: change the levels like this:\n\ndat$train$y &lt;- recode_factor(dat$train$y, \"2\"=\"two\", \"7\"=\"seven\")\ndat$test$y &lt;- recode_factor(dat$test$y, \"2\"=\"two\", \"7\"=\"seven\")\n\n33. In this chapter, we illustrated a couple of machine learning algorithms on a subset of the MNIST dataset. Try fitting a model to the entire dataset.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  },
  {
    "objectID": "ml/ml-in-practice.html#footnotes",
    "href": "ml/ml-in-practice.html#footnotes",
    "title": "32  Machine learning in practice",
    "section": "",
    "text": "https://topepo.github.io/caret/available-models.html↩︎\nhttp://topepo.github.io/caret/available-models.html↩︎\nhttps://topepo.github.io/caret/available-models.html↩︎\nhttps://topepo.github.io/caret/using-your-own-model-in-train.html↩︎",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Machine learning in practice</span>"
    ]
  }
]