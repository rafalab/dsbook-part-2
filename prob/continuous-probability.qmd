# Continuous Probability

In @sec-ecdf-intro, we discussed why it is not practical to assign a probability to every possible continuous outcome, such as an exact height, since there are infinitely many possible values. The same idea extends to outcomes that take values on a continuous scale: each individual value has probability zero. Instead, we describe their behavior through probability density functions, which let us compute probabilities for intervals of values rather than single points.  

In this chapter, we introduce the mathematical framework for continuous probability distributions and present several useful approximations that frequently appear in data analysis.

## Cumulative distribution functions {#sec-cdf-intro}

We return to our example using the heights of adult male students:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
x <- heights |> filter(sex == "Male") |> pull(height)
```

We previously defined the empirical cumulative distribution function (eCDF) as

```{r}
F <- function(a) mean(x <= a)
```

which, for any value `a`, gives the proportion of values in the list `x` that are less than or equal to `a`.

To connect the eCDF to probability, imagine randomly selecting one of the male students. What is the chance that he is taller than 70.5 inches? Because each student is equally likely to be chosen, this probability is simply the proportion of students taller than 70.5 inches. Using the eCDF, we can compute it as:

```{r}
1 - F(70.5)
```

The cumulative distribution function (CDF) is the theoretical counterpart of the eCDF. Rather than relying on observed data, it assigns probabilities to ranges of values for a random outcome $X$. Specifically, the CDF gives, for any number $a$, the probability that $X$ is less than or equal to $a$:

$$
F(a) = \Pr(X \leq a)
$$

Once the CDF is defined, we can compute the probability that $X$ falls within any interval. For example, the probability that a student's height is between $a$ and $b$ is:

$$
\Pr(a < X \leq b) = F(b) - F(a)
$$

Because we can determine the probability of any event from the CDF, it fully defines the probability distribution of a continuous outcome.


## Probability density function

For most continuous distributions, we can describe the cumulative distribution function (CDF) in terms of another function, $f(x)$, such that

$$
F(b) - F(a) = \int_a^b f(x)\,dx
$$ 

This function $f(x)$ is called the *probability density function* (PDF).  

The PDF plays a role similar to the relative frequency distribution for discrete data. However, instead of assigning probabilities to individual outcomes, which would all be zero for a continuous variable, the PDF describes how probability is distributed across values of $x$. We can think of it as defining the shape of the distribution: wider regions under the curve correspond to more likely ranges of values.  

To build intuition, imagine dividing the range of possible outcomes into many tiny intervals. The width of each interval forms the base of a rectangle, and $f(x)$ determines its height. The total area of all rectangles approximates the probability of observing a value between $a$ and $b$. In the limit, this approximation becomes an integral:

```{r probability-density-explanation, echo=FALSE}
cont <- data.frame(x = seq(0, 5, len = 300), y = dgamma(seq(0, 5, len = 300), 2, 2))
disc <- data.frame(x = seq(0, 5, 0.075), y = dgamma(seq(0, 5, 0.075), 2, 2))
ggplot(mapping = aes(x, y)) +
  geom_col(data = disc, fill = "grey") +
  geom_line(data = cont) +
  ylab("f(x)")
```

An important example is the normal distribution, introduced in @sec-normal-distribution. Its  probability density function is

$$
f(x) = \frac{1}{\sqrt{2\pi}\,\sigma} \exp\left(-\frac{1}{2}\left(\frac{x - m}{s}\right)^2\right)
$$

Integrating this function gives the CDF of the normal distribution. In R, the corresponding function is `pnorm`. A random outcome is said to be *normally distributed* with mean `m` and standard deviation `s` if its CDF is defined by

```{r}
#| eval: false
F(a) <- pnorm(a, mean = m, sd = s)
```

This is particularly useful in practice. If we are willing to assume that a variable such as height follows a normal distribution, we can answer probability questions without needing the full dataset. For example, to find the probability that a randomly selected student is taller than 70 inches, we only need the sample mean and standard deviation:

```{r}
m <- mean(x)
s <- sd(x)
1 - pnorm(70.5, m, s)
```


## Theoretical distributions as approximations

The normal distribution is defined mathematically, without relying on data. In practice, however, almost all the quantities we analyze come from discrete observations. For instance, our height data can be viewed as categorical, with each unique height representing a category and its probability given by its relative frequency. However, as we described in @sec-ecdf-intro, while these reported values appear discrete, this discreteness arises from rounding. A few students reported exact metric conversions, such as 177 cm = 69.685 inches, while most rounded to the nearest inch. It is therefore more useful to treat height as a *continuous* variable, recognizing that no one is exactly 70 inches tall—the higher frequency at 70 simply reflects rounding.

In continuous distributions, individual points have probability zero. Instead, we work with intervals, asking questions such as: what is the probability that a height falls between 69.5 and 70.5 inches? For rounded data, this matches the natural interval corresponding to a single reported inch.

The normal distribution provides a convenient way to approximate these probabilities. For example:

```{r}
mean(x < 70.5) - mean(x <= 69.5)
pnorm(70.5, m, s) - pnorm(69.5, m, s)
```

The approximation is close for intervals aligned with the rounding, though the approximation deteriorates for smaller, uneven ranges. This discrepancy reflects *discretization* rather than a flaw in the normal model itself. As long as we are aware of this limitation, treating rounded data as continuous and using normal approximations remains an effective and practical approach.

## Monte Carlo simulations

Simulation is a powerful way to understand randomness, approximate probabilities, and explore how theoretical models behave in practice. Many of the probability models used in data analysis are continuous distributions, which describe outcomes that can take any value within a range. 
In R, all random-number–generating functions that produce simulated data from a continuous distribution, begin with the letter `r`. These functions form part of a consistent family that allows us to simulate from nearly any probability distribution.  In this section, we illustrate Monte Carlo techniques using continuous distributions—starting with the normal distribution and extending to other commonly used models.

### Normal distribution

R provides the `rnorm` function to generate normally distributed outcomes. It takes three arguments, sample size, mean (default 0), and standard deviation (default 1), and produces random numbers that follow a normal distribution.

Here is an example of how we could generate data that resemble our reported heights:

```{r}
n <- length(x)
m <- mean(x)
s <- sd(x)
simulated_heights <- rnorm(n, m, s)
```

Not surprisingly, the distribution of the simulated heights looks normal:

```{r simulated-heights, echo=FALSE}
data.frame(simulated_heights = simulated_heights) |>
  ggplot(aes(simulated_heights)) + 
  geom_histogram(color = "black", binwidth = 2) 
```

This is one of the most useful functions in R because it lets us generate data that mimic natural variation and explore what outcomes might occur by chance through Monte Carlo simulations.

### Example: extreme values

How rare is it to find a seven-footer among 800 men? Suppose we repeatedly sample 800 male students at random and record the tallest in each group. What does the distribution of these tallest height look like?  The following Monte Carlo simulation helps us find out:

```{r}
tallest <- replicate(10000, max(rnorm(800, m, s)))
```

Having a seven-footer is quite rare:

```{r}
mean(tallest >= 7*12)
```

Here is the resulting distribution of the tallest person's height:

```{r simulated-tallest-height, echo=FALSE}
data.frame(tallest = tallest) |> ggplot(aes(tallest)) + 
  geom_histogram(color = "black", binwidth = 1) 
```

Although deriving this distribution analytically is possible, it is not straightforward. Once derived, it provides a faster way to evaluate probabilities than simulation. However, when analytic derivations are too complex or infeasible, Monte Carlo simulation offers a practical alternative. By repeatedly generating random samples, we can approximate the distribution of almost any statistic and obtain reliable estimates even when theoretical results are unavailable.

### Other continuous distributions

The normal distribution is not the only useful theoretical model. Other continuous distributions that often appear in data analysis include the Student's *t*, chi-square, exponential, gamma, and beta distributions. Their corresponding shorthand names in R are `t`, `chisq`, `exp`, `gamma`, and `beta`.

Each of these distributions has an associated `r` function, such as `rt`, `rchisq`, `rexp`, `rgamma`, and `rbeta`, that allows you to generate random samples for simulation. This consistency makes it easy to apply Monte Carlo methods to a wide variety of data-generating processes.


## Calculating densities, probabilities, and quantiles

Beyond simulation, R provides companion functions to evaluate and summarize continuous distributions. Each distribution in base R follows a simple naming convention:

* `d` - density function
* `p` - cumulative distribution function (CDF)
* `q` - quantile function
* `r` - random number generation (used in Monte Carlo)

For example, the Student's *t* distribution uses `dt`, `pt`, `qt`, and `rt`. Base R includes the most common continuous distributions, normal, *t*, chi-square, exponential, gamma, and beta, but many additional distributions are available in specialized packages such as **extraDistr** and **actuar**. While there are dozens of continuous distributions in use, the ones provided by base R cover the majority of applications encountered in practice.


## Exercises

1\. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?

2\. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?

3\. Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?

4\. Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?

5\. Notice that the answer to the question does not change when you change units. This makes sense since the standard deviations from the average for an entry in a list are not affected by what units we use. In fact, if you look closely, you notice that 61 and 67 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.

6\. To understand the mathematical rationale that explains why the answers to exercises 3, 4, and 5, suppose we have a random variable $X$ with average $m$ and standard error $s$. Suppose we ask the probability o$X$ being smaller or equal to $a$. Remember that, by definition, $a$ is $(a - m)/s$ standard deviations $s$ away from the average $m$. The probability is:

$$
\mathrm{Pr}(X \leq a)
$$

Now we subtract $\mu$ to both sides and then divide both sides by $\sigma$:

$$
\mathrm{Pr}\left(\frac{X-m}{s} \leq \frac{a-m}{s} \right)
$$

The quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it $Z$:

$$
\mathrm{Pr}\left(Z \leq \frac{a-\mu}{\sigma} \right)
$$

So, no matter the units, the probability of $X\leq a$ is the same as the probability of a standard normal variable being less than $(a - m)/s$. If `m` is the average and `s` the standard error, which of the following R code would give us the right answer in every situation?

a.  `mean(X <= a)`
b.  `pnorm((a - m)/s)`
c.  `pnorm((a - m)/s, m, s)`
d.  `pnorm(a)`

7\. Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use `qnorm`.

8\. IQ scores are approximately normally distributed, with a mean of 100 and a standard deviation of 15. Suppose we want to know what the distribution of the highest IQ would look like across all high school graduating classes if 10,000 people are born each year in a school district. Run a Monte Carlo simulation with `B = 1000`, where each iteration generates 10,000 IQ scores and records the highest value. Then, create a histogram to visualize the distribution of the maximum IQ in the district.
