# Distributions {#sec-summaries-distributions}

```{r}
#| cache: false
#| warning: false
#| message: false
#| echo: false

library(tidyverse)
library(dslabs)
```

Before we begin, we clarify that this chapter will not cover distributions in the context of probability, which is the mathematical framework used to model uncertainty. We postpone that discussion until the next part of the book, where we will use it throughout. Here, we focus on _distributions_ as a way to describe and summarize data we’ve already collected. Our goal is to understand the variation and patterns present in a dataset, without yet making probabilistic claims or predictions. The concepts introduced in this chapter, such as histograms, averages, and standard deviations, will later serve as the foundation for understanding probability distributions, where similar tools are used to describe the likely outcomes of random processes.

To illustrate the concepts needed to understand distribution and how they relate to summary statistics, we will pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the `heights` data frame included in the **dslabs** package.

```{r, eval = FALSE}
library(dslabs)
str(heights)
```


One way to convey the heights to ET is to simply send him this list of `r nrow(heights)` heights. However, there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights. We examine the female height data in @sec-stratification.

At this stage, we are thinking of distributions purely as tools for summarizing and describing data. In the next part of the book, we will connect these same ideas to probability, where distributions help us model uncertainty. For now, our goal is simply to explore the patterns and variation present in the data we do have.

It turns out that, in some cases, the two summary statistics, the average and the standard deviation are all we need to understand the data. We will learn data visualization techniques that will help us determine when this two-number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.

## Variable types 

Knowing what variable type we are working with is important because it will help us determining the most effective way to summarize our data and display its distribution.

We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.

When data values represent groups instead of numbers, we refer to the data as _categorical_. Two simple examples are sex (male or female) and US regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers, such as spiciness (mild, medium, hot). These ordered categorical data are referred to as _ordinal_ data.

Numerical data can be continuous or discrete. A continuous variable can take on any value within a range. For example, if measured with sufficient precision, height is continuous because the difference between two individuals (even twins) can be an arbitrarily small fraction. However, if height is rounded to the nearest inch, it becomes discrete since values must be a whole number. Other examples of discrete data are the number of heads when tossing 10 coins, the number of cigarettes a person smokes a day, or the number of customers visiting a store in an hour. In these cases, the values are countable and cannot take on fractional values. Unlike continuous data, which can be measured with increasing precision, discrete data are limited to distinct, separate values.

In practice, discrete variables are often treated as continuous. For example, a digital measuring instrument might record data as 16-bit integers. However, because it is rare to obtain exactly the same reading more than once, it is often convenient to treat the data as continuous. Another example is country population size. Although population counts are always whole numbers, it is uncommon for two jurisdictions to have exactly the same population. For this reason, population size is also commonly treated as continuous.


## Relative frequency distribution {#sec-pmf-intro}

The most basic statistical summary of a list of numbers or categories is its *distribution*. The simplest way to think of a distribution is as a compact description of values with many entries. This concept should not be new for readers of this book. For example, with categorical data, the _relative frequency distribution_ simply describes the proportion of each unique category:

$$
p_k = \frac{\mbox{number of times category }k \mbox{ appears in the list}}{n}
$$

with $n$ the length of the list.

Here is an example with US state regions:

```{r summaries-state-region-distribution}
prop.table(table(state.region))
```

When the data are discrete numerical values, the relative frequency distribution is defined in the same way, except it is more common to use this notation since it can be considered a function of numerical values:

$$
f(x) = \frac{\mbox{number of times value } x \mbox{ appears in the list}}{n}
$$


Here is the distribution for US state populations rounded to the nearest million, which are discrete numerical values:

```{r summaries-rounded-heights-distribution}
#| echo: false
murders |> mutate(x = round(population/10^6)) |> count(x) |> mutate(f = n/sum(n)) |>
  ggplot(aes(x, f)) + geom_col() + 
  scale_x_continuous(breaks = seq(1, 37, by = 1)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 8)) +
  ylab("f(x)")
```

## Empirical cumulative distribution functions {#sec-ecdf-intro}

When the data is continuous, the task of constructing a summary based on a distribution is more challenging because reporting the frequency of each entry is not an effective summary: most entries are unique so the relative frequency distribution does not summarize much.

Here are the relative frequencies of US state populations:

```{r relative-frequencies-of-US}
#| echo: false
murders |> filter(state %in% state.name) |>
  count(population) |> mutate(f = n/sum(n)) |> ggplot(aes(population, f)) + 
  geom_col() + xlab("x") + ylab("f(x)")
```

Since there are 50 states, each with a unique population value $x$, the plot shows $f(x) = \frac{1}{50} = 0.02$ for each state.

To define a distribution for continuous numeric data we define a function that reports the proportion of the data entries $x$ that are below $a$, for all possible values of $a$. This function is called the empirical cumulative distribution function (eCDF) and often denoted with $F$:

$$ F(a) = \mbox{Proportion of data points that are less than or equal to }a$$

Here is the eCDF for state population data above:

```{r state-population-ecdf}
#| echo: false
murders |> filter(state %in% state.name) |>
  ggplot(aes(population)) + 
  stat_ecdf() +
  labs(x = "a", y = "F(a)")
```

Similar to how a frequency table summarizes categorical data, the empirical cumulative distribution function (eCDF) summarizes continuous data. The eCDF provides a clear picture of how the values are distributed and highlights key characteristics of the dataset. For instance, in the population data, we can see that about half of the states have populations greater than 5,000,000, while most states fall below 20,000,000. This type of summary helps us quickly identify medians, ranges, and other important distributional features.

In our heights case study, most students reported heights rounded to the nearest inch. However, some did not. For example, one student reported a height of `68.503937007874` inches, and another reported `68.8976377952756` inches. These values correspond to 174 cm and 175 cm converted to inches, respectively, suggesting that some students entered their heights in centimeters and relied on a conversion to inches.

As a result the data is continuos and the relative frequency distribution is somewhat confusing:

```{r frequency-of-continuous-data-not-good}
#| echo: false
heights |> filter(sex == "Male") |> 
  count(height) |> mutate(f = n/sum(n)) |> ggplot(aes(height, f)) + 
  geom_col(width = 0.1) +
  xlab("x = height") + ylab("f(x)")
```

Although this approach preserves all the information from the list of observations, it can give misleading impressions if extrapolating insights to a broader population. For example, it might suggest there are no people with heights between 74 and 74.5 inches or that significantly more people are exactly 72 inches tall than 71.5 inches, patterns we know aren't accurate. In contrast, the eCDF provides a more useful summary:

```{r summaries-ecdf}
#| echo: false
heights |> filter(sex == "Male") |> 
  ggplot(aes(height)) + 
  stat_ecdf() +
  labs(x = "a", y = "F(a)")
```

From the plot, we can see that `r round(ecdf(heights$height[heights$sex=="Male"])(66)*100)`% of the values are below 65, since $F(66)=$ `r round(ecdf(heights$height[heights$sex=="Male"])(66), 2)`, or that `r round(ecdf(heights$height[heights$sex=="Male"])(72)*100)`% of the values are below 72, since $F(72)=$ `r round(ecdf(heights$height[heights$sex=="Male"])(72), 2)`, and so on. In fact, we can report the proportion of values between any two heights, say $a$ and $b$, by computing $F(b) - F(a)$. This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression "a picture is worth a thousand words", in this case, a picture is as informative as `r sum(heights$sex=="Male")` numbers.


:::{.callout-note}
The reason we add the word *empirical* is because, as we will see in @sec-cdf-intro, the cumulative distribution function (CDF) can be defined mathematically, meaning without any data.
:::

## Histograms

Although the empirical cumulative distribution function (eCDF) is a fundamental concept in statistics, it is not commonly used in practice. The main reason is that it does not easily highlight key distribution characteristics, such as the central tendency, symmetry, or the range that contains 95% of the values. In contrast, histograms are widely preferred because they make these features much more interpretable. While histograms sacrifice a small amount of information, they provide a much clearer summary of the data.

To construct a histogram, we first divide the range of the data into non-overlapping bins. Each bin acts as a category, and its height is chosen so that the area of the bar, calculated as the height multiplied by the bin width, represents the relative frequency of observations within that bin. Although a histogram resembles a bar plot, it differs in that the x-axis represents numerical values rather than categorical ones.

If we round the reported heights to the nearest inch and create a relative frequency distribution, we obtain a histogram with bins defined in one-inch intervals:

$$(49.5, 50.5], (50.5, 51.5], (51.5, 52.5], (52.5, 53.5], \dots, (82.5, 83.5]$$

```{r frequency-distribution-rounded-heights}
heights |> filter(sex == "Male") |> 
  mutate(height = round(height)) |>
  count(height) |> mutate(f = n / sum(n)) |> 
  ggplot(aes(height, f)) + geom_col()
```

The `hist` function allows us quickly construct a histogram. We can define custom bin intervals which is particularly useful for grouping extreme values and avoiding empty bins:

```{r hist-custom-bin-example}
with(heights, hist(height[sex == "Male"], breaks = c(50, 55, 60:80, 85)))
```

If we were to send either of these plots to an extraterrestrial unfamiliar with human height distributions, they would immediately gain key insights about the data. First, the height values range from 50 to 84 inches, with over 95% of observations falling between 63 and 75 inches. Second, the distribution is approximately symmetric around 69 inches. Additionally, by summing bin counts, one could estimate the proportion of data falling within any given interval. Thus, a histogram provides an intuitive summary that preserves most of the information from the original `r sum(heights$sex=="Male")` height measurements using only about 30 bin counts.

What information is lost?  A histogram treats all values within a bin as if they were the same when computing bar heights. For instance, values of 64.0, 64.1, and 64.2 inches are grouped together. However, since these differences are barely perceptible, the practical impact is negligible, and we achieve a meaningful summary using just 23 numbers.

:::{.callout-note}
By default, histograms often display frequency on the y-axis instead of relative frequency. The **ggplot2** package follows this convention:

```{r summaries-height-histogram}
#| message: false
heights |> filter(sex == "Male") |> ggplot(aes(height)) + geom_histogram()
```

When displaying frequencies, it is important to use equally spaced bins, as larger bins will naturally contain more observations and may distort the visual representation of the data. To instead display relative frequency, you can modify the code by setting `y = after_stat(density)`:

```{r summaries-height-histogram-ggplot2}
#| eval: false
heights |> filter(sex == "Male") |> 
  ggplot(aes(x = height, y = after_stat(density))) + 
  geom_histogram(breaks = c(50, 55, 60:80, 85), fill = "grey", color = "black")
```

We use `fill` and `color` to mimic the behavior or `hist`.
:::

## Smoothed densitiy plots

Smooth density plots are similar to histograms, but the data is not divided into bins. Here is what a smooth density plot looks like for our heights data:

```{r summaries-example-of-smoothed-density}
heights |> filter(sex == "Male") |> 
  ggplot(aes(height)) + geom_density(alpha = 0.2, fill = "#00BFC4")
```

In this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. 

To understand the smooth densities, we have to understand *estimates*, a topic we don't cover until later. However, we provide a heuristic explanation to help you understand the basics.

The main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of `r sum(heights$sex=="Male")` male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let's say there are 1,000,000 of these measurements. This list of values has a distribution, like any other list of values, and what we truly want to report to ET is this larger distribution, as it is much more general. Unfortunately, we don't get to see it.

However, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don't have big jumps in the heights of consecutive bins. Below, we present a hypothetical histogram with bins of size 1:

```{r summaries--simulated-data-histogram-1, echo = FALSE}
set.seed(1988)
x <- data.frame(height = c(rnorm(1000000,69,3), rnorm(1000000,65,3)))
x |> ggplot(aes(height)) + geom_histogram(binwidth = 1, fill = "grey", color = "black")
```

The smaller we make the bins, the smoother the histogram becomes. Below are the histograms with bin width of 1, 0.5, and 0.1:

```{r summaries-simulated-data-histogram-2, fig.width = 9, fig.height = 3,  out.width = "100%",echo = FALSE, message = FALSE}
p1 <- x |> 
  ggplot(aes(height)) + 
  geom_histogram(binwidth = 1, fill = "grey", color = "black") + 
  ggtitle("binwidth = 1")
p2 <- x |> 
  ggplot(aes(height)) + 
  geom_histogram(binwidth = 0.5, fill = "grey", color = "black") + 
  ggtitle("binwidth = 0.5")
p3 <- x |> 
  ggplot(aes(height)) + 
  geom_histogram(binwidth = 0.25, fill = "grey", color = "black") + 
  ggtitle("binwidth = 0.25")
library(gridExtra)
grid.arrange(p1, p2, p3, nrow = 1)
```

The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts:

```{r, simulated-density-1, echo = FALSE}
x |> ggplot(aes(height)) + 
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.25) +
  geom_line(stat = 'density')
```

Now, back to reality. We don't have millions of measurements. Instead, we have `r sum(heights$sex=="Male")` and we can't make a histogram with very small bins.

Therefore, we make a histogram using bin sizes appropriate for our data, computing frequencies rather than counts. Additionally, we draw a smooth curve that passes through the tops of the histogram bars. The following plots demonstrate the steps that lead to a smooth density:

```{r summaries-smooth-density-2, echo = FALSE, out.width = "100%"}
hist1 <- heights |> 
  filter(sex == "Male") |> 
  ggplot(aes(height)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 1, color = "black") 
hist2 <- hist1 +
  geom_line(stat = 'density')
hist3 <- hist1 + 
  geom_point(data = ggplot_build(hist2)$data[[1]], aes(x,y), col = "blue")
hist4 <- ggplot() + geom_point(data = ggplot_build(hist2)$data[[1]], aes(x,y), col = "blue") + 
  xlab("height") + ylab("density")
hist5 <- hist4 + geom_line(data = ggplot_build(hist2)$data[[2]], aes(x,y))
hist6 <- heights |> 
  filter(sex == "Male") |> 
  ggplot(aes(height)) +
  geom_density(alpha = 0.2, fill = "#00BFC4", col = 0) +
  geom_line(stat = 'density') +
  scale_y_continuous(limits = layer_scales(hist2)$y$range$range)
  
grid.arrange(hist1, hist3, hist4, hist5, hist2, hist6, nrow = 2)
```

However, remember that *smooth* is a relative term. We can actually control the *smoothness* of the curve that defines the smooth density through an option in the function that computes the smooth density curve. Here are two examples using different degrees of smoothness on the same histogram:

```{r summaries-densities-different-smoothness, out.width = "100%", fig.width = 6, fig.height = 3, cache=FALSE, message=FALSE}
p <- heights |> filter(sex == "Male") |> 
  ggplot(aes(height)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 1, alpha = 0.5) 

p1 <- p +  geom_line(stat = 'density', adjust = 0.5)
p2 <- p +  geom_line(stat = 'density', adjust = 2) 

library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
```

We need to make this choice with care as the resulting summary can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71 than to the proportion that is 78 or 65. This implies that the curve should be relatively smooth, resembling the example on the right more than the one on the left.

While the histogram is an assumption-free summary, the smoothed density is based on some assumptions.

Note that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine that we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:

```{r summaries-area-under-curve, echo = FALSE}
d <- with(heights, density(height[sex == "Male"]))
tmp <- data.frame(height = d$x, density = d$y)
tmp |> ggplot(aes(height,density)) + geom_line() + 
  geom_area(aes(x = height, y = density), data = filter(tmp, between(height, 65, 68)), alpha = 0.2, fill = "#00BFC4")
```

The proportion of this area is about `r round(mean(dplyr::between(heights$height[heights$sex=="Male"], 65, 68)), 2)`, meaning that about `r noquote(paste0(round(mean(dplyr::between(heights$height[heights$sex=="Male"], 65, 68)), 2)*100, '%'))` of male heights are between 65 and 68 inches.

By understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:

```{r summaries-example-of-smoothed-density-2, echo = FALSE}
heights |> 
  filter(sex == "Male") |> 
  ggplot(aes(height)) + 
  geom_density(alpha = 0.2, fill = "#00BFC4")
```

:::{.callout-note}
With the material covered up to this point, you can complete exercises 1 through 10.
:::

## The normal distribution {#sec-normal-distribution}

Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.

The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. One reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for these occurrences, which we will describe later. Here we focus on how the normal distribution helps us summarize data.

Rather than using data, the normal distribution is defined with a mathematical formula. For any interval $(a,b)$, the proportion of values in that interval can be computed using this formula:

$$\mathrm{Pr}(a < x \leq b) = \int_a^b \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} \, dx$$

You don't need to memorize the formula to use the normal distribution in practics. The most important characterstics is that it is completely defined by just two parameters: $\mu$ and $\sigma$. The rest of the symbols in the formula represent the interval ends, $a$ and $b$, and known mathematical constants $\pi$ and $e$. These two parameters, $\mu$ and $\sigma$, are referred to as the *mean* and the *standard deviation* (SD) of the distribution, respectively (\mu and \sigma are the Greek letters for $m$ and $s$).

The distribution is symmetric, centered at $\mu$, and most values (about 95%) are within $2\sigma$ from $\mu$. Here is what the normal distribution looks like when the $\mu = 0$ and $\sigma = 1$:

```{r summaries-normal-distribution-density, echo = FALSE}
m <- 0; s <- 1
norm_dist <- data.frame(x = seq(-4, 4,len = 50)*s + m) |> mutate(density = dnorm(x, m, s))
norm_dist |> ggplot(aes(x,density)) + geom_line()
```

The fact that the distribution is defined by just two parameters implies that if a dataset's distribution is approximated by the normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the mean and the standard deviation. 

## The average and standard deviation

Discrete distributions also have means and standard deviations. We will discuss these in more detail in [@sec-random-variables]. For a list of values $x_1, \dots, x_n$ the mean, call it $\mu_x$ is defined by 

$$
\mu_x = \frac{1}{n}\sum_{i=1}^n x_i
$$
Note that this equivalent to the average of the $x$s. 

The standard deviation is defined with 

$$
\sigma_x = \sqrt{\frac{1}{n}\sum_{i=1}^n (x_i - \mu_x)^2}
$$
which can thought of as the average distance of the points $x_i$ to the mean $\mu_x$. 


Let's compute the mean and stardard deviations for the height for males which we will store in the object `x`:

```{r}
x <- with(heights, height[sex == "Male"])
```

The pre-built functions `mean` and `sd` can be used here:

```{r}
m <- mean(x)
s <- sd(x)
```

:::{.callout-warning}
For reasons explained in @sec-population-sd, `sd(x)` divides by `length(x)-1` rather than `length(x)`. But note that when `length(x)` is large, `sd(x)` and `sqrt(sum((x-mu)^2) / length(x))` are practically equal.
:::

If the distribution of the values stored in `x` is approximated with a normal distribution, it make sense that it is a normal distribution with mean `m <- mean(x)` and stadnard deviation `s <- sd(x)`. This implies that the distribution of `x` can be summarized with just these two numbers!

Here is a plot of the smooth density and the normal distribution with mean = `r round(m,1)` and SD = `r round(s,1)` plotted as a black line with our student height smooth density in blue:

```{r summaries-data-and-normal-densities, echo = FALSE}
norm_dist <- data.frame(x = seq(-4, 4, len = 50)*s + m) |> 
  mutate(density = dnorm(x, m, s))

heights |> filter(sex == "Male") |> ggplot(aes(height)) +
  geom_density(fill = "#00BFC4") +
  geom_line(aes(x, density),  data = norm_dist) 
```

The normal distribution does appear to be quite a good approximation here. We will now see how well this approximation works at predicting the proportion of values within intervals.

## Standard units

For data that is approximately normally distributed, it is convenient to think in terms of *standard units*. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value `x` from a vector `X`, we define the value of `x` in standard units as `z = (x - m)/s` with `m` and `s` the average and standard deviation of `X`, respectively. Why is this convenient?

First, revisit the formula for the normal distribution and observe that what is being exponentiated is $-z^2/2$ with $z$ equivalent to $x$ in standard units. Because the maximum of $e^{-z^2/2}$ is when $z = 0$, this explains why the maximum of the distribution occurs at the mean. It also explains the symmetry since $- z^2/2$ is symmetric around 0. Secondly, note that by converting the normally distributed data to standard units, we can quickly ascertain whether, for example, a person is about average ($z = 0$), one of the tallest ($z \approx 2$), one of the shortest ($z \approx -2$), or an extremely rare occurrence ($z > 3$ or $z < -3$). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.

In R, we can obtain standard units using the function `scale`:

```{r}
z <- scale(x)
```

To see how many males are within 2 SDs from the average, we simply type:

```{r}
mean(abs(z) < 2)
```

The proportion is about 95%, which is what the normal distribution predicts! To further validate this approximation, we can use quantile-quantile plots.

## Quantile-quantile plots

A systematic way to check whether one distribution is well approximated by another is to compare their cumulative distribution functions (CDFs). A practical way to do this is through their _quantiles_.

For a proportion $0 \leq p \leq 1$, the $p$-th quantile of a distribution $F(x)$ is the value $q$ such that $F(q) = p$, or $q = F^{-1}(p)$. For example:

* $p = 0$ gives the minimum,
* $p = 0.5$ gives the median,
* $p = 1$ gives the maximum.

If two distributions $F_1(x)$ and $F_2(x)$ are similar, then their quantiles at any $p$ should also be similar.

A *quantile-quantile plot (qqplot)* lets us visually check this by plotting the quantiles of one distribution against the quantiles of another. Specifically, we plot $F_1^{-1}(p_1), \dots, F_1^{-1}(p_m)$ versus $F_2^{-1}(p_1), \dots, F_2^{-1}(p_m)$ for a set of proportions $p_1, \dots, p_m$.

A common choice for these proportions is

$$
p_i = \frac{i - 0.5}{n}, \quad i=1,\dots,n
$$

with $n$ the sample size. This avoids exactly $p=0$ or $p=1$, which would correspond to $-\infty$ or $+\infty$ in distributions like the normal.

A common use of qqplots is to compare data to a theoretical normal distribution.

The CDF of the standard normal is usually written as $\Phi(x)$, which gives the probability that a normal random variable is less than $x$. For example, $\Phi(-1.96) = 0.025$ and $\Phi(1.96) = 0.975$. In R we compute these with `pnorm`:

```{r}
pnorm(-1.96)
```

The inverse of $\Phi$, denoted $\Phi^{-1}(p)$, gives the theoretical quantiles. For example, $\Phi^{-1}(0.975) = 1.96$. In R this is `qnorm`:

```{r}
qnorm(0.975)
```

By default `pnorm` and `qnorm` use the standard normal (mean 0, sd 1). You can change this with the `mean` and `sd` arguments:

```{r}
qnorm(0.975, mean = 5, sd = 2)
```

We can compute quantiles for a list of observations too. If we have a vector `x`, the quantile $q_p$ is the value such that $p$ proportion of the data is below it:

```{r}
p <- 0.5
q_p <- quantile(x, p)
```

For example, in the heights dataset:

```{r}
mean(x <= 69.5)
```

About 50% of men are shorter than 69.5 inches, so this is the median ($p=0.5$).


To compare make a qqplot to assess if our observations follow a normal distribution:

1. Define a vector of proportions $p_1, \dots, p_m$.
2. Compute sample quantiles from the data.
3. Compute theoretical quantiles from a normal distribution with the same mean and SD as the data.
4. Plot them against each other.

Example:

```{r qqplot-example}
p <- seq(0.05, 0.95, 0.05)
sample_quantiles <- quantile(x, p)
theoretical_quantiles <- qnorm(p, mean = mean(x), sd = sd(x))
plot(theoretical_quantiles, sample_quantiles)
abline(0,1)
```

If the data are normally distributed, the points should fall along the 45° line.

It is even simpler if we first **standardize** the data:

```{r summaries-qqplot-standardized, eval = FALSE}
sample_quantiles <- quantile(z, p)
theoretical_quantiles <- qnorm(p) 
plot(theoretical_quantiles, sample_quantiles)
abline(0,1)
```


Instead of writing this by hand, R provides convenient functions:

```{r}
#| eval: false
qqnorm(x)
qqline(x)
```

Or with **ggplot2**:

```{r geom_qq-example, eval = FALSE}
heights |> filter(sex == "Male") |>
  ggplot(aes(sample = scale(height))) + 
  geom_qq() +
  geom_abline()
```

By default, `qqnorm` and `geom_qq` use all $n$ quantiles, $p_i = (i-0.5)/n$.


qqplots can also compare **two empirical distributions**, not just data to a theoretical model. For example, to compare male and female heights:

```{r male-vs-female-qqplot}
with(heights, qqplot(height[sex == "Female"], height[sex == "Male"]))
abline(0,1)
```

In summary: qqplots give a simple but powerful way to check whether two distributions match, most often by comparing data to a theoretical distribution such as the normal.


## Percentiles

Before we move on, let's define some terms that are commonly used in exploratory data analysis.

*Percentiles* are special cases of *quantiles* that are commonly used. The percentiles are the quantiles you obtain when setting the $p$ at $0.01, 0.02, ..., 0.99$. For example, we refer to the case of $p = 0.25$ as the 25th percentile, representing a value below which 25% of the data falls. The most famous percentile is the 50th, also known as the *median*.
Another special case that receives a name are the *quartiles*, which are obtained when setting $p = 0.25,0.50$, and $0.75$.

## Boxplots

To introduce boxplots, we will use a dataset of US murders by state. Suppose we want to summarize the murder rate distribution. Using the techniques we have learned, we can quickly see that the normal approximation does not apply in this case:

```{r summaries-hist-qqplot-non-normal-data, out.width = "100%",  fig.width = 6, fig.height = 3, echo = FALSE}
murders <- murders |> mutate(rate = total/population*100000)
library(gridExtra)
p1 <- murders |> ggplot(aes(x = rate)) + geom_histogram(binwidth = 0.5, color = "black") + ggtitle("Histogram")
p2 <- murders |> ggplot(aes(sample = rate)) + 
  geom_qq(dparams = summarize(murders, mean = mean(rate), sd = sd(rate))) +
  geom_abline() + ggtitle("QQ-plot")
grid.arrange(p1, p2, ncol = 2)
```

In this instance, the histogram above or a smooth density plot would serve as a relatively succinct summary.

Now suppose those used to receiving just two numbers as summaries ask us for a more compact numerical summary.

The boxplot provides a five-number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). The boxplot often ignores *outliers* when computing the range and instead plots these as independent points. We will provide a detailed explanation of outliers in @sec-robust-summaries.  Finally, we plot these numbers as a "box" with "whiskers" like this:

```{r summaries-first-boxplot, echo = FALSE}
murders |> ggplot(aes("",rate)) + geom_boxplot() +
  coord_cartesian(xlim = c(0, 2)) + xlab("")
```

The box spans from the first quartile to the third quartile. The horizontal line inside the box marks the median (the 50th percentile). The whiskers extend to the smallest and largest values that are not considered outliers. Outliers are shown as individual points beyond the whiskers. Note that the height of the box is the difference between the third and first quartile. This difference is called the interquartile range (IQR), and it measures the spread of the middle 50% of the data.

## Stratification {#sec-stratification}

In data analysis, we often divide observations into groups based on the values of one or more variables associated with those observations. For example, in the next section, we divide the height values into groups based on a sex variable: females and males. We call this procedure *stratification* and refer to the resulting groups as *strata*.

Stratification is common in data visualization because we are often interested in how the distribution of variables differs across different subgroups. 

Using the histogram, density plots, and qqplots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of `r round(m, 1)` inches and a SD of `r round(s,1)` inches. With this information, ET will have a good idea of what to expect when he meets our male students. However, to provide a complete picture we need to also provide a summary of the female heights.

We learned that boxplots are useful when we want to quickly compare two or more distributions. Here are the heights for females and males:

```{r summaries-female-male-boxplots}
heights |> ggplot(aes(sex, height, fill = sex)) + geom_boxplot()
```

The plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. But does the normal approximation also work for the female height data collected by the survey? We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful:

```{r summaries-histogram-qqplot-female-heights, echo=FALSE, out.width="100%",  fig.width = 6, fig.height = 3}
p1 <- heights |> filter(sex == "Female") |>
  ggplot(aes(height)) +
  geom_density(fill = "#F8766D") 
p2 <- heights |> filter(sex == "Female") |> 
  ggplot(aes(sample = scale(height))) +
  geom_qq() + geom_abline() + ylab("Standard Units")
grid.arrange(p1, p2, ncol = 2)
```

We see something we did not see for the males: the density plot has a second *bump*. Also, the qqplot shows that the highest points tend to be taller than expected by the normal distribution. Finally, we also see five points in the qqplot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights.

We have noticed what we didn't expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, `Female` was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data.

Regarding the five smallest values, note that these are:

```{r}
heights |> filter(sex == "Female") |> 
  top_n(5, desc(height)) |>
  pull(height)
```

Because these are reported heights, a possibility is that the student meant to enter `5'1"`, `5'2"`, `5'3"` or `5'5"`.

## Exercises

1\. In the `murders` dataset, the region is a categorical variable and the following is its distribution:

```{r summaries-barplot-exercise, echo = FALSE}
library(dslabs)
ds_theme_set()
murders |> group_by(region) |>
  summarize(n = n()) |>
  mutate(Proportion = n/sum(n), 
         region = reorder(region, Proportion)) |>
  ggplot(aes(x = region, y = Proportion, fill = region)) + 
  geom_bar(stat = "identity", show.legend = FALSE) + 
  xlab("")
```

To the closest 5%, what proportion of the states are in the North Central region?

2\. Which of the following is true:

a.  The graph above is a histogram.
b.  The graph above shows only four numbers with a bar plot.
c.  Categories are not numbers, so it does not make sense to graph the distribution.
d.  The colors, not the height of the bars, describe the distribution.

3\. The plot below shows the eCDF for male heights:

```{r summaries-ecdf-exercise, echo = FALSE}
heights |> filter(sex == "Male") |> ggplot(aes(height)) + 
  stat_ecdf() +
  ylab("F(a)") + xlab("a")
```

Based on the plot, what percentage of males are shorter than 75 inches?

a.  100%
b.  95%
c.  80%
d.  72 inches

4\. To the closest inch, what height `m` has the property that 1/2 of the male students are taller than `m` and 1/2 are shorter?

a.  61 inches
b.  64 inches
c.  69 inches
d.  74 inches

5\. Here is an eCDF of the murder rates across states:

```{r summaries-ecdf-exercise-2, echo = FALSE}
murders |> mutate(murder_rate = total/population * 10^5) |>
  ggplot(aes(murder_rate)) + 
  stat_ecdf() +
  ylab("F(a)") + xlab("a")
```

Knowing that there are 51 states (counting DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people?

a.  1
b.  5
c.  10
d.  50

6\. Based on the eCDF above, which of the following statements are true:

a.  About half the states have murder rates above 7 per 100,000 and the other half below.
b.  Most states have murder rates below 2 per 100,000.
c.  All the states have murder rates above 2 per 100,000.
d.  With the exception of 4 states, the murder rates are below 5 per 100,000.

7\. Below is a histogram of male heights in our `heights` dataset:

```{r summaries-height-histogram-exercise, echo = FALSE}
heights |> 
  filter(sex == "Male") |> 
  ggplot(aes(height)) + 
  geom_histogram(binwidth = 1, color = "black")
```

Based on this plot, how many males are between 63.5 and 65.5?

a.  10
b.  24
c.  47
d.  100

8\. About what **percentage** are shorter than 60 inches?

a.  1%
b.  10%
c.  25%
d.  50%

9\. Based on the density plot below, about what proportion of US states have populations larger than 10 million?

```{r summaries-density-exercise, echo = FALSE}
murders |> ggplot(aes(x = population/10^6)) + 
  geom_density(fill = "grey") + 
  scale_x_log10() +
  xlab("Population in millions")
```

a.  0.02
b.  0.15
c.  0.50
d.  0.55

10\. Below are three density plots. Is it possible that they are from the same dataset?

```{r summaries-density-exercise-2, echo = FALSE, warning = FALSE, message = FALSE}
library(gridExtra)
p1 <- murders |> ggplot(aes(x = population/10^6)) + 
  geom_density(fill = "grey", bw = 5) + xlab("Population in millions") + ggtitle("1")
p2 <- murders |> ggplot(aes(x = population/10^6)) + 
  geom_density(fill = "grey", bw = .05) + scale_x_log10() + xlab("Population in millions") + ggtitle("2")
p3 <- murders |> ggplot(aes(x = population/10^6)) + 
  geom_density(fill = "grey", bw = 1) + scale_x_log10() + xlab("Population in millions") + ggtitle("3")
grid.arrange(p1,p2,p3,ncol = 2)
```

Which of the following statements is true:

a.  It is impossible that they are from the same dataset.
b.  They are from the same dataset, but the plots are different due to code errors.
c.  They are the same dataset, but the first and second plot undersmooth and the third oversmooths.
d.  They are the same dataset, but the first is not in the log scale, the second undersmooths, and the third oversmooths.

11\. Define variables containing the heights of males and females as follows:

```{r, eval = FALSE}
library(dslabs)
male <- heights$height[heights$sex == "Male"]
female <- heights$height[heights$sex == "Female"]
```

How many measurements do we have for each?

12\. Suppose we can't make a plot and want to compare the distributions side by side. We can't just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing `female_percentiles` and `male_percentiles` with the 10th, 30th, 50th, 70th, & 90th percentiles for each sex. Then create a data frame with these two as columns.

13\. Study the following boxplots showing population sizes by country:

```{r summaries-boxplot-exercise, echo = FALSE, message = FALSE}
library(tidyverse)
library(dslabs)
ds_theme_set()
tab <- gapminder |> filter(year == 2010) |> group_by(continent) |> select(continent, population)  
tab |> ggplot(aes(x = continent, y = population/10^6)) + 
  geom_boxplot() + 
  scale_y_continuous(trans = "log10", breaks = c(1,10,100,1000)) + ylab("Population in millions")
```

Which continent has the country with the biggest population size?

14\. Which continent has the largest median population size?

15\. What is median population size for Africa to the nearest million?

16\. What proportion of countries in Europe have populations below 14 million?

a.  0.99
b.  0.75
c.  0.50
d.  0.25

17\. If we use a log transformation, which continent shown above has the largest interquartile range?

18\. Load the height dataset and create a vector `x` with just the male heights:

```{r, eval = FALSE}
library(dslabs)
x <- heights$height[heights$sex=="Male"]
```

What proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and `mean`.

19\. Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the `pnorm` function to predict the proportions.

20\. Notice that the approximation calculated in question 19 is very close to the exact calculation in question 18. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81\]. How many times larger is the actual proportion than the approximation?

21\. Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as *seven-footers*. Hint: use the `pnorm` function.

22\. There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many of these men (18-40 year olds) are seven feet tall or taller in the world?

23\. There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world's 18-to-40-year-old *seven-footers* are in the NBA?

24\. Repeat the calculations performed in the previous question for Lebron James' height: 6 feet 8 inches. There are about 150 players that are at least that tall.

25\. In answering the previous questions, we found that it is not uncommon for a seven-footer to become an NBA player. What would be a fair critique of our calculations:

a.  Practice and talent are what make a great basketball player, not height.
b.  The normal approximation is not appropriate for heights.
c.  The normal approximation tends to underestimate the extreme values. It's possible that there are more seven-footers than we predicted.
d.  The normal approximation tends to overestimate the extreme values. It's possible that there are fewer seven-footers than we predicted.
